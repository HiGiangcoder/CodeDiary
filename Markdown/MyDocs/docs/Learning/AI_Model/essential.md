# C√°c ki·∫øn th·ª©c c·∫ßn ngo√†i l·ªÅ c·∫ßn thi·∫øt khi x√¢y d·ª±ng ki·∫øn tr√∫c UNet

### 1. Histogram v√† ƒë∆∞·ªùng KDE:

[Histogram v√† KDE](essential/histogram_kde.ipynb)

 **a. Kernel Density Estimation (KDE) l√† g√¨?**  
KDE (**∆Ø·ªõc l∆∞·ª£ng m·∫≠t ƒë·ªô h·∫°t nh√¢n**) l√† m·ªôt ph∆∞∆°ng ph√°p th·ªëng k√™ ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng **h√†m m·∫≠t ƒë·ªô x√°c su·∫•t** (Probability Density Function - PDF) c·ªßa m·ªôt t·∫≠p d·ªØ li·ªáu.  

N√≥i ƒë∆°n gi·∫£n, KDE gi√∫p b·∫°n th·∫•y **ph√¢n ph·ªëi th·ª±c s·ª±** c·ªßa d·ªØ li·ªáu thay v√¨ ch·ªâ d·ª±a v√†o bi·ªÉu ƒë·ªì c·ªôt (histogram).  

V√≠ d·ª•:  
- Histogram chia d·ªØ li·ªáu th√†nh t·ª´ng nh√≥m (bins) v√† ƒë·∫øm s·ªë l∆∞·ª£ng ƒëi·ªÉm trong m·ªói nh√≥m.  
- KDE v·∫Ω m·ªôt **ƒë∆∞·ªùng cong li√™n t·ª•c** m√¥ t·∫£ x√°c su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ng gi√° tr·ªã.  

![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Comparison_of_1D_histogram_and_KDE.png/640px-Comparison_of_1D_histogram_and_KDE.png)  
_(H√¨nh minh h·ªça: Histogram vs KDE)_  

---

 **b. C√°ch KDE ho·∫°t ƒë·ªông?**  
KDE s·ª≠ d·ª•ng **h·∫°t nh√¢n (kernel)** ‚Äì m·ªôt h√†m to√°n h·ªçc gi√∫p ∆∞·ªõc l∆∞·ª£ng m·∫≠t ƒë·ªô c·ªßa t·ª´ng ƒëi·ªÉm d·ªØ li·ªáu. C√¥ng th·ª©c t·ªïng qu√°t c·ªßa KDE:  

\[
\hat{f}(x) = \frac{1}{n h} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)
\]

Trong ƒë√≥:  
- \( \hat{f}(x) \) l√† ∆∞·ªõc l∆∞·ª£ng m·∫≠t ƒë·ªô t·∫°i ƒëi·ªÉm \( x \).  
- \( n \) l√† s·ªë l∆∞·ª£ng m·∫´u d·ªØ li·ªáu.  
- \( h \) l√† **bƒÉng th√¥ng (bandwidth)**, x√°c ƒë·ªãnh ƒë·ªô m∆∞·ª£t c·ªßa ƒë∆∞·ªùng KDE.  
- \( K(\cdot) \) l√† **h√†m h·∫°t nh√¢n (kernel function)**, th∆∞·ªùng l√† m·ªôt h√†m chu·∫©n nh∆∞:  
  - Gaussian (ph·ªï bi·∫øn nh·∫•t)  
  - Epanechnikov  
  - Uniform  

**Vai tr√≤ c·ªßa bƒÉng th√¥ng (bandwidth)**
- N·∫øu **bƒÉng th√¥ng nh·ªè**, KDE c√≥ nhi·ªÅu dao ƒë·ªông, d·ªÖ b·ªã nhi·ªÖu.  
- N·∫øu **bƒÉng th√¥ng l·ªõn**, KDE tr·ªü n√™n qu√° m∆∞·ª£t, m·∫•t chi ti·∫øt quan tr·ªçng.  

![](https://upload.wikimedia.org/wikipedia/commons/5/50/Kernel_density_estimation_banwidth_comparison.svg)  
_(H√¨nh minh h·ªça: BƒÉng th√¥ng nh·ªè vs v·ª´a vs l·ªõn)_  

---

 **c. So s√°nh Histogram v√† KDE**
| ƒê·∫∑c ƒëi·ªÉm | Histogram | KDE |
|----------|------------|------------|
| D·∫°ng bi·ªÉu di·ªÖn | C·ªôt r·ªùi r·∫°c | ƒê∆∞·ªùng cong li√™n t·ª•c |
| Ph·ª• thu·ªôc v√†o bins | C√≥ | Kh√¥ng |
| ·∫¢nh h∆∞·ªüng b·ªüi nhi·ªÖu | √çt (nh∆∞ng b·ªã chia theo bins) | C√≥ th·ªÉ b·ªã nhi·ªÖu n·∫øu bandwidth nh·ªè |
| Th·ªÉ hi·ªán ph√¢n ph·ªëi | Th√¥ s∆° | M∆∞·ª£t h∆°n, d·ªÖ quan s√°t |

üëâ **Histogram ph√π h·ª£p khi c·∫ßn ƒë·∫øm s·ªë l∆∞·ª£ng**  
üëâ **KDE ph√π h·ª£p khi mu·ªën hi·ªÉu r√µ h∆°n v·ªÅ ph√¢n ph·ªëi th·ª±c s·ª± c·ªßa d·ªØ li·ªáu**  

### 2. upconv `nn.ConvTranspose2d`:
```python
def upconv_block(self, in_channels, out_channels):
    return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)
```

![Convolution Transpose](unet/convtranspose.png)


### 3. crop image `Crop tensor`:
```python
def crop_tensor(self, tensor, target_size):
    _, _, H, W = tensor.size()
    diffY = (H - target_size[0]) // 2
    diffX = (W - target_size[1]) // 2
    return tensor[:, :, diffY:(diffY + target_size[0]), diffX:(diffX + target_size[1])]
```

- H√†m tr√™n s·∫Ω c·∫Øt c√°c ph·∫ßn d∆∞ th·ª´a c·ªßa tensor ƒë∆∞·ª£c truy·ªÉn v√†o sao cho c√≥ size gi·ªëng v·ªõi target_size

**Gi·∫£i th√≠ch v·ªÅ: `dec1.shape[2:]`:**

- dec1.shape c√≥ d·∫°ng $(batch_size, channels, height, width)$.

- `dec1.shape[2:]` tr·∫£ v·ªÅ m·ªôt tuple ch·ª©a hai gi√° tr·ªã: $(height, width)$ c·ªßa tensor dec1.

- V√≠ d·ª•: N·∫øu dec1 c√≥ k√≠ch th∆∞·ªõc $(1, 64, 388, 388)$, th√¨ `dec1.shape[2:]` s·∫Ω tr·∫£ v·ªÅ $(388, 388)$.



### 4. `torch.cat()`:
```python
torch.cat((dec4, enc4), dim=1)
```

- G·ªôp 2 dec4 v√† enc4 l·∫°i theo chi·ªÅu 1 (t·ª©c l√† tƒÉng s·ªë l∆∞·ª£ng channel l√™n).


### 5.  `nn.BatchNorm2d()`:

**Batch Normalization l√† g√¨?**  

Batch Normalization (BatchNorm) l√† m·ªôt k·ªπ thu·∫≠t chu·∫©n h√≥a d·ªØ li·ªáu ƒë·∫ßu v√†o t·∫°i m·ªói l·ªõp c·ªßa m·∫°ng n∆°-ron ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t v√† t·ªëc ƒë·ªô hu·∫•n luy·ªán. N√≥ gi√∫p ·ªïn ƒë·ªãnh qu√° tr√¨nh hu·∫•n luy·ªán b·∫±ng c√°ch gi·∫£m s·ª± thay ƒë·ªïi c·ªßa c√°c ph√¢n ph·ªëi ƒë·∫ßu v√†o, l√†m cho m√¥ h√¨nh h·ªçc nhanh h∆°n v√† t·ªïng qu√°t t·ªët h∆°n.

---

**C√°ch ho·∫°t ƒë·ªông c·ªßa BatchNorm**  

Gi·∫£ s·ª≠ ta c√≥ ƒë·∫ßu v√†o $x$ v·ªõi k√≠ch th∆∞·ªõc $(batch\_size, channels, height, width)$. BatchNorm th·ª±c hi·ªán c√°c b∆∞·ªõc sau tr√™n m·ªói k√™nh ri√™ng l·∫ª:

**T√≠nh trung b√¨nh (mean)** c·ªßa t·ª´ng k√™nh:  $\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i$.

- Trong ƒë√≥ $m$ l√† s·ªë l∆∞·ª£ng m·∫´u trong batch.

**T√≠nh ph∆∞∆°ng sai (variance)** c·ªßa t·ª´ng k√™nh: $\sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2$.

- ƒêi·ªÅu n√†y ƒëo l∆∞·ªùng m·ª©c ƒë·ªô ph√¢n t√°n c·ªßa d·ªØ li·ªáu trong batch.

**Chu·∫©n h√≥a d·ªØ li·ªáu** b·∫±ng c√°ch ƒë∆∞a v·ªÅ ph√¢n ph·ªëi c√≥ trung b√¨nh 0 v√† ph∆∞∆°ng sai 1: $\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$.

- Trong ƒë√≥ $\epsilon$ l√† m·ªôt s·ªë r·∫•t nh·ªè ƒë·ªÉ tr√°nh chia cho 0.

**Th√™m tham s·ªë trainable (scale & shift)**:  $y_i = \gamma \hat{x}_i + \beta$.

- $\gamma$ (scale) v√† $\beta$ (shift) l√† c√°c tham s·ªë h·ªçc ƒë∆∞·ª£c, gi√∫p BatchNorm c√≥ th·ªÉ kh√¥i ph·ª•c l·∫°i kh·∫£ nƒÉng bi·ªÉu di·ªÖn ban ƒë·∫ßu n·∫øu c·∫ßn.

**V√≠ d·ª• v·ªÅ c√°ch ho·∫°t ƒë·ªông c·ªßa `nn.BatchNorm2d(out_channels)`**
```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import seaborn as sns

batch_size = 1000  # TƒÉng batch_size ƒë·ªÉ c√≥ ph√¢n ph·ªëi t·ªët h∆°n
out_channels = 16
height, width = 32, 32

# L·ªõp BatchNorm2d
bn = nn.BatchNorm2d(out_channels)

# T·∫°o d·ªØ li·ªáu gi·∫£ v·ªõi ph√¢n ph·ªëi kh√¥ng ƒë·ªìng ƒë·ªÅu (ph√¢n b·ªë l·ªách tr√°i, exponential)
x = torch.rand(batch_size, out_channels, height, width) ** 1.5  # N√¢ng l≈©y th·ª´a ƒë·ªÉ t·∫°o ph√¢n b·ªë l·ªách

# √Åp d·ª•ng BatchNorm
output = bn(x)

print(output.shape)

def plot_distribution(ax, data, title, color):
    """H√†m v·∫Ω bi·ªÉu ƒë·ªì ph√¢n ph·ªëi c·ªßa d·ªØ li·ªáu tr√™n subplot"""
    data_flat = data.view(-1).detach().numpy()
    sns.histplot(data_flat, bins=100, kde=True, color=color, ax=ax)
    ax.set_title(title)
    ax.set_xlabel('Gi√° tr·ªã k√≠ch ho·∫°t')
    ax.set_ylabel('T·∫ßn su·∫•t')

# T·∫°o figure v√† subplot
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# V·∫Ω bi·ªÉu ƒë·ªì tr∆∞·ªõc v√† sau BatchNorm tr√™n c√°c subplot kh√°c nhau
plot_distribution(axes[0], x, 'Tr∆∞·ªõc BatchNorm', 'blue')
plot_distribution(axes[1], output, 'Sau BatchNorm', 'red')

plt.tight_layout()
plt.show()
```
**Output:**
> torch.Size([1000, 16, 32, 32])
![batch normalization 2d](unet/batchnorm2d.png)

 **·∫¢nh h∆∞·ªüng c·ªßa BatchNorm ƒë·∫øn ph√¢n ph·ªëi d·ªØ li·ªáu**
- Tr∆∞·ªõc BatchNorm: D·ªØ li·ªáu c√≥ th·ªÉ c√≥ b·∫•t k·ª≥ d·∫°ng ph√¢n ph·ªëi n√†o (ph√¢n b·ªë l·ªách, ph√¢n b·ªë d·ªëc, v.v.).  
- Sau BatchNorm: D·ªØ li·ªáu ƒë∆∞·ª£c chuy·ªÉn v·ªÅ d·∫°ng **ph√¢n ph·ªëi chu·∫©n h∆°n** v·ªõi trung b√¨nh x·∫•p x·ªâ `0` v√† ph∆∞∆°ng sai `1`.

 **L·ª£i √≠ch c·ªßa Batch Normalization**
1. **Gi√∫p hu·∫•n luy·ªán nhanh h∆°n**: Do d·ªØ li·ªáu ƒë∆∞·ª£c chu·∫©n h√≥a, m√¥ h√¨nh c√≥ th·ªÉ h·ªçc nhanh h∆°n v√† gi·∫£m s·ª± c·∫ßn thi·∫øt c·ªßa learning rate th·∫•p.  
2. **·ªîn ƒë·ªãnh qu√° tr√¨nh hu·∫•n luy·ªán**: Tr√°nh t√¨nh tr·∫°ng "Exploding Gradients" ho·∫∑c "Vanishing Gradients".  
3. **Gi·∫£m ph·ª• thu·ªôc v√†o kh·ªüi t·∫°o tr·ªçng s·ªë**: V√¨ d·ªØ li·ªáu ƒë·∫ßu v√†o ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a, m√¥ h√¨nh √≠t nh·∫°y c·∫£m h∆°n v·ªõi c√°ch kh·ªüi t·∫°o ban ƒë·∫ßu.  
4. **T·ªïng qu√°t h√≥a t·ªët h∆°n**: BatchNorm gi√∫p m√¥ h√¨nh tr√°nh b·ªã overfitting nh·ªù t√°c d·ª•ng t∆∞∆°ng t·ª± dropout.  


### 6. 