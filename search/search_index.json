{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Learning/OOP_Practice/","title":"OOP Practice","text":"In\u00a0[1]: Copied! <pre># B\u00e0i 1: T\u1ea1o m\u1ed9t l\u1edbp \u0111\u01a1n gi\u1ea3n\n# 1. T\u1ea1o m\u1ed9t l\u1edbp t\u00ean l\u00e0 `Person` v\u1edbi c\u00e1c thu\u1ed9c t\u00ednh: t\u00ean (`name`) v\u00e0 tu\u1ed5i (`age`).\n# 2. Vi\u1ebft m\u1ed9t ph\u01b0\u01a1ng th\u1ee9c `introduce` \u0111\u1ec3 in ra l\u1eddi gi\u1edbi thi\u1ec7u b\u1ea3n th\u00e2n c\u1ee7a \u0111\u1ed1i t\u01b0\u1ee3ng.\n\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def introduce(self):\n        print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name} v\u00e0 t\u00f4i {self.age} tu\u1ed5i.\")\n\n# T\u1ea1o \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c\nperson1 = Person(\"Nguyen Van A\", 25)\nperson1.introduce()\n</pre> # B\u00e0i 1: T\u1ea1o m\u1ed9t l\u1edbp \u0111\u01a1n gi\u1ea3n # 1. T\u1ea1o m\u1ed9t l\u1edbp t\u00ean l\u00e0 `Person` v\u1edbi c\u00e1c thu\u1ed9c t\u00ednh: t\u00ean (`name`) v\u00e0 tu\u1ed5i (`age`). # 2. Vi\u1ebft m\u1ed9t ph\u01b0\u01a1ng th\u1ee9c `introduce` \u0111\u1ec3 in ra l\u1eddi gi\u1edbi thi\u1ec7u b\u1ea3n th\u00e2n c\u1ee7a \u0111\u1ed1i t\u01b0\u1ee3ng.  class Person:     def __init__(self, name, age):         self.name = name         self.age = age      def introduce(self):         print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name} v\u00e0 t\u00f4i {self.age} tu\u1ed5i.\")  # T\u1ea1o \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c person1 = Person(\"Nguyen Van A\", 25) person1.introduce() <pre>Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 Nguyen Van A v\u00e0 t\u00f4i 25 tu\u1ed5i.\n</pre> In\u00a0[2]: Copied! <pre>class Animal:\n    def __init__(self, species, age):\n        self.species = species\n        self.age = age\n\n    def describe(self):\n        print(f\"\u0110\u00e2y l\u00e0 m\u1ed9t con {self.species} v\u00e0 n\u00f3 {self.age} tu\u1ed5i.\")\n\n# T\u1ea1o \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c\nanimal1 = Animal(\"ch\u00f3\", 3)\nanimal1.describe()\n</pre> class Animal:     def __init__(self, species, age):         self.species = species         self.age = age      def describe(self):         print(f\"\u0110\u00e2y l\u00e0 m\u1ed9t con {self.species} v\u00e0 n\u00f3 {self.age} tu\u1ed5i.\")  # T\u1ea1o \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c animal1 = Animal(\"ch\u00f3\", 3) animal1.describe() <pre>\u0110\u00e2y l\u00e0 m\u1ed9t con ch\u00f3 v\u00e0 n\u00f3 3 tu\u1ed5i.\n</pre> In\u00a0[3]: Copied! <pre># B\u00e0i 2: K\u1ebf th\u1eeba\n# 1. T\u1ea1o m\u1ed9t l\u1edbp con t\u00ean l\u00e0 `Student` k\u1ebf th\u1eeba t\u1eeb l\u1edbp `Person`.\n# 2. Th\u00eam thu\u1ed9c t\u00ednh m\u1edbi `student_id` v\u00e0 ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c `introduce` \u0111\u1ec3 bao g\u1ed3m m\u00e3 sinh vi\u00ean.\n\nclass Student(Person):\n    def __init__(self, name, age, student_id):\n        super().__init__(name, age)\n        self.student_id = student_id\n\n    def introduce(self):\n        print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name}, t\u00f4i {self.age} tu\u1ed5i v\u00e0 m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 {self.student_id}.\")\n\n# T\u1ea1o \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c\nstudent1 = Student(\"Le Thi B\", 20, \"SV12345\")\nstudent1.introduce()\n</pre> # B\u00e0i 2: K\u1ebf th\u1eeba # 1. T\u1ea1o m\u1ed9t l\u1edbp con t\u00ean l\u00e0 `Student` k\u1ebf th\u1eeba t\u1eeb l\u1edbp `Person`. # 2. Th\u00eam thu\u1ed9c t\u00ednh m\u1edbi `student_id` v\u00e0 ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c `introduce` \u0111\u1ec3 bao g\u1ed3m m\u00e3 sinh vi\u00ean.  class Student(Person):     def __init__(self, name, age, student_id):         super().__init__(name, age)         self.student_id = student_id      def introduce(self):         print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name}, t\u00f4i {self.age} tu\u1ed5i v\u00e0 m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 {self.student_id}.\")  # T\u1ea1o \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c student1 = Student(\"Le Thi B\", 20, \"SV12345\") student1.introduce() <pre>Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 Le Thi B, t\u00f4i 20 tu\u1ed5i v\u00e0 m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 SV12345.\n</pre> In\u00a0[4]: Copied! <pre>class Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def introduce(self):\n        print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name} v\u00e0 t\u00f4i {self.age} tu\u1ed5i.\")\n\nclass Student(Person):\n    def __init__(self, name, age, student_id):\n        super().__init__(name, age)\n        self.student_id = student_id\n\n    def introduce(self):\n        print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name}, t\u00f4i {self.age} tu\u1ed5i v\u00e0 m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 {self.student_id}.\")\n\nclass GraduateStudent(Student):\n    def __init__(self, name, age, student_id, thesis_title):\n        super().__init__(name, age, student_id)\n        self.thesis_title = thesis_title\n\n    def introduce(self):\n        print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name}, t\u00f4i {self.age} tu\u1ed5i, m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 {self.student_id}, v\u00e0 t\u00ean lu\u1eadn v\u0103n c\u1ee7a t\u00f4i l\u00e0 '{self.thesis_title}'.\")\n\ndef introduce_people(people):\n    for person in people:\n        person.introduce()\n\n# T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Person, Student v\u00e0 GraduateStudent\npeople = [\n    Person(\"Nguyen Van A\", 25),\n    Student(\"Le Thi B\", 20, \"SV123\"),\n    GraduateStudent(\"Tran Van C\", 30, \"SV456\", \"Nghi\u00ean c\u1ee9u v\u1ec1 AI\")\n]\n\n# G\u1ecdi h\u00e0m introduce_people\nintroduce_people(people)\n</pre> class Person:     def __init__(self, name, age):         self.name = name         self.age = age      def introduce(self):         print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name} v\u00e0 t\u00f4i {self.age} tu\u1ed5i.\")  class Student(Person):     def __init__(self, name, age, student_id):         super().__init__(name, age)         self.student_id = student_id      def introduce(self):         print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name}, t\u00f4i {self.age} tu\u1ed5i v\u00e0 m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 {self.student_id}.\")  class GraduateStudent(Student):     def __init__(self, name, age, student_id, thesis_title):         super().__init__(name, age, student_id)         self.thesis_title = thesis_title      def introduce(self):         print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name}, t\u00f4i {self.age} tu\u1ed5i, m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 {self.student_id}, v\u00e0 t\u00ean lu\u1eadn v\u0103n c\u1ee7a t\u00f4i l\u00e0 '{self.thesis_title}'.\")  def introduce_people(people):     for person in people:         person.introduce()  # T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Person, Student v\u00e0 GraduateStudent people = [     Person(\"Nguyen Van A\", 25),     Student(\"Le Thi B\", 20, \"SV123\"),     GraduateStudent(\"Tran Van C\", 30, \"SV456\", \"Nghi\u00ean c\u1ee9u v\u1ec1 AI\") ]  # G\u1ecdi h\u00e0m introduce_people introduce_people(people) <pre>Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 Nguyen Van A v\u00e0 t\u00f4i 25 tu\u1ed5i.\nXin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 Le Thi B, t\u00f4i 20 tu\u1ed5i v\u00e0 m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 SV123.\nXin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 Tran Van C, t\u00f4i 30 tu\u1ed5i, m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 SV456, v\u00e0 t\u00ean lu\u1eadn v\u0103n c\u1ee7a t\u00f4i l\u00e0 'Nghi\u00ean c\u1ee9u v\u1ec1 AI'.\n</pre> In\u00a0[5]: Copied! <pre># B\u00e0i 3: \u0110a h\u00ecnh\n# Vi\u1ebft m\u1ed9t h\u00e0m nh\u1eadn v\u00e0o m\u1ed9t danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng `Person` v\u00e0 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c `introduce` c\u1ee7a ch\u00fang.\n\ndef introduce_all(people):\n    for person in people:\n        person.introduce()\n\n# T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Person v\u00e0 Student\npeople = [\n    Person(\"Nguyen Van C\", 30),\n    Student(\"Tran Van D\", 22, \"SV54321\")\n]\n\nintroduce_all(people)\n</pre> # B\u00e0i 3: \u0110a h\u00ecnh # Vi\u1ebft m\u1ed9t h\u00e0m nh\u1eadn v\u00e0o m\u1ed9t danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng `Person` v\u00e0 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c `introduce` c\u1ee7a ch\u00fang.  def introduce_all(people):     for person in people:         person.introduce()  # T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Person v\u00e0 Student people = [     Person(\"Nguyen Van C\", 30),     Student(\"Tran Van D\", 22, \"SV54321\") ]  introduce_all(people) <pre>Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 Nguyen Van C v\u00e0 t\u00f4i 30 tu\u1ed5i.\nXin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 Tran Van D, t\u00f4i 22 tu\u1ed5i v\u00e0 m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 SV54321.\n</pre> In\u00a0[6]: Copied! <pre>class Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def introduce(self):\n        print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name} v\u00e0 t\u00f4i {self.age} tu\u1ed5i.\")\n\nclass Student(Person):\n    def __init__(self, name, age, student_id):\n        super().__init__(name, age)\n        self.student_id = student_id\n\n    def introduce(self):\n        print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name}, t\u00f4i {self.age} tu\u1ed5i v\u00e0 m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 {self.student_id}.\")\n\nclass Teacher(Person):\n    def __init__(self, name, age, subject):\n        super().__init__(name, age)\n        self.subject = subject\n\n    def introduce(self):\n        print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name}, t\u00f4i {self.age} tu\u1ed5i v\u00e0 t\u00f4i d\u1ea1y m\u00f4n {self.subject}.\")\n\ndef introduce_people(people):\n    for person in people:\n        person.introduce()\n\n# T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Person, Student v\u00e0 Teacher\npeople = [\n    Person(\"Nguyen Van A\", 25),\n    Student(\"Le Thi B\", 20, \"SV123\"),\n    Teacher(\"Tran Van C\", 40, \"To\u00e1n\")\n]\n\n# G\u1ecdi h\u00e0m introduce_people\nintroduce_people(people)\n</pre> class Person:     def __init__(self, name, age):         self.name = name         self.age = age      def introduce(self):         print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name} v\u00e0 t\u00f4i {self.age} tu\u1ed5i.\")  class Student(Person):     def __init__(self, name, age, student_id):         super().__init__(name, age)         self.student_id = student_id      def introduce(self):         print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name}, t\u00f4i {self.age} tu\u1ed5i v\u00e0 m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 {self.student_id}.\")  class Teacher(Person):     def __init__(self, name, age, subject):         super().__init__(name, age)         self.subject = subject      def introduce(self):         print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name}, t\u00f4i {self.age} tu\u1ed5i v\u00e0 t\u00f4i d\u1ea1y m\u00f4n {self.subject}.\")  def introduce_people(people):     for person in people:         person.introduce()  # T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Person, Student v\u00e0 Teacher people = [     Person(\"Nguyen Van A\", 25),     Student(\"Le Thi B\", 20, \"SV123\"),     Teacher(\"Tran Van C\", 40, \"To\u00e1n\") ]  # G\u1ecdi h\u00e0m introduce_people introduce_people(people) <pre>Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 Nguyen Van A v\u00e0 t\u00f4i 25 tu\u1ed5i.\nXin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 Le Thi B, t\u00f4i 20 tu\u1ed5i v\u00e0 m\u00e3 sinh vi\u00ean c\u1ee7a t\u00f4i l\u00e0 SV123.\nXin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 Tran Van C, t\u00f4i 40 tu\u1ed5i v\u00e0 t\u00f4i d\u1ea1y m\u00f4n To\u00e1n.\n</pre> In\u00a0[7]: Copied! <pre># B\u00e0i 4: \u0110\u00f3ng g\u00f3i d\u1eef li\u1ec7u\n# 1. Th\u00eam thu\u1ed9c t\u00ednh ri\u00eang t\u01b0 `_password` v\u00e0o l\u1edbp `Person`.\n# 2. Vi\u1ebft c\u00e1c ph\u01b0\u01a1ng th\u1ee9c getter v\u00e0 setter \u0111\u1ec3 truy c\u1eadp v\u00e0 thay \u0111\u1ed5i gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh n\u00e0y.\n\nclass PersonWithPrivate:\n    def __init__(self, name, age, password):\n        self.name = name\n        self.age = age\n        self._password = password\n\n    def get_password(self):\n        return self._password\n\n    def set_password(self, new_password):\n        self._password = new_password\n\n# T\u1ea1o \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 th\u1eed s\u1eed d\u1ee5ng getter v\u00e0 setter\nperson2 = PersonWithPrivate(\"Pham Thi E\", 28, \"mypassword\")\nprint(\"M\u1eadt kh\u1ea9u ban \u0111\u1ea7u:\", person2.get_password())\nperson2.set_password(\"newpassword\")\nprint(\"M\u1eadt kh\u1ea9u sau khi thay \u0111\u1ed5i:\", person2.get_password())\n</pre> # B\u00e0i 4: \u0110\u00f3ng g\u00f3i d\u1eef li\u1ec7u # 1. Th\u00eam thu\u1ed9c t\u00ednh ri\u00eang t\u01b0 `_password` v\u00e0o l\u1edbp `Person`. # 2. Vi\u1ebft c\u00e1c ph\u01b0\u01a1ng th\u1ee9c getter v\u00e0 setter \u0111\u1ec3 truy c\u1eadp v\u00e0 thay \u0111\u1ed5i gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh n\u00e0y.  class PersonWithPrivate:     def __init__(self, name, age, password):         self.name = name         self.age = age         self._password = password      def get_password(self):         return self._password      def set_password(self, new_password):         self._password = new_password  # T\u1ea1o \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 th\u1eed s\u1eed d\u1ee5ng getter v\u00e0 setter person2 = PersonWithPrivate(\"Pham Thi E\", 28, \"mypassword\") print(\"M\u1eadt kh\u1ea9u ban \u0111\u1ea7u:\", person2.get_password()) person2.set_password(\"newpassword\") print(\"M\u1eadt kh\u1ea9u sau khi thay \u0111\u1ed5i:\", person2.get_password()) <pre>M\u1eadt kh\u1ea9u ban \u0111\u1ea7u: mypassword\nM\u1eadt kh\u1ea9u sau khi thay \u0111\u1ed5i: newpassword\n</pre> In\u00a0[8]: Copied! <pre>class Person:\n    def __init__(self, name, age, password):\n        self.name = name\n        self.age = age\n        self._password = password\n\n    def get_password(self):\n        return self._password\n\n    def set_password(self, new_password):\n        if self.validate_password(new_password):\n            self._password = new_password\n        else:\n            print(\"M\u1eadt kh\u1ea9u kh\u00f4ng h\u1ee3p l\u1ec7. M\u1eadt kh\u1ea9u ph\u1ea3i c\u00f3 \u00edt nh\u1ea5t 8 k\u00fd t\u1ef1, bao g\u1ed3m c\u1ea3 ch\u1eef c\u00e1i v\u00e0 s\u1ed1.\")\n\n    def validate_password(self, password):\n        if len(password) &lt; 8:\n            return False\n        has_letter = any(char.isalpha() for char in password)\n        has_digit = any(char.isdigit() for char in password)\n        return has_letter and has_digit\n\n    def introduce(self):\n        print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name} v\u00e0 t\u00f4i {self.age} tu\u1ed5i.\")\n\n    def __str__(self):\n        return f\"Person(name={self.name}, age={self.age})\"\n\n# T\u1ea1o \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 th\u1eed s\u1eed d\u1ee5ng getter, setter, v\u00e0 validate_password\nperson = Person(\"Pham Thi E\", 28, \"mypassword\")\nprint(\"M\u1eadt kh\u1ea9u ban \u0111\u1ea7u:\", person.get_password())\nperson.set_password(\"newpassword123\")\nprint(\"M\u1eadt kh\u1ea9u sau khi thay \u0111\u1ed5i:\", person.get_password())\nprint(person)\n</pre> class Person:     def __init__(self, name, age, password):         self.name = name         self.age = age         self._password = password      def get_password(self):         return self._password      def set_password(self, new_password):         if self.validate_password(new_password):             self._password = new_password         else:             print(\"M\u1eadt kh\u1ea9u kh\u00f4ng h\u1ee3p l\u1ec7. M\u1eadt kh\u1ea9u ph\u1ea3i c\u00f3 \u00edt nh\u1ea5t 8 k\u00fd t\u1ef1, bao g\u1ed3m c\u1ea3 ch\u1eef c\u00e1i v\u00e0 s\u1ed1.\")      def validate_password(self, password):         if len(password) &lt; 8:             return False         has_letter = any(char.isalpha() for char in password)         has_digit = any(char.isdigit() for char in password)         return has_letter and has_digit      def introduce(self):         print(f\"Xin ch\u00e0o, t\u00f4i t\u00ean l\u00e0 {self.name} v\u00e0 t\u00f4i {self.age} tu\u1ed5i.\")      def __str__(self):         return f\"Person(name={self.name}, age={self.age})\"  # T\u1ea1o \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 th\u1eed s\u1eed d\u1ee5ng getter, setter, v\u00e0 validate_password person = Person(\"Pham Thi E\", 28, \"mypassword\") print(\"M\u1eadt kh\u1ea9u ban \u0111\u1ea7u:\", person.get_password()) person.set_password(\"newpassword123\") print(\"M\u1eadt kh\u1ea9u sau khi thay \u0111\u1ed5i:\", person.get_password()) print(person) <pre>M\u1eadt kh\u1ea9u ban \u0111\u1ea7u: mypassword\nM\u1eadt kh\u1ea9u sau khi thay \u0111\u1ed5i: newpassword123\nPerson(name=Pham Thi E, age=28)\n</pre> In\u00a0[9]: Copied! <pre>from abc import ABC, abstractmethod\n\nclass Shape(ABC):\n    @abstractmethod\n    def area(self):\n        pass\n\nclass Circle(Shape):\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return 3.14 * self.radius * self.radius\n\nclass Rectangle(Shape):\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\ndef total_area(shapes):\n    total = 0\n    for shape in shapes:\n        total += shape.area()\n    return total\n\n# T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Shape\nshapes = [\n    Circle(5),\n    Rectangle(4, 6),\n    Circle(3)\n]\n\n# T\u00ednh t\u1ed5ng di\u1ec7n t\u00edch\nprint(\"T\u1ed5ng di\u1ec7n t\u00edch:\", total_area(shapes))\n</pre> from abc import ABC, abstractmethod  class Shape(ABC):     @abstractmethod     def area(self):         pass  class Circle(Shape):     def __init__(self, radius):         self.radius = radius      def area(self):         return 3.14 * self.radius * self.radius  class Rectangle(Shape):     def __init__(self, width, height):         self.width = width         self.height = height      def area(self):         return self.width * self.height  def total_area(shapes):     total = 0     for shape in shapes:         total += shape.area()     return total  # T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Shape shapes = [     Circle(5),     Rectangle(4, 6),     Circle(3) ]  # T\u00ednh t\u1ed5ng di\u1ec7n t\u00edch print(\"T\u1ed5ng di\u1ec7n t\u00edch:\", total_area(shapes)) <pre>T\u1ed5ng di\u1ec7n t\u00edch: 130.76\n</pre> In\u00a0[10]: Copied! <pre>from abc import ABC, abstractmethod\n\nclass Shape(ABC):\n    @abstractmethod\n    def area(self):\n        pass\n\nclass Circle(Shape):\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return 3.14 * self.radius * self.radius\n\nclass Rectangle(Shape):\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\nclass Triangle(Shape):\n    def __init__(self, base, height):\n        self.base = base\n        self.height = height\n\n    def area(self):\n        return 0.5 * self.base * self.height\n\nclass Square(Shape):\n    def __init__(self, side):\n        self.side = side\n\n    def area(self):\n        return self.side * self.side\n\ndef total_area(shapes):\n    total = 0\n    for shape in shapes:\n        total += shape.area()\n    return total\n\n# T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Shape\nshapes = [\n    Circle(5),\n    Rectangle(4, 6),\n    Triangle(3, 4),\n    Square(4)\n]\n\n# T\u00ednh t\u1ed5ng di\u1ec7n t\u00edch\nprint(\"T\u1ed5ng di\u1ec7n t\u00edch:\", total_area(shapes))\n</pre> from abc import ABC, abstractmethod  class Shape(ABC):     @abstractmethod     def area(self):         pass  class Circle(Shape):     def __init__(self, radius):         self.radius = radius      def area(self):         return 3.14 * self.radius * self.radius  class Rectangle(Shape):     def __init__(self, width, height):         self.width = width         self.height = height      def area(self):         return self.width * self.height  class Triangle(Shape):     def __init__(self, base, height):         self.base = base         self.height = height      def area(self):         return 0.5 * self.base * self.height  class Square(Shape):     def __init__(self, side):         self.side = side      def area(self):         return self.side * self.side  def total_area(shapes):     total = 0     for shape in shapes:         total += shape.area()     return total  # T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Shape shapes = [     Circle(5),     Rectangle(4, 6),     Triangle(3, 4),     Square(4) ]  # T\u00ednh t\u1ed5ng di\u1ec7n t\u00edch print(\"T\u1ed5ng di\u1ec7n t\u00edch:\", total_area(shapes)) <pre>T\u1ed5ng di\u1ec7n t\u00edch: 124.5\n</pre> In\u00a0[11]: Copied! <pre>from abc import ABC, abstractmethod\n\nclass Employee(ABC):\n    def __init__(self, name, salary):\n        self.name = name\n        self.salary = salary\n        self._bonus = 0\n\n    @abstractmethod\n    def calculate_bonus(self):\n        pass\n\n    def get_bonus(self):\n        return self._bonus\n\n    def set_bonus(self, bonus):\n        self._bonus = bonus\n\nclass Manager(Employee):\n    def __init__(self, name, salary, department):\n        super().__init__(name, salary)\n        self.department = department\n\n    def calculate_bonus(self):\n        self._bonus = self.salary * 0.1\n        return self._bonus\n\nclass Developer(Employee):\n    def __init__(self, name, salary, programming_language):\n        super().__init__(name, salary)\n        self.programming_language = programming_language\n\n    def calculate_bonus(self):\n        self._bonus = self.salary * 0.2\n        return self._bonus\n\ndef total_bonus(employees):\n    total = 0\n    for employee in employees:\n        total += employee.calculate_bonus()\n    return total\n\n# T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Employee\nemployees = [\n    Manager(\"Nguyen Van A\", 5000, \"Sales\"),\n    Developer(\"Le Thi B\", 4000, \"Python\"),\n    Manager(\"Tran Van C\", 6000, \"HR\"),\n    Developer(\"Pham Thi D\", 4500, \"Java\")\n]\n\n# T\u00ednh t\u1ed5ng ti\u1ec1n th\u01b0\u1edfng\nprint(\"T\u1ed5ng ti\u1ec1n th\u01b0\u1edfng:\", total_bonus(employees))\n</pre> from abc import ABC, abstractmethod  class Employee(ABC):     def __init__(self, name, salary):         self.name = name         self.salary = salary         self._bonus = 0      @abstractmethod     def calculate_bonus(self):         pass      def get_bonus(self):         return self._bonus      def set_bonus(self, bonus):         self._bonus = bonus  class Manager(Employee):     def __init__(self, name, salary, department):         super().__init__(name, salary)         self.department = department      def calculate_bonus(self):         self._bonus = self.salary * 0.1         return self._bonus  class Developer(Employee):     def __init__(self, name, salary, programming_language):         super().__init__(name, salary)         self.programming_language = programming_language      def calculate_bonus(self):         self._bonus = self.salary * 0.2         return self._bonus  def total_bonus(employees):     total = 0     for employee in employees:         total += employee.calculate_bonus()     return total  # T\u1ea1o danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng Employee employees = [     Manager(\"Nguyen Van A\", 5000, \"Sales\"),     Developer(\"Le Thi B\", 4000, \"Python\"),     Manager(\"Tran Van C\", 6000, \"HR\"),     Developer(\"Pham Thi D\", 4500, \"Java\") ]  # T\u00ednh t\u1ed5ng ti\u1ec1n th\u01b0\u1edfng print(\"T\u1ed5ng ti\u1ec1n th\u01b0\u1edfng:\", total_bonus(employees)) <pre>T\u1ed5ng ti\u1ec1n th\u01b0\u1edfng: 2800.0\n</pre>"},{"location":"Learning/OOP_Practice/#thuc-hanh-lap-trinh-huong-oi-tuong-oop-voi-python","title":"Th\u1ef1c h\u00e0nh L\u1eadp tr\u00ecnh H\u01b0\u1edbng \u0111\u1ed1i t\u01b0\u1ee3ng (OOP) v\u1edbi Python\u00b6","text":""},{"location":"Learning/OOP_Practice/#gioi-thieu","title":"Gi\u1edbi thi\u1ec7u\u00b6","text":"<p>Trong notebook n\u00e0y, b\u1ea1n s\u1ebd th\u1ef1c h\u00e0nh c\u00e1c kh\u00e1i ni\u1ec7m c\u01a1 b\u1ea3n v\u1ec1 l\u1eadp tr\u00ecnh h\u01b0\u1edbng \u0111\u1ed1i t\u01b0\u1ee3ng (OOP) trong Python, bao g\u1ed3m:</p> <ul> <li>L\u1edbp v\u00e0 \u0111\u1ed1i t\u01b0\u1ee3ng</li> <li>Thu\u1ed9c t\u00ednh v\u00e0 ph\u01b0\u01a1ng th\u1ee9c</li> <li>K\u1ebf th\u1eeba</li> <li>\u0110a h\u00ecnh</li> <li>Tr\u1eebu t\u01b0\u1ee3ng</li> <li>\u0110\u00f3ng g\u00f3i d\u1eef li\u1ec7u</li> </ul> <p>H\u00e3y th\u1ef1c hi\u1ec7n c\u00e1c b\u00e0i t\u1eadp theo t\u1eebng ph\u1ea7n d\u01b0\u1edbi \u0111\u00e2y. L\u01b0u \u00fd h\u00e3y t\u1ef1 code tr\u01b0\u1edbc khi xem code m\u1eabu.</p>"},{"location":"Learning/OOP_Practice/#0-ly-thuyet","title":"0. L\u00fd thuy\u1ebft\u00b6","text":""},{"location":"Learning/OOP_Practice/#cac-tinh-chat-cua-lap-trinh-huong-oi-tuong-oop","title":"C\u00e1c t\u00ednh ch\u1ea5t c\u1ee7a L\u1eadp tr\u00ecnh H\u01b0\u1edbng \u0111\u1ed1i t\u01b0\u1ee3ng (OOP)\u00b6","text":"<ol> <li><p>\u0110\u00f3ng g\u00f3i (Encapsulation):</p> <ul> <li>\u0110\u00f3ng g\u00f3i l\u00e0 vi\u1ec7c g\u00f3i g\u1ecdn d\u1eef li\u1ec7u v\u00e0 c\u00e1c ph\u01b0\u01a1ng th\u1ee9c thao t\u00e1c tr\u00ean d\u1eef li\u1ec7u \u0111\u00f3 v\u00e0o trong m\u1ed9t \u0111\u1ed1i t\u01b0\u1ee3ng. \u0110i\u1ec1u n\u00e0y gi\u00fap b\u1ea3o v\u1ec7 d\u1eef li\u1ec7u kh\u1ecfi s\u1ef1 truy c\u1eadp v\u00e0 thay \u0111\u1ed5i t\u1eeb b\u00ean ngo\u00e0i, ch\u1ec9 cho ph\u00e9p truy c\u1eadp th\u00f4ng qua c\u00e1c ph\u01b0\u01a1ng th\u1ee9c \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a.</li> <li>V\u00ed d\u1ee5: S\u1eed d\u1ee5ng c\u00e1c thu\u1ed9c t\u00ednh ri\u00eang t\u01b0 (private) v\u00e0 c\u00e1c ph\u01b0\u01a1ng th\u1ee9c getter, setter \u0111\u1ec3 truy c\u1eadp v\u00e0 thay \u0111\u1ed5i gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh.</li> </ul> </li> <li><p>K\u1ebf th\u1eeba (Inheritance):</p> <ul> <li>K\u1ebf th\u1eeba cho ph\u00e9p m\u1ed9t l\u1edbp con (subclass) k\u1ebf th\u1eeba c\u00e1c thu\u1ed9c t\u00ednh v\u00e0 ph\u01b0\u01a1ng th\u1ee9c t\u1eeb m\u1ed9t l\u1edbp cha (superclass). \u0110i\u1ec1u n\u00e0y gi\u00fap t\u00e1i s\u1eed d\u1ee5ng m\u00e3 ngu\u1ed3n v\u00e0 t\u1ea1o ra c\u00e1c m\u1ed1i quan h\u1ec7 ph\u00e2n c\u1ea5p gi\u1eefa c\u00e1c l\u1edbp.</li> <li>V\u00ed d\u1ee5: L\u1edbp <code>Student</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Person</code> v\u00e0 th\u00eam thu\u1ed9c t\u00ednh <code>student_id</code>.</li> </ul> </li> <li><p>\u0110a h\u00ecnh (Polymorphism):</p> <ul> <li>\u0110a h\u00ecnh cho ph\u00e9p c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng thu\u1ed9c c\u00e1c l\u1edbp kh\u00e1c nhau c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd th\u00f4ng qua c\u00f9ng m\u1ed9t giao di\u1ec7n. \u0110i\u1ec1u n\u00e0y c\u00f3 ngh\u0129a l\u00e0 c\u00f9ng m\u1ed9t ph\u01b0\u01a1ng th\u1ee9c c\u00f3 th\u1ec3 c\u00f3 c\u00e1c h\u00e0nh vi kh\u00e1c nhau t\u00f9y thu\u1ed9c v\u00e0o \u0111\u1ed1i t\u01b0\u1ee3ng g\u1ecdi n\u00f3.</li> <li>V\u00ed d\u1ee5: Ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code> c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c ghi \u0111\u00e8 trong c\u00e1c l\u1edbp con \u0111\u1ec3 c\u00f3 h\u00e0nh vi kh\u00e1c nhau.</li> </ul> </li> <li><p>Tr\u1eebu t\u01b0\u1ee3ng (Abstraction):</p> <ul> <li>Tr\u1eebu t\u01b0\u1ee3ng l\u00e0 vi\u1ec7c \u1ea9n \u0111i c\u00e1c chi ti\u1ebft tri\u1ec3n khai c\u1ee5 th\u1ec3 v\u00e0 ch\u1ec9 hi\u1ec3n th\u1ecb c\u00e1c t\u00ednh n\u0103ng c\u1ea7n thi\u1ebft c\u1ee7a \u0111\u1ed1i t\u01b0\u1ee3ng. \u0110i\u1ec1u n\u00e0y gi\u00fap \u0111\u01a1n gi\u1ea3n h\u00f3a vi\u1ec7c s\u1eed d\u1ee5ng \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 t\u1eadp trung v\u00e0o c\u00e1c t\u00ednh n\u0103ng ch\u00ednh.</li> <li>V\u00ed d\u1ee5: S\u1eed d\u1ee5ng c\u00e1c l\u1edbp tr\u1eebu t\u01b0\u1ee3ng (abstract class) v\u00e0 c\u00e1c ph\u01b0\u01a1ng th\u1ee9c tr\u1eebu t\u01b0\u1ee3ng (abstract method) \u0111\u1ec3 \u0111\u1ecbnh ngh\u0129a c\u00e1c h\u00e0nh vi m\u00e0 c\u00e1c l\u1edbp con ph\u1ea3i tri\u1ec3n khai.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#1-create-simple-class","title":"1. Create simple class\u00b6","text":""},{"location":"Learning/OOP_Practice/#bai-1-tao-mot-lop-on-gian","title":"B\u00e0i 1: T\u1ea1o m\u1ed9t l\u1edbp \u0111\u01a1n gi\u1ea3n\u00b6","text":"<ol> <li><p>T\u1ea1o l\u1edbp <code>Person</code>:</p> <ul> <li>T\u1ea1o m\u1ed9t l\u1edbp c\u00f3 t\u00ean l\u00e0 <code>Person</code>.</li> <li>L\u1edbp n\u00e0y c\u00f3 hai thu\u1ed9c t\u00ednh: <code>name</code> (t\u00ean) v\u00e0 <code>age</code> (tu\u1ed5i).</li> </ul> </li> <li><p>Vi\u1ebft ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code>:</p> <ul> <li>Vi\u1ebft m\u1ed9t ph\u01b0\u01a1ng th\u1ee9c c\u00f3 t\u00ean l\u00e0 <code>introduce</code>.</li> <li>Ph\u01b0\u01a1ng th\u1ee9c n\u00e0y s\u1ebd in ra l\u1eddi gi\u1edbi thi\u1ec7u b\u1ea3n th\u00e2n c\u1ee7a \u0111\u1ed1i t\u01b0\u1ee3ng, bao g\u1ed3m t\u00ean v\u00e0 tu\u1ed5i.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#bai-2-tao-mot-lop-on-gian-khac","title":"B\u00e0i 2: T\u1ea1o m\u1ed9t l\u1edbp \u0111\u01a1n gi\u1ea3n kh\u00e1c\u00b6","text":"<ol> <li><p>T\u1ea1o l\u1edbp <code>Animal</code>:</p> <ul> <li>T\u1ea1o m\u1ed9t l\u1edbp c\u00f3 t\u00ean l\u00e0 <code>Animal</code>.</li> <li>L\u1edbp n\u00e0y c\u00f3 hai thu\u1ed9c t\u00ednh: <code>species</code> (lo\u00e0i) v\u00e0 <code>age</code> (tu\u1ed5i).</li> </ul> </li> <li><p>Vi\u1ebft ph\u01b0\u01a1ng th\u1ee9c <code>describe</code>:</p> <ul> <li>Vi\u1ebft m\u1ed9t ph\u01b0\u01a1ng th\u1ee9c c\u00f3 t\u00ean l\u00e0 <code>describe</code>.</li> <li>Ph\u01b0\u01a1ng th\u1ee9c n\u00e0y s\u1ebd in ra m\u00f4 t\u1ea3 v\u1ec1 con v\u1eadt, bao g\u1ed3m lo\u00e0i v\u00e0 tu\u1ed5i.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#2-inheritance-ke-thua","title":"2. Inheritance (K\u1ebf th\u1eeba)\u00b6","text":""},{"location":"Learning/OOP_Practice/#bai-21-ke-thua","title":"B\u00e0i 2.1: K\u1ebf th\u1eeba\u00b6","text":"<ol> <li><p>T\u1ea1o l\u1edbp con <code>Student</code>:</p> <ul> <li>T\u1ea1o m\u1ed9t l\u1edbp con c\u00f3 t\u00ean l\u00e0 <code>Student</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Person</code>.</li> </ul> </li> <li><p>Th\u00eam thu\u1ed9c t\u00ednh <code>student_id</code> v\u00e0 ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code>:</p> <ul> <li>Th\u00eam thu\u1ed9c t\u00ednh m\u1edbi c\u00f3 t\u00ean l\u00e0 <code>student_id</code>.</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code> \u0111\u1ec3 bao g\u1ed3m m\u00e3 sinh vi\u00ean trong l\u1eddi gi\u1edbi thi\u1ec7u.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#bai-22-nang-cao","title":"B\u00e0i 2.2: N\u00e2ng cao\u00b6","text":"<ol> <li><p>T\u1ea1o l\u1edbp con <code>Student</code>:</p> <ul> <li>T\u1ea1o m\u1ed9t l\u1edbp con c\u00f3 t\u00ean l\u00e0 <code>Student</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Person</code>.</li> </ul> </li> <li><p>Th\u00eam thu\u1ed9c t\u00ednh <code>student_id</code> v\u00e0 ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code>:</p> <ul> <li>Th\u00eam thu\u1ed9c t\u00ednh m\u1edbi c\u00f3 t\u00ean l\u00e0 <code>student_id</code>.</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code> \u0111\u1ec3 bao g\u1ed3m m\u00e3 sinh vi\u00ean trong l\u1eddi gi\u1edbi thi\u1ec7u.</li> </ul> </li> <li><p>B\u00e0i t\u1eadp n\u00e2ng cao:</p> <ul> <li>T\u1ea1o th\u00eam m\u1ed9t l\u1edbp con <code>GraduateStudent</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Student</code> v\u1edbi thu\u1ed9c t\u00ednh <code>thesis_title</code> (t\u00ean lu\u1eadn v\u0103n).</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code> trong l\u1edbp <code>GraduateStudent</code> \u0111\u1ec3 bao g\u1ed3m th\u00f4ng tin v\u1ec1 t\u00ean lu\u1eadn v\u0103n.</li> <li>T\u1ea1o m\u1ed9t danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng <code>Person</code>, <code>Student</code>, v\u00e0 <code>GraduateStudent</code>, sau \u0111\u00f3 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code> c\u1ee7a t\u1eebng \u0111\u1ed1i t\u01b0\u1ee3ng trong danh s\u00e1ch.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#3-polymorphism-a-hinh","title":"3. Polymorphism (\u0110a h\u00ecnh)\u00b6","text":""},{"location":"Learning/OOP_Practice/#bai-31-a-hinh","title":"B\u00e0i 3.1: \u0110a h\u00ecnh\u00b6","text":"<ol> <li>Vi\u1ebft h\u00e0m s\u1eed d\u1ee5ng \u0111a h\u00ecnh:<ul> <li>Vi\u1ebft m\u1ed9t h\u00e0m nh\u1eadn v\u00e0o m\u1ed9t danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng <code>Person</code>.</li> <li>H\u00e0m n\u00e0y s\u1ebd g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code> c\u1ee7a t\u1eebng \u0111\u1ed1i t\u01b0\u1ee3ng trong danh s\u00e1ch.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#bai-32-bai-tap-nang-cao","title":"B\u00e0i 3.2: B\u00e0i t\u1eadp n\u00e2ng cao:\u00b6","text":"<ul> <li>T\u1ea1o th\u00eam m\u1ed9t l\u1edbp con <code>Teacher</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Person</code> v\u1edbi thu\u1ed9c t\u00ednh <code>subject</code> (m\u00f4n h\u1ecdc).</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code> trong l\u1edbp <code>Teacher</code> \u0111\u1ec3 bao g\u1ed3m th\u00f4ng tin v\u1ec1 m\u00f4n h\u1ecdc.</li> <li>Vi\u1ebft m\u1ed9t h\u00e0m nh\u1eadn v\u00e0o m\u1ed9t danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng <code>Person</code> v\u00e0 <code>Teacher</code>, sau \u0111\u00f3 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c <code>introduce</code> c\u1ee7a t\u1eebng \u0111\u1ed1i t\u01b0\u1ee3ng trong danh s\u00e1ch.</li> </ul>"},{"location":"Learning/OOP_Practice/#4-encapsulation-ong-goi","title":"4. Encapsulation (\u0110\u00f3ng g\u00f3i)\u00b6","text":""},{"location":"Learning/OOP_Practice/#bai-41-ong-goi-du-lieu","title":"B\u00e0i 4.1: \u0110\u00f3ng g\u00f3i d\u1eef li\u1ec7u\u00b6","text":"<ol> <li><p>Th\u00eam thu\u1ed9c t\u00ednh ri\u00eang t\u01b0 <code>_password</code>:</p> <ul> <li>Th\u00eam thu\u1ed9c t\u00ednh ri\u00eang t\u01b0 c\u00f3 t\u00ean l\u00e0 <code>_password</code> v\u00e0o l\u1edbp <code>Person</code>.</li> </ul> </li> <li><p>Vi\u1ebft c\u00e1c ph\u01b0\u01a1ng th\u1ee9c getter v\u00e0 setter:</p> <ul> <li>Vi\u1ebft ph\u01b0\u01a1ng th\u1ee9c getter \u0111\u1ec3 truy c\u1eadp gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh <code>_password</code>.</li> <li>Vi\u1ebft ph\u01b0\u01a1ng th\u1ee9c setter \u0111\u1ec3 thay \u0111\u1ed5i gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh <code>_password</code>.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#bai-42-ong-goi-du-lieu","title":"B\u00e0i 4.2: \u0110\u00f3ng g\u00f3i d\u1eef li\u1ec7u\u00b6","text":"<ol> <li><p>Th\u00eam thu\u1ed9c t\u00ednh ri\u00eang t\u01b0 <code>_password</code>:</p> <ul> <li>Th\u00eam thu\u1ed9c t\u00ednh ri\u00eang t\u01b0 c\u00f3 t\u00ean l\u00e0 <code>_password</code> v\u00e0o l\u1edbp <code>Person</code>.</li> </ul> </li> <li><p>Vi\u1ebft c\u00e1c ph\u01b0\u01a1ng th\u1ee9c getter v\u00e0 setter:</p> <ul> <li>Vi\u1ebft ph\u01b0\u01a1ng th\u1ee9c getter \u0111\u1ec3 truy c\u1eadp gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh <code>_password</code>.</li> <li>Vi\u1ebft ph\u01b0\u01a1ng th\u1ee9c setter \u0111\u1ec3 thay \u0111\u1ed5i gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh <code>_password</code>.</li> </ul> </li> <li><p>B\u00e0i t\u1eadp n\u00e2ng cao:</p> <ul> <li>Th\u00eam ph\u01b0\u01a1ng th\u1ee9c <code>validate_password</code> \u0111\u1ec3 ki\u1ec3m tra t\u00ednh h\u1ee3p l\u1ec7 c\u1ee7a m\u1eadt kh\u1ea9u m\u1edbi tr\u01b0\u1edbc khi thay \u0111\u1ed5i. M\u1eadt kh\u1ea9u h\u1ee3p l\u1ec7 ph\u1ea3i c\u00f3 \u00edt nh\u1ea5t 8 k\u00fd t\u1ef1, bao g\u1ed3m c\u1ea3 ch\u1eef c\u00e1i v\u00e0 s\u1ed1.</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>__str__</code> \u0111\u1ec3 kh\u00f4ng hi\u1ec3n th\u1ecb thu\u1ed9c t\u00ednh <code>_password</code> khi in \u0111\u1ed1i t\u01b0\u1ee3ng <code>Person</code>.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#5-abstraction","title":"5. Abstraction\u00b6","text":""},{"location":"Learning/OOP_Practice/#bai-51-truu-tuong","title":"B\u00e0i 5.1: Tr\u1eebu t\u01b0\u1ee3ng\u00b6","text":"<ol> <li><p>T\u1ea1o l\u1edbp tr\u1eebu t\u01b0\u1ee3ng <code>Shape</code>:</p> <ul> <li>T\u1ea1o m\u1ed9t l\u1edbp tr\u1eebu t\u01b0\u1ee3ng c\u00f3 t\u00ean l\u00e0 <code>Shape</code>.</li> <li>L\u1edbp n\u00e0y c\u00f3 m\u1ed9t ph\u01b0\u01a1ng th\u1ee9c tr\u1eebu t\u01b0\u1ee3ng <code>area</code> \u0111\u1ec3 t\u00ednh di\u1ec7n t\u00edch.</li> </ul> </li> <li><p>T\u1ea1o c\u00e1c l\u1edbp con <code>Circle</code> v\u00e0 <code>Rectangle</code>:</p> <ul> <li>T\u1ea1o l\u1edbp <code>Circle</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Shape</code> v\u1edbi thu\u1ed9c t\u00ednh <code>radius</code> (b\u00e1n k\u00ednh).</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>area</code> trong l\u1edbp <code>Circle</code> \u0111\u1ec3 t\u00ednh di\u1ec7n t\u00edch h\u00ecnh tr\u00f2n.</li> <li>T\u1ea1o l\u1edbp <code>Rectangle</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Shape</code> v\u1edbi c\u00e1c thu\u1ed9c t\u00ednh <code>width</code> (chi\u1ec1u r\u1ed9ng) v\u00e0 <code>height</code> (chi\u1ec1u cao).</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>area</code> trong l\u1edbp <code>Rectangle</code> \u0111\u1ec3 t\u00ednh di\u1ec7n t\u00edch h\u00ecnh ch\u1eef nh\u1eadt.</li> </ul> </li> <li><p>Vi\u1ebft h\u00e0m t\u00ednh t\u1ed5ng di\u1ec7n t\u00edch:</p> <ul> <li>Vi\u1ebft m\u1ed9t h\u00e0m nh\u1eadn v\u00e0o m\u1ed9t danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng <code>Shape</code> v\u00e0 tr\u1ea3 v\u1ec1 t\u1ed5ng di\u1ec7n t\u00edch c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c h\u00ecnh trong danh s\u00e1ch.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#bai-52-truu-tuong-nang-cao","title":"B\u00e0i 5.2: Tr\u1eebu t\u01b0\u1ee3ng n\u00e2ng cao\u00b6","text":"<ol> <li><p>T\u1ea1o l\u1edbp con <code>Triangle</code>:</p> <ul> <li>T\u1ea1o l\u1edbp <code>Triangle</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Shape</code> v\u1edbi c\u00e1c thu\u1ed9c t\u00ednh <code>base</code> (\u0111\u00e1y) v\u00e0 <code>height</code> (chi\u1ec1u cao).</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>area</code> trong l\u1edbp <code>Triangle</code> \u0111\u1ec3 t\u00ednh di\u1ec7n t\u00edch h\u00ecnh tam gi\u00e1c.</li> </ul> </li> <li><p>T\u1ea1o l\u1edbp con <code>Square</code>:</p> <ul> <li>T\u1ea1o l\u1edbp <code>Square</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Shape</code> v\u1edbi thu\u1ed9c t\u00ednh <code>side</code> (c\u1ea1nh).</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>area</code> trong l\u1edbp <code>Square</code> \u0111\u1ec3 t\u00ednh di\u1ec7n t\u00edch h\u00ecnh vu\u00f4ng.</li> </ul> </li> <li><p>Vi\u1ebft h\u00e0m t\u00ednh t\u1ed5ng di\u1ec7n t\u00edch n\u00e2ng cao:</p> <ul> <li>Vi\u1ebft m\u1ed9t h\u00e0m nh\u1eadn v\u00e0o m\u1ed9t danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng <code>Shape</code> (bao g\u1ed3m <code>Circle</code>, <code>Rectangle</code>, <code>Triangle</code>, v\u00e0 <code>Square</code>) v\u00e0 tr\u1ea3 v\u1ec1 t\u1ed5ng di\u1ec7n t\u00edch c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c h\u00ecnh trong danh s\u00e1ch.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#6-final-exercise","title":"6. Final Exercise\u00b6","text":""},{"location":"Learning/OOP_Practice/#bai-tap-tong-hop-ap-dung-ca-4-thuoc-tinh-oop","title":"B\u00e0i t\u1eadp t\u1ed5ng h\u1ee3p: \u00c1p d\u1ee5ng c\u1ea3 4 thu\u1ed9c t\u00ednh OOP\u00b6","text":"<ol> <li><p>T\u1ea1o l\u1edbp tr\u1eebu t\u01b0\u1ee3ng <code>Employee</code>:</p> <ul> <li>T\u1ea1o m\u1ed9t l\u1edbp tr\u1eebu t\u01b0\u1ee3ng c\u00f3 t\u00ean l\u00e0 <code>Employee</code>.</li> <li>L\u1edbp n\u00e0y c\u00f3 c\u00e1c thu\u1ed9c t\u00ednh: <code>name</code> (t\u00ean) v\u00e0 <code>salary</code> (l\u01b0\u01a1ng).</li> <li>L\u1edbp n\u00e0y c\u00f3 m\u1ed9t ph\u01b0\u01a1ng th\u1ee9c tr\u1eebu t\u01b0\u1ee3ng <code>calculate_bonus</code> \u0111\u1ec3 t\u00ednh ti\u1ec1n th\u01b0\u1edfng.</li> </ul> </li> <li><p>T\u1ea1o c\u00e1c l\u1edbp con <code>Manager</code> v\u00e0 <code>Developer</code>:</p> <ul> <li>T\u1ea1o l\u1edbp <code>Manager</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Employee</code> v\u1edbi thu\u1ed9c t\u00ednh <code>department</code> (ph\u00f2ng ban).</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>calculate_bonus</code> trong l\u1edbp <code>Manager</code> \u0111\u1ec3 t\u00ednh ti\u1ec1n th\u01b0\u1edfng d\u1ef1a tr\u00ean l\u01b0\u01a1ng v\u00e0 m\u1ed9t h\u1ec7 s\u1ed1 c\u1ed1 \u0111\u1ecbnh.</li> <li>T\u1ea1o l\u1edbp <code>Developer</code> k\u1ebf th\u1eeba t\u1eeb l\u1edbp <code>Employee</code> v\u1edbi thu\u1ed9c t\u00ednh <code>programming_language</code> (ng\u00f4n ng\u1eef l\u1eadp tr\u00ecnh).</li> <li>Ghi \u0111\u00e8 ph\u01b0\u01a1ng th\u1ee9c <code>calculate_bonus</code> trong l\u1edbp <code>Developer</code> \u0111\u1ec3 t\u00ednh ti\u1ec1n th\u01b0\u1edfng d\u1ef1a tr\u00ean l\u01b0\u01a1ng v\u00e0 m\u1ed9t h\u1ec7 s\u1ed1 c\u1ed1 \u0111\u1ecbnh.</li> </ul> </li> <li><p>\u0110\u00f3ng g\u00f3i d\u1eef li\u1ec7u:</p> <ul> <li>Th\u00eam thu\u1ed9c t\u00ednh ri\u00eang t\u01b0 <code>_bonus</code> v\u00e0o l\u1edbp <code>Employee</code>.</li> <li>Vi\u1ebft c\u00e1c ph\u01b0\u01a1ng th\u1ee9c getter v\u00e0 setter \u0111\u1ec3 truy c\u1eadp v\u00e0 thay \u0111\u1ed5i gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh <code>_bonus</code>.</li> </ul> </li> <li><p>\u0110a h\u00ecnh:</p> <ul> <li>Vi\u1ebft m\u1ed9t h\u00e0m nh\u1eadn v\u00e0o m\u1ed9t danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng <code>Employee</code> v\u00e0 g\u1ecdi ph\u01b0\u01a1ng th\u1ee9c <code>calculate_bonus</code> c\u1ee7a t\u1eebng \u0111\u1ed1i t\u01b0\u1ee3ng trong danh s\u00e1ch.</li> <li>H\u00e0m n\u00e0y s\u1ebd tr\u1ea3 v\u1ec1 t\u1ed5ng ti\u1ec1n th\u01b0\u1edfng c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c nh\u00e2n vi\u00ean trong danh s\u00e1ch.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#7-do-by-yourself-hehe","title":"7. Do by yourself hehe\u00b6","text":""},{"location":"Learning/OOP_Practice/#bai-tap-71-he-thong-quan-ly-nhan-vien","title":"B\u00e0i t\u1eadp 7.1: H\u1ec7 th\u1ed1ng Qu\u1ea3n L\u00fd Nh\u00e2n Vi\u00ean\u00b6","text":"<p>H\u00e3y vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh qu\u1ea3n l\u00fd nh\u00e2n vi\u00ean trong c\u00f4ng ty. C\u00f4ng ty c\u00f3 nhi\u1ec1u lo\u1ea1i nh\u00e2n vi\u00ean kh\u00e1c nhau (nh\u00e2n vi\u00ean v\u0103n ph\u00f2ng, nh\u00e2n vi\u00ean k\u1ef9 thu\u1eadt, nh\u00e2n vi\u00ean b\u00e1n h\u00e0ng). M\u1ed7i lo\u1ea1i nh\u00e2n vi\u00ean c\u00f3 c\u00e1ch t\u00ednh l\u01b0\u01a1ng ri\u00eang, nh\u01b0ng t\u1ea5t c\u1ea3 \u0111\u1ec1u c\u00f3 c\u00e1c th\u00f4ng tin chung nh\u01b0 h\u1ecd t\u00ean, m\u00e3 nh\u00e2n vi\u00ean, v\u00e0 l\u01b0\u01a1ng.</p> <ol> <li><p>\u0110\u00f3ng g\u00f3i:</p> <ul> <li>C\u00e1c thu\u1ed9c t\u00ednh c\u1ee7a nh\u00e2n vi\u00ean ph\u1ea3i \u0111\u01b0\u1ee3c b\u1ea3o v\u1ec7 (private/protected) v\u00e0 ch\u1ec9 c\u00f3 th\u1ec3 truy c\u1eadp th\u00f4ng qua ph\u01b0\u01a1ng th\u1ee9c getter/setter.</li> </ul> </li> <li><p>K\u1ebf th\u1eeba:</p> <ul> <li>T\u1ea1o l\u1edbp cha <code>Employee</code> v\u00e0 c\u00e1c l\u1edbp con k\u1ebf th\u1eeba (<code>OfficeEmployee</code>, <code>TechnicalEmployee</code>, <code>SalesEmployee</code>).</li> </ul> </li> <li><p>\u0110a h\u00ecnh:</p> <ul> <li>M\u1ed7i l\u1edbp con s\u1ebd \u0111\u1ecbnh ngh\u0129a l\u1ea1i ph\u01b0\u01a1ng th\u1ee9c <code>calculate_salary()</code> \u0111\u1ec3 t\u00ednh l\u01b0\u01a1ng theo c\u00e1ch ri\u00eang.</li> </ul> </li> <li><p>Tr\u1eebu t\u01b0\u1ee3ng h\u00f3a:</p> <ul> <li>L\u1edbp cha <code>Employee</code> s\u1ebd l\u00e0 m\u1ed9t l\u1edbp tr\u1eebu t\u01b0\u1ee3ng.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#bai-tap-72-he-thong-quan-ly-nhan-vien","title":"B\u00e0i t\u1eadp 7.2: H\u1ec7 th\u1ed1ng Qu\u1ea3n L\u00fd Nh\u00e2n Vi\u00ean\u00b6","text":"<p>H\u00e3y vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh qu\u1ea3n l\u00fd nh\u00e2n vi\u00ean trong c\u00f4ng ty. C\u00f4ng ty c\u00f3 nhi\u1ec1u lo\u1ea1i nh\u00e2n vi\u00ean kh\u00e1c nhau (nh\u00e2n vi\u00ean v\u0103n ph\u00f2ng, nh\u00e2n vi\u00ean k\u1ef9 thu\u1eadt, nh\u00e2n vi\u00ean b\u00e1n h\u00e0ng). M\u1ed7i lo\u1ea1i nh\u00e2n vi\u00ean c\u00f3 c\u00e1ch t\u00ednh l\u01b0\u01a1ng ri\u00eang, nh\u01b0ng t\u1ea5t c\u1ea3 \u0111\u1ec1u c\u00f3 c\u00e1c th\u00f4ng tin chung nh\u01b0 h\u1ecd t\u00ean, m\u00e3 nh\u00e2n vi\u00ean, v\u00e0 l\u01b0\u01a1ng.</p> <ol> <li><p>\u0110\u00f3ng g\u00f3i:</p> <ul> <li>C\u00e1c thu\u1ed9c t\u00ednh c\u1ee7a nh\u00e2n vi\u00ean ph\u1ea3i \u0111\u01b0\u1ee3c b\u1ea3o v\u1ec7 (private/protected) v\u00e0 ch\u1ec9 c\u00f3 th\u1ec3 truy c\u1eadp th\u00f4ng qua ph\u01b0\u01a1ng th\u1ee9c getter/setter.</li> <li>Th\u00eam thu\u1ed9c t\u00ednh ri\u00eang t\u01b0 <code>_bonus</code> \u0111\u1ec3 l\u01b0u tr\u1eef ti\u1ec1n th\u01b0\u1edfng c\u1ee7a nh\u00e2n vi\u00ean v\u00e0 c\u00e1c ph\u01b0\u01a1ng th\u1ee9c getter/setter t\u01b0\u01a1ng \u1ee9ng.</li> </ul> </li> <li><p>K\u1ebf th\u1eeba:</p> <ul> <li>T\u1ea1o l\u1edbp cha <code>Employee</code> v\u00e0 c\u00e1c l\u1edbp con k\u1ebf th\u1eeba (<code>OfficeEmployee</code>, <code>TechnicalEmployee</code>, <code>SalesEmployee</code>).</li> <li>M\u1ed7i l\u1edbp con s\u1ebd c\u00f3 th\u00eam c\u00e1c thu\u1ed9c t\u00ednh \u0111\u1eb7c tr\u01b0ng ri\u00eang (v\u00ed d\u1ee5: <code>OfficeEmployee</code> c\u00f3 thu\u1ed9c t\u00ednh <code>office_hours</code>, <code>TechnicalEmployee</code> c\u00f3 thu\u1ed9c t\u00ednh <code>projects</code>, <code>SalesEmployee</code> c\u00f3 thu\u1ed9c t\u00ednh <code>sales</code>).</li> </ul> </li> <li><p>\u0110a h\u00ecnh:</p> <ul> <li>M\u1ed7i l\u1edbp con s\u1ebd \u0111\u1ecbnh ngh\u0129a l\u1ea1i ph\u01b0\u01a1ng th\u1ee9c <code>calculate_salary()</code> \u0111\u1ec3 t\u00ednh l\u01b0\u01a1ng theo c\u00e1ch ri\u00eang.</li> <li>Th\u00eam ph\u01b0\u01a1ng th\u1ee9c <code>calculate_bonus()</code> \u0111\u1ec3 t\u00ednh ti\u1ec1n th\u01b0\u1edfng d\u1ef1a tr\u00ean hi\u1ec7u su\u1ea5t l\u00e0m vi\u1ec7c c\u1ee7a t\u1eebng lo\u1ea1i nh\u00e2n vi\u00ean.</li> </ul> </li> <li><p>Tr\u1eebu t\u01b0\u1ee3ng h\u00f3a:</p> <ul> <li>L\u1edbp cha <code>Employee</code> s\u1ebd l\u00e0 m\u1ed9t l\u1edbp tr\u1eebu t\u01b0\u1ee3ng v\u1edbi c\u00e1c ph\u01b0\u01a1ng th\u1ee9c tr\u1eebu t\u01b0\u1ee3ng <code>calculate_salary()</code> v\u00e0 <code>calculate_bonus()</code>.</li> </ul> </li> <li><p>B\u00e0i t\u1eadp n\u00e2ng cao:</p> <ul> <li>Vi\u1ebft m\u1ed9t h\u00e0m nh\u1eadn v\u00e0o m\u1ed9t danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng <code>Employee</code> v\u00e0 tr\u1ea3 v\u1ec1 t\u1ed5ng l\u01b0\u01a1ng v\u00e0 t\u1ed5ng ti\u1ec1n th\u01b0\u1edfng c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c nh\u00e2n vi\u00ean trong danh s\u00e1ch.</li> <li>Vi\u1ebft m\u1ed9t h\u00e0m \u0111\u1ec3 t\u00ecm ki\u1ebfm nh\u00e2n vi\u00ean theo m\u00e3 nh\u00e2n vi\u00ean v\u00e0 tr\u1ea3 v\u1ec1 th\u00f4ng tin c\u1ee7a nh\u00e2n vi\u00ean \u0111\u00f3.</li> <li>Vi\u1ebft m\u1ed9t h\u00e0m \u0111\u1ec3 s\u1eafp x\u1ebfp danh s\u00e1ch nh\u00e2n vi\u00ean theo l\u01b0\u01a1ng t\u1eeb cao \u0111\u1ebfn th\u1ea5p.</li> </ul> </li> </ol>"},{"location":"Learning/OOP_Practice/#bai-tap-73-he-thong-quan-ly-thu-vien","title":"B\u00e0i t\u1eadp 7.3: H\u1ec7 th\u1ed1ng Qu\u1ea3n L\u00fd Th\u01b0 Vi\u1ec7n\u00b6","text":"<p>H\u00e3y vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh qu\u1ea3n l\u00fd th\u01b0 vi\u1ec7n. Th\u01b0 vi\u1ec7n c\u00f3 nhi\u1ec1u lo\u1ea1i t\u00e0i li\u1ec7u kh\u00e1c nhau (s\u00e1ch, t\u1ea1p ch\u00ed, b\u00e1o). M\u1ed7i lo\u1ea1i t\u00e0i li\u1ec7u c\u00f3 c\u00e1c thu\u1ed9c t\u00ednh v\u00e0 ph\u01b0\u01a1ng th\u1ee9c ri\u00eang, nh\u01b0ng t\u1ea5t c\u1ea3 \u0111\u1ec1u c\u00f3 c\u00e1c th\u00f4ng tin chung nh\u01b0 m\u00e3 t\u00e0i li\u1ec7u, t\u00ean t\u00e0i li\u1ec7u, v\u00e0 nh\u00e0 xu\u1ea5t b\u1ea3n.</p> <ol> <li><p>\u0110\u00f3ng g\u00f3i:</p> <ul> <li>C\u00e1c thu\u1ed9c t\u00ednh c\u1ee7a t\u00e0i li\u1ec7u ph\u1ea3i \u0111\u01b0\u1ee3c b\u1ea3o v\u1ec7 (private/protected) v\u00e0 ch\u1ec9 c\u00f3 th\u1ec3 truy c\u1eadp th\u00f4ng qua ph\u01b0\u01a1ng th\u1ee9c getter/setter.</li> <li>Th\u00eam thu\u1ed9c t\u00ednh ri\u00eang t\u01b0 <code>_availability</code> \u0111\u1ec3 l\u01b0u tr\u1eef tr\u1ea1ng th\u00e1i s\u1eb5n c\u00f3 c\u1ee7a t\u00e0i li\u1ec7u v\u00e0 c\u00e1c ph\u01b0\u01a1ng th\u1ee9c getter/setter t\u01b0\u01a1ng \u1ee9ng.</li> </ul> </li> <li><p>K\u1ebf th\u1eeba:</p> <ul> <li>T\u1ea1o l\u1edbp cha <code>Document</code> v\u00e0 c\u00e1c l\u1edbp con k\u1ebf th\u1eeba (<code>Book</code>, <code>Magazine</code>, <code>Newspaper</code>).</li> <li>M\u1ed7i l\u1edbp con s\u1ebd c\u00f3 th\u00eam c\u00e1c thu\u1ed9c t\u00ednh \u0111\u1eb7c tr\u01b0ng ri\u00eang (v\u00ed d\u1ee5: <code>Book</code> c\u00f3 thu\u1ed9c t\u00ednh <code>author</code> v\u00e0 <code>number_of_pages</code>, <code>Magazine</code> c\u00f3 thu\u1ed9c t\u00ednh <code>issue_number</code> v\u00e0 <code>month</code>, <code>Newspaper</code> c\u00f3 thu\u1ed9c t\u00ednh <code>date</code>).</li> </ul> </li> <li><p>\u0110a h\u00ecnh:</p> <ul> <li>M\u1ed7i l\u1edbp con s\u1ebd \u0111\u1ecbnh ngh\u0129a l\u1ea1i ph\u01b0\u01a1ng th\u1ee9c <code>display_info()</code> \u0111\u1ec3 hi\u1ec3n th\u1ecb th\u00f4ng tin chi ti\u1ebft c\u1ee7a t\u00e0i li\u1ec7u.</li> <li>Th\u00eam ph\u01b0\u01a1ng th\u1ee9c <code>check_availability()</code> \u0111\u1ec3 ki\u1ec3m tra tr\u1ea1ng th\u00e1i s\u1eb5n c\u00f3 c\u1ee7a t\u00e0i li\u1ec7u.</li> </ul> </li> <li><p>Tr\u1eebu t\u01b0\u1ee3ng h\u00f3a:</p> <ul> <li>L\u1edbp cha <code>Document</code> s\u1ebd l\u00e0 m\u1ed9t l\u1edbp tr\u1eebu t\u01b0\u1ee3ng v\u1edbi c\u00e1c ph\u01b0\u01a1ng th\u1ee9c tr\u1eebu t\u01b0\u1ee3ng <code>display_info()</code> v\u00e0 <code>check_availability()</code>.</li> </ul> </li> <li><p>B\u00e0i t\u1eadp n\u00e2ng cao:</p> <ul> <li>Vi\u1ebft m\u1ed9t h\u00e0m nh\u1eadn v\u00e0o m\u1ed9t danh s\u00e1ch c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng <code>Document</code> v\u00e0 hi\u1ec3n th\u1ecb th\u00f4ng tin c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c t\u00e0i li\u1ec7u trong danh s\u00e1ch.</li> <li>Vi\u1ebft m\u1ed9t h\u00e0m \u0111\u1ec3 t\u00ecm ki\u1ebfm t\u00e0i li\u1ec7u theo m\u00e3 t\u00e0i li\u1ec7u v\u00e0 tr\u1ea3 v\u1ec1 th\u00f4ng tin c\u1ee7a t\u00e0i li\u1ec7u \u0111\u00f3.</li> <li>Vi\u1ebft m\u1ed9t h\u00e0m \u0111\u1ec3 s\u1eafp x\u1ebfp danh s\u00e1ch t\u00e0i li\u1ec7u theo t\u00ean t\u00e0i li\u1ec7u theo th\u1ee9 t\u1ef1 b\u1ea3ng ch\u1eef c\u00e1i.</li> <li>Vi\u1ebft m\u1ed9t h\u00e0m \u0111\u1ec3 ki\u1ec3m tra v\u00e0 c\u1eadp nh\u1eadt tr\u1ea1ng th\u00e1i s\u1eb5n c\u00f3 c\u1ee7a t\u00e0i li\u1ec7u khi t\u00e0i li\u1ec7u \u0111\u01b0\u1ee3c m\u01b0\u1ee3n ho\u1eb7c tr\u1ea3 l\u1ea1i.</li> </ul> </li> </ol>"},{"location":"Learning/Visualization/","title":"Visualization","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt %matplotlib inline <ul> <li>line</li> <li>bar</li> <li>histogram</li> <li>scatter</li> </ul> In\u00a0[\u00a0]: Copied! <pre>arr = np.array([10, 20, 60, 35, 75, 100, 65])\n</pre> arr = np.array([10, 20, 60, 35, 75, 100, 65]) In\u00a0[\u00a0]: Copied! <pre>arr = np.array([10, 20, 60, 35, 75, 100, 65])\n\nplt.plot(arr, 'go-')\n</pre> arr = np.array([10, 20, 60, 35, 75, 100, 65])  plt.plot(arr, 'go-') Out[\u00a0]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f239ece0590&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre>arr = np.array([10, 20, 60, 35, 75, 100, 65])\n</pre> arr = np.array([10, 20, 60, 35, 75, 100, 65]) In\u00a0[\u00a0]: Copied! <pre>arr = np.array([10, 20, 60, 35, 75, 100, 65])\n\nplt.bar(x=np.arange(len(arr)), height=arr, color='red')\n</pre> arr = np.array([10, 20, 60, 35, 75, 100, 65])  plt.bar(x=np.arange(len(arr)), height=arr, color='red') Out[\u00a0]: <pre>&lt;BarContainer object of 7 artists&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>arr = np.array([10, 20, 30, 10, 10, 20])\n</pre> arr = np.array([10, 20, 30, 10, 10, 20]) In\u00a0[\u00a0]: Copied! <pre>def calculate_histogram(arr):\n    \n    # O(n^2)\n    \n    hist = {}\n    \n    for i in set(arr):\n        \n        count = 0\n        \n        for j in arr:\n            if j == i:\n                count += 1 \n        \n        hist[i] = count \n    \n    return hist\n</pre> def calculate_histogram(arr):          # O(n^2)          hist = {}          for i in set(arr):                  count = 0                  for j in arr:             if j == i:                 count += 1                   hist[i] = count           return hist In\u00a0[\u00a0]: Copied! <pre>calculate_histogram(arr)\n</pre> calculate_histogram(arr) Out[\u00a0]: <pre>{10: 3, 20: 2, 30: 1}</pre> In\u00a0[\u00a0]: Copied! <pre>def calculate_histogram(arr):\n    \n    # O(n)\n    \n    hist = {}\n    \n    for i in range(len(arr)):\n         \n        if arr[i] in hist:\n            hist[arr[i]] += 1\n        else:\n            hist[arr[i]] = 1\n            \n    return hist\n</pre> def calculate_histogram(arr):          # O(n)          hist = {}          for i in range(len(arr)):                   if arr[i] in hist:             hist[arr[i]] += 1         else:             hist[arr[i]] = 1                  return hist In\u00a0[\u00a0]: Copied! <pre>def calculate_histogram(arr):\n    \"\"\"\n    Function to calculate histogram of an array\n    \n    * Parameters:\n    arr -- an numpy array, with shape (n,)\n    \n    * Return:\n    hist --  a dictionary, indicate result histogram\n    \n    1. \n    2. \n    3. \n    \"\"\"\n    \n    hist = {}\n    \n    for i in range(len(arr)):\n        hist[arr[i]] = hist.get(arr[i], 0) + 1\n            \n    return hist\n</pre> def calculate_histogram(arr):     \"\"\"     Function to calculate histogram of an array          * Parameters:     arr -- an numpy array, with shape (n,)          * Return:     hist --  a dictionary, indicate result histogram          1.      2.      3.      \"\"\"          hist = {}          for i in range(len(arr)):         hist[arr[i]] = hist.get(arr[i], 0) + 1                  return hist In\u00a0[\u00a0]: Copied! <pre>hist = calculate_histogram(arr)\n</pre> hist = calculate_histogram(arr) In\u00a0[\u00a0]: Copied! <pre>plt.xticks(ticks=[10,15, 20,25, 30])\nplt.yticks(ticks=[1, 2, 3])\nplt.bar(x = hist.keys(), height = hist.values())\n</pre> plt.xticks(ticks=[10,15, 20,25, 30]) plt.yticks(ticks=[1, 2, 3]) plt.bar(x = hist.keys(), height = hist.values()) Out[\u00a0]: <pre>&lt;BarContainer object of 3 artists&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('bank.csv')\n\narr_1 = df.annual_income.values\narr_2 = df.spending_score.values\n\nplt.scatter(arr_1, arr_2, color='red')\n</pre> df = pd.read_csv('bank.csv')  arr_1 = df.annual_income.values arr_2 = df.spending_score.values  plt.scatter(arr_1, arr_2, color='red') Out[\u00a0]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f239eca2690&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>plt.scatter(arr_1, arr_2, color='red')\n</pre> plt.scatter(arr_1, arr_2, color='red') Out[\u00a0]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f239e4e6e50&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=(10, 6))\n\nax1 = fig.add_subplot(3, 3, 1)\nax2 = fig.add_subplot(3, 3, 2)\nax3 = fig.add_subplot(3, 3, 3)\n\nfig.show()\n</pre> fig = plt.figure(figsize=(10, 6))  ax1 = fig.add_subplot(3, 3, 1) ax2 = fig.add_subplot(3, 3, 2) ax3 = fig.add_subplot(3, 3, 3)  fig.show() In\u00a0[\u00a0]: Copied! <pre>arr = np.array([10, 20, 60, 35, 75, 100, 65])\n\nax1.plot(arr)\nax2.bar(x = np.arange(len(arr)), height=arr)\nax3.scatter(arr_1, arr_2)\n\nfig\n</pre> arr = np.array([10, 20, 60, 35, 75, 100, 65])  ax1.plot(arr) ax2.bar(x = np.arange(len(arr)), height=arr) ax3.scatter(arr_1, arr_2)  fig Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>arr = np.array([10, 20, 60, 35, 75, 100, 65])\n\nplt.plot(arr)\n</pre> arr = np.array([10, 20, 60, 35, 75, 100, 65])  plt.plot(arr) Out[\u00a0]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f239e3b9fd0&gt;]</pre> <p>$y = x^2$</p> In\u00a0[\u00a0]: Copied! <pre>step = 0.01\n\nx = np.arange(-1, 1+step, step)\ny = x * x\n\nplt.plot(x, y)\n</pre> step = 0.01  x = np.arange(-1, 1+step, step) y = x * x  plt.plot(x, y) Out[\u00a0]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f239e2ba510&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre>plt.plot(x, y)\n</pre> plt.plot(x, y) Out[\u00a0]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f239e136790&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre>arr_1 = np.array([10, 20, 60, 35, 75, 100, 65])\narr_2 = np.array([5, 7, 10, 8, 6, 6, 8])\n\nplt.bar(x=np.arange(len(arr_1)), height=arr_1, color='blue', label='L\u01b0\u1ee3ng m\u01b0a')\nplt.plot(arr_2, color='red', label='Nhi\u1ec7t \u0111\u1ed9')\n\nplt.xlabel('th\u1ee9')\nplt.ylabel('Gi\u00e1 tr\u1ecb')\nplt.title(\"Bi\u1ec3u \u0111\u1ed3 l\u01b0\u1ee3ng m\u01b0a nhi\u1ec7t \u0111\u1ed9\")\nplt.legend()\n\nplt.savefig('my_figure.jpg')\n</pre> arr_1 = np.array([10, 20, 60, 35, 75, 100, 65]) arr_2 = np.array([5, 7, 10, 8, 6, 6, 8])  plt.bar(x=np.arange(len(arr_1)), height=arr_1, color='blue', label='L\u01b0\u1ee3ng m\u01b0a') plt.plot(arr_2, color='red', label='Nhi\u1ec7t \u0111\u1ed9')  plt.xlabel('th\u1ee9') plt.ylabel('Gi\u00e1 tr\u1ecb') plt.title(\"Bi\u1ec3u \u0111\u1ed3 l\u01b0\u1ee3ng m\u01b0a nhi\u1ec7t \u0111\u1ed9\") plt.legend()  plt.savefig('my_figure.jpg') In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('bank.csv')\n\narr_1 = df.annual_income.values\narr_2 = df.spending_score.values\n\nplt.xticks(np.arange(0, 250, 50))\nplt.yticks(np.arange(0, 110, 10))\n\nplt.xlim((0, 150))\nplt.ylim((0, 150))\n\nplt.scatter(arr_1, arr_2, color='red')\n</pre> df = pd.read_csv('bank.csv')  arr_1 = df.annual_income.values arr_2 = df.spending_score.values  plt.xticks(np.arange(0, 250, 50)) plt.yticks(np.arange(0, 110, 10))  plt.xlim((0, 150)) plt.ylim((0, 150))  plt.scatter(arr_1, arr_2, color='red') Out[\u00a0]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f239df79050&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(1, 1, 1)\n\nrect = plt.Rectangle((0, 0), 1, 1)\ncircle = plt.Circle((0, 0), 2)\n\nax.add_patch(rect)\nax.add_patch(circle)\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\n</pre> import matplotlib.pyplot as plt fig = plt.figure(figsize=(5, 5)) ax = fig.add_subplot(1, 1, 1)  rect = plt.Rectangle((0, 0), 1, 1) circle = plt.Circle((0, 0), 2)  ax.add_patch(rect) ax.add_patch(circle) ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) Out[\u00a0]: <pre>(-2.0, 2.0)</pre> In\u00a0[\u00a0]: Copied! <pre>arr_1 = np.array([10, 20, 60, 35, 75, 100, 65])\narr_2 = np.array([10, 20, 60, 35, 75, 100, 65])\n\nplt.bar(x=np.arange(len(arr_1)), height=arr_1, color='red', label='s\u1ed1 1')\nplt.bar(x=np.arange(len(arr_2)), height=arr_2, color='green', bottom=arr_1, label='s\u1ed1 2')\nplt.legend()\n</pre> arr_1 = np.array([10, 20, 60, 35, 75, 100, 65]) arr_2 = np.array([10, 20, 60, 35, 75, 100, 65])  plt.bar(x=np.arange(len(arr_1)), height=arr_1, color='red', label='s\u1ed1 1') plt.bar(x=np.arange(len(arr_2)), height=arr_2, color='green', bottom=arr_1, label='s\u1ed1 2') plt.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x7f239de9a690&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># x = [0, 1, 2, 3, 4, 5, 6]\n# x = [0, 1, 2, 3, 4, 5, 6] + 0.2\n\nWIDTH = 0.3\n\narr_1 = np.array([100, 20, 60, 350, 75, 100, 65])\narr_2 = np.array([10, 20, 60, 35, 75, 100, 65])\narr_3 = np.array([10, 20, 60, 35, 75, 100, 65])\n\nplt.bar(x=np.arange(len(arr_1)), height=arr_1, color='red', label='s\u1ed1 1', width=WIDTH)\nplt.bar(x=np.arange(len(arr_2)) + WIDTH, height=arr_2, color='green', label='s\u1ed1 2', width=WIDTH)\nplt.bar(x=np.arange(len(arr_3)) + 2*WIDTH, height=arr_3, color='blue', label='s\u1ed1 3', width=WIDTH)\nplt.legend()\n</pre> # x = [0, 1, 2, 3, 4, 5, 6] # x = [0, 1, 2, 3, 4, 5, 6] + 0.2  WIDTH = 0.3  arr_1 = np.array([100, 20, 60, 350, 75, 100, 65]) arr_2 = np.array([10, 20, 60, 35, 75, 100, 65]) arr_3 = np.array([10, 20, 60, 35, 75, 100, 65])  plt.bar(x=np.arange(len(arr_1)), height=arr_1, color='red', label='s\u1ed1 1', width=WIDTH) plt.bar(x=np.arange(len(arr_2)) + WIDTH, height=arr_2, color='green', label='s\u1ed1 2', width=WIDTH) plt.bar(x=np.arange(len(arr_3)) + 2*WIDTH, height=arr_3, color='blue', label='s\u1ed1 3', width=WIDTH) plt.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x7f239df07790&gt;</pre> <ul> <li>Python</li> <li>\u0110\u1ec7 quy</li> <li>numpy</li> <li>pandas</li> <li>matplotlib</li> </ul> In\u00a0[\u00a0]: Copied! <pre>series_1 = pd.Series([100, 20, 60, 350, 75, 100, 65])\n</pre> series_1 = pd.Series([100, 20, 60, 350, 75, 100, 65]) In\u00a0[\u00a0]: Copied! <pre>series_1.plot(kind='line')\n</pre> series_1.plot(kind='line') Out[\u00a0]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f239dcbd890&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('bank.csv')\ndf.columns\n</pre> df = pd.read_csv('bank.csv') df.columns In\u00a0[\u00a0]: Copied! <pre>df.plot(x = 'annual_income', y = 'spending_score', kind='scatter')\n</pre> df.plot(x = 'annual_income', y = 'spending_score', kind='scatter')"},{"location":"Learning/Visualization/#1-matplotlib","title":"1. Matplotlib\u00b6","text":""},{"location":"Learning/Visualization/#line-chart","title":"Line chart\u00b6","text":""},{"location":"Learning/Visualization/#bar-chart","title":"Bar chart\u00b6","text":""},{"location":"Learning/Visualization/#histogram","title":"Histogram\u00b6","text":"<p>{gia tri: so lan xuat hien}</p>"},{"location":"Learning/Visualization/#scatter","title":"Scatter\u00b6","text":""},{"location":"Learning/Visualization/#figure-object","title":"Figure object\u00b6","text":""},{"location":"Learning/Visualization/#colors-markers-line-styles","title":"Colors, markers, line styles\u00b6","text":""},{"location":"Learning/Visualization/#nhieu-bieu-o-tren-cung-1-anh","title":"Nhi\u1ec1u bi\u1ec3u \u0111\u1ed3 tr\u00ean c\u00f9ng 1 \u1ea3nh\u00b6","text":""},{"location":"Learning/Visualization/#xticks-yticks-xlim-ylim","title":"xticks, yticks, xlim, ylim\u00b6","text":""},{"location":"Learning/Visualization/#plttext-pltrectangle-pltcircle-pltpolygon","title":"plt.text, plt.Rectangle, plt.Circle, plt.Polygon\u00b6","text":""},{"location":"Learning/Visualization/#bar-chart-chong-len-nhau","title":"Bar chart ch\u1ed3ng l\u00ean nhau\u00b6","text":""},{"location":"Learning/Visualization/#bar-chart-lien-ke-nhau","title":"Bar chart li\u1ec1n k\u1ec1 nhau\u00b6","text":""},{"location":"Learning/Visualization/#2-pandas","title":"2. Pandas\u00b6","text":""},{"location":"Learning/Visualization/#3-seaborn","title":"3. Seaborn\u00b6","text":""},{"location":"Learning/AI_Model/essential/","title":"C\u00e1c ki\u1ebfn th\u1ee9c c\u1ea7n ngo\u00e0i l\u1ec1 c\u1ea7n thi\u1ebft khi x\u00e2y d\u1ef1ng ki\u1ebfn tr\u00fac UNet","text":""},{"location":"Learning/AI_Model/essential/#1-histogram-va-uong-kde","title":"1. Histogram v\u00e0 \u0111\u01b0\u1eddng KDE:","text":"<p>Histogram v\u00e0 KDE</p> <p>a. Kernel Density Estimation (KDE) l\u00e0 g\u00ec? KDE (\u01af\u1edbc l\u01b0\u1ee3ng m\u1eadt \u0111\u1ed9 h\u1ea1t nh\u00e2n) l\u00e0 m\u1ed9t ph\u01b0\u01a1ng ph\u00e1p th\u1ed1ng k\u00ea \u0111\u1ec3 \u01b0\u1edbc l\u01b0\u1ee3ng h\u00e0m m\u1eadt \u0111\u1ed9 x\u00e1c su\u1ea5t (Probability Density Function - PDF) c\u1ee7a m\u1ed9t t\u1eadp d\u1eef li\u1ec7u.  </p> <p>N\u00f3i \u0111\u01a1n gi\u1ea3n, KDE gi\u00fap b\u1ea1n th\u1ea5y ph\u00e2n ph\u1ed1i th\u1ef1c s\u1ef1 c\u1ee7a d\u1eef li\u1ec7u thay v\u00ec ch\u1ec9 d\u1ef1a v\u00e0o bi\u1ec3u \u0111\u1ed3 c\u1ed9t (histogram).  </p> <p>V\u00ed d\u1ee5: - Histogram chia d\u1eef li\u1ec7u th\u00e0nh t\u1eebng nh\u00f3m (bins) v\u00e0 \u0111\u1ebfm s\u1ed1 l\u01b0\u1ee3ng \u0111i\u1ec3m trong m\u1ed7i nh\u00f3m. - KDE v\u1ebd m\u1ed9t \u0111\u01b0\u1eddng cong li\u00ean t\u1ee5c m\u00f4 t\u1ea3 x\u00e1c su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eebng gi\u00e1 tr\u1ecb.  </p> <p> (H\u00ecnh minh h\u1ecda: Histogram vs KDE) </p> <p>b. C\u00e1ch KDE ho\u1ea1t \u0111\u1ed9ng? KDE s\u1eed d\u1ee5ng h\u1ea1t nh\u00e2n (kernel) \u2013 m\u1ed9t h\u00e0m to\u00e1n h\u1ecdc gi\u00fap \u01b0\u1edbc l\u01b0\u1ee3ng m\u1eadt \u0111\u1ed9 c\u1ee7a t\u1eebng \u0111i\u1ec3m d\u1eef li\u1ec7u. C\u00f4ng th\u1ee9c t\u1ed5ng qu\u00e1t c\u1ee7a KDE:  </p> \\[ \\hat{f}(x) = \\frac{1}{n h} \\sum_{i=1}^{n} K\\left(\\frac{x - x_i}{h}\\right) \\] <p>Trong \u0111\u00f3: - \\( \\hat{f}(x) \\) l\u00e0 \u01b0\u1edbc l\u01b0\u1ee3ng m\u1eadt \u0111\u1ed9 t\u1ea1i \u0111i\u1ec3m \\( x \\). - \\( n \\) l\u00e0 s\u1ed1 l\u01b0\u1ee3ng m\u1eabu d\u1eef li\u1ec7u. - \\( h \\) l\u00e0 b\u0103ng th\u00f4ng (bandwidth), x\u00e1c \u0111\u1ecbnh \u0111\u1ed9 m\u01b0\u1ee3t c\u1ee7a \u0111\u01b0\u1eddng KDE. - \\( K(\\cdot) \\) l\u00e0 h\u00e0m h\u1ea1t nh\u00e2n (kernel function), th\u01b0\u1eddng l\u00e0 m\u1ed9t h\u00e0m chu\u1ea9n nh\u01b0:   - Gaussian (ph\u1ed5 bi\u1ebfn nh\u1ea5t)   - Epanechnikov   - Uniform  </p> <p>Vai tr\u00f2 c\u1ee7a b\u0103ng th\u00f4ng (bandwidth) - N\u1ebfu b\u0103ng th\u00f4ng nh\u1ecf, KDE c\u00f3 nhi\u1ec1u dao \u0111\u1ed9ng, d\u1ec5 b\u1ecb nhi\u1ec5u. - N\u1ebfu b\u0103ng th\u00f4ng l\u1edbn, KDE tr\u1edf n\u00ean qu\u00e1 m\u01b0\u1ee3t, m\u1ea5t chi ti\u1ebft quan tr\u1ecdng.  </p> <p> (H\u00ecnh minh h\u1ecda: B\u0103ng th\u00f4ng nh\u1ecf vs v\u1eeba vs l\u1edbn) </p> <p>c. So s\u00e1nh Histogram v\u00e0 KDE</p> \u0110\u1eb7c \u0111i\u1ec3m Histogram KDE D\u1ea1ng bi\u1ec3u di\u1ec5n C\u1ed9t r\u1eddi r\u1ea1c \u0110\u01b0\u1eddng cong li\u00ean t\u1ee5c Ph\u1ee5 thu\u1ed9c v\u00e0o bins C\u00f3 Kh\u00f4ng \u1ea2nh h\u01b0\u1edfng b\u1edfi nhi\u1ec5u \u00cdt (nh\u01b0ng b\u1ecb chia theo bins) C\u00f3 th\u1ec3 b\u1ecb nhi\u1ec5u n\u1ebfu bandwidth nh\u1ecf Th\u1ec3 hi\u1ec7n ph\u00e2n ph\u1ed1i Th\u00f4 s\u01a1 M\u01b0\u1ee3t h\u01a1n, d\u1ec5 quan s\u00e1t <p>\ud83d\udc49 Histogram ph\u00f9 h\u1ee3p khi c\u1ea7n \u0111\u1ebfm s\u1ed1 l\u01b0\u1ee3ng \ud83d\udc49 KDE ph\u00f9 h\u1ee3p khi mu\u1ed1n hi\u1ec3u r\u00f5 h\u01a1n v\u1ec1 ph\u00e2n ph\u1ed1i th\u1ef1c s\u1ef1 c\u1ee7a d\u1eef li\u1ec7u </p>"},{"location":"Learning/AI_Model/essential/#2-upconv-convolution-transpose-nnconvtranspose2d","title":"2. Upconv (Convolution transpose) <code>nn.ConvTranspose2d</code>:","text":"<pre><code>def upconv_block(self, in_channels, out_channels):\n    return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n</code></pre>"},{"location":"Learning/AI_Model/essential/#3-upsampling-upsample-torchnnupsamplescale_factor-mode","title":"3. Upsampling (Upsample) <code>torch.nn.Upsample(scale_factor, mode)</code>","text":"<p>Upsampling trong PyTorch \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 t\u0103ng k\u00edch th\u01b0\u1edbc tensor b\u1eb1ng ph\u01b0\u01a1ng ph\u00e1p n\u1ed9i suy. D\u01b0\u1edbi \u0111\u00e2y l\u00e0 c\u00e1c ch\u1ebf \u0111\u1ed9 (<code>mode</code>) ph\u1ed5 bi\u1ebfn c\u1ee7a <code>nn.Upsample</code>:  </p>"},{"location":"Learning/AI_Model/essential/#1-nearest-neighbor-nearest","title":"1. Nearest Neighbor (<code>'nearest'</code>)","text":"<ul> <li>\u0110\u01a1n gi\u1ea3n nh\u1ea5t, l\u1eb7p l\u1ea1i gi\u00e1 tr\u1ecb c\u1ee7a pixel l\u00e2n c\u1eadn g\u1ea7n nh\u1ea5t.  </li> <li>Kh\u00f4ng t\u1ea1o hi\u1ec7u \u1ee9ng l\u00e0m m\u1ecbn, c\u00f3 th\u1ec3 g\u00e2y r\u0103ng c\u01b0a.  </li> <li>C\u00fa ph\u00e1p: <pre><code>import torch.nn as nn\n\nupsample = nn.Upsample(scale_factor=2, mode='nearest')\n</code></pre></li> </ul>"},{"location":"Learning/AI_Model/essential/#2-bilinear-bilinear","title":"2. Bilinear (<code>'bilinear'</code>)","text":"<ul> <li>D\u00f9ng n\u1ed9i suy tuy\u1ebfn t\u00ednh 2D \u0111\u1ec3 t\u1ea1o pixel m\u1edbi d\u1ef1a tr\u00ean c\u00e1c gi\u00e1 tr\u1ecb xung quanh.  </li> <li>M\u1ecbn h\u01a1n so v\u1edbi <code>'nearest'</code>.  </li> <li>C\u1ea7n <code>align_corners=True/False</code> \u0111\u1ec3 ki\u1ec3m so\u00e1t c\u00e1ch n\u1ed9i suy. </li> <li>C\u00fa ph\u00e1p: <pre><code>upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n</code></pre></li> </ul>"},{"location":"Learning/AI_Model/essential/#3-bicubic-bicubic","title":"3. Bicubic (<code>'bicubic'</code>)","text":"<ul> <li>N\u1ed9i suy b\u1eadc ba, m\u1ecbn h\u01a1n bilinear.  </li> <li>Th\u01b0\u1eddng d\u00f9ng \u0111\u1ec3 upscale \u1ea3nh ch\u1ea5t l\u01b0\u1ee3ng cao.  </li> <li>C\u00fa ph\u00e1p: <pre><code>upsample = nn.Upsample(scale_factor=2, mode='bicubic', align_corners=True)\n</code></pre></li> </ul>"},{"location":"Learning/AI_Model/essential/#4-trilinear-trilinear","title":"4. Trilinear (<code>'trilinear'</code>)","text":"<ul> <li>N\u1ed9i suy tuy\u1ebfn t\u00ednh trong 3D (cho d\u1eef li\u1ec7u 3D nh\u01b0 video, y t\u1ebf).  </li> <li>C\u00fa ph\u00e1p: <pre><code>upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n</code></pre></li> </ul>"},{"location":"Learning/AI_Model/essential/#so-sanh-nhanh","title":"So s\u00e1nh nhanh","text":"Mode L\u00e0m m\u1ecbn? \u0110\u1ed9 ph\u1ee9c t\u1ea1p \u1ee8ng d\u1ee5ng ch\u00ednh <code>'nearest'</code> \u274c Th\u1ea5p Upsampling nhanh, kh\u00f4ng l\u00e0m m\u1ecbn <code>'bilinear'</code> \u2705 Trung b\u00ecnh L\u00e0m m\u1ecbn \u1ea3nh 2D <code>'bicubic'</code> \u2705\u2705 Cao T\u0103ng \u0111\u1ed9 ph\u00e2n gi\u1ea3i ch\u1ea5t l\u01b0\u1ee3ng cao <code>'trilinear'</code> \u2705 Cao N\u1ed9i suy tr\u00ean d\u1eef li\u1ec7u 3D"},{"location":"Learning/AI_Model/essential/#4-crop-image-crop-tensor","title":"4. crop image <code>Crop tensor</code>:","text":"<pre><code>def crop_tensor(self, tensor, target_size):\n    _, _, H, W = tensor.size()\n    diffY = (H - target_size[0]) // 2\n    diffX = (W - target_size[1]) // 2\n    return tensor[:, :, diffY:(diffY + target_size[0]), diffX:(diffX + target_size[1])]\n</code></pre> <ul> <li>H\u00e0m tr\u00ean s\u1ebd c\u1eaft c\u00e1c ph\u1ea7n d\u01b0 th\u1eeba c\u1ee7a tensor \u0111\u01b0\u1ee3c truy\u1ec3n v\u00e0o sao cho c\u00f3 size gi\u1ed1ng v\u1edbi target_size</li> </ul> <p>Gi\u1ea3i th\u00edch v\u1ec1: <code>dec1.shape[2:]</code>:</p> <ul> <li> <p>dec1.shape c\u00f3 d\u1ea1ng \\((batch_size, channels, height, width)\\).</p> </li> <li> <p><code>dec1.shape[2:]</code> tr\u1ea3 v\u1ec1 m\u1ed9t tuple ch\u1ee9a hai gi\u00e1 tr\u1ecb: \\((height, width)\\) c\u1ee7a tensor dec1.</p> </li> <li> <p>V\u00ed d\u1ee5: N\u1ebfu dec1 c\u00f3 k\u00edch th\u01b0\u1edbc \\((1, 64, 388, 388)\\), th\u00ec <code>dec1.shape[2:]</code> s\u1ebd tr\u1ea3 v\u1ec1 \\((388, 388)\\).</p> </li> </ul>"},{"location":"Learning/AI_Model/essential/#5-torchcat","title":"5. <code>torch.cat()</code>:","text":"<pre><code>torch.cat((dec4, enc4), dim=1)\n</code></pre> <ul> <li>G\u1ed9p 2 dec4 v\u00e0 enc4 l\u1ea1i theo chi\u1ec1u 1 (t\u1ee9c l\u00e0 t\u0103ng s\u1ed1 l\u01b0\u1ee3ng channel l\u00ean).</li> </ul>"},{"location":"Learning/AI_Model/essential/#6-nnbatchnorm2d","title":"6.  <code>nn.BatchNorm2d()</code>:","text":"<p>Batch Normalization l\u00e0 g\u00ec? </p> <p>Batch Normalization (BatchNorm) l\u00e0 m\u1ed9t k\u1ef9 thu\u1eadt chu\u1ea9n h\u00f3a d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o t\u1ea1i m\u1ed7i l\u1edbp c\u1ee7a m\u1ea1ng n\u01a1-ron \u0111\u1ec3 c\u1ea3i thi\u1ec7n hi\u1ec7u su\u1ea5t v\u00e0 t\u1ed1c \u0111\u1ed9 hu\u1ea5n luy\u1ec7n. N\u00f3 gi\u00fap \u1ed5n \u0111\u1ecbnh qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n b\u1eb1ng c\u00e1ch gi\u1ea3m s\u1ef1 thay \u0111\u1ed5i c\u1ee7a c\u00e1c ph\u00e2n ph\u1ed1i \u0111\u1ea7u v\u00e0o, l\u00e0m cho m\u00f4 h\u00ecnh h\u1ecdc nhanh h\u01a1n v\u00e0 t\u1ed5ng qu\u00e1t t\u1ed1t h\u01a1n.</p> <p>C\u00e1ch ho\u1ea1t \u0111\u1ed9ng c\u1ee7a BatchNorm </p> <p>Gi\u1ea3 s\u1eed ta c\u00f3 \u0111\u1ea7u v\u00e0o \\(x\\) v\u1edbi k\u00edch th\u01b0\u1edbc \\((batch\\_size, channels, height, width)\\). BatchNorm th\u1ef1c hi\u1ec7n c\u00e1c b\u01b0\u1edbc sau tr\u00ean m\u1ed7i k\u00eanh ri\u00eang l\u1ebb:</p> <p>T\u00ednh trung b\u00ecnh (mean) c\u1ee7a t\u1eebng k\u00eanh:  \\(\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\\).</p> <ul> <li>Trong \u0111\u00f3 \\(m\\) l\u00e0 s\u1ed1 l\u01b0\u1ee3ng m\u1eabu trong batch.</li> </ul> <p>T\u00ednh ph\u01b0\u01a1ng sai (variance) c\u1ee7a t\u1eebng k\u00eanh: \\(\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\\).</p> <ul> <li>\u0110i\u1ec1u n\u00e0y \u0111o l\u01b0\u1eddng m\u1ee9c \u0111\u1ed9 ph\u00e2n t\u00e1n c\u1ee7a d\u1eef li\u1ec7u trong batch.</li> </ul> <p>Chu\u1ea9n h\u00f3a d\u1eef li\u1ec7u b\u1eb1ng c\u00e1ch \u0111\u01b0a v\u1ec1 ph\u00e2n ph\u1ed1i c\u00f3 trung b\u00ecnh 0 v\u00e0 ph\u01b0\u01a1ng sai 1: \\(\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\).</p> <ul> <li>Trong \u0111\u00f3 \\(\\epsilon\\) l\u00e0 m\u1ed9t s\u1ed1 r\u1ea5t nh\u1ecf \u0111\u1ec3 tr\u00e1nh chia cho 0.</li> </ul> <p>Th\u00eam tham s\u1ed1 trainable (scale &amp; shift):  \\(y_i = \\gamma \\hat{x}_i + \\beta\\).</p> <ul> <li>\\(\\gamma\\) (scale) v\u00e0 \\(\\beta\\) (shift) l\u00e0 c\u00e1c tham s\u1ed1 h\u1ecdc \u0111\u01b0\u1ee3c, gi\u00fap BatchNorm c\u00f3 th\u1ec3 kh\u00f4i ph\u1ee5c l\u1ea1i kh\u1ea3 n\u0103ng bi\u1ec3u di\u1ec5n ban \u0111\u1ea7u n\u1ebfu c\u1ea7n.</li> </ul> <p>V\u00ed d\u1ee5 v\u1ec1 c\u00e1ch ho\u1ea1t \u0111\u1ed9ng c\u1ee7a <code>nn.BatchNorm2d(out_channels)</code> <pre><code>import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nbatch_size = 1000  # T\u0103ng batch_size \u0111\u1ec3 c\u00f3 ph\u00e2n ph\u1ed1i t\u1ed1t h\u01a1n\nout_channels = 16\nheight, width = 32, 32\n\n# L\u1edbp BatchNorm2d\nbn = nn.BatchNorm2d(out_channels)\n\n# T\u1ea1o d\u1eef li\u1ec7u gi\u1ea3 v\u1edbi ph\u00e2n ph\u1ed1i kh\u00f4ng \u0111\u1ed3ng \u0111\u1ec1u (ph\u00e2n b\u1ed1 l\u1ec7ch tr\u00e1i, exponential)\nx = torch.rand(batch_size, out_channels, height, width) ** 1.5  # N\u00e2ng l\u0169y th\u1eeba \u0111\u1ec3 t\u1ea1o ph\u00e2n b\u1ed1 l\u1ec7ch\n\n# \u00c1p d\u1ee5ng BatchNorm\noutput = bn(x)\n\nprint(output.shape)\n\ndef plot_distribution(ax, data, title, color):\n    \"\"\"H\u00e0m v\u1ebd bi\u1ec3u \u0111\u1ed3 ph\u00e2n ph\u1ed1i c\u1ee7a d\u1eef li\u1ec7u tr\u00ean subplot\"\"\"\n    data_flat = data.view(-1).detach().numpy()\n    sns.histplot(data_flat, bins=100, kde=True, color=color, ax=ax)\n    ax.set_title(title)\n    ax.set_xlabel('Gi\u00e1 tr\u1ecb k\u00edch ho\u1ea1t')\n    ax.set_ylabel('T\u1ea7n su\u1ea5t')\n\n# T\u1ea1o figure v\u00e0 subplot\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# V\u1ebd bi\u1ec3u \u0111\u1ed3 tr\u01b0\u1edbc v\u00e0 sau BatchNorm tr\u00ean c\u00e1c subplot kh\u00e1c nhau\nplot_distribution(axes[0], x, 'Tr\u01b0\u1edbc BatchNorm', 'blue')\nplot_distribution(axes[1], output, 'Sau BatchNorm', 'red')\n\nplt.tight_layout()\nplt.show()\n</code></pre> Output:</p> <p>torch.Size([1000, 16, 32, 32]) </p> <p>\u1ea2nh h\u01b0\u1edfng c\u1ee7a BatchNorm \u0111\u1ebfn ph\u00e2n ph\u1ed1i d\u1eef li\u1ec7u - Tr\u01b0\u1edbc BatchNorm: D\u1eef li\u1ec7u c\u00f3 th\u1ec3 c\u00f3 b\u1ea5t k\u1ef3 d\u1ea1ng ph\u00e2n ph\u1ed1i n\u00e0o (ph\u00e2n b\u1ed1 l\u1ec7ch, ph\u00e2n b\u1ed1 d\u1ed1c, v.v.). - Sau BatchNorm: D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c chuy\u1ec3n v\u1ec1 d\u1ea1ng ph\u00e2n ph\u1ed1i chu\u1ea9n h\u01a1n v\u1edbi trung b\u00ecnh x\u1ea5p x\u1ec9 <code>0</code> v\u00e0 ph\u01b0\u01a1ng sai <code>1</code>.</p> <p>L\u1ee3i \u00edch c\u1ee7a Batch Normalization 1. Gi\u00fap hu\u1ea5n luy\u1ec7n nhanh h\u01a1n: Do d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c chu\u1ea9n h\u00f3a, m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 h\u1ecdc nhanh h\u01a1n v\u00e0 gi\u1ea3m s\u1ef1 c\u1ea7n thi\u1ebft c\u1ee7a learning rate th\u1ea5p. 2. \u1ed4n \u0111\u1ecbnh qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n: Tr\u00e1nh t\u00ecnh tr\u1ea1ng \"Exploding Gradients\" ho\u1eb7c \"Vanishing Gradients\". 3. Gi\u1ea3m ph\u1ee5 thu\u1ed9c v\u00e0o kh\u1edfi t\u1ea1o tr\u1ecdng s\u1ed1: V\u00ec d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o \u0111\u00e3 \u0111\u01b0\u1ee3c chu\u1ea9n h\u00f3a, m\u00f4 h\u00ecnh \u00edt nh\u1ea1y c\u1ea3m h\u01a1n v\u1edbi c\u00e1ch kh\u1edfi t\u1ea1o ban \u0111\u1ea7u. 4. T\u1ed5ng qu\u00e1t h\u00f3a t\u1ed1t h\u01a1n: BatchNorm gi\u00fap m\u00f4 h\u00ecnh tr\u00e1nh b\u1ecb overfitting nh\u1edd t\u00e1c d\u1ee5ng t\u01b0\u01a1ng t\u1ef1 dropout.  </p>"},{"location":"Learning/AI_Model/essential/#7","title":"7.","text":""},{"location":"Learning/AI_Model/lenet5/","title":"Overview","text":"<p>CNN model tutorial part 1</p> <p>CNN model tutorial part 2</p> <p>CNN</p> <p>Lenet build from scratch</p> <p>Report</p> <p>Unet medical segmentation</p> <p>LeNet-5 MNIST (jupyter notebook)</p> <p>C\u00e1c t\u00e0i li\u1ec7u li\u00ean quan v\u1ec1 CNN - LeNet-5</p>"},{"location":"Learning/AI_Model/unet/","title":"Gi\u1ea3i th\u00edch v\u1ec1 ki\u1ebfn tr\u00fac UNet v\u00e0 c\u00e1ch c\u00e0i \u0111\u1eb7t.","text":""},{"location":"Learning/AI_Model/unet/#phan-i-gioi-thieu","title":"Ph\u1ea7n I: Gi\u1edbi thi\u1ec7u","text":""},{"location":"Learning/AI_Model/unet/#1-unet-la-gi","title":"1. UNet l\u00e0 g\u00ec?","text":"<p>UNet l\u00e0 m\u1ed9t m\u00f4 h\u00ecnh deep learning thu\u1ed9c ki\u1ebfn tr\u00fac CNN (Convolution Neural Network), \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf \u0111\u1eb7c bi\u1ec7t cho c\u00e1c t\u00e1c v\u1ee5 ph\u00e2n \u0111o\u1ea1n \u1ea3nh (image segmentation).</p> <p></p> <p>H\u00ecnh 1: Minh h\u1ecda v\u1ec1 m\u00f4 h\u00ecnh UNet v\u00e0 v\u00ed d\u1ee5 v\u1ec1 image segmentation.</p>"},{"location":"Learning/AI_Model/unet/#2-kien-truc-cua-unet","title":"2. Ki\u1ebfn tr\u00fac c\u1ee7a UNet","text":"<p>Ki\u1ebfn tr\u00fac UNet c\u00f3 h\u00ecnh d\u00e1ng gi\u1ed1ng ch\u1eef \"U\", g\u1ed3m 2 ph\u1ea7n ch\u00ednh:</p>"},{"location":"Learning/AI_Model/unet/#21-encoder-contraction-path","title":"2.1. Encoder (Contraction Path)","text":"<ul> <li> <p>G\u1ed3m c\u00e1c convolution layer v\u00e0 pooling layer \u0111\u1ec3 gi\u1ea3m k\u00edch th\u01b0\u1edbc kh\u00f4ng gian c\u1ee7a \u1ea3nh v\u00e0 t\u0103ng s\u1ed1 l\u01b0\u1ee3ng \u0111\u1eb7c tr\u01b0ng (feature maps).</p> </li> <li> <p>Gi\u00fap tr\u00edch xu\u1ea5t c\u00e1c \u0111\u1eb7c tr\u01b0ng quan tr\u1ecdng.</p> </li> </ul>"},{"location":"Learning/AI_Model/unet/#22-decoder-expansion-path","title":"2.2. Decoder (Expansion Path)","text":"<ul> <li> <p>S\u1eed d\u1ee5ng c\u00e1c l\u1edbp upsampling (deconvolution) \u0111\u1ec3 kh\u00f4i ph\u1ee5c k\u00edch th\u01b0\u1edbc ban \u0111\u1ea7u c\u1ee7a \u1ea3nh.</p> </li> <li> <p>K\u1ebft h\u1ee3p v\u1edbi c\u00e1c \u0111\u1eb7c tr\u01b0ng t\u1eeb Encoder (skip connections) gi\u00fap t\u00e1i t\u1ea1o chi ti\u1ebft t\u1ed1t h\u01a1n.</p> </li> </ul> <p></p> <p>H\u00ecnh 2: C\u1ea5u tr\u00fac ki\u1ebfn tr\u00fac UNet.</p>"},{"location":"Learning/AI_Model/unet/#3-ac-iem-noi-bat","title":"3. \u0110\u1eb7c \u0111i\u1ec3m n\u1ed5i b\u1eadt","text":"<ul> <li> <p>Skip connections: Gi\u00fap gi\u1eef l\u1ea1i th\u00f4ng tin chi ti\u1ebft t\u1eeb c\u00e1c l\u1edbp ban \u0111\u1ea7u v\u00e0 c\u1ea3i thi\u1ec7n k\u1ebft qu\u1ea3 ph\u00e2n \u0111o\u1ea1n.</p> </li> <li> <p>Kh\u00f4ng y\u00eau c\u1ea7u qu\u00e1 nhi\u1ec1u d\u1eef li\u1ec7u: Do ki\u1ebfn tr\u00fac \u0111\u1ed1i x\u1ee9ng v\u00e0 c\u00e1ch t\u1eadn d\u1ee5ng d\u1eef li\u1ec7u hi\u1ec7u qu\u1ea3.</p> </li> <li> <p>\u1ee8ng d\u1ee5ng trong ph\u00e2n \u0111o\u1ea1n \u1ea3nh y khoa: \u0110\u01b0\u1ee3c thi\u1ebft k\u1ebf ban \u0111\u1ea7u cho l\u0129nh v\u1ef1c y t\u1ebf, nh\u01b0 ph\u00e2n \u0111o\u1ea1n t\u1ebf b\u00e0o, m\u00f4 b\u1ec7nh, n\u00e3o MRI, v.v.</p> </li> </ul>"},{"location":"Learning/AI_Model/unet/#4-ung-dung-cua-unet","title":"4. \u1ee8ng d\u1ee5ng c\u1ee7a UNet","text":"<ul> <li> <p>Y t\u1ebf: Ph\u00e2n \u0111o\u1ea1n h\u00ecnh \u1ea3nh y khoa nh\u01b0 X-ray, MRI, CT Scan.</p> </li> <li> <p>Vi\u1ec5n th\u00e1m: Ph\u00e2n t\u00edch \u1ea3nh v\u1ec7 tinh, b\u1ea3n \u0111\u1ed3 \u0111\u1ecba l\u00fd.</p> </li> <li>Ngh\u1ec7 thu\u1eadt v\u00e0 x\u1eed l\u00fd \u1ea3nh: T\u1ea1o b\u1ea3n \u0111\u1ed3 s\u00e2u (depth maps), t\u00e1ch n\u1ec1n.</li> </ul> <p>UNet v\u1eabn l\u00e0 m\u1ed9t trong nh\u1eefng ki\u1ebfn tr\u00fac m \u1ea1nh m\u1ebd nh\u1ea5t cho ph\u00e2n \u0111o\u1ea1n \u1ea3nh v\u00e0 c\u00f3 nhi\u1ec1u bi\u1ebfn th\u1ec3 nh\u01b0 UNet++, Attention UNet, 3D UNet,... \u0111\u1ec3 c\u1ea3i thi\u1ec7n hi\u1ec7u su\u1ea5t.</p>"},{"location":"Learning/AI_Model/unet/#phan-ii-mo-ta-chi-tiet","title":"Ph\u1ea7n II: M\u00f4 t\u1ea3 chi ti\u1ebft","text":""},{"location":"Learning/AI_Model/unet/#1-vi-du-ve-mo-hinh-unet","title":"1. V\u00ed d\u1ee5 v\u1ec1 m\u00f4 h\u00ecnh UNet:","text":"<p>D\u01b0\u1edbi \u0111\u00e2y l\u00e0 m\u1ed9t \u0111o\u1ea1n code c\u01a1 b\u1ea3n s\u1eed d\u1ee5ng ki\u1ebfn tr\u00fac UNet trong PyTorch. UNet l\u00e0 m\u1ed9t ki\u1ebfn tr\u00fac m\u1ea1ng n\u01a1-ron ph\u1ed5 bi\u1ebfn trong c\u00e1c b\u00e0i to\u00e1n ph\u00e2n \u0111o\u1ea1n \u1ea3nh (image segmentation).</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UNet, self).__init__()\n\n        # Encoder\n        self.encoder1 = self.conv_block(in_channels, 64)\n        self.encoder2 = self.conv_block(64, 128)\n        self.encoder3 = self.conv_block(128, 256)\n        self.encoder4 = self.conv_block(256, 512)\n\n        # Bottleneck\n        self.bottleneck = self.conv_block(512, 1024)\n\n        # Decoder\n        self.upconv4 = self.upconv_block(1024, 512)\n        self.decoder4 = self.conv_block(1024, 512)\n        self.upconv3 = self.upconv_block(512, 256)\n        self.decoder3 = self.conv_block(512, 256)\n        self.upconv2 = self.upconv_block(256, 128)\n        self.decoder2 = self.conv_block(256, 128)\n        self.upconv1 = self.upconv_block(128, 64)\n        self.decoder1 = self.conv_block(128, 64)\n\n        # Final layer\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def upconv_block(self, in_channels, out_channels):\n        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n\n    def crop_tensor(self, tensor, target_size):\n        _, _, H, W = tensor.size()\n        diffY = (H - target_size[0]) // 2\n        diffX = (W - target_size[1]) // 2\n        return tensor[:, :, diffY:(diffY + target_size[0]), diffX:(diffX + target_size[1])]\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(F.max_pool2d(enc1, 2))\n        enc3 = self.encoder3(F.max_pool2d(enc2, 2))\n        enc4 = self.encoder4(F.max_pool2d(enc3, 2))\n\n        # Bottleneck\n        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))\n\n        # Decoder\n        dec4 = self.upconv4(bottleneck)\n        enc4 = self.crop_tensor(enc4, dec4.shape[2:])\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n\n        dec3 = self.upconv3(dec4)\n        enc3 = self.crop_tensor(enc3, dec3.shape[2:])\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = self.upconv2(dec3)\n        enc2 = self.crop_tensor(enc2, dec2.shape[2:])\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = self.upconv1(dec2)\n        enc1 = self.crop_tensor(enc1, dec1.shape[2:])\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # Final layer\n        out = self.final_conv(dec1)\n\n        return out\n\n# Ki\u1ec3m tra xem GPU c\u00f3 s\u1eb5n kh\u00f4ng\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# T\u1ea1o m\u00f4 h\u00ecnh v\u00e0 di chuy\u1ec3n sang GPU\nmodel = UNet(in_channels=1, out_channels=1).to(device)\n\n# T\u1ea1o d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o v\u00e0 di chuy\u1ec3n sang GPU\nx = torch.randn((1, 1, 572, 572)).to(device)\n\n# Ch\u1ea1y m\u00f4 h\u00ecnh\noutput = model(x)\nprint(output.shape)  # K\u00edch th\u01b0\u1edbc \u0111\u1ea7u ra: (1, 1, 388, 388)\n</code></pre>"},{"location":"Learning/AI_Model/unet/#giai-thich-so-qua","title":"Gi\u1ea3i th\u00edch s\u01a1 qua:","text":"<ul> <li>Encoder: Ph\u1ea7n encoder bao g\u1ed3m c\u00e1c kh\u1ed1i convolution (<code>conv_block</code>) v\u00e0 max pooling \u0111\u1ec3 gi\u1ea3m k\u00edch th\u01b0\u1edbc kh\u00f4ng gian c\u1ee7a \u1ea3nh v\u00e0 t\u0103ng s\u1ed1 l\u01b0\u1ee3ng k\u00eanh (channels).</li> <li>Bottleneck: \u0110\u00e2y l\u00e0 ph\u1ea7n n\u1eb1m gi\u1eefa encoder v\u00e0 decoder, th\u01b0\u1eddng l\u00e0 m\u1ed9t kh\u1ed1i convolution v\u1edbi s\u1ed1 l\u01b0\u1ee3ng k\u00eanh l\u1edbn nh\u1ea5t.</li> <li>Decoder: Ph\u1ea7n decoder s\u1eed d\u1ee5ng c\u00e1c kh\u1ed1i up-convolution (<code>upconv_block</code>) \u0111\u1ec3 t\u0103ng k\u00edch th\u01b0\u1edbc kh\u00f4ng gian c\u1ee7a \u1ea3nh v\u00e0 gi\u1ea3m s\u1ed1 l\u01b0\u1ee3ng k\u00eanh. C\u00e1c k\u1ebft n\u1ed1i skip (skip connections) \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 k\u1ebft h\u1ee3p th\u00f4ng tin t\u1eeb encoder t\u01b0\u01a1ng \u1ee9ng.</li> <li>Final layer: L\u1edbp convolution cu\u1ed1i c\u00f9ng \u0111\u1ec3 \u00e1nh x\u1ea1 s\u1ed1 l\u01b0\u1ee3ng k\u00eanh \u0111\u1ea7u ra th\u00e0nh s\u1ed1 l\u1edbp c\u1ea7n ph\u00e2n \u0111o\u1ea1n.</li> </ul>"},{"location":"Learning/AI_Model/unet/#luu-y","title":"L\u01b0u \u00fd:","text":"<ul> <li>K\u00edch th\u01b0\u1edbc \u0111\u1ea7u v\u00e0o v\u00e0 \u0111\u1ea7u ra c\u1ee7a UNet ph\u1ee5 thu\u1ed9c v\u00e0o k\u00edch th\u01b0\u1edbc c\u1ee7a \u1ea3nh \u0111\u1ea7u v\u00e0o v\u00e0 s\u1ed1 l\u01b0\u1ee3ng l\u1edbp c\u1ea7n ph\u00e2n \u0111o\u1ea1n.</li> <li>B\u1ea1n c\u00f3 th\u1ec3 \u0111i\u1ec1u ch\u1ec9nh s\u1ed1 l\u01b0\u1ee3ng k\u00eanh, s\u1ed1 l\u01b0\u1ee3ng l\u1edbp, v\u00e0 c\u00e1c tham s\u1ed1 kh\u00e1c \u0111\u1ec3 ph\u00f9 h\u1ee3p v\u1edbi b\u00e0i to\u00e1n c\u1ee5 th\u1ec3 c\u1ee7a m\u00ecnh.</li> </ul>"},{"location":"Learning/AI_Model/unet/#2-ban-ve-tay-giai-thich-ve-unet-va-cac-layer","title":"2. B\u1ea3n v\u1ebd tay gi\u1ea3i th\u00edch v\u1ec1 UNet v\u00e0 c\u00e1c layer.","text":""},{"location":"Learning/AI_Model/unet/#3-u-net-for-building-segmenttation-pytorch-kaggle","title":"3. U-Net for Building Segmenttation - PyTorch (Kaggle)","text":"<ul> <li>link</li> </ul>"},{"location":"Learning/AI_Model/unet/#4-unet-architecture-image-segmentation","title":"4. Unet Architecture image segmentation","text":"<ul> <li>UNet (Jupyter notebook)</li> </ul>"},{"location":"Learning/AI_Model/unet/#phan-iii-cach-cai-at-unet-bang-pytorch","title":"Ph\u1ea7n III: C\u00e1ch c\u00e0i \u0111\u1eb7t UNet b\u1eb1ng PyTorch","text":""},{"location":"Learning/AI_Model/essential/histogram_kde/","title":"C\u00e1ch v\u1ebd Histogram v\u00e0 KDE b\u1eb1ng seaborn","text":"In\u00a0[3]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# T\u1ea1o d\u1eef li\u1ec7u ng\u1eabu nhi\u00ean\ndata = np.random.rand(1000)  # 1000 gi\u00e1 tr\u1ecb t\u1eeb ph\u00e2n ph\u1ed1i chu\u1ea9n\n\n# V\u1ebd bi\u1ec3u \u0111\u1ed3 histogram v\u00e0 KDE\nsns.histplot(data, bins=30, kde=True, color='blue')\n\n# Th\u00eam ti\u00eau \u0111\u1ec1 v\u00e0 nh\u00e3n tr\u1ee5c\nplt.title('Histogram &amp; KDE')\nplt.xlabel('Gi\u00e1 tr\u1ecb')\nplt.ylabel('T\u1ea7n su\u1ea5t')\n\n# Hi\u1ec3n th\u1ecb bi\u1ec3u \u0111\u1ed3\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt import seaborn as sns  # T\u1ea1o d\u1eef li\u1ec7u ng\u1eabu nhi\u00ean data = np.random.rand(1000)  # 1000 gi\u00e1 tr\u1ecb t\u1eeb ph\u00e2n ph\u1ed1i chu\u1ea9n  # V\u1ebd bi\u1ec3u \u0111\u1ed3 histogram v\u00e0 KDE sns.histplot(data, bins=30, kde=True, color='blue')  # Th\u00eam ti\u00eau \u0111\u1ec1 v\u00e0 nh\u00e3n tr\u1ee5c plt.title('Histogram &amp; KDE') plt.xlabel('Gi\u00e1 tr\u1ecb') plt.ylabel('T\u1ea7n su\u1ea5t')  # Hi\u1ec3n th\u1ecb bi\u1ec3u \u0111\u1ed3 plt.show()  In\u00a0[5]: Copied! <pre>data2 = (data - data.mean()) / data.std()  # Sau BatchNorm (chu\u1ea9n h\u00f3a)\n\n# T\u1ea1o figure v\u1edbi 2 subplot (1 h\u00e0ng, 2 c\u1ed9t)\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# V\u1ebd histogram + KDE tr\u00ean subplot \u0111\u1ea7u ti\u00ean\nsns.histplot(data, bins=30, kde=True, color='blue', ax=axes[0])\naxes[0].set_title('Tr\u01b0\u1edbc BatchNorm')\n\n# V\u1ebd histogram + KDE tr\u00ean subplot th\u1ee9 hai\nsns.histplot(data2, bins=30, kde=True, color='red', ax=axes[1])\naxes[1].set_title('Sau BatchNorm')\n\n# C\u0103n ch\u1ec9nh kho\u1ea3ng c\u00e1ch gi\u1eefa c\u00e1c subplot\nplt.tight_layout()\n\n# Hi\u1ec3n th\u1ecb bi\u1ec3u \u0111\u1ed3\nplt.show()\n</pre> data2 = (data - data.mean()) / data.std()  # Sau BatchNorm (chu\u1ea9n h\u00f3a)  # T\u1ea1o figure v\u1edbi 2 subplot (1 h\u00e0ng, 2 c\u1ed9t) fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # V\u1ebd histogram + KDE tr\u00ean subplot \u0111\u1ea7u ti\u00ean sns.histplot(data, bins=30, kde=True, color='blue', ax=axes[0]) axes[0].set_title('Tr\u01b0\u1edbc BatchNorm')  # V\u1ebd histogram + KDE tr\u00ean subplot th\u1ee9 hai sns.histplot(data2, bins=30, kde=True, color='red', ax=axes[1]) axes[1].set_title('Sau BatchNorm')  # C\u0103n ch\u1ec9nh kho\u1ea3ng c\u00e1ch gi\u1eefa c\u00e1c subplot plt.tight_layout()  # Hi\u1ec3n th\u1ecb bi\u1ec3u \u0111\u1ed3 plt.show()"},{"location":"Learning/AI_Model/essential/histogram_kde/#cach-ve-histogram-va-kde-bang-seaborn","title":"C\u00e1ch v\u1ebd Histogram v\u00e0 KDE b\u1eb1ng seaborn\u00b6","text":""},{"location":"Learning/AI_Model/essential/histogram_kde/#cach-ve-nhieu-bieu-o-tren-cung-mot-figure-subplot","title":"C\u00e1ch v\u1ebd nhi\u1ec1u bi\u1ec3u \u0111\u1ed3 tr\u00ean c\u00f9ng m\u1ed9t figure (subplot)\u00b6","text":""},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/","title":"X\u00e2y d\u1ef1ng LeNet-5 model v\u1edbi Pytorch hu\u1ea5n luy\u1ec7n tr\u00ean t\u1eadp d\u1eef li\u1ec7u MNIST.","text":"In\u00a0[\u00a0]: Copied! <pre>import torchvision\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n\ntransform_augmented = transforms.Compose([\n    transforms.RandomAffine(\n        degrees=10, # G\u00f3c xoay \u1ea3nh qua l\u1ea1i (qua tr\u00e1i ho\u1eb7c qua ph\u1ea3i 10 \u0111\u1ed9)\n        translate = (0.1, 0.1), # T\u1ecbnh ti\u1ebfn \u1ea3nh l\u00ean xu\u1ed1ng tr\u00e1i ph\u1ea3i\n        scale = (0.9, 1.1), # T\u1ec9 l\u1ec7 so v\u1edbi \u1ea3nh g\u1ed1c\n        shear = 10 # \u0110\u1ed9 nghi\u00eang\n    ), \n    transforms.ToTensor() # Chuy\u1ec3n sang tensor \u0111\u1ec3 s\u1eed d\u1ee5ng PyTorch\n])\n\n# T\u1ea3i d\u1eef li\u1ec7u MNIST\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_augmented)\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_augmented)\n\n\nindices = list(range(len(train_dataset)))\n\ntrain_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=6969)\n\ntrain_subset = torch.utils.data.Subset(train_dataset, train_indices)\nval_subset = torch.utils.data.Subset(train_dataset, val_indices)\n\n\n# T\u1ea1o DataLoader\ntrainloader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=3)\nvalloader = torch.utils.data.DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=3)\ntestloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=3)\n\n\n# Ki\u1ec3m tra k\u00edch th\u01b0\u1edbc\nprint(f\"Train size: {len(train_subset)}, Validation size: {len(val_subset)} Test size: {len(test_dataset)}\")\n</pre> import torchvision import torch import torch.nn as nn import torchvision.transforms as transforms import torch.optim as optim import numpy as np import matplotlib.pyplot as plt  from sklearn.metrics import confusion_matrix, classification_report from sklearn.model_selection import train_test_split   transform_augmented = transforms.Compose([     transforms.RandomAffine(         degrees=10, # G\u00f3c xoay \u1ea3nh qua l\u1ea1i (qua tr\u00e1i ho\u1eb7c qua ph\u1ea3i 10 \u0111\u1ed9)         translate = (0.1, 0.1), # T\u1ecbnh ti\u1ebfn \u1ea3nh l\u00ean xu\u1ed1ng tr\u00e1i ph\u1ea3i         scale = (0.9, 1.1), # T\u1ec9 l\u1ec7 so v\u1edbi \u1ea3nh g\u1ed1c         shear = 10 # \u0110\u1ed9 nghi\u00eang     ),      transforms.ToTensor() # Chuy\u1ec3n sang tensor \u0111\u1ec3 s\u1eed d\u1ee5ng PyTorch ])  # T\u1ea3i d\u1eef li\u1ec7u MNIST train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_augmented) test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_augmented)   indices = list(range(len(train_dataset)))  train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=6969)  train_subset = torch.utils.data.Subset(train_dataset, train_indices) val_subset = torch.utils.data.Subset(train_dataset, val_indices)   # T\u1ea1o DataLoader trainloader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=3) valloader = torch.utils.data.DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=3) testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=3)   # Ki\u1ec3m tra k\u00edch th\u01b0\u1edbc print(f\"Train size: {len(train_subset)}, Validation size: {len(val_subset)} Test size: {len(test_dataset)}\") <pre>Train size: 48000, Validation size: 12000 Test size: 10000\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig, axes = plt.subplots(7, 10, figsize=(10, 7))\nj = 0\n\nfor i, ax in enumerate(axes.flat):\n    image, label = train_subset[j]\n    while label != i % 10:\n        j += 1\n        image, label = train_subset[j]\n\n    ax.imshow(image[0], cmap='gray')\n    # ax.set_title(label)\n    ax.axis('off')\n\nplt.show()\n</pre> fig, axes = plt.subplots(7, 10, figsize=(10, 7)) j = 0  for i, ax in enumerate(axes.flat):     image, label = train_subset[j]     while label != i % 10:         j += 1         image, label = train_subset[j]      ax.imshow(image[0], cmap='gray')     # ax.set_title(label)     ax.axis('off')  plt.show() In\u00a0[\u00a0]: Copied! <pre>class LeNet5(nn.Module):\n    def __init__(self):\n        super(LeNet5, self).__init__()\n        # (32, 1, 28, 28)\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=9, kernel_size=3, padding=1) # (32, 9, 28, 28)\n        self.AvgPool1 = nn.AvgPool2d(kernel_size=2, stride=2) # (32, 9, 14, 14) \n\n        self.conv2 = nn.Conv2d(in_channels=9, out_channels=36, kernel_size=5) # (32, 36, 10, 10) \n        self.AvgPool2 = nn.AvgPool2d(kernel_size=2, stride=2) #(32, 36, 5, 5)\n\n        self.conv3 = nn.Conv2d(in_channels=36, out_channels=72, kernel_size=2) # (32, 72, 4, 4)\n\n        self.Flatten = nn.Flatten() # (32, 72, 4, 4) -&gt; (32, 72*4*4)\n\n        self.fc1 = nn.Linear(72*4*4, 96) # (32, 96)\n        self.fc2 = nn.Linear(96, 10) # (32, 10)\n\n        self.tanh = nn.Tanh()\n\n    \n    def forward(self, x):\n        x = self.tanh(self.conv1(x))\n        x = self.AvgPool1(x)\n        x = self.tanh(self.conv2(x))\n        x = self.AvgPool2(x)\n        x = self.tanh(self.conv3(x))\n        \n        x = self.Flatten(x)\n        x = self.tanh(self.fc1(x))\n        x = torch.softmax(self.fc2(x), dim=1)\n\n        return x\n</pre> class LeNet5(nn.Module):     def __init__(self):         super(LeNet5, self).__init__()         # (32, 1, 28, 28)         self.conv1 = nn.Conv2d(in_channels=1, out_channels=9, kernel_size=3, padding=1) # (32, 9, 28, 28)         self.AvgPool1 = nn.AvgPool2d(kernel_size=2, stride=2) # (32, 9, 14, 14)           self.conv2 = nn.Conv2d(in_channels=9, out_channels=36, kernel_size=5) # (32, 36, 10, 10)          self.AvgPool2 = nn.AvgPool2d(kernel_size=2, stride=2) #(32, 36, 5, 5)          self.conv3 = nn.Conv2d(in_channels=36, out_channels=72, kernel_size=2) # (32, 72, 4, 4)          self.Flatten = nn.Flatten() # (32, 72, 4, 4) -&gt; (32, 72*4*4)          self.fc1 = nn.Linear(72*4*4, 96) # (32, 96)         self.fc2 = nn.Linear(96, 10) # (32, 10)          self.tanh = nn.Tanh()           def forward(self, x):         x = self.tanh(self.conv1(x))         x = self.AvgPool1(x)         x = self.tanh(self.conv2(x))         x = self.AvgPool2(x)         x = self.tanh(self.conv3(x))                  x = self.Flatten(x)         x = self.tanh(self.fc1(x))         x = torch.softmax(self.fc2(x), dim=1)          return x In\u00a0[\u00a0]: Copied! <pre>class EarlyStopping():\n    def __init__(self, patience=7, delta=0):\n        self.patience = patience\n        self.delta = delta\n        \n        self.counter = 0\n        self.best_score = None # minimize val_loss\n        self.early_stop = False\n        self.best_model_state = None\n\n\n    def __call__(self, val_loss, model):\n        if self.best_score is None:\n            self.best_score = val_loss\n            self.best_model_state = model.state_dict()\n        elif val_loss &lt; self.best_score - self.delta:\n            self.best_score = val_loss\n            self.counter = 0\n            self.best_model_state = model.state_dict()\n        else:\n            self.counter += 1\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n    \n    \n    def load_best_model(self, model):\n        model.load_state_dict(self.best_model_state)\n</pre> class EarlyStopping():     def __init__(self, patience=7, delta=0):         self.patience = patience         self.delta = delta                  self.counter = 0         self.best_score = None # minimize val_loss         self.early_stop = False         self.best_model_state = None       def __call__(self, val_loss, model):         if self.best_score is None:             self.best_score = val_loss             self.best_model_state = model.state_dict()         elif val_loss &lt; self.best_score - self.delta:             self.best_score = val_loss             self.counter = 0             self.best_model_state = model.state_dict()         else:             self.counter += 1             if self.counter &gt;= self.patience:                 self.early_stop = True               def load_best_model(self, model):         model.load_state_dict(self.best_model_state) In\u00a0[\u00a0]: Copied! <pre>def Train_model():\n    model.train()\n    total_loss = 0.0\n    \n\n    for inputs, labels in trainloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        predictions = model(inputs) # forward pass\n        loss = loss_fn(predictions, labels)\n\n        optimizer.zero_grad() # zero out the gradients\n        loss.backward() # backpropagation\n        optimizer.step()\n        total_loss += loss.item()\n    \n    \n    avg_loss = total_loss / len(trainloader)\n    return avg_loss\n</pre> def Train_model():     model.train()     total_loss = 0.0           for inputs, labels in trainloader:         inputs, labels = inputs.to(device), labels.to(device)          predictions = model(inputs) # forward pass         loss = loss_fn(predictions, labels)          optimizer.zero_grad() # zero out the gradients         loss.backward() # backpropagation         optimizer.step()         total_loss += loss.item()               avg_loss = total_loss / len(trainloader)     return avg_loss In\u00a0[\u00a0]: Copied! <pre>def Validate_model():\n    model.eval()\n    total_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n\n    with torch.no_grad(): # Kh\u00f4ng t\u00ednh gradient \u0111\u1ec3 ti\u1ebft ki\u1ec7m b\u1ed9 nh\u1edb &amp; t\u0103ng t\u1ed1c \u0111\u1ed9 t\u00ednh  to\u00e1n.\n        for inputs, labels in valloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            predictions = model(inputs)\n            loss = loss_fn(predictions, labels)\n            \n            total_loss += loss.item() * inputs.size(0)\n\n            _, predicted = torch.max(predictions, 1)\n            correct_predictions += (predicted == labels).sum().item() # (TP + TN)\n            total_samples += inputs.size(0) # (TP + TN + FP + FN)\n    \n    \n    avg_loss = total_loss / total_samples\n    accuracy = correct_predictions / total_samples # accuracy = (TP + TN) / (TP + TN + FP + FN)\n\n    return avg_loss, accuracy\n</pre> def Validate_model():     model.eval()     total_loss = 0.0     correct_predictions = 0     total_samples = 0      with torch.no_grad(): # Kh\u00f4ng t\u00ednh gradient \u0111\u1ec3 ti\u1ebft ki\u1ec7m b\u1ed9 nh\u1edb &amp; t\u0103ng t\u1ed1c \u0111\u1ed9 t\u00ednh  to\u00e1n.         for inputs, labels in valloader:             inputs, labels = inputs.to(device), labels.to(device)              predictions = model(inputs)             loss = loss_fn(predictions, labels)                          total_loss += loss.item() * inputs.size(0)              _, predicted = torch.max(predictions, 1)             correct_predictions += (predicted == labels).sum().item() # (TP + TN)             total_samples += inputs.size(0) # (TP + TN + FP + FN)               avg_loss = total_loss / total_samples     accuracy = correct_predictions / total_samples # accuracy = (TP + TN) / (TP + TN + FP + FN)      return avg_loss, accuracy In\u00a0[15]: Copied! <pre>device = torch.device('cuda:0')\nmodel = LeNet5().to(device)\n\nloss_fn = nn.CrossEntropyLoss() # loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer\nearly_stopping = EarlyStopping(patience=100, delta=0)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n</pre> device = torch.device('cuda:0') model = LeNet5().to(device)  loss_fn = nn.CrossEntropyLoss() # loss function optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer early_stopping = EarlyStopping(patience=100, delta=0) scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5) In\u00a0[\u00a0]: Copied! <pre>num_epochs = 10000\ntrainloss_list = []\nvalloss_list = []\nvalaccuracy_list = []\n\nfor epoch in range(num_epochs):\n    train_loss = Train_model()\n    val_loss, val_accuracy = Validate_model()\n    \n    scheduler.step(val_loss)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break\n    \n    trainloss_list.append(train_loss)\n    valloss_list.append(val_loss)\n    valaccuracy_list.append(val_accuracy)\n    \n    print(f\"Epoch: {epoch + 1} / {num_epochs}, Average training loss: {train_loss:.9f},  Validation loss: {val_loss:.9f}, Validation accuracy: {val_accuracy:.9f}\", end=', ')\n    \n    # In learning rate\n    for param_group in optimizer.param_groups:\n        print(f\"Learning Rate = {param_group['lr']}\")\n\n\nearly_stopping.load_best_model(model)\nprint(\"Done training\")\n</pre> num_epochs = 10000 trainloss_list = [] valloss_list = [] valaccuracy_list = []  for epoch in range(num_epochs):     train_loss = Train_model()     val_loss, val_accuracy = Validate_model()          scheduler.step(val_loss)     early_stopping(val_loss, model)     if early_stopping.early_stop:         print(\"Early stopping\")         break          trainloss_list.append(train_loss)     valloss_list.append(val_loss)     valaccuracy_list.append(val_accuracy)          print(f\"Epoch: {epoch + 1} / {num_epochs}, Average training loss: {train_loss:.9f},  Validation loss: {val_loss:.9f}, Validation accuracy: {val_accuracy:.9f}\", end=', ')          # In learning rate     for param_group in optimizer.param_groups:         print(f\"Learning Rate = {param_group['lr']}\")   early_stopping.load_best_model(model) print(\"Done training\") <pre>Epoch: 1 / 10000, Average training loss: 1.711298673,  Validation loss: 1.570720556, Validation accuracy: 0.909750000, Learning Rate = 0.001\nEpoch: 2 / 10000, Average training loss: 1.545805686,  Validation loss: 1.527351081, Validation accuracy: 0.940250000, Learning Rate = 0.001\nEpoch: 3 / 10000, Average training loss: 1.523345617,  Validation loss: 1.515968457, Validation accuracy: 0.950083333, Learning Rate = 0.001\nEpoch: 4 / 10000, Average training loss: 1.515791066,  Validation loss: 1.511762278, Validation accuracy: 0.953750000, Learning Rate = 0.001\nEpoch: 5 / 10000, Average training loss: 1.510854689,  Validation loss: 1.515518937, Validation accuracy: 0.951250000, Learning Rate = 0.001\nEpoch: 6 / 10000, Average training loss: 1.508081745,  Validation loss: 1.502138111, Validation accuracy: 0.962583333, Learning Rate = 0.001\nEpoch: 7 / 10000, Average training loss: 1.505355273,  Validation loss: 1.501209243, Validation accuracy: 0.963916667, Learning Rate = 0.001\nEpoch: 8 / 10000, Average training loss: 1.500591198,  Validation loss: 1.502096941, Validation accuracy: 0.961750000, Learning Rate = 0.001\nEpoch: 9 / 10000, Average training loss: 1.499723890,  Validation loss: 1.499058124, Validation accuracy: 0.963833333, Learning Rate = 0.001\nEpoch: 10 / 10000, Average training loss: 1.499659726,  Validation loss: 1.496785388, Validation accuracy: 0.966916667, Learning Rate = 0.001\nEpoch: 11 / 10000, Average training loss: 1.497729574,  Validation loss: 1.496266824, Validation accuracy: 0.966416667, Learning Rate = 0.001\nEpoch: 12 / 10000, Average training loss: 1.498528179,  Validation loss: 1.494793018, Validation accuracy: 0.968500000, Learning Rate = 0.001\nEpoch: 13 / 10000, Average training loss: 1.497244548,  Validation loss: 1.499055842, Validation accuracy: 0.963583333, Learning Rate = 0.001\nEpoch: 14 / 10000, Average training loss: 1.495950928,  Validation loss: 1.495952556, Validation accuracy: 0.967500000, Learning Rate = 0.001\nEpoch: 15 / 10000, Average training loss: 1.493710838,  Validation loss: 1.491649203, Validation accuracy: 0.970750000, Learning Rate = 0.001\nEpoch: 16 / 10000, Average training loss: 1.494703108,  Validation loss: 1.490854134, Validation accuracy: 0.971833333, Learning Rate = 0.001\nEpoch: 17 / 10000, Average training loss: 1.493939047,  Validation loss: 1.497583027, Validation accuracy: 0.965083333, Learning Rate = 0.001\nEpoch: 18 / 10000, Average training loss: 1.493649021,  Validation loss: 1.493686789, Validation accuracy: 0.968333333, Learning Rate = 0.001\nEpoch: 19 / 10000, Average training loss: 1.493538829,  Validation loss: 1.489992214, Validation accuracy: 0.972833333, Learning Rate = 0.001\nEpoch: 20 / 10000, Average training loss: 1.493383412,  Validation loss: 1.488233262, Validation accuracy: 0.974083333, Learning Rate = 0.001\nEpoch: 21 / 10000, Average training loss: 1.491899186,  Validation loss: 1.492954819, Validation accuracy: 0.970333333, Learning Rate = 0.001\nEpoch: 22 / 10000, Average training loss: 1.492179828,  Validation loss: 1.490479735, Validation accuracy: 0.972166667, Learning Rate = 0.001\nEpoch: 23 / 10000, Average training loss: 1.490565386,  Validation loss: 1.489736879, Validation accuracy: 0.973000000, Learning Rate = 0.001\nEpoch: 24 / 10000, Average training loss: 1.491799639,  Validation loss: 1.492394189, Validation accuracy: 0.969833333, Learning Rate = 0.001\nEpoch: 25 / 10000, Average training loss: 1.491425840,  Validation loss: 1.489310807, Validation accuracy: 0.972833333, Learning Rate = 0.001\nEpoch: 26 / 10000, Average training loss: 1.489910583,  Validation loss: 1.489644904, Validation accuracy: 0.972333333, Learning Rate = 0.0001\nEpoch: 27 / 10000, Average training loss: 1.486524782,  Validation loss: 1.483460267, Validation accuracy: 0.979000000, Learning Rate = 0.0001\nEpoch: 28 / 10000, Average training loss: 1.484213956,  Validation loss: 1.483390267, Validation accuracy: 0.978500000, Learning Rate = 0.0001\nEpoch: 29 / 10000, Average training loss: 1.483561140,  Validation loss: 1.483384705, Validation accuracy: 0.978833333, Learning Rate = 0.0001\nEpoch: 30 / 10000, Average training loss: 1.482882334,  Validation loss: 1.483032667, Validation accuracy: 0.979416667, Learning Rate = 0.0001\nEpoch: 31 / 10000, Average training loss: 1.482381139,  Validation loss: 1.483168055, Validation accuracy: 0.978666667, Learning Rate = 0.0001\nEpoch: 32 / 10000, Average training loss: 1.481872785,  Validation loss: 1.481478404, Validation accuracy: 0.979416667, Learning Rate = 0.0001\nEpoch: 33 / 10000, Average training loss: 1.481215484,  Validation loss: 1.481203439, Validation accuracy: 0.980916667, Learning Rate = 0.0001\nEpoch: 34 / 10000, Average training loss: 1.481704401,  Validation loss: 1.480331378, Validation accuracy: 0.981750000, Learning Rate = 0.0001\nEpoch: 35 / 10000, Average training loss: 1.480607539,  Validation loss: 1.482099520, Validation accuracy: 0.979750000, Learning Rate = 0.0001\nEpoch: 36 / 10000, Average training loss: 1.481160646,  Validation loss: 1.479216059, Validation accuracy: 0.982583333, Learning Rate = 0.0001\nEpoch: 37 / 10000, Average training loss: 1.480495407,  Validation loss: 1.481189580, Validation accuracy: 0.980583333, Learning Rate = 0.0001\nEpoch: 38 / 10000, Average training loss: 1.480576204,  Validation loss: 1.481000502, Validation accuracy: 0.981000000, Learning Rate = 0.0001\nEpoch: 39 / 10000, Average training loss: 1.480809554,  Validation loss: 1.480039725, Validation accuracy: 0.981583333, Learning Rate = 0.0001\nEpoch: 40 / 10000, Average training loss: 1.479817109,  Validation loss: 1.480003026, Validation accuracy: 0.982166667, Learning Rate = 0.0001\nEpoch: 41 / 10000, Average training loss: 1.480577956,  Validation loss: 1.478754793, Validation accuracy: 0.983000000, Learning Rate = 0.0001\nEpoch: 42 / 10000, Average training loss: 1.479158375,  Validation loss: 1.480182472, Validation accuracy: 0.981666667, Learning Rate = 0.0001\nEpoch: 43 / 10000, Average training loss: 1.479873056,  Validation loss: 1.481755441, Validation accuracy: 0.979666667, Learning Rate = 0.0001\nEpoch: 44 / 10000, Average training loss: 1.479326307,  Validation loss: 1.479817016, Validation accuracy: 0.982166667, Learning Rate = 0.0001\nEpoch: 45 / 10000, Average training loss: 1.478551007,  Validation loss: 1.479362654, Validation accuracy: 0.982500000, Learning Rate = 0.0001\nEpoch: 46 / 10000, Average training loss: 1.479585331,  Validation loss: 1.479587564, Validation accuracy: 0.982750000, Learning Rate = 0.0001\nEpoch: 47 / 10000, Average training loss: 1.479173001,  Validation loss: 1.480204200, Validation accuracy: 0.981166667, Learning Rate = 1e-05\nEpoch: 48 / 10000, Average training loss: 1.478481606,  Validation loss: 1.479017972, Validation accuracy: 0.983166667, Learning Rate = 1e-05\nEpoch: 49 / 10000, Average training loss: 1.479050801,  Validation loss: 1.479842488, Validation accuracy: 0.981833333, Learning Rate = 1e-05\nEpoch: 50 / 10000, Average training loss: 1.477854023,  Validation loss: 1.480494069, Validation accuracy: 0.981166667, Learning Rate = 1e-05\nEpoch: 51 / 10000, Average training loss: 1.478615464,  Validation loss: 1.477589977, Validation accuracy: 0.984416667, Learning Rate = 1e-05\nEpoch: 52 / 10000, Average training loss: 1.478740357,  Validation loss: 1.478401830, Validation accuracy: 0.983083333, Learning Rate = 1e-05\nEpoch: 53 / 10000, Average training loss: 1.478531585,  Validation loss: 1.479511240, Validation accuracy: 0.982416667, Learning Rate = 1e-05\nEpoch: 54 / 10000, Average training loss: 1.477914494,  Validation loss: 1.477875246, Validation accuracy: 0.984166667, Learning Rate = 1e-05\nEpoch: 55 / 10000, Average training loss: 1.477958612,  Validation loss: 1.479526916, Validation accuracy: 0.982333333, Learning Rate = 1e-05\nEpoch: 56 / 10000, Average training loss: 1.477544259,  Validation loss: 1.478656245, Validation accuracy: 0.983166667, Learning Rate = 1e-05\nEpoch: 57 / 10000, Average training loss: 1.478594944,  Validation loss: 1.478552052, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000002e-06\nEpoch: 58 / 10000, Average training loss: 1.478027245,  Validation loss: 1.478645734, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000002e-06\nEpoch: 59 / 10000, Average training loss: 1.478559281,  Validation loss: 1.477567656, Validation accuracy: 0.984250000, Learning Rate = 1.0000000000000002e-06\nEpoch: 60 / 10000, Average training loss: 1.477488290,  Validation loss: 1.478282434, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000002e-06\nEpoch: 61 / 10000, Average training loss: 1.477915743,  Validation loss: 1.480260523, Validation accuracy: 0.980833333, Learning Rate = 1.0000000000000002e-06\nEpoch: 62 / 10000, Average training loss: 1.477846198,  Validation loss: 1.477244551, Validation accuracy: 0.985166667, Learning Rate = 1.0000000000000002e-06\nEpoch: 63 / 10000, Average training loss: 1.477955353,  Validation loss: 1.480428439, Validation accuracy: 0.981416667, Learning Rate = 1.0000000000000002e-06\nEpoch: 64 / 10000, Average training loss: 1.478676523,  Validation loss: 1.479950106, Validation accuracy: 0.981750000, Learning Rate = 1.0000000000000002e-06\nEpoch: 65 / 10000, Average training loss: 1.477937676,  Validation loss: 1.478950701, Validation accuracy: 0.982666667, Learning Rate = 1.0000000000000002e-06\nEpoch: 66 / 10000, Average training loss: 1.478513218,  Validation loss: 1.478361980, Validation accuracy: 0.983250000, Learning Rate = 1.0000000000000002e-06\nEpoch: 67 / 10000, Average training loss: 1.478624663,  Validation loss: 1.478067612, Validation accuracy: 0.983166667, Learning Rate = 1.0000000000000002e-06\nEpoch: 68 / 10000, Average training loss: 1.478018022,  Validation loss: 1.478157911, Validation accuracy: 0.983750000, Learning Rate = 1.0000000000000002e-07\nEpoch: 69 / 10000, Average training loss: 1.478312078,  Validation loss: 1.478823566, Validation accuracy: 0.983166667, Learning Rate = 1.0000000000000002e-07\nEpoch: 70 / 10000, Average training loss: 1.477755107,  Validation loss: 1.479224939, Validation accuracy: 0.982250000, Learning Rate = 1.0000000000000002e-07\nEpoch: 71 / 10000, Average training loss: 1.477897309,  Validation loss: 1.479360752, Validation accuracy: 0.982500000, Learning Rate = 1.0000000000000002e-07\nEpoch: 72 / 10000, Average training loss: 1.478121186,  Validation loss: 1.478910200, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000002e-07\nEpoch: 73 / 10000, Average training loss: 1.478199806,  Validation loss: 1.477432704, Validation accuracy: 0.984916667, Learning Rate = 1.0000000000000002e-07\nEpoch: 74 / 10000, Average training loss: 1.478643792,  Validation loss: 1.477706615, Validation accuracy: 0.984166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 75 / 10000, Average training loss: 1.479025278,  Validation loss: 1.478041023, Validation accuracy: 0.984250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 76 / 10000, Average training loss: 1.478323245,  Validation loss: 1.479632699, Validation accuracy: 0.982166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 77 / 10000, Average training loss: 1.477977127,  Validation loss: 1.477704630, Validation accuracy: 0.984250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 78 / 10000, Average training loss: 1.477340162,  Validation loss: 1.479285761, Validation accuracy: 0.982166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 79 / 10000, Average training loss: 1.478480024,  Validation loss: 1.478115533, Validation accuracy: 0.983583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 80 / 10000, Average training loss: 1.477960693,  Validation loss: 1.477915361, Validation accuracy: 0.983833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 81 / 10000, Average training loss: 1.477761649,  Validation loss: 1.478216903, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 82 / 10000, Average training loss: 1.478333407,  Validation loss: 1.478133688, Validation accuracy: 0.983666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 83 / 10000, Average training loss: 1.478370915,  Validation loss: 1.477969742, Validation accuracy: 0.984083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 84 / 10000, Average training loss: 1.477818390,  Validation loss: 1.479127992, Validation accuracy: 0.982500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 85 / 10000, Average training loss: 1.478062133,  Validation loss: 1.479069940, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 86 / 10000, Average training loss: 1.477872230,  Validation loss: 1.477006078, Validation accuracy: 0.984750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 87 / 10000, Average training loss: 1.478234522,  Validation loss: 1.478014402, Validation accuracy: 0.983583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 88 / 10000, Average training loss: 1.477435340,  Validation loss: 1.478932402, Validation accuracy: 0.982750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 89 / 10000, Average training loss: 1.477132710,  Validation loss: 1.478985333, Validation accuracy: 0.982750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 90 / 10000, Average training loss: 1.478187788,  Validation loss: 1.478642316, Validation accuracy: 0.982833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 91 / 10000, Average training loss: 1.477662168,  Validation loss: 1.479593359, Validation accuracy: 0.982000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 92 / 10000, Average training loss: 1.478917232,  Validation loss: 1.477579238, Validation accuracy: 0.984583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 93 / 10000, Average training loss: 1.477986281,  Validation loss: 1.477917233, Validation accuracy: 0.984083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 94 / 10000, Average training loss: 1.478071522,  Validation loss: 1.478484676, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 95 / 10000, Average training loss: 1.478921007,  Validation loss: 1.477614128, Validation accuracy: 0.984250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 96 / 10000, Average training loss: 1.478461999,  Validation loss: 1.479136455, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 97 / 10000, Average training loss: 1.477980642,  Validation loss: 1.478273974, Validation accuracy: 0.983500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 98 / 10000, Average training loss: 1.477656556,  Validation loss: 1.479635625, Validation accuracy: 0.981750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 99 / 10000, Average training loss: 1.478296545,  Validation loss: 1.477522306, Validation accuracy: 0.984333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 100 / 10000, Average training loss: 1.478175492,  Validation loss: 1.480036931, Validation accuracy: 0.981333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 101 / 10000, Average training loss: 1.478356365,  Validation loss: 1.479634486, Validation accuracy: 0.982083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 102 / 10000, Average training loss: 1.477886447,  Validation loss: 1.478140086, Validation accuracy: 0.983916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 103 / 10000, Average training loss: 1.478087797,  Validation loss: 1.479096370, Validation accuracy: 0.982500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 104 / 10000, Average training loss: 1.477932431,  Validation loss: 1.477997558, Validation accuracy: 0.984416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 105 / 10000, Average training loss: 1.478482462,  Validation loss: 1.479283571, Validation accuracy: 0.982416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 106 / 10000, Average training loss: 1.478748086,  Validation loss: 1.477534870, Validation accuracy: 0.984500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 107 / 10000, Average training loss: 1.477837162,  Validation loss: 1.478702887, Validation accuracy: 0.982833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 108 / 10000, Average training loss: 1.478065484,  Validation loss: 1.478044408, Validation accuracy: 0.983833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 109 / 10000, Average training loss: 1.478418663,  Validation loss: 1.478911922, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 110 / 10000, Average training loss: 1.478079016,  Validation loss: 1.479223401, Validation accuracy: 0.982250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 111 / 10000, Average training loss: 1.477638987,  Validation loss: 1.478405184, Validation accuracy: 0.983750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 112 / 10000, Average training loss: 1.477987425,  Validation loss: 1.478757937, Validation accuracy: 0.982583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 113 / 10000, Average training loss: 1.477943535,  Validation loss: 1.479042393, Validation accuracy: 0.982416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 114 / 10000, Average training loss: 1.477682178,  Validation loss: 1.477775918, Validation accuracy: 0.984166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 115 / 10000, Average training loss: 1.477778677,  Validation loss: 1.478497750, Validation accuracy: 0.983166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 116 / 10000, Average training loss: 1.477744128,  Validation loss: 1.479158196, Validation accuracy: 0.982083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 117 / 10000, Average training loss: 1.477807653,  Validation loss: 1.478518238, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 118 / 10000, Average training loss: 1.478271483,  Validation loss: 1.478993903, Validation accuracy: 0.982500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 119 / 10000, Average training loss: 1.478387554,  Validation loss: 1.478537981, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 120 / 10000, Average training loss: 1.478204545,  Validation loss: 1.478177635, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 121 / 10000, Average training loss: 1.478234409,  Validation loss: 1.479405423, Validation accuracy: 0.982750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 122 / 10000, Average training loss: 1.477915259,  Validation loss: 1.478371004, Validation accuracy: 0.983333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 123 / 10000, Average training loss: 1.478126070,  Validation loss: 1.478656468, Validation accuracy: 0.982416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 124 / 10000, Average training loss: 1.477648165,  Validation loss: 1.479492529, Validation accuracy: 0.981833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 125 / 10000, Average training loss: 1.477916238,  Validation loss: 1.478969254, Validation accuracy: 0.982500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 126 / 10000, Average training loss: 1.478042741,  Validation loss: 1.476938092, Validation accuracy: 0.985333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 127 / 10000, Average training loss: 1.477672371,  Validation loss: 1.477712111, Validation accuracy: 0.984500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 128 / 10000, Average training loss: 1.479014091,  Validation loss: 1.479283867, Validation accuracy: 0.982666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 129 / 10000, Average training loss: 1.477967463,  Validation loss: 1.479753308, Validation accuracy: 0.981583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 130 / 10000, Average training loss: 1.477435040,  Validation loss: 1.477842693, Validation accuracy: 0.984250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 131 / 10000, Average training loss: 1.478352657,  Validation loss: 1.480790501, Validation accuracy: 0.980333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 132 / 10000, Average training loss: 1.477889454,  Validation loss: 1.479229886, Validation accuracy: 0.982333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 133 / 10000, Average training loss: 1.477911753,  Validation loss: 1.479899280, Validation accuracy: 0.981916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 134 / 10000, Average training loss: 1.478179787,  Validation loss: 1.478723875, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 135 / 10000, Average training loss: 1.478061510,  Validation loss: 1.478302839, Validation accuracy: 0.983833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 136 / 10000, Average training loss: 1.477946187,  Validation loss: 1.479108185, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 137 / 10000, Average training loss: 1.477609954,  Validation loss: 1.478064883, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 138 / 10000, Average training loss: 1.478263264,  Validation loss: 1.478595345, Validation accuracy: 0.983166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 139 / 10000, Average training loss: 1.478032888,  Validation loss: 1.478912293, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 140 / 10000, Average training loss: 1.478240353,  Validation loss: 1.478531292, Validation accuracy: 0.983166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 141 / 10000, Average training loss: 1.478342167,  Validation loss: 1.478916184, Validation accuracy: 0.983250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 142 / 10000, Average training loss: 1.478528056,  Validation loss: 1.478054660, Validation accuracy: 0.983833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 143 / 10000, Average training loss: 1.478083527,  Validation loss: 1.478854635, Validation accuracy: 0.982833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 144 / 10000, Average training loss: 1.478251144,  Validation loss: 1.479737836, Validation accuracy: 0.982000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 145 / 10000, Average training loss: 1.477686652,  Validation loss: 1.477106491, Validation accuracy: 0.985333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 146 / 10000, Average training loss: 1.478113347,  Validation loss: 1.478719416, Validation accuracy: 0.983333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 147 / 10000, Average training loss: 1.478398651,  Validation loss: 1.478962710, Validation accuracy: 0.982666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 148 / 10000, Average training loss: 1.477871657,  Validation loss: 1.479124036, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 149 / 10000, Average training loss: 1.478077688,  Validation loss: 1.479107216, Validation accuracy: 0.982750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 150 / 10000, Average training loss: 1.478394183,  Validation loss: 1.477642370, Validation accuracy: 0.984000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 151 / 10000, Average training loss: 1.478573891,  Validation loss: 1.478403901, Validation accuracy: 0.983500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 152 / 10000, Average training loss: 1.478750961,  Validation loss: 1.479015793, Validation accuracy: 0.982333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 153 / 10000, Average training loss: 1.478486382,  Validation loss: 1.479960945, Validation accuracy: 0.981166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 154 / 10000, Average training loss: 1.477490959,  Validation loss: 1.478332095, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 155 / 10000, Average training loss: 1.478071337,  Validation loss: 1.478876573, Validation accuracy: 0.982666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 156 / 10000, Average training loss: 1.477950056,  Validation loss: 1.478791166, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 157 / 10000, Average training loss: 1.478283628,  Validation loss: 1.479096667, Validation accuracy: 0.982500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 158 / 10000, Average training loss: 1.478055052,  Validation loss: 1.479081295, Validation accuracy: 0.983166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 159 / 10000, Average training loss: 1.478140139,  Validation loss: 1.479399000, Validation accuracy: 0.982000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 160 / 10000, Average training loss: 1.477898212,  Validation loss: 1.477984406, Validation accuracy: 0.983833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 161 / 10000, Average training loss: 1.478068462,  Validation loss: 1.479935238, Validation accuracy: 0.981833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 162 / 10000, Average training loss: 1.478682903,  Validation loss: 1.478365801, Validation accuracy: 0.983333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 163 / 10000, Average training loss: 1.478288248,  Validation loss: 1.478563182, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 164 / 10000, Average training loss: 1.478217936,  Validation loss: 1.479550058, Validation accuracy: 0.982166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 165 / 10000, Average training loss: 1.477855734,  Validation loss: 1.479715741, Validation accuracy: 0.982166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 166 / 10000, Average training loss: 1.478438535,  Validation loss: 1.479778784, Validation accuracy: 0.982083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 167 / 10000, Average training loss: 1.478365014,  Validation loss: 1.478581922, Validation accuracy: 0.983333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 168 / 10000, Average training loss: 1.478582100,  Validation loss: 1.478375047, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 169 / 10000, Average training loss: 1.477942979,  Validation loss: 1.479245805, Validation accuracy: 0.982750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 170 / 10000, Average training loss: 1.477732531,  Validation loss: 1.479362920, Validation accuracy: 0.982000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 171 / 10000, Average training loss: 1.478835567,  Validation loss: 1.478381348, Validation accuracy: 0.983500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 172 / 10000, Average training loss: 1.477917109,  Validation loss: 1.477653322, Validation accuracy: 0.983916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 173 / 10000, Average training loss: 1.477880944,  Validation loss: 1.478099075, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 174 / 10000, Average training loss: 1.478273886,  Validation loss: 1.477688865, Validation accuracy: 0.983583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 175 / 10000, Average training loss: 1.478493143,  Validation loss: 1.477882240, Validation accuracy: 0.983333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 176 / 10000, Average training loss: 1.478638858,  Validation loss: 1.477978071, Validation accuracy: 0.984250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 177 / 10000, Average training loss: 1.478073541,  Validation loss: 1.478025577, Validation accuracy: 0.983916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 178 / 10000, Average training loss: 1.478614603,  Validation loss: 1.479055292, Validation accuracy: 0.982000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 179 / 10000, Average training loss: 1.478289701,  Validation loss: 1.478236800, Validation accuracy: 0.984000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 180 / 10000, Average training loss: 1.478171046,  Validation loss: 1.479331498, Validation accuracy: 0.982333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 181 / 10000, Average training loss: 1.478724976,  Validation loss: 1.479077293, Validation accuracy: 0.982416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 182 / 10000, Average training loss: 1.478512027,  Validation loss: 1.480420596, Validation accuracy: 0.980916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 183 / 10000, Average training loss: 1.477717696,  Validation loss: 1.478379808, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 184 / 10000, Average training loss: 1.477741859,  Validation loss: 1.479454691, Validation accuracy: 0.982166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 185 / 10000, Average training loss: 1.478599086,  Validation loss: 1.478693300, Validation accuracy: 0.982666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 186 / 10000, Average training loss: 1.478252590,  Validation loss: 1.479402884, Validation accuracy: 0.982500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 187 / 10000, Average training loss: 1.477937685,  Validation loss: 1.479156491, Validation accuracy: 0.982083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 188 / 10000, Average training loss: 1.478360628,  Validation loss: 1.479057078, Validation accuracy: 0.981916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 189 / 10000, Average training loss: 1.478563123,  Validation loss: 1.478086025, Validation accuracy: 0.983833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 190 / 10000, Average training loss: 1.478241512,  Validation loss: 1.477180254, Validation accuracy: 0.985333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 191 / 10000, Average training loss: 1.478663851,  Validation loss: 1.476892586, Validation accuracy: 0.984750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 192 / 10000, Average training loss: 1.478176799,  Validation loss: 1.478955438, Validation accuracy: 0.982583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 193 / 10000, Average training loss: 1.478623381,  Validation loss: 1.479745248, Validation accuracy: 0.981583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 194 / 10000, Average training loss: 1.478796603,  Validation loss: 1.478023360, Validation accuracy: 0.983833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 195 / 10000, Average training loss: 1.478278518,  Validation loss: 1.479779728, Validation accuracy: 0.981750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 196 / 10000, Average training loss: 1.477644928,  Validation loss: 1.478716229, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 197 / 10000, Average training loss: 1.477859357,  Validation loss: 1.479573404, Validation accuracy: 0.981916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 198 / 10000, Average training loss: 1.478600427,  Validation loss: 1.478081298, Validation accuracy: 0.983583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 199 / 10000, Average training loss: 1.478378093,  Validation loss: 1.476924424, Validation accuracy: 0.985000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 200 / 10000, Average training loss: 1.478099958,  Validation loss: 1.480681755, Validation accuracy: 0.980833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 201 / 10000, Average training loss: 1.477826816,  Validation loss: 1.477882140, Validation accuracy: 0.984000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 202 / 10000, Average training loss: 1.477901855,  Validation loss: 1.479006075, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 203 / 10000, Average training loss: 1.478328228,  Validation loss: 1.478661324, Validation accuracy: 0.983500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 204 / 10000, Average training loss: 1.478146700,  Validation loss: 1.480803011, Validation accuracy: 0.980833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 205 / 10000, Average training loss: 1.478128230,  Validation loss: 1.479821969, Validation accuracy: 0.981833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 206 / 10000, Average training loss: 1.477607057,  Validation loss: 1.478974406, Validation accuracy: 0.982416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 207 / 10000, Average training loss: 1.477567083,  Validation loss: 1.478930053, Validation accuracy: 0.983500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 208 / 10000, Average training loss: 1.478670274,  Validation loss: 1.478425886, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 209 / 10000, Average training loss: 1.477844214,  Validation loss: 1.478320312, Validation accuracy: 0.983583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 210 / 10000, Average training loss: 1.478453615,  Validation loss: 1.479549114, Validation accuracy: 0.982166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 211 / 10000, Average training loss: 1.477894775,  Validation loss: 1.479798800, Validation accuracy: 0.982583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 212 / 10000, Average training loss: 1.478250636,  Validation loss: 1.479582486, Validation accuracy: 0.982416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 213 / 10000, Average training loss: 1.478657826,  Validation loss: 1.478360919, Validation accuracy: 0.983583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 214 / 10000, Average training loss: 1.477510223,  Validation loss: 1.479079091, Validation accuracy: 0.982500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 215 / 10000, Average training loss: 1.477621213,  Validation loss: 1.477060917, Validation accuracy: 0.985083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 216 / 10000, Average training loss: 1.477720130,  Validation loss: 1.476727850, Validation accuracy: 0.985166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 217 / 10000, Average training loss: 1.478319353,  Validation loss: 1.478822557, Validation accuracy: 0.982666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 218 / 10000, Average training loss: 1.477490141,  Validation loss: 1.477281222, Validation accuracy: 0.984916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 219 / 10000, Average training loss: 1.478600171,  Validation loss: 1.479000379, Validation accuracy: 0.982833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 220 / 10000, Average training loss: 1.478482388,  Validation loss: 1.478953336, Validation accuracy: 0.982500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 221 / 10000, Average training loss: 1.477587137,  Validation loss: 1.479066634, Validation accuracy: 0.982250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 222 / 10000, Average training loss: 1.478219077,  Validation loss: 1.478841247, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 223 / 10000, Average training loss: 1.477920734,  Validation loss: 1.478512491, Validation accuracy: 0.983500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 224 / 10000, Average training loss: 1.478226087,  Validation loss: 1.479995059, Validation accuracy: 0.981500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 225 / 10000, Average training loss: 1.477791037,  Validation loss: 1.478874385, Validation accuracy: 0.982750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 226 / 10000, Average training loss: 1.478026933,  Validation loss: 1.478037441, Validation accuracy: 0.983750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 227 / 10000, Average training loss: 1.478083809,  Validation loss: 1.477982217, Validation accuracy: 0.983833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 228 / 10000, Average training loss: 1.478221686,  Validation loss: 1.479559025, Validation accuracy: 0.982083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 229 / 10000, Average training loss: 1.478163615,  Validation loss: 1.478447411, Validation accuracy: 0.983583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 230 / 10000, Average training loss: 1.477993318,  Validation loss: 1.477295377, Validation accuracy: 0.984166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 231 / 10000, Average training loss: 1.477717666,  Validation loss: 1.479255171, Validation accuracy: 0.982250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 232 / 10000, Average training loss: 1.478018204,  Validation loss: 1.479184117, Validation accuracy: 0.982583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 233 / 10000, Average training loss: 1.477883673,  Validation loss: 1.478955062, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 234 / 10000, Average training loss: 1.478366417,  Validation loss: 1.478915554, Validation accuracy: 0.982166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 235 / 10000, Average training loss: 1.477637145,  Validation loss: 1.478997232, Validation accuracy: 0.982750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 236 / 10000, Average training loss: 1.478160016,  Validation loss: 1.478195104, Validation accuracy: 0.983666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 237 / 10000, Average training loss: 1.478079841,  Validation loss: 1.478213876, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 238 / 10000, Average training loss: 1.478300392,  Validation loss: 1.479358685, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 239 / 10000, Average training loss: 1.477981562,  Validation loss: 1.479715470, Validation accuracy: 0.981916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 240 / 10000, Average training loss: 1.477068834,  Validation loss: 1.480038931, Validation accuracy: 0.981166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 241 / 10000, Average training loss: 1.478047986,  Validation loss: 1.478251887, Validation accuracy: 0.983166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 242 / 10000, Average training loss: 1.477570399,  Validation loss: 1.479764832, Validation accuracy: 0.981750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 243 / 10000, Average training loss: 1.478316175,  Validation loss: 1.477430354, Validation accuracy: 0.984666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 244 / 10000, Average training loss: 1.478205913,  Validation loss: 1.478546873, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 245 / 10000, Average training loss: 1.478402752,  Validation loss: 1.478801593, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 246 / 10000, Average training loss: 1.478268897,  Validation loss: 1.478711735, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 247 / 10000, Average training loss: 1.477266531,  Validation loss: 1.479614534, Validation accuracy: 0.982833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 248 / 10000, Average training loss: 1.478606850,  Validation loss: 1.478779030, Validation accuracy: 0.982666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 249 / 10000, Average training loss: 1.478219641,  Validation loss: 1.477892029, Validation accuracy: 0.984250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 250 / 10000, Average training loss: 1.477728886,  Validation loss: 1.477941297, Validation accuracy: 0.983750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 251 / 10000, Average training loss: 1.478740435,  Validation loss: 1.477913118, Validation accuracy: 0.983916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 252 / 10000, Average training loss: 1.478434249,  Validation loss: 1.479311325, Validation accuracy: 0.982416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 253 / 10000, Average training loss: 1.478832516,  Validation loss: 1.479120392, Validation accuracy: 0.981750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 254 / 10000, Average training loss: 1.478673310,  Validation loss: 1.478532191, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 255 / 10000, Average training loss: 1.478320111,  Validation loss: 1.478997377, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 256 / 10000, Average training loss: 1.478086469,  Validation loss: 1.477938708, Validation accuracy: 0.983833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 257 / 10000, Average training loss: 1.478075610,  Validation loss: 1.477163974, Validation accuracy: 0.984750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 258 / 10000, Average training loss: 1.477844625,  Validation loss: 1.478529273, Validation accuracy: 0.982750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 259 / 10000, Average training loss: 1.478063138,  Validation loss: 1.477441319, Validation accuracy: 0.984583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 260 / 10000, Average training loss: 1.477980482,  Validation loss: 1.479407628, Validation accuracy: 0.982500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 261 / 10000, Average training loss: 1.478423980,  Validation loss: 1.479495557, Validation accuracy: 0.982083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 262 / 10000, Average training loss: 1.477481103,  Validation loss: 1.478198387, Validation accuracy: 0.983666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 263 / 10000, Average training loss: 1.478054312,  Validation loss: 1.478676733, Validation accuracy: 0.983250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 264 / 10000, Average training loss: 1.478447395,  Validation loss: 1.479072470, Validation accuracy: 0.982750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 265 / 10000, Average training loss: 1.477818181,  Validation loss: 1.479098312, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 266 / 10000, Average training loss: 1.478246792,  Validation loss: 1.479912766, Validation accuracy: 0.982250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 267 / 10000, Average training loss: 1.478409924,  Validation loss: 1.477186774, Validation accuracy: 0.984250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 268 / 10000, Average training loss: 1.478444713,  Validation loss: 1.479124023, Validation accuracy: 0.983166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 269 / 10000, Average training loss: 1.478243810,  Validation loss: 1.478204676, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 270 / 10000, Average training loss: 1.477873312,  Validation loss: 1.478124276, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 271 / 10000, Average training loss: 1.478386715,  Validation loss: 1.481109414, Validation accuracy: 0.980583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 272 / 10000, Average training loss: 1.477495743,  Validation loss: 1.478621246, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 273 / 10000, Average training loss: 1.477363336,  Validation loss: 1.478893972, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 274 / 10000, Average training loss: 1.477638872,  Validation loss: 1.479932394, Validation accuracy: 0.981333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 275 / 10000, Average training loss: 1.478114545,  Validation loss: 1.477850215, Validation accuracy: 0.984000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 276 / 10000, Average training loss: 1.477729624,  Validation loss: 1.478791241, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 277 / 10000, Average training loss: 1.478174719,  Validation loss: 1.479138529, Validation accuracy: 0.982416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 278 / 10000, Average training loss: 1.478212689,  Validation loss: 1.478694506, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 279 / 10000, Average training loss: 1.478296462,  Validation loss: 1.478427456, Validation accuracy: 0.983250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 280 / 10000, Average training loss: 1.477692042,  Validation loss: 1.479376903, Validation accuracy: 0.982416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 281 / 10000, Average training loss: 1.477318026,  Validation loss: 1.478214988, Validation accuracy: 0.983666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 282 / 10000, Average training loss: 1.477665855,  Validation loss: 1.479384226, Validation accuracy: 0.982416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 283 / 10000, Average training loss: 1.478163363,  Validation loss: 1.479720383, Validation accuracy: 0.982000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 284 / 10000, Average training loss: 1.478022559,  Validation loss: 1.477624038, Validation accuracy: 0.984416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 285 / 10000, Average training loss: 1.478437044,  Validation loss: 1.479107230, Validation accuracy: 0.982916667, Learning Rate = 1.0000000000000004e-08\nEpoch: 286 / 10000, Average training loss: 1.477690847,  Validation loss: 1.478804671, Validation accuracy: 0.983166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 287 / 10000, Average training loss: 1.478143956,  Validation loss: 1.478767123, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 288 / 10000, Average training loss: 1.477540056,  Validation loss: 1.478340059, Validation accuracy: 0.983583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 289 / 10000, Average training loss: 1.478213684,  Validation loss: 1.478438838, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 290 / 10000, Average training loss: 1.478405197,  Validation loss: 1.479192549, Validation accuracy: 0.982666667, Learning Rate = 1.0000000000000004e-08\nEpoch: 291 / 10000, Average training loss: 1.478030966,  Validation loss: 1.479332692, Validation accuracy: 0.981750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 292 / 10000, Average training loss: 1.478132984,  Validation loss: 1.478433163, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 293 / 10000, Average training loss: 1.478190381,  Validation loss: 1.478244013, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 294 / 10000, Average training loss: 1.478123207,  Validation loss: 1.478885219, Validation accuracy: 0.982833333, Learning Rate = 1.0000000000000004e-08\nEpoch: 295 / 10000, Average training loss: 1.478478462,  Validation loss: 1.477590005, Validation accuracy: 0.984166667, Learning Rate = 1.0000000000000004e-08\nEpoch: 296 / 10000, Average training loss: 1.478427953,  Validation loss: 1.478761493, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 297 / 10000, Average training loss: 1.478215072,  Validation loss: 1.478360154, Validation accuracy: 0.983416667, Learning Rate = 1.0000000000000004e-08\nEpoch: 298 / 10000, Average training loss: 1.478155405,  Validation loss: 1.478843109, Validation accuracy: 0.982750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 299 / 10000, Average training loss: 1.478818813,  Validation loss: 1.478631260, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 300 / 10000, Average training loss: 1.478634989,  Validation loss: 1.480198911, Validation accuracy: 0.981750000, Learning Rate = 1.0000000000000004e-08\nEpoch: 301 / 10000, Average training loss: 1.478081931,  Validation loss: 1.478070218, Validation accuracy: 0.983583333, Learning Rate = 1.0000000000000004e-08\nEpoch: 302 / 10000, Average training loss: 1.477815454,  Validation loss: 1.479248855, Validation accuracy: 0.982000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 303 / 10000, Average training loss: 1.478462558,  Validation loss: 1.478307550, Validation accuracy: 0.983250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 304 / 10000, Average training loss: 1.477102849,  Validation loss: 1.477790376, Validation accuracy: 0.984083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 305 / 10000, Average training loss: 1.478099036,  Validation loss: 1.478131897, Validation accuracy: 0.983333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 306 / 10000, Average training loss: 1.478221912,  Validation loss: 1.478807648, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 307 / 10000, Average training loss: 1.478062746,  Validation loss: 1.479640464, Validation accuracy: 0.982083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 308 / 10000, Average training loss: 1.478038467,  Validation loss: 1.477755854, Validation accuracy: 0.984250000, Learning Rate = 1.0000000000000004e-08\nEpoch: 309 / 10000, Average training loss: 1.478221975,  Validation loss: 1.479668510, Validation accuracy: 0.982000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 310 / 10000, Average training loss: 1.478290200,  Validation loss: 1.478967078, Validation accuracy: 0.983000000, Learning Rate = 1.0000000000000004e-08\nEpoch: 311 / 10000, Average training loss: 1.477676318,  Validation loss: 1.478024742, Validation accuracy: 0.983500000, Learning Rate = 1.0000000000000004e-08\nEpoch: 312 / 10000, Average training loss: 1.478760458,  Validation loss: 1.479283237, Validation accuracy: 0.982333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 313 / 10000, Average training loss: 1.478568107,  Validation loss: 1.478987576, Validation accuracy: 0.982333333, Learning Rate = 1.0000000000000004e-08\nEpoch: 314 / 10000, Average training loss: 1.477834647,  Validation loss: 1.478659576, Validation accuracy: 0.983083333, Learning Rate = 1.0000000000000004e-08\nEpoch: 315 / 10000, Average training loss: 1.478212060,  Validation loss: 1.477930449, Validation accuracy: 0.983916667, Learning Rate = 1.0000000000000004e-08\nEarly stopping\nDone training\n</pre> In\u00a0[17]: Copied! <pre># H\u00e0m t\u00ednh accuracy, v\u00e0 \u0111\u1ed3ng th\u1eddi l\u01b0u predict v\u00e0 output khi model d\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp test\ndef Evaludate_model(): \n    model.eval() \n    correct = 0\n    total = 0\n\n    output_labels = np.array([])\n    predict_labels = np.array([])\n\n\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            predictions = model(inputs)\n            _, predicted = torch.max(predictions, 1)\n\n            total += labels.size(0)\n            \n            correct += (predicted == labels).sum().item()\n\n            output_labels = np.append(output_labels, labels.cpu().numpy())\n            predict_labels = np.append(predict_labels, predicted.cpu().numpy())\n\n\n    return correct / total, output_labels, predict_labels\n</pre> # H\u00e0m t\u00ednh accuracy, v\u00e0 \u0111\u1ed3ng th\u1eddi l\u01b0u predict v\u00e0 output khi model d\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp test def Evaludate_model():      model.eval()      correct = 0     total = 0      output_labels = np.array([])     predict_labels = np.array([])       with torch.no_grad():         for inputs, labels in testloader:             inputs, labels = inputs.to(device), labels.to(device)                          predictions = model(inputs)             _, predicted = torch.max(predictions, 1)              total += labels.size(0)                          correct += (predicted == labels).sum().item()              output_labels = np.append(output_labels, labels.cpu().numpy())             predict_labels = np.append(predict_labels, predicted.cpu().numpy())       return correct / total, output_labels, predict_labels In\u00a0[18]: Copied! <pre>accuracy, output_label, predict_label = Evaludate_model()        \n\n# in ra accuracy\nprint(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n\n# in ra confusion matrix\ncm = confusion_matrix(output_label, predict_label)\nprint('Confusion matrix:\\n', cm)\n\n# in ra classification report\nreport = classification_report(output_label, predict_label, target_names=[\"number \" + str(i) for i in range(10)])\nprint('Classification report:\\n', report)\n</pre> accuracy, output_label, predict_label = Evaludate_model()          # in ra accuracy print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")  # in ra confusion matrix cm = confusion_matrix(output_label, predict_label) print('Confusion matrix:\\n', cm)  # in ra classification report report = classification_report(output_label, predict_label, target_names=[\"number \" + str(i) for i in range(10)]) print('Classification report:\\n', report) <pre>Accuracy on test set: 98.53%\nConfusion matrix:\n [[ 970    0    0    0    1    1    6    1    1    0]\n [   0 1125    2    2    0    0    3    2    1    0]\n [   0    1 1023    0    0    0    1    6    1    0]\n [   0    1    2  996    0    5    0    2    3    1]\n [   0    0    0    0  975    0    0    0    0    7]\n [   2    0    0    3    0  878    4    2    1    2]\n [   6    1    1    1    1    1  944    0    3    0]\n [   0    5    6    1    0    0    0 1010    2    4]\n [   1    1    5    2    1    4    1    3  950    6]\n [   1    3    1    3   14    1    0    2    2  982]]\nClassification report:\n               precision    recall  f1-score   support\n\n    number 0       0.99      0.99      0.99       980\n    number 1       0.99      0.99      0.99      1135\n    number 2       0.98      0.99      0.99      1032\n    number 3       0.99      0.99      0.99      1010\n    number 4       0.98      0.99      0.99       982\n    number 5       0.99      0.98      0.99       892\n    number 6       0.98      0.99      0.98       958\n    number 7       0.98      0.98      0.98      1028\n    number 8       0.99      0.98      0.98       974\n    number 9       0.98      0.97      0.98      1009\n\n    accuracy                           0.99     10000\n   macro avg       0.99      0.99      0.99     10000\nweighted avg       0.99      0.99      0.99     10000\n\n</pre> In\u00a0[19]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\naxes[0].plot(trainloss_list, label='Training loss', color='blue')\naxes[0].plot(valloss_list, label='Validation loss', color='red')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Bi\u1ec3u \u0111\u1ed3 v\u1ec1 loss c\u1ee7a t\u1eadp train v\u00e0 t\u1eadp validation')\naxes[0].legend()\n\naxes[1].plot(valaccuracy_list, label='Validation accuracy', color='green')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')  \naxes[1].set_title('Bi\u1ec3u \u0111\u1ed3 v\u1ec1 accuracy c\u1ee7a t\u1eadp validation')\naxes[1].legend()\n\nplt.show()\n</pre> fig, axes = plt.subplots(1, 2, figsize=(16, 7))  axes[0].plot(trainloss_list, label='Training loss', color='blue') axes[0].plot(valloss_list, label='Validation loss', color='red') axes[0].set_xlabel('Epoch') axes[0].set_ylabel('Loss') axes[0].set_title('Bi\u1ec3u \u0111\u1ed3 v\u1ec1 loss c\u1ee7a t\u1eadp train v\u00e0 t\u1eadp validation') axes[0].legend()  axes[1].plot(valaccuracy_list, label='Validation accuracy', color='green') axes[1].set_xlabel('Epoch') axes[1].set_ylabel('Accuracy')   axes[1].set_title('Bi\u1ec3u \u0111\u1ed3 v\u1ec1 accuracy c\u1ee7a t\u1eadp validation') axes[1].legend()  plt.show()"},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#xay-dung-lenet-5-model-voi-pytorch-huan-luyen-tren-tap-du-lieu-mnist","title":"X\u00e2y d\u1ef1ng LeNet-5 model v\u1edbi Pytorch hu\u1ea5n luy\u1ec7n tr\u00ean t\u1eadp d\u1eef li\u1ec7u MNIST.\u00b6","text":"<ul> <li><p>MNIST l\u00e0 m\u1ed9t dataset bao g\u1ed3m 70000 \u1ea3nh s\u1ed1  vi\u1ebft tay t\u1eeb 0 \u0111\u1ebfn 9 c\u00f3 k\u00edch th\u01b0\u1edbc 28 x 28 pixels, thu\u1ed9c d\u1ea1ng \u1ea3nh grayscale (t\u1ee9c l\u00e0 \u1ea3nh \u0111en tr\u1eafng(1 channel)).</p> </li> <li><p>D\u01b0\u1edbi \u0111\u00e2y l\u00e0 \u1ea3nh m\u1ed9t s\u1ed1 m\u1eabu input c\u1ee7a dataset MNIST.</p> </li> </ul> <p></p> <ul> <li>Output bao g\u1ed3m c\u00e1c Label: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9</li> </ul>"},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#thuc-hien-buoc-chuan-bi-du-lieu","title":"Th\u1ef1c hi\u1ec7n b\u01b0\u1edbc chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u:\u00b6","text":"<ul> <li>Th\u1ef1c hi\u1ec7n b\u01b0\u1edbc Data Augmentation.</li> <li>Bao g\u1ed3m chia d\u1eef li\u1ec7u ra 3 t\u1eadp Train, Validaion v\u00e0 Test.</li> <li>Trong \u0111\u00f3, Train set c\u00f3 48000 samples, Validation set c\u00f3 12000 samples, v\u00e0 Test set c\u00f3 10000</li> </ul>"},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#mo-ta-du-lieu","title":"M\u00f4 t\u1ea3 d\u1eef li\u1ec7u\u00b6","text":"<ul> <li>Trainloader \u0111ang c\u00f3 d\u1ea1ng m\u1ed9t t\u1eadp c\u00e1c ph\u1ea7n t\u1eed (input, label)</li> <li>M\u1ed7i <code>inputs</code> n\u1eb1m trong <code>trainloader</code> ho\u1eb7c <code>testloader</code> \u0111\u1ec1u c\u00f3 shape = (32, 1, 28, 28), ki\u1ec3u torch, t\u1ee9c l\u00e0 batch=32, channel=1, height $\\times$ weight = 28 $\\times$ 28</li> </ul>"},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#visualizaion-mot-so-anh-trong-tap-du-lieu","title":"Visualizaion m\u1ed9t s\u1ed1 \u1ea3nh trong t\u1eadp d\u1eef li\u1ec7u:\u00b6","text":""},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#inh-nghia-mot-lenet-5-model-nhan-dien-chu-viet","title":"\u0110\u1ecbnh ngh\u0129a m\u1ed9t LeNet-5 model nh\u1eadn di\u1ec7n ch\u1eef vi\u1ebft\u00b6","text":""},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#define-early-stopping-function","title":"Define <code>Early Stopping</code> function\u00b6","text":""},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#define-train_model-function","title":"Define <code>Train_model()</code> function\u00b6","text":""},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#define-validate_model-function","title":"Define <code>Validate_model()</code> function\u00b6","text":""},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#khoi-tao-model-loss-function-optimizer-early-stopping-va-learning-rate-schedule","title":"Kh\u1edfi t\u1ea1o model, loss function, optimizer, Early stopping v\u00e0 Learning rate schedule\u00b6","text":""},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#train-model","title":"Train model\u00b6","text":""},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#evaluate-model","title":"Evaluate model\u00b6","text":"<ul> <li>T\u00ednh Accuracy, Confusion matrix, Classification report</li> </ul>"},{"location":"Learning/AI_Model/lenet/Lenet5_MNIST/#plot-charts","title":"Plot charts\u00b6","text":""},{"location":"Learning/AI_Model/lenet/lenetpart1/","title":"LeNet-5 Model Tutorial (PART 1)","text":""},{"location":"Learning/AI_Model/lenet/lenetpart1/#nguyen-truong-giang","title":"Nguy\u1ec5n Tr\u01b0\u1eddng Giang","text":""},{"location":"Learning/AI_Model/lenet/lenetpart1/#ngay-6-thang-2-nam-2025","title":"Ng\u00e0y 6 th\u00e1ng 2 n\u0103m 2025","text":""},{"location":"Learning/AI_Model/lenet/lenetpart1/#muc-luc","title":"M\u1ee5c l\u1ee5c","text":"<ul> <li>LeNet-5 Model Tutorial (PART 1)</li> <li>Nguy\u1ec5n Tr\u01b0\u1eddng Giang<ul> <li>Ng\u00e0y 6 th\u00e1ng 2 n\u0103m 2025</li> </ul> </li> <li>M\u1ee5c l\u1ee5c</li> <li>Gi\u1edbi thi\u1ec7u v\u1ec1 t\u00e0i li\u1ec7u n\u00e0y</li> <li>Gi\u1edbi thi\u1ec7u v\u1ec1 LeNet-5</li> <li>C\u1ea5u tr\u00fac t\u1ed5ng quan c\u1ee7a LeNet-5</li> <li>M\u1ed9t s\u1ed1 gi\u1edbi h\u1ea1n</li> <li>M\u00f4 t\u1ea3 chi ti\u1ebft v\u1ec1 c\u00e1c Layer<ul> <li>Convolutional Layer</li> <li>Pooling layer (Downsampling)</li> <li>H\u00e0m <code>nn.Flatten()</code></li> <li>H\u00e0m <code>nn.Linear()</code></li> </ul> </li> <li>M\u1ed9t s\u1ed1 h\u00e0m c\u1ea7n thi\u1ebft cho vi\u1ec7c build LeNet-5 model<ul> <li>Loss functions:</li> <li>C\u00e1c h\u00e0m optimizers:</li> </ul> </li> <li>M\u1ed9t s\u1ed1 Activation function phi tuy\u1ebfn ph\u1ed5 bi\u1ebfn<ul> <li>ReLU: <code>max(0, x)</code></li> <li>Leaky ReLU: <code>max(\u03b1x, x)</code></li> <li>Sigmoid: <code>1 / (1 + e^-x)</code></li> <li>Tanh: <code>(e^x - e^-x) / (e^x + e^-x)</code></li> <li>Softmax: <code>e^x_i / sum(e^x_j)</code></li> </ul> </li> <li>V\u00ed d\u1ee5 v\u1ec1 m\u1ed9t LeNet-5 ph\u00e2n lo\u1ea1i th\u1eddi trang</li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#gioi-thieu-ve-tai-lieu-nay","title":"Gi\u1edbi thi\u1ec7u v\u1ec1 t\u00e0i li\u1ec7u n\u00e0y","text":"<p>\u0110\u00e2y l\u00e0 t\u00e0i li\u1ec7u \u0111\u01b0\u1ee3c t\u1ed5ng h\u1ee3p b\u1edfi nhi\u1ec1u ngu\u1ed3n, t\u00f3m t\u1eaft v\u00e0 vi\u1ebft trong qu\u00e1 tr\u00ecnh h\u1ecdc c\u1ee7a m\u00ecnh. N\u1ebfu c\u00f3 sai s\u00f3t mong m\u1ecdi ng\u01b0\u1eddi th\u00f4ng c\u1ea3m.</p>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#gioi-thieu-ve-lenet-5","title":"Gi\u1edbi thi\u1ec7u v\u1ec1 LeNet-5","text":"<p>LeNet-5 l\u00e0 m\u1ed9t ki\u1ebfn tr\u00fac m\u1ea1ng n\u01a1-ron t\u00edch ch\u1eadp (CNN) \u0111\u01b0\u1ee3c ph\u00e1t tri\u1ec3n b\u1edfi Yann LeCun v\u00e0o n\u0103m 1998. \u0110\u00e2y l\u00e0 m\u1ed9t trong nh\u1eefng m\u00f4 h\u00ecnh CNN \u0111\u1ea7u ti\u00ean, \u0111\u1eb7t n\u1ec1n t\u1ea3ng cho s\u1ef1 ph\u00e1t tri\u1ec3n c\u1ee7a Deep Learning trong nh\u1eadn d\u1ea1ng h\u00ecnh \u1ea3nh.</p>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#cau-truc-tong-quan-cua-lenet-5","title":"C\u1ea5u tr\u00fac t\u1ed5ng quan c\u1ee7a LeNet-5","text":"<p>LeNet-5 \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf ch\u1ee7 y\u1ebfu \u0111\u1ec3 nh\u1eadn d\u1ea1ng ch\u1eef s\u1ed1 vi\u1ebft tay v\u00e0 h\u00ecnh \u1ea3nh nh\u1ecf. Ki\u1ebfn tr\u00fac g\u1ed3m 7 l\u1edbp (kh\u00f4ng t\u00ednh input), g\u1ed3m: - Convolutional Layer (T\u00edch ch\u1eadp) - Pooling Layer (L\u1ea5y m\u1eabu) - Fully Connected Layer (K\u1ebft n\u1ed1i \u0111\u1ea7y \u0111\u1ee7)</p>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#mot-so-gioi-han","title":"M\u1ed9t s\u1ed1 gi\u1edbi h\u1ea1n","text":"<ul> <li>Kh\u00f4ng hi\u1ec7u qu\u1ea3 v\u1edbi d\u1eef li\u1ec7u l\u1edbn, ph\u1ee9c t\u1ea1p (vd: \u1ea3nh RGB k\u00edch th\u01b0\u1edbc l\u1edbn).</li> <li>C\u00e1c m\u00f4 h\u00ecnh hi\u1ec7n \u0111\u1ea1i nh\u01b0 ResNet, VGG \u0111\u00e3 thay th\u1ebf LeNet trong nhi\u1ec1u \u1ee9ng d\u1ee5ng.</li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#mo-ta-chi-tiet-ve-cac-layer","title":"M\u00f4 t\u1ea3 chi ti\u1ebft v\u1ec1 c\u00e1c Layer","text":""},{"location":"Learning/AI_Model/lenet/lenetpart1/#convolutional-layer","title":"Convolutional Layer","text":"<ul> <li>Kh\u00e1i ni\u1ec7m: Th\u1ef1c hi\u1ec7n ph\u00e9p t\u00edch ch\u1eadp gi\u1eefa d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o v\u00e0 m\u1ed9t ho\u1eb7c nhi\u1ec1u kernel \u0111\u1ec3 tr\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng.</li> <li>Qu\u00e1 tr\u00ecnh ho\u1ea1t \u0111\u1ed9ng:</li> <li>Input: H\u00ecnh \u1ea3nh (grayscale 100\u00d7200\u00d71 ho\u1eb7c RGB 69\u00d796\u00d73).</li> <li>Kernel: Ma tr\u1eadn nh\u1ecf (vd: 3\u00d73, 5\u00d75), c\u00f3 tham s\u1ed1 h\u1ecdc \u0111\u01b0\u1ee3c.</li> <li>Ph\u00e9p to\u00e1n Convolution: Kernel tr\u01b0\u1ee3t qua \u1ea3nh v\u00e0 t\u00ednh t\u00edch v\u00f4 h\u01b0\u1edbng.</li> <li>Activation function: S\u1eed d\u1ee5ng ReLU ho\u1eb7c Sigmoid.</li> <li> <p>Feature Maps: M\u1ed7i kernel t\u1ea1o ra m\u1ed9t feature map.</p> </li> <li> <p>C\u00f4ng th\u1ee9c t\u00ednh Output size:   <pre><code>Output size = (Input size - Kernel size + 2 * Padding) / Stride + 1\n</code></pre></p> </li> <li> <p>C\u00fa ph\u00e1p trong PyTorch:   <pre><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n</code></pre></p> </li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#pooling-layer-downsampling","title":"Pooling layer (Downsampling)","text":"<ul> <li>Kh\u00e1i ni\u1ec7m: Gi\u1ea3m s\u1ed1 l\u01b0\u1ee3ng tham s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh.</li> <li>C\u00e1c lo\u1ea1i pooling:</li> <li>Max Pooling</li> <li> <p>Average Pooling</p> </li> <li> <p>C\u00fa ph\u00e1p trong PyTorch:   <pre><code>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0)\n</code></pre></p> </li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#ham-nnflatten","title":"H\u00e0m <code>nn.Flatten()</code>","text":"<ul> <li>Chuy\u1ec3n tensor nhi\u1ec1u chi\u1ec1u th\u00e0nh tensor m\u1ed9t chi\u1ec1u.</li> <li>C\u00fa ph\u00e1p:   <pre><code>torch.nn.Flatten(start_dim=1, end_dim=-1)\n</code></pre></li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#ham-nnlinear","title":"H\u00e0m <code>nn.Linear()</code>","text":"<ul> <li>T\u1ea1o m\u1ed9t l\u1edbp fully connected.</li> <li>C\u00fa ph\u00e1p:   <pre><code>torch.nn.Linear(in_features, out_features, bias=True)\n</code></pre></li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#mot-so-ham-can-thiet-cho-viec-build-lenet-5-model","title":"M\u1ed9t s\u1ed1 h\u00e0m c\u1ea7n thi\u1ebft cho vi\u1ec7c build LeNet-5 model","text":""},{"location":"Learning/AI_Model/lenet/lenetpart1/#loss-functions","title":"Loss functions:","text":"<ul> <li><code>nn.MSELoss()</code></li> <li><code>nn.L1Loss()</code></li> <li><code>nn.CrossEntropyLoss()</code></li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#cac-ham-optimizers","title":"C\u00e1c h\u00e0m optimizers:","text":"<ul> <li><code>optim.SGD()</code></li> <li><code>optim.Adam()</code></li> <li><code>optim.AdamW()</code></li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#mot-so-activation-function-phi-tuyen-pho-bien","title":"M\u1ed9t s\u1ed1 Activation function phi tuy\u1ebfn ph\u1ed5 bi\u1ebfn","text":""},{"location":"Learning/AI_Model/lenet/lenetpart1/#relu-max0-x","title":"ReLU: <code>max(0, x)</code>","text":"<pre><code>import torch\nimport torch.nn.functional as F\nx = torch.tensor([-1.0, 0.0, 1.0])\nrelu_output = F.relu(x)\nprint(relu_output)\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#leaky-relu-maxx-x","title":"Leaky ReLU: <code>max(\u03b1x, x)</code>","text":"<pre><code>leaky_relu = F.leaky_relu(x, negative_slope=0.01)\nprint(leaky_relu)\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#sigmoid-1-1-e-x","title":"Sigmoid: <code>1 / (1 + e^-x)</code>","text":"<pre><code>sigmoid_output = torch.sigmoid(x)\nprint(sigmoid_output)\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#tanh-ex-e-x-ex-e-x","title":"Tanh: <code>(e^x - e^-x) / (e^x + e^-x)</code>","text":"<pre><code>tanh_output = torch.tanh(x)\nprint(tanh_output)\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#softmax-ex_i-sumex_j","title":"Softmax: <code>e^x_i / sum(e^x_j)</code>","text":"<pre><code>softmax_output = F.softmax(x, dim=0)\nprint(softmax_output)\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart1/#vi-du-ve-mot-lenet-5-phan-loai-thoi-trang","title":"V\u00ed d\u1ee5 v\u1ec1 m\u1ed9t LeNet-5 ph\u00e2n lo\u1ea1i th\u1eddi trang","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n# \u0110\u1ecbnh ngh\u0129a m\u00f4 h\u00ecnh LeNet-5\nclass LeNet5(nn.Module):\n    def __init__(self):\n        super(LeNet5, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = torch.tanh(self.conv1(x))\n        x = self.pool1(x)\n        x = torch.tanh(self.conv2(x))\n        x = self.pool2(x)\n        x = x.view(x.size(0), -1)\n        x = torch.tanh(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n        x = self.fc3(x)\n        return x\n</code></pre> <p>T\u00e0i li\u1ec7u n\u00e0y cung c\u1ea5p ki\u1ebfn th\u1ee9c c\u01a1 b\u1ea3n v\u1ec1 LeNet-5 v\u00e0 c\u00e1ch tri\u1ec3n khai m\u00f4 h\u00ecnh b\u1eb1ng PyTorch. Hy v\u1ecdng n\u00f3 s\u1ebd h\u1eefu \u00edch cho b\u1ea1n!</p>"},{"location":"Learning/AI_Model/lenet/lenetpart2/","title":"CNN Model Tutorial - Part 2","text":""},{"location":"Learning/AI_Model/lenet/lenetpart2/#muc-luc","title":"M\u1ee5c l\u1ee5c","text":"<ul> <li>CNN Model Tutorial - Part 2</li> <li>M\u1ee5c l\u1ee5c</li> <li>C\u1ea5u tr\u00fac m\u1ed9t m\u00f4 h\u00ecnh t\u1ea1o b\u1edfi PyTorch b\u1eb1ng OOP<ul> <li>Gi\u1ea3i th\u00edch</li> </ul> </li> <li>Early stopping</li> <li>Learning rate scheduler</li> <li>H\u00e0m <code>train_model()</code></li> <li>\u0110\u1ed9 \u0111o \u0111\u00e1nh gi\u00e1 (c\u00e1c lo\u1ea1i \u0111\u00e1nh gi\u00e1)</li> <li>Confusion Matrix (ma tr\u1eadn nh\u1ea7m l\u1eabn)</li> <li>Data Augmentation (t\u0103ng c\u01b0\u1eddng d\u1eef li\u1ec7u)</li> <li>V\u00ed d\u1ee5 ho\u00e0n ch\u1ec9nh</li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart2/#cau-truc-mot-mo-hinh-tao-boi-pytorch-bang-oop","title":"C\u1ea5u tr\u00fac m\u1ed9t m\u00f4 h\u00ecnh t\u1ea1o b\u1edfi PyTorch b\u1eb1ng OOP","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=9, kernel_size=3, padding=2)\n        self.AvgPool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.Flatten = nn.Flatten()\n        self.fc1 = nn.Linear(9*15*15, 36)\n        self.fc2 = nn.Linear(36, 10)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = self.AvgPool1(x)\n        x = self.Flatten(x)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart2/#giai-thich","title":"Gi\u1ea3i th\u00edch","text":"<ul> <li><code>__init__</code>: Khai b\u00e1o c\u00e1c layers.</li> <li><code>forward</code>: X\u00e2y d\u1ef1ng lu\u1ed3ng t\u00ednh to\u00e1n khi truy\u1ec1n d\u1eef li\u1ec7u qua m\u00f4 h\u00ecnh.</li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart2/#early-stopping","title":"Early stopping","text":"<p>Early Stopping gi\u00fap tr\u00e1nh overfitting b\u1eb1ng c\u00e1ch d\u1eebng training n\u1ebfu hi\u1ec7u su\u1ea5t kh\u00f4ng c\u1ea3i thi\u1ec7n sau m\u1ed9t s\u1ed1 epoch nh\u1ea5t \u0111\u1ecbnh.</p> <pre><code>class EarlyStopping:\n    def __init__(self, patience=7, delta=0):\n        self.patience = patience\n        self.delta = delta\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.best_model_state = None\n\n    def __call__(self, val_loss, model):\n        if self.best_score is None or val_loss &lt; self.best_score - self.delta:\n            self.best_score = val_loss\n            self.counter = 0\n            self.best_model_state = model.state_dict()\n        else:\n            self.counter += 1\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n\n    def load_best_model(self, model):\n        model.load_state_dict(self.best_model_state)\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart2/#learning-rate-scheduler","title":"Learning rate scheduler","text":"<pre><code>import torch.optim as optim\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n</code></pre> <p>S\u1eed d\u1ee5ng trong v\u00f2ng l\u1eb7p training:</p> <pre><code>for epoch in range(100):\n    train_loss = train_model()\n    val_loss = validate_model()\n    scheduler.step(val_loss)\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart2/#ham-train_model","title":"H\u00e0m <code>train_model()</code>","text":"<pre><code>def train_model(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n    return running_loss / len(dataloader.dataset)\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart2/#o-o-anh-gia-cac-loai-anh-gia","title":"\u0110\u1ed9 \u0111o \u0111\u00e1nh gi\u00e1 (c\u00e1c lo\u1ea1i \u0111\u00e1nh gi\u00e1)","text":"<ul> <li> <p>Accuracy:   <pre><code>Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}\n</code></pre></p> </li> <li> <p>Precision:   <pre><code>Precision = \\frac{TP}{TP + FP}\n</code></pre></p> </li> <li> <p>Recall:   <pre><code>Recall = \\frac{TP}{TP + FN}\n</code></pre></p> </li> <li> <p>F1-score:   <pre><code>F1-score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}\n</code></pre></p> </li> </ul>"},{"location":"Learning/AI_Model/lenet/lenetpart2/#confusion-matrix-ma-tran-nham-lan","title":"Confusion Matrix (ma tr\u1eadn nh\u1ea7m l\u1eabn)","text":"<pre><code>from sklearn.metrics import confusion_matrix\nimport numpy as np\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0])\ncm = confusion_matrix(y_true, y_pred)\nprint(\"Confusion Matrix:\\n\", cm)\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart2/#data-augmentation-tang-cuong-du-lieu","title":"Data Augmentation (t\u0103ng c\u01b0\u1eddng d\u1eef li\u1ec7u)","text":"<pre><code>import torchvision.transforms as transforms\ntransform_augmented = transforms.Compose([\n    transforms.RandomRotation(15),\n    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n    transforms.ToTensor()\n])\n</code></pre>"},{"location":"Learning/AI_Model/lenet/lenetpart2/#vi-du-hoan-chinh","title":"V\u00ed d\u1ee5 ho\u00e0n ch\u1ec9nh","text":"<p>M\u1ed9t m\u00f4 h\u00ecnh ho\u00e0n ch\u1ec9nh s\u1eed d\u1ee5ng <code>EarlyStopping</code>, <code>train_model</code>, <code>validate_model</code>, v\u00e0 <code>ReduceLROnPlateau</code> c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c t\u00ecm th\u1ea5y trong t\u00e0i li\u1ec7u g\u1ed1c.</p> <p>T\u00e0i li\u1ec7u h\u01b0\u1edbng d\u1eabn b\u1edfi Nguy\u1ec5n Tr\u01b0\u1eddng Giang - Ng\u00e0y 6 th\u00e1ng 2 n\u0103m 2025.</p>"},{"location":"Learning/AI_Model/unet/UNet/","title":"UNet Architecture","text":"In\u00a0[1]: Copied! <pre>import opendatasets as od\n\ndata_path = 'https://www.kaggle.com/datasets/abdallahwagih/kvasir-dataset-for-classification-and-segmentation'\nod.download(data_path)\n</pre> import opendatasets as od  data_path = 'https://www.kaggle.com/datasets/abdallahwagih/kvasir-dataset-for-classification-and-segmentation' od.download(data_path) <pre>Skipping, found downloaded files in \"./kvasir-dataset-for-classification-and-segmentation\" (use force=True to force download)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Read image and mask paths\nimage_paths = []\nmask_paths = []\nimport os\n\nimages_root = './kvasir-dataset-for-classification-and-segmentation/kvasir-seg/Kvasir-SEG/images'\nmasks_root = './kvasir-dataset-for-classification-and-segmentation/kvasir-seg/Kvasir-SEG/masks'\n\nfor image, mask in zip(os.listdir(images_root), os.listdir(masks_root)):\n    image_paths.append(os.path.join(images_root, image))\n    mask_paths.append(os.path.join(masks_root, mask))\n</pre> # Read image and mask paths image_paths = [] mask_paths = [] import os  images_root = './kvasir-dataset-for-classification-and-segmentation/kvasir-seg/Kvasir-SEG/images' masks_root = './kvasir-dataset-for-classification-and-segmentation/kvasir-seg/Kvasir-SEG/masks'  for image, mask in zip(os.listdir(images_root), os.listdir(masks_root)):     image_paths.append(os.path.join(images_root, image))     mask_paths.append(os.path.join(masks_root, mask)) In\u00a0[3]: Copied! <pre>from PIL import Image\n\nprint('Image: ', Image.open(image_paths[0]).convert('RGB').getbands())\nprint('Mask: ', Image.open(mask_paths[0]).convert('L').getbands())\n</pre> from PIL import Image  print('Image: ', Image.open(image_paths[0]).convert('RGB').getbands()) print('Mask: ', Image.open(mask_paths[0]).convert('L').getbands()) <pre>Image:  ('R', 'G', 'B')\nMask:  ('L',)\n</pre> In\u00a0[4]: Copied! <pre>import numpy as np\n\nprint('Images: ', np.array(Image.open(image_paths[0]).convert('RGB')).shape)\nprint('Masks: ', np.array(Image.open(mask_paths[0]).convert('L')).shape)\n</pre> import numpy as np  print('Images: ', np.array(Image.open(image_paths[0]).convert('RGB')).shape) print('Masks: ', np.array(Image.open(mask_paths[0]).convert('L')).shape) <pre>Images:  (530, 571, 3)\nMasks:  (530, 571)\n</pre> In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\ndef Visualize_Data():\n    fig, axes = plt.subplots(10, 14, figsize=(24, 20))\n    \n    for i in range(0, 10, 2):\n        for j in range(0, 14):\n            num = i // 2 * 10 + j\n            image = Image.open(image_paths[num])\n            mask = Image.open(mask_paths[num])\n            \n            axes[i, j].imshow(image)\n            axes[i, j].axis('off')\n            \n            axes[i + 1, j].imshow(mask)\n            axes[i + 1, j].axis('off')\n    plt.show()\n\nVisualize_Data()\n            \n</pre> import matplotlib.pyplot as plt def Visualize_Data():     fig, axes = plt.subplots(10, 14, figsize=(24, 20))          for i in range(0, 10, 2):         for j in range(0, 14):             num = i // 2 * 10 + j             image = Image.open(image_paths[num])             mask = Image.open(mask_paths[num])                          axes[i, j].imshow(image)             axes[i, j].axis('off')                          axes[i + 1, j].imshow(mask)             axes[i + 1, j].axis('off')     plt.show()  Visualize_Data()              In\u00a0[6]: Copied! <pre>from sklearn.model_selection import train_test_split\n# seed = 0\nval_size = 0.2\ntest_size = 0.25\nis_shuffle = True\n\nX_train, X_val, y_train, y_val = train_test_split(\n    image_paths,\n    mask_paths,\n    test_size=val_size,\n    random_state=42,\n    shuffle=is_shuffle\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_train,\n    y_train,\n    test_size=test_size,\n    random_state=42,\n    shuffle=is_shuffle\n)\n\nprint('Train: ', len(X_train))\nprint('Val: ', len(X_val))\nprint('Test: ', len(X_test))\n</pre> from sklearn.model_selection import train_test_split # seed = 0 val_size = 0.2 test_size = 0.25 is_shuffle = True  X_train, X_val, y_train, y_val = train_test_split(     image_paths,     mask_paths,     test_size=val_size,     random_state=42,     shuffle=is_shuffle )  X_train, X_test, y_train, y_test = train_test_split(     X_train,     y_train,     test_size=test_size,     random_state=42,     shuffle=is_shuffle )  print('Train: ', len(X_train)) print('Val: ', len(X_val)) print('Test: ', len(X_test)) <pre>Train:  600\nVal:  200\nTest:  200\n</pre> In\u00a0[7]: Copied! <pre>from torch.utils.data import Dataset, DataLoader\n\nclass KvasirDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, transform=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    # Load image and mask\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        mask_path = self.mask_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        mask = Image.open(mask_path).convert('L')\n        \n        if self.transform:\n            image, mask = self.transform(image, mask)\n            \n        return image, mask\n</pre> from torch.utils.data import Dataset, DataLoader  class KvasirDataset(Dataset):     def __init__(self, image_paths, mask_paths, transform=None):         self.image_paths = image_paths         self.mask_paths = mask_paths         self.transform = transform              def __len__(self):         return len(self.image_paths)          # Load image and mask     def __getitem__(self, idx):         image_path = self.image_paths[idx]         mask_path = self.mask_paths[idx]         image = Image.open(image_path).convert('RGB')         mask = Image.open(mask_path).convert('L')                  if self.transform:             image, mask = self.transform(image, mask)                      return image, mask In\u00a0[8]: Copied! <pre># Create non transform dataset\nfull_data = KvasirDataset(image_paths, mask_paths)\nimg_data = []\nfor img, _ in full_data:\n    img_array = np.array(img.resize((224, 224))) / 255.0\n    img_data.append(img_array)\n    \nimg_data = np.array(img_data)\n\n# Calculate mean and std\nmean = np.mean(img_data, axis=(0, 1, 2))\nstd = np.std(img_data, axis=(0, 1, 2))\n\nprint('Mean: ', mean)\nprint('Std: ', std)\n</pre> # Create non transform dataset full_data = KvasirDataset(image_paths, mask_paths) img_data = [] for img, _ in full_data:     img_array = np.array(img.resize((224, 224))) / 255.0     img_data.append(img_array)      img_data = np.array(img_data)  # Calculate mean and std mean = np.mean(img_data, axis=(0, 1, 2)) std = np.std(img_data, axis=(0, 1, 2))  print('Mean: ', mean) print('Std: ', std) <pre>Mean:  [0.55714883 0.32170294 0.23581956]\nStd:  [0.31774015 0.22082197 0.18651856]\n</pre> In\u00a0[9]: Copied! <pre>import torchvision.transforms as transforms\nimport torch\n\n# Define a class transform for image and mask\nclass CustomTransform:\n    def __init__(self, image_transform, mask_transform):\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n\n    def __call__(self, image, mask):\n        seed = torch.random.initial_seed()\n        torch.manual_seed(seed)\n        image = self.image_transform(image)\n\n        torch.manual_seed(seed)\n        mask = self.mask_transform(mask)\n\n        return image, mask\n\n# Train Image transforms\ntrain_image_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    # TODO: Chu\u1ea9n h\u00f3a \u1ea3nh v\u1edbi gi\u00e1 tr\u1ecb mean v\u00e0 std, c\u00e1c gi\u00e1 tr\u1ecb mean v\u00e0 std d\u00f9ng l\u1ea1i mean v\u00e0 std \u0111\u00e3 t\u00ednh \u1edf tr\u00ean\n    transforms.Normalize(mean=[0.55714883, 0.32170294, 0.23581956], std=[0.31774015, 0.22082197, 0.18651856])\n])\n\n# Train Mask Transforms\ntrain_mask_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    # TODO: Chuy\u1ec3n mask th\u00e0nh nh\u1ecb ph\u00e2n, g\u1ed3m c\u00e1c gi\u00e1 tr\u1ecb 0 v\u00e0 1\n    transforms.Lambda(lambda x: (x &gt; 0.5).float())\n])\n\n# Val Image transforms\nval_image_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    # TODO: Chu\u1ea9n h\u00f3a \u1ea3nh v\u1edbi gi\u00e1 tr\u1ecb mean v\u00e0 std, c\u00e1c gi\u00e1 tr\u1ecb mean v\u00e0 std d\u00f9ng l\u1ea1i mean v\u00e0 std \u0111\u00e3 t\u00ednh \u1edf tr\u00ean\n    transforms.Normalize(mean=[0.55714883, 0.32170294, 0.23581956], std=[0.31774015, 0.22082197, 0.18651856])\n])\n\n# Val Mask transforms\nval_mask_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    # TODO: Chuy\u1ec3n mask th\u00e0nh nh\u1ecb ph\u00e2n, g\u1ed3m c\u00e1c gi\u00e1 tr\u1ecb 0 v\u00e0 1\n    transforms.Lambda(lambda x: (x &gt; 0.5).float())\n])\n\n# Transforms dictionary\ndata_transforms = {\n    'train': CustomTransform(train_image_transforms, train_mask_transforms),\n    'val': CustomTransform(val_image_transforms, val_mask_transforms),\n}\n</pre> import torchvision.transforms as transforms import torch  # Define a class transform for image and mask class CustomTransform:     def __init__(self, image_transform, mask_transform):         self.image_transform = image_transform         self.mask_transform = mask_transform      def __call__(self, image, mask):         seed = torch.random.initial_seed()         torch.manual_seed(seed)         image = self.image_transform(image)          torch.manual_seed(seed)         mask = self.mask_transform(mask)          return image, mask  # Train Image transforms train_image_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     # TODO: Chu\u1ea9n h\u00f3a \u1ea3nh v\u1edbi gi\u00e1 tr\u1ecb mean v\u00e0 std, c\u00e1c gi\u00e1 tr\u1ecb mean v\u00e0 std d\u00f9ng l\u1ea1i mean v\u00e0 std \u0111\u00e3 t\u00ednh \u1edf tr\u00ean     transforms.Normalize(mean=[0.55714883, 0.32170294, 0.23581956], std=[0.31774015, 0.22082197, 0.18651856]) ])  # Train Mask Transforms train_mask_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     # TODO: Chuy\u1ec3n mask th\u00e0nh nh\u1ecb ph\u00e2n, g\u1ed3m c\u00e1c gi\u00e1 tr\u1ecb 0 v\u00e0 1     transforms.Lambda(lambda x: (x &gt; 0.5).float()) ])  # Val Image transforms val_image_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     # TODO: Chu\u1ea9n h\u00f3a \u1ea3nh v\u1edbi gi\u00e1 tr\u1ecb mean v\u00e0 std, c\u00e1c gi\u00e1 tr\u1ecb mean v\u00e0 std d\u00f9ng l\u1ea1i mean v\u00e0 std \u0111\u00e3 t\u00ednh \u1edf tr\u00ean     transforms.Normalize(mean=[0.55714883, 0.32170294, 0.23581956], std=[0.31774015, 0.22082197, 0.18651856]) ])  # Val Mask transforms val_mask_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     # TODO: Chuy\u1ec3n mask th\u00e0nh nh\u1ecb ph\u00e2n, g\u1ed3m c\u00e1c gi\u00e1 tr\u1ecb 0 v\u00e0 1     transforms.Lambda(lambda x: (x &gt; 0.5).float()) ])  # Transforms dictionary data_transforms = {     'train': CustomTransform(train_image_transforms, train_mask_transforms),     'val': CustomTransform(val_image_transforms, val_mask_transforms), }  In\u00a0[10]: Copied! <pre># Create datasets\ntrain_dataset = KvasirDataset(\n    X_train,\n    y_train,\n    transform=data_transforms['train']\n)\n\nval_dataset = KvasirDataset(\n    X_val,\n    y_val,\n    transform=data_transforms['val']\n)\n\ntest_dataset = KvasirDataset(\n    X_test,\n    y_test,\n    transform=data_transforms['val']\n)\n</pre> # Create datasets train_dataset = KvasirDataset(     X_train,     y_train,     transform=data_transforms['train'] )  val_dataset = KvasirDataset(     X_val,     y_val,     transform=data_transforms['val'] )  test_dataset = KvasirDataset(     X_test,     y_test,     transform=data_transforms['val'] ) In\u00a0[11]: Copied! <pre># Create dataloaders\ntrain_batch_size = 8\ntest_batch_size = 4\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=train_batch_size,\n    shuffle=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=test_batch_size,\n    shuffle=False\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=test_batch_size,\n    shuffle=False\n)\n</pre> # Create dataloaders train_batch_size = 8 test_batch_size = 4  train_loader = DataLoader(     train_dataset,     batch_size=train_batch_size,     shuffle=True )  val_loader = DataLoader(     val_dataset,     batch_size=test_batch_size,     shuffle=False )  test_loader = DataLoader(     test_dataset,     batch_size=test_batch_size,     shuffle=False ) In\u00a0[12]: Copied! <pre>import torchvision\n\ntrain_features, train_labels = next(iter(train_loader))\n\n# Use torchvision to display a grid of images\ndef show_batch(images, masks):\n    grid_images = torchvision.utils.make_grid(images, nrow=4, normalize=True)\n    grid_masks = torchvision.utils.make_grid(masks, nrow=4, normalize=True)\n\n    plt.figure(figsize=(20, 20))\n    plt.imshow(np.transpose(grid_images, (1, 2, 0)), )\n    plt.show()\n    \n    plt.figure(figsize=(20, 20))\n    plt.imshow(np.transpose(grid_masks, (1, 2, 0)), )\n    plt.show()\n</pre> import torchvision  train_features, train_labels = next(iter(train_loader))  # Use torchvision to display a grid of images def show_batch(images, masks):     grid_images = torchvision.utils.make_grid(images, nrow=4, normalize=True)     grid_masks = torchvision.utils.make_grid(masks, nrow=4, normalize=True)      plt.figure(figsize=(20, 20))     plt.imshow(np.transpose(grid_images, (1, 2, 0)), )     plt.show()          plt.figure(figsize=(20, 20))     plt.imshow(np.transpose(grid_masks, (1, 2, 0)), )     plt.show() In\u00a0[13]: Copied! <pre># Check the shape of the images and masks\ntrain_features[0].shape\n</pre> # Check the shape of the images and masks train_features[0].shape Out[13]: <pre>torch.Size([3, 224, 224])</pre> In\u00a0[14]: Copied! <pre># Display a batch of images and masks\nshow_batch(train_features, train_labels)\n</pre> # Display a batch of images and masks show_batch(train_features, train_labels) In\u00a0[15]: Copied! <pre># Check device availability\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n</pre> # Check device availability import torch device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print(device) <pre>cuda\n</pre> In\u00a0[16]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import models\nimport torch.optim as optim\nfrom time import time\n\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = (15,4)\n</pre> import torch import torch.nn as nn import torch.nn.functional as F import torchvision from torchvision import models import torch.optim as optim from time import time  from matplotlib import rcParams rcParams['figure.figsize'] = (15,4) In\u00a0[\u00a0]: Copied! <pre># Define the convolutional block\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\n# Define the ConvRelu\nclass ConvRelu(nn.Module):\n    def __init__(self, in_: int, out: int):\n        super().__init__()\n        self.conv = conv3x3(in_, out) # TODO: T\u1ea1o m\u1ed9t l\u1edbp convolution s\u1eed d\u1ee5ng h\u00e0m conv3x3 \u1edf tr\u00ean\n        self.relu = nn.ReLU(inplace=True) # TODO: T\u1ea1o m\u1ed9t l\u1edbp ReLU activation v\u1edbi inplace=True\n    def forward(self, x):\n        x = self.conv(x) # TODO: \u00c1p d\u1ee5ng l\u1edbp convolution l\u00ean input x\n        x = self.relu(x) # TODO: \u00c1p d\u1ee5ng activation function l\u00ean output t\u1eeb convolution\n        return x\n</pre> # Define the convolutional block def conv3x3(in_, out):     return nn.Conv2d(in_, out, 3, padding=1)  # Define the ConvRelu class ConvRelu(nn.Module):     def __init__(self, in_: int, out: int):         super().__init__()         self.conv = conv3x3(in_, out) # TODO: T\u1ea1o m\u1ed9t l\u1edbp convolution s\u1eed d\u1ee5ng h\u00e0m conv3x3 \u1edf tr\u00ean         self.relu = nn.ReLU(inplace=True) # TODO: T\u1ea1o m\u1ed9t l\u1edbp ReLU activation v\u1edbi inplace=True     def forward(self, x):         x = self.conv(x) # TODO: \u00c1p d\u1ee5ng l\u1edbp convolution l\u00ean input x         x = self.relu(x) # TODO: \u00c1p d\u1ee5ng activation function l\u00ean output t\u1eeb convolution         return x In\u00a0[\u00a0]: Copied! <pre>class DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n        super(DecoderBlock, self).__init__()\n        # TODO: L\u01b0u l\u1ea1i gi\u00e1 tr\u1ecb in_channels\n        self.in_channels = in_channels\n        \n        # TODO: N\u1ebfu is_deconv l\u00e0 True, s\u1eed d\u1ee5ng ConvTranspose2d \u0111\u1ec3 t\u0103ng k\u00edch th\u01b0\u1edbc\n        if is_deconv:\n            self.block = nn.Sequential(\n                # TODO: Th\u00eam m\u1ed9t l\u1edbp ConvRelu v\u1edbi in_channels v\u00e0 middle_channels\n                nn.conv2d(in_channels=in_channels, out_channels=middle_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(middle_channels),\n                nn.ReLU(inplace=True)\n                \n                # TODO: Th\u00eam m\u1ed9t l\u1edbp ConvTranspose2d v\u1edbi middle_channels v\u00e0 out_channels\n                nn.ConvTranspose2d(in_channels=middle_channels, out_channels=out_channels, kernel_size=2, stride=2)\n                \n                # TODO: Th\u00eam m\u1ed9t l\u1edbp ReLU activation v\u1edbi inplace=True\n            )\n        # TODO: N\u1ebfu is_deconv l\u00e0 False, s\u1eed d\u1ee5ng Upsample \u0111\u1ec3 t\u0103ng k\u00edch th\u01b0\u1edbc\n        else:\n            self.block = nn.Sequential(\n                # TODO: Th\u00eam m\u1ed9t l\u1edbp Upsample v\u1edbi scale_factor=2 v\u00e0 mode='bilinear'\n                # TODO: Th\u00eam m\u1ed9t l\u1edbp ConvRelu v\u1edbi in_channels v\u00e0 middle_channels\n                # TODO: Th\u00eam m\u1ed9t l\u1edbp ConvRelu v\u1edbi middle_channels v\u00e0 out_channels\n            )\n\n    def forward(self, x):\n        return self.block(x)\n</pre> class DecoderBlock(nn.Module):     def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):         super(DecoderBlock, self).__init__()         # TODO: L\u01b0u l\u1ea1i gi\u00e1 tr\u1ecb in_channels         self.in_channels = in_channels                  # TODO: N\u1ebfu is_deconv l\u00e0 True, s\u1eed d\u1ee5ng ConvTranspose2d \u0111\u1ec3 t\u0103ng k\u00edch th\u01b0\u1edbc         if is_deconv:             self.block = nn.Sequential(                 # TODO: Th\u00eam m\u1ed9t l\u1edbp ConvRelu v\u1edbi in_channels v\u00e0 middle_channels                 nn.conv2d(in_channels=in_channels, out_channels=middle_channels, kernel_size=3, padding=1),                 nn.BatchNorm2d(middle_channels),                 nn.ReLU(inplace=True)                                  # TODO: Th\u00eam m\u1ed9t l\u1edbp ConvTranspose2d v\u1edbi middle_channels v\u00e0 out_channels                 nn.ConvTranspose2d(in_channels=middle_channels, out_channels=out_channels, kernel_size=2, stride=2)                                  # TODO: Th\u00eam m\u1ed9t l\u1edbp ReLU activation v\u1edbi inplace=True             )         # TODO: N\u1ebfu is_deconv l\u00e0 False, s\u1eed d\u1ee5ng Upsample \u0111\u1ec3 t\u0103ng k\u00edch th\u01b0\u1edbc         else:             self.block = nn.Sequential(                 # TODO: Th\u00eam m\u1ed9t l\u1edbp Upsample v\u1edbi scale_factor=2 v\u00e0 mode='bilinear'                 # TODO: Th\u00eam m\u1ed9t l\u1edbp ConvRelu v\u1edbi in_channels v\u00e0 middle_channels                 # TODO: Th\u00eam m\u1ed9t l\u1edbp ConvRelu v\u1edbi middle_channels v\u00e0 out_channels             )      def forward(self, x):         return self.block(x) In\u00a0[\u00a0]: Copied! <pre># TODO: \u0110\u1ecbnh ngh\u0129a l\u1edbp UNet11 k\u1ebf th\u1eeba t\u1eeb nn.Module\nclass UNet11(nn.Module):\n    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n        \"\"\"\n        :param num_classes: s\u1ed1 l\u01b0\u1ee3ng l\u1edbp \u0111\u1ea7u ra\n        :param num_filters: s\u1ed1 l\u01b0\u1ee3ng b\u1ed9 l\u1ecdc s\u1eed d\u1ee5ng trong m\u1ea1ng\n        :param pretrained: s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh VGG11 c\u00f3 tr\u1ecdng s\u1ed1 pre-trained hay kh\u00f4ng\n        \"\"\"\n        super().__init__()\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # TODO: L\u01b0u l\u1ea1i s\u1ed1 l\u01b0\u1ee3ng l\u1edbp \u0111\u1ea7u ra\n\n        # TODO: Ki\u1ec3m tra n\u1ebfu pretrained=True, s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh VGG11 v\u1edbi tr\u1ecdng s\u1ed1 c\u00f3 s\u1eb5n\n        if pretrained:\n            self.encoder = models.vgg11(weights=models.vgg.VGG11_Weights.DEFAULT).features\n        else:\n            self.encoder = models.vgg11().features\n\n        # TODO: Kh\u1edfi t\u1ea1o h\u00e0m k\u00edch ho\u1ea1t ReLU\n        # TODO: X\u00e2y d\u1ef1ng c\u00e1c kh\u1ed1i convolutional t\u1eeb c\u00e1c l\u1edbp trong VGG11\n        self.conv1 = nn.Sequential(\n            self.encoder[0],\n            # TODO: Th\u00eam h\u00e0m k\u00edch ho\u1ea1t ReLU\n        )\n\n        self.conv2 = nn.Sequential(\n            self.encoder[3],\n            # TODO: Th\u00eam h\u00e0m k\u00edch ho\u1ea1t ReLU\n        )\n\n        # TODO: Ti\u1ebfp t\u1ee5c x\u00e2y d\u1ef1ng c\u00e1c kh\u1ed1i convolutional cho conv3, conv4, conv5\n        self.conv3 = nn.Sequential(\n            self.encoder[6],\n            # TODO: Th\u00eam h\u00e0m k\u00edch ho\u1ea1t ReLU\n            self.encoder[8],\n            # TODO: Th\u00eam h\u00e0m k\u00edch ho\u1ea1t ReLU\n        )\n\n        # TODO: X\u00e2y d\u1ef1ng kh\u1ed1i center v\u00e0 decoder t\u01b0\u01a1ng \u1ee9ng\n        self.center = DecoderBlock(256 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv=is_deconv)\n        \n        # TODO: X\u00e2y d\u1ef1ng c\u00e1c l\u1edbp decoder dec5, dec4, dec3, dec2, dec1 theo ki\u1ebfn tr\u00fac U-Net\n\n        # TODO: X\u00e2y d\u1ef1ng l\u1edbp final convolution\n        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        # TODO: Th\u1ef1c hi\u1ec7n forward pass theo th\u1ee9 t\u1ef1 c\u00e1c t\u1ea7ng conv1 -&gt; conv5, pool gi\u1eefa c\u00e1c t\u1ea7ng\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(self.pool(conv1))\n        conv3 = self.conv3(self.pool(conv2))\n        # TODO: Ti\u1ebfp t\u1ee5c cho \u0111\u1ebfn conv5\n\n        # TODO: Truy\u1ec1n qua t\u1ea7ng center\n        center = self.center(self.pool(conv5))\n\n        # TODO: Gh\u00e9p n\u1ed1i (concat) c\u00e1c \u0111\u1eb7c tr\u01b0ng t\u1eeb encoder v\u1edbi decoder theo t\u1eebng t\u1ea7ng\n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n        # TODO: Ti\u1ebfp t\u1ee5c v\u1edbi dec4, dec3, dec2, dec1\n\n        # TODO: Ki\u1ec3m tra n\u1ebfu num_classes &gt; 1, s\u1eed d\u1ee5ng log_softmax, ng\u01b0\u1ee3c l\u1ea1i s\u1eed d\u1ee5ng conv \u0111\u1ea7u ra\n        if self.num_classes &gt; 1:\n            x_out = F.log_softmax(self.final(dec1), dim=1)\n        else:\n            x_out = self.final(dec1)\n\n        return x_out\n</pre>  # TODO: \u0110\u1ecbnh ngh\u0129a l\u1edbp UNet11 k\u1ebf th\u1eeba t\u1eeb nn.Module class UNet11(nn.Module):     def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):         \"\"\"         :param num_classes: s\u1ed1 l\u01b0\u1ee3ng l\u1edbp \u0111\u1ea7u ra         :param num_filters: s\u1ed1 l\u01b0\u1ee3ng b\u1ed9 l\u1ecdc s\u1eed d\u1ee5ng trong m\u1ea1ng         :param pretrained: s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh VGG11 c\u00f3 tr\u1ecdng s\u1ed1 pre-trained hay kh\u00f4ng         \"\"\"         super().__init__()         self.pool = nn.MaxPool2d(2, 2)          # TODO: L\u01b0u l\u1ea1i s\u1ed1 l\u01b0\u1ee3ng l\u1edbp \u0111\u1ea7u ra          # TODO: Ki\u1ec3m tra n\u1ebfu pretrained=True, s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh VGG11 v\u1edbi tr\u1ecdng s\u1ed1 c\u00f3 s\u1eb5n         if pretrained:             self.encoder = models.vgg11(weights=models.vgg.VGG11_Weights.DEFAULT).features         else:             self.encoder = models.vgg11().features          # TODO: Kh\u1edfi t\u1ea1o h\u00e0m k\u00edch ho\u1ea1t ReLU         # TODO: X\u00e2y d\u1ef1ng c\u00e1c kh\u1ed1i convolutional t\u1eeb c\u00e1c l\u1edbp trong VGG11         self.conv1 = nn.Sequential(             self.encoder[0],             # TODO: Th\u00eam h\u00e0m k\u00edch ho\u1ea1t ReLU         )          self.conv2 = nn.Sequential(             self.encoder[3],             # TODO: Th\u00eam h\u00e0m k\u00edch ho\u1ea1t ReLU         )          # TODO: Ti\u1ebfp t\u1ee5c x\u00e2y d\u1ef1ng c\u00e1c kh\u1ed1i convolutional cho conv3, conv4, conv5         self.conv3 = nn.Sequential(             self.encoder[6],             # TODO: Th\u00eam h\u00e0m k\u00edch ho\u1ea1t ReLU             self.encoder[8],             # TODO: Th\u00eam h\u00e0m k\u00edch ho\u1ea1t ReLU         )          # TODO: X\u00e2y d\u1ef1ng kh\u1ed1i center v\u00e0 decoder t\u01b0\u01a1ng \u1ee9ng         self.center = DecoderBlock(256 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv=is_deconv)                  # TODO: X\u00e2y d\u1ef1ng c\u00e1c l\u1edbp decoder dec5, dec4, dec3, dec2, dec1 theo ki\u1ebfn tr\u00fac U-Net          # TODO: X\u00e2y d\u1ef1ng l\u1edbp final convolution         self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)      def forward(self, x):         # TODO: Th\u1ef1c hi\u1ec7n forward pass theo th\u1ee9 t\u1ef1 c\u00e1c t\u1ea7ng conv1 -&gt; conv5, pool gi\u1eefa c\u00e1c t\u1ea7ng         conv1 = self.conv1(x)         conv2 = self.conv2(self.pool(conv1))         conv3 = self.conv3(self.pool(conv2))         # TODO: Ti\u1ebfp t\u1ee5c cho \u0111\u1ebfn conv5          # TODO: Truy\u1ec1n qua t\u1ea7ng center         center = self.center(self.pool(conv5))          # TODO: Gh\u00e9p n\u1ed1i (concat) c\u00e1c \u0111\u1eb7c tr\u01b0ng t\u1eeb encoder v\u1edbi decoder theo t\u1eebng t\u1ea7ng         dec5 = self.dec5(torch.cat([center, conv5], 1))         # TODO: Ti\u1ebfp t\u1ee5c v\u1edbi dec4, dec3, dec2, dec1          # TODO: Ki\u1ec3m tra n\u1ebfu num_classes &gt; 1, s\u1eed d\u1ee5ng log_softmax, ng\u01b0\u1ee3c l\u1ea1i s\u1eed d\u1ee5ng conv \u0111\u1ea7u ra         if self.num_classes &gt; 1:             x_out = F.log_softmax(self.final(dec1), dim=1)         else:             x_out = self.final(dec1)          return x_out In\u00a0[\u00a0]: Copied! <pre># Create the model\nfrom torchsummary import summary\n\nmodel = UNet11(pretrained=True)\nmodel.to(device)\nsummary(model, (3, 224, 224))\n</pre> # Create the model from torchsummary import summary  model = UNet11(pretrained=True) model.to(device) summary(model, (3, 224, 224)) In\u00a0[\u00a0]: Copied! <pre>def iou_pytorch(outputs, labels):\n    outputs = outputs.squeeze().byte()  # BATCH x 1 x H x W =&gt; BATCH x H x W\n    labels = labels.squeeze().byte()\n\n    SMOOTH = 1e-8\n\n    # TODO: T\u00ednh to\u00e1n ph\u1ea7n giao nhau gi\u1eefa outputs v\u00e0 labels\n    # intersection = ...\n\n    # TODO: T\u00ednh to\u00e1n ph\u1ea7n h\u1ee3p gi\u1eefa outputs v\u00e0 labels\n    # union = ...\n\n    # TODO: T\u00ednh to\u00e1n IoU theo c\u00f4ng th\u1ee9c iou theo c\u00f4ng th\u1ee9c (intersection + SMOOTH) / (union + SMOOTH)\n    # iou = ...\n    return iou\n</pre> def iou_pytorch(outputs, labels):     outputs = outputs.squeeze().byte()  # BATCH x 1 x H x W =&gt; BATCH x H x W     labels = labels.squeeze().byte()      SMOOTH = 1e-8      # TODO: T\u00ednh to\u00e1n ph\u1ea7n giao nhau gi\u1eefa outputs v\u00e0 labels     # intersection = ...      # TODO: T\u00ednh to\u00e1n ph\u1ea7n h\u1ee3p gi\u1eefa outputs v\u00e0 labels     # union = ...      # TODO: T\u00ednh to\u00e1n IoU theo c\u00f4ng th\u1ee9c iou theo c\u00f4ng th\u1ee9c (intersection + SMOOTH) / (union + SMOOTH)     # iou = ...     return iou  In\u00a0[\u00a0]: Copied! <pre>import torch.nn as nn\n\nnn.BCEWithLogitsLoss()\n</pre> import torch.nn as nn  nn.BCEWithLogitsLoss() In\u00a0[\u00a0]: Copied! <pre># Function to denormalize image\ndef de_normalize(img, mean=mean, std=std):\n    result = img * std + mean\n    result = np.clip(result, 0.0, 1.0)\n    \n    return result\n</pre> # Function to denormalize image def de_normalize(img, mean=mean, std=std):     result = img * std + mean     result = np.clip(result, 0.0, 1.0)          return result In\u00a0[\u00a0]: Copied! <pre># Function to display the result\n@torch.inference_mode()\ndef display_prediction(model, image, target):\n    model.eval()\n    img = image[None,...].to(device)\n    output = model(img)\n    pred = (torch.sigmoid(output.squeeze()) &gt; 0.5).to(int)\n\n\n    plt.figure(figsize=(10, 5))\n\n    plt.subplot(1,3,1)\n    plt.axis('off')\n    plt.title(\"Input Image\")\n    plt.imshow(de_normalize(image.numpy().transpose(1,2,0)))\n\n    plt.subplot(1,3,2)\n    plt.axis('off')\n    plt.title(\"Prediction\")\n    plt.imshow(pred.cpu().squeeze(), cmap='gray')\n\n    plt.subplot(1,3,3)\n    plt.axis('off')\n    plt.title(\"Ground Truth\")\n    plt.imshow(target.numpy().transpose(1, 2, 0), cmap='gray')\n\n    plt.show()\n</pre> # Function to display the result @torch.inference_mode() def display_prediction(model, image, target):     model.eval()     img = image[None,...].to(device)     output = model(img)     pred = (torch.sigmoid(output.squeeze()) &gt; 0.5).to(int)       plt.figure(figsize=(10, 5))      plt.subplot(1,3,1)     plt.axis('off')     plt.title(\"Input Image\")     plt.imshow(de_normalize(image.numpy().transpose(1,2,0)))      plt.subplot(1,3,2)     plt.axis('off')     plt.title(\"Prediction\")     plt.imshow(pred.cpu().squeeze(), cmap='gray')      plt.subplot(1,3,3)     plt.axis('off')     plt.title(\"Ground Truth\")     plt.imshow(target.numpy().transpose(1, 2, 0), cmap='gray')      plt.show() In\u00a0[\u00a0]: Copied! <pre># Choose one set of image, mask in train set to display\nidx = np.random.randint(0, len(train_dataset) - 1)\ndisplay_image = train_dataset[idx][0]\ndisplay_mask = train_dataset[idx][1]\n</pre> # Choose one set of image, mask in train set to display idx = np.random.randint(0, len(train_dataset) - 1) display_image = train_dataset[idx][0] display_mask = train_dataset[idx][1] In\u00a0[\u00a0]: Copied! <pre># Function to evaluate model\ndef evaluate(model, val_loader, criterion):\n    model.eval()\n    val_loss = 0.0\n    val_IOU = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            \n            total += labels.size(0)\n            predict_labels = (torch.sigmoid(outputs.squeeze()) &gt; 0.5).to(int)\n            val_IOU += iou_pytorch(predict_labels, labels).sum().item()\n\n    val_loss = val_loss / len(val_loader)\n    val_IOU = val_IOU / total\n    return val_loss, val_IOU\n</pre> # Function to evaluate model def evaluate(model, val_loader, criterion):     model.eval()     val_loss = 0.0     val_IOU = 0     total = 0     with torch.no_grad():         for inputs, labels in val_loader:             inputs, labels = inputs.to(device), labels.to(device)              outputs = model(inputs)             loss = criterion(outputs, labels)             val_loss += loss.item()                          total += labels.size(0)             predict_labels = (torch.sigmoid(outputs.squeeze()) &gt; 0.5).to(int)             val_IOU += iou_pytorch(predict_labels, labels).sum().item()      val_loss = val_loss / len(val_loader)     val_IOU = val_IOU / total     return val_loss, val_IOU In\u00a0[\u00a0]: Copied! <pre># Function to train model\ndef train(model, train_loader, val_loader, criterion, optimizer, max_epoch=50, output_path='best_model.pt'):\n    # save progress\n    train_losses = []\n    val_losses = []\n    train_IOUs = []\n    val_IOUs = []\n    best_IOU = 0.0\n    \n    # Train\n    for epoch in range(max_epoch):\n        model.train()\n\n        running_loss = 0.0\n        running_IOU = 0\n        total = 0\n        \n        for i, (inputs, labels) in enumerate(train_loader, 0):\n            # Move inputs and labels to the device\n            inputs, labels = inputs.to(device), labels.to(device) \n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item()\n            \n            # Calculate IOU\n            total += labels.size(0)\n            predict_labels = (torch.sigmoid(outputs.squeeze()) &gt; 0.5).to(int)\n\n            running_IOU += iou_pytorch(predict_labels, labels).sum().item()\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            \n        epoch_loss = running_loss / len(train_loader)\n        epoch_IOU = running_IOU / total\n        val_loss, val_IOU = evaluate(model, val_loader, criterion)\n        # Save best model\n        if val_IOU &gt; best_IOU:\n            best_IOU = val_IOU\n            model_scripted = torch.jit.script(model) # Export to TorchScript\n            model_scripted.save(output_path) # Save\n        \n        print(f\"Epoch [{epoch + 1}/{max_epoch}], Loss: {epoch_loss:.4f}, IOU: {epoch_IOU:.2f}, Val Loss: {val_loss:.4f}, Val IOU: {val_IOU:.2f}\")\n        display_prediction(model, display_image, display_mask)\n        \n\n        # Save for plot\n        train_losses.append(epoch_loss)\n        train_IOUs.append(epoch_IOU)\n        val_losses.append(val_loss)\n        val_IOUs.append(val_IOU)\n        \n    return [train_losses, val_losses, train_IOUs, val_IOUs]\n        \n</pre> # Function to train model def train(model, train_loader, val_loader, criterion, optimizer, max_epoch=50, output_path='best_model.pt'):     # save progress     train_losses = []     val_losses = []     train_IOUs = []     val_IOUs = []     best_IOU = 0.0          # Train     for epoch in range(max_epoch):         model.train()          running_loss = 0.0         running_IOU = 0         total = 0                  for i, (inputs, labels) in enumerate(train_loader, 0):             # Move inputs and labels to the device             inputs, labels = inputs.to(device), labels.to(device)               # Zero the parameter gradients             optimizer.zero_grad()              # Forward pass             outputs = model(inputs)             loss = criterion(outputs, labels)              running_loss += loss.item()                          # Calculate IOU             total += labels.size(0)             predict_labels = (torch.sigmoid(outputs.squeeze()) &gt; 0.5).to(int)              running_IOU += iou_pytorch(predict_labels, labels).sum().item()              # Backward pass and optimization             loss.backward()             optimizer.step()                      epoch_loss = running_loss / len(train_loader)         epoch_IOU = running_IOU / total         val_loss, val_IOU = evaluate(model, val_loader, criterion)         # Save best model         if val_IOU &gt; best_IOU:             best_IOU = val_IOU             model_scripted = torch.jit.script(model) # Export to TorchScript             model_scripted.save(output_path) # Save                  print(f\"Epoch [{epoch + 1}/{max_epoch}], Loss: {epoch_loss:.4f}, IOU: {epoch_IOU:.2f}, Val Loss: {val_loss:.4f}, Val IOU: {val_IOU:.2f}\")         display_prediction(model, display_image, display_mask)                   # Save for plot         train_losses.append(epoch_loss)         train_IOUs.append(epoch_IOU)         val_losses.append(val_loss)         val_IOUs.append(val_IOU)              return [train_losses, val_losses, train_IOUs, val_IOUs]          In\u00a0[\u00a0]: Copied! <pre># Clear the GPU memory just in case\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()\n</pre> # Clear the GPU memory just in case import gc gc.collect() torch.cuda.empty_cache() torch.cuda.memory_allocated() In\u00a0[\u00a0]: Copied! <pre># Train the model\nmax_epoch = 20\nlr = 4e-5\nweight_decay = 1e-4\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\nhistory_bce = train(model, train_loader, val_loader, criterion, optimizer, max_epoch, 'best_unet11_model.pt')\n</pre> # Train the model max_epoch = 20 lr = 4e-5 weight_decay = 1e-4 criterion = nn.BCEWithLogitsLoss() optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = weight_decay) history_bce = train(model, train_loader, val_loader, criterion, optimizer, max_epoch, 'best_unet11_model.pt')"},{"location":"Learning/AI_Model/unet/UNet/#unet-architecture","title":"UNet Architecture\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#1-prepare-data","title":"1. Prepare data\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#try-converting-images-and-masks-to-their-true-channel-and-check-number-of-channels-of-images-and-masks","title":"Try converting images and masks to their true channel and check number of channels of images and masks\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#check-the-shape-of-images-and-masks","title":"Check the shape of images and masks\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#show-examples-of-data","title":"Show examples of data\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#bai-tap-1-plot-anh-tu-dataset-ra-oi-tuong-can-load-la-1-cap-mau-du-lieu-moi-cap-bao-gom-1-image-va-1-mask","title":"B\u00e0i t\u1eadp 1: Plot \u1ea3nh t\u1eeb dataset ra, \u0111\u1ed1i t\u01b0\u1ee3ng c\u1ea7n load l\u00e0 1 c\u1eb7p m\u1eabu d\u1eef li\u1ec7u, m\u1ed7i c\u1eb7p bao g\u1ed3m 1 image v\u00e0 1 mask\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#split-data-into-three-sets","title":"Split data into three sets\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#create-kvasir-dataset","title":"Create Kvasir Dataset\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#calculate-mean-and-std-of-the-dataset","title":"Calculate mean and std of the dataset\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#data-transform-and-augmentation","title":"Data transform and augmentation\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#bai-tap-2-hoan-thien-cac-phep-bien-oi-du-lieu","title":"B\u00e0i t\u1eadp 2: Ho\u00e0n thi\u1ec7n c\u00e1c ph\u00e9p bi\u1ebfn \u0111\u1ed5i d\u1eef li\u1ec7u\u00b6","text":"<p>Trong b\u00e0i t\u1eadp n\u00e0y, b\u1ea1n s\u1ebd ho\u00e0n thi\u1ec7n c\u00e1c ph\u00e9p bi\u1ebfn \u0111\u1ed5i (transforms) cho \u1ea3nh v\u00e0 mask. C\u00e1c ph\u00e9p bi\u1ebfn \u0111\u1ed5i n\u00e0y s\u1ebd gi\u00fap chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u tr\u01b0\u1edbc khi \u0111\u01b0a v\u00e0o m\u00f4 h\u00ecnh h\u1ecdc s\u00e2u. G\u1ee3i \u00fd: d\u00f9ng c\u00e1c h\u00e0m trong \"transforms\"</p>"},{"location":"Learning/AI_Model/unet/UNet/#create-dataset","title":"Create dataset\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#make-dataloader","title":"Make dataloader\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#show-batch","title":"Show batch\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#2-model","title":"2. Model\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#bai-tap-3-hoan-thanh-cac-oan-code-sau-theo-goi-y","title":"B\u00e0i t\u1eadp 3: Ho\u00e0n th\u00e0nh c\u00e1c \u0111o\u1ea1n code sau theo g\u1ee3i \u00fd\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#iou-metric","title":"IOU metric\u00b6","text":"<p>For quality assessment, we will use the IoU (intersection over union) metric defined as:</p> <p>$I o U=\\frac{\\text {target } \\cap \\text { prediction }}{\\text {target } \\cup{prediction }}$</p>"},{"location":"Learning/AI_Model/unet/UNet/#examples","title":"Examples:\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#intersection-and-union","title":"Intersection and union:\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#bai-tap-4-hoan-thanh-ham-iou-ben-duoi","title":"B\u00e0i t\u1eadp 4: Ho\u00e0n th\u00e0nh h\u00e0m IOU b\u00ean d\u01b0\u1edbi\u00b6","text":""},{"location":"Learning/AI_Model/unet/UNet/#loss-function","title":"Loss function\u00b6","text":"<p>To start the training of our model, we will implement a loss function: Binary cross-enthropy loss (BCE)</p> <p>$$\\mathcal L_{BCE}(y, \\hat y) = -\\sum_i \\left[y_i\\log\\sigma(\\hat y_i) + (1-y_i)\\log(1-\\sigma(\\hat y_i))\\right].$$</p> <p>where $y$ is our target and $\\hat y$ is an output of the model. $\\sigma$ is a logistic function</p>"},{"location":"Learning/AI_Model/unet/UNet/#3-train","title":"3. Train\u00b6","text":""},{"location":"Learning/Python/Numpy_1/","title":"Numpy 1","text":"In\u00a0[\u00a0]: Copied! <pre>#Numpy \nimport numpy as np\nmy_arr = np.arange(10000)\nprint(my_arr)\nmy_arr?\n</pre> #Numpy  import numpy as np my_arr = np.arange(10000) print(my_arr) my_arr? In\u00a0[\u00a0]: Copied! <pre>#Multidimensional array object\nmy_np_data = np.random.randn(2,3)\nprint(my_np_data)\n</pre> #Multidimensional array object my_np_data = np.random.randn(2,3) print(my_np_data) In\u00a0[\u00a0]: Copied! <pre>my_list = [[-0.14660949,  0.10938535 , 0.73807359],\n [ 0.23479429, -0.18461008 ,-1.22907498]]\n</pre> my_list = [[-0.14660949,  0.10938535 , 0.73807359],  [ 0.23479429, -0.18461008 ,-1.22907498]]  In\u00a0[\u00a0]: Copied! <pre>my_np_data * 10\n</pre> my_np_data * 10 In\u00a0[\u00a0]: Copied! <pre>my_list * 2\n</pre> my_list * 2 In\u00a0[\u00a0]: Copied! <pre>my_np_data+my_np_data\n</pre> my_np_data+my_np_data In\u00a0[\u00a0]: Copied! <pre>my_np_data.shape\n</pre> my_np_data.shape In\u00a0[\u00a0]: Copied! <pre>my_np_data.dtype\n</pre> my_np_data.dtype In\u00a0[\u00a0]: Copied! <pre>#create a ndarrays (numpy arrays)\ndata1 = [ 5,6,2,1.5,5]\narr1 = np.array(data1)\narr1\n</pre> #create a ndarrays (numpy arrays) data1 = [ 5,6,2,1.5,5] arr1 = np.array(data1) arr1 In\u00a0[\u00a0]: Copied! <pre>data2 = [[1,2,3,4],[5,6,7,4]]\narr2 = np.array(data2)\narr2\n</pre> data2 = [[1,2,3,4],[5,6,7,4]] arr2 = np.array(data2) arr2 In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nnp.zeros(10) # array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\nnp.zeros((3, 6)) \nprint(np.zeros((3,4)))\n</pre> import numpy as np np.zeros(10) # array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])  np.zeros((3, 6))  print(np.zeros((3,4))) In\u00a0[\u00a0]: Copied! <pre>##Data types for ndarrays\narr1 = np.array([1,2,3],dtype=np.float64)\narr2 = np.array([1,2,3],dtype=np.int32)\n</pre> ##Data types for ndarrays arr1 = np.array([1,2,3],dtype=np.float64) arr2 = np.array([1,2,3],dtype=np.int32)  In\u00a0[\u00a0]: Copied! <pre>#type cast\narr = np.array([1.5,2,3,4.5,5]) #dtype = float64\nint_arr = arr.astype(np.int64) #dtype = int64\nint_arr\n</pre> #type cast arr = np.array([1.5,2,3,4.5,5]) #dtype = float64 int_arr = arr.astype(np.int64) #dtype = int64 int_arr  In\u00a0[\u00a0]: Copied! <pre>#Arithmetic with Numpy arrays\narr = np.array([[1., 2., 3.], [4., 5., 6.]])\n#Apply operation element-wise\narr * arr \narr - arr\n\n#with scalars\n1/arr\narr ** 0.5\n\n#compare between arrays\narr2 = np.array([[0., 4., 1.], [7., 2., 12.]])\n\narr&gt;arr2\n</pre> #Arithmetic with Numpy arrays arr = np.array([[1., 2., 3.], [4., 5., 6.]]) #Apply operation element-wise arr * arr  arr - arr  #with scalars 1/arr arr ** 0.5  #compare between arrays arr2 = np.array([[0., 4., 1.], [7., 2., 12.]])  arr&gt;arr2 In\u00a0[\u00a0]: Copied! <pre>#Indexing &amp; slicing\n\narr = np.arange(10) # array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\narr[5] # 5\narr[5:8] # array([5, 6, 7])\narr[5:8] = 12 # array([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9])\n\n# Two dimensions\narr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\narr2d[2] #array([7, 8, 9])\narr2d[0][2] # 3\narr2d[0,2] # 3\narr2d[:2,1:] # array([[2, 3],[5, 6]]\n\n\n# Three dimensions\narr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\narr3d.shape # (2,2,3)\n\ntemp = arr3d[0]\n#temp = 0\ntemp[:] = 0\narr3d\n</pre> #Indexing &amp; slicing  arr = np.arange(10) # array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) arr[5] # 5 arr[5:8] # array([5, 6, 7]) arr[5:8] = 12 # array([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9])  # Two dimensions arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) arr2d[2] #array([7, 8, 9]) arr2d[0][2] # 3 arr2d[0,2] # 3 arr2d[:2,1:] # array([[2, 3],[5, 6]]   # Three dimensions arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]) arr3d.shape # (2,2,3)  temp = arr3d[0] #temp = 0 temp[:] = 0 arr3d    In\u00a0[\u00a0]: Copied! <pre>#Use copy \narr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\narr3d.shape # (2,2,3)\n\ntemp = arr3d[0].copy()\n\ntemp[:] = 0\narr3d\n</pre> #Use copy  arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]) arr3d.shape # (2,2,3)  temp = arr3d[0].copy()  temp[:] = 0 arr3d  In\u00a0[\u00a0]: Copied! <pre>#Boolean indexing\nnames = np.array(['Bob', 'Will','Joe',  'Bob', 'Will', 'Joe', 'Joe'])\ndata = np.random.randn(7, 4)\n\ndata\n</pre> #Boolean indexing names = np.array(['Bob', 'Will','Joe',  'Bob', 'Will', 'Joe', 'Joe']) data = np.random.randn(7, 4)  data In\u00a0[\u00a0]: Copied! <pre>names = np.array(['Bob', 'Will','Joe',  'Bob', 'Will', 'Joe', 'Joe'])\nprint(names == 'Bob')\n</pre> names = np.array(['Bob', 'Will','Joe',  'Bob', 'Will', 'Joe', 'Joe']) print(names == 'Bob') In\u00a0[\u00a0]: Copied! <pre>names == 'Bob'\n</pre> names == 'Bob' In\u00a0[\u00a0]: Copied! <pre>data[names == 'Bob']\n</pre> data[names == 'Bob'] In\u00a0[\u00a0]: Copied! <pre>data[names == 'Bob',2:]\nprint(data)\n</pre>  data[names == 'Bob',2:] print(data) In\u00a0[\u00a0]: Copied! <pre>#Select everything but 'Bob'\ndata[names != 'Bob']\n</pre> #Select everything but 'Bob' data[names != 'Bob'] In\u00a0[\u00a0]: Copied! <pre>#negate condition\ndata[~(names == 'Bob')]\n</pre> #negate condition data[~(names == 'Bob')] In\u00a0[\u00a0]: Copied! <pre>m=5\nn=6\narr = np.random.randn(m,n)\n# arr[arr&lt;0] = 0\n# print(arr&lt;0)\narr2 = arr&lt;0\nprint(arr2)\narr2[::,::2] = False\narr[arr2] = 0\nprint(arr2)\nprint(arr)\n</pre> m=5 n=6 arr = np.random.randn(m,n) # arr[arr&lt;0] = 0 # print(arr&lt;0) arr2 = arr&lt;0 print(arr2) arr2[::,::2] = False arr[arr2] = 0 print(arr2) print(arr) In\u00a0[\u00a0]: Copied! <pre>m=4\nn=5\na = np.random.randn(m,n)\nprint(a)\n# a[:,::2] = 0\nb = a[:,::2]\n# print(b)\nb = a&lt;0\nb[:,::2] = False\na[b] = 0\nprint(a)\n</pre> m=4 n=5 a = np.random.randn(m,n) print(a) # a[:,::2] = 0 b = a[:,::2] # print(b) b = a&lt;0 b[:,::2] = False a[b] = 0 print(a) In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nm=3\nn=4\narr1 = np.random.randn(m,n)\narr2 = arr1[::2,:]\narr2[arr2&lt;0]=0\nprint(arr2)\n</pre> import numpy as np m=3 n=4 arr1 = np.random.randn(m,n) arr2 = arr1[::2,:] arr2[arr2&lt;0]=0 print(arr2) In\u00a0[\u00a0]: Copied! <pre>#Fancy Indexing\narr = np.array([[ 0.,  0.,  0.,  0.],\n           [ 1.,  1.,  1.,  1.],\n           [ 2.,  2.,  2.,  2.],\n           [ 3.,  3.,  3.,  3.],\n           [ 4.,  4.,  4.,  4.],\n           [ 5.,  5.,  5.,  5.],\n           [ 6.,  6.,  6.,  6.],\n           [ 7.,  7.,  7.,  7.]])\n\narr[[1,5,2]]\n</pre> #Fancy Indexing arr = np.array([[ 0.,  0.,  0.,  0.],            [ 1.,  1.,  1.,  1.],            [ 2.,  2.,  2.,  2.],            [ 3.,  3.,  3.,  3.],            [ 4.,  4.,  4.,  4.],            [ 5.,  5.,  5.,  5.],            [ 6.,  6.,  6.,  6.],            [ 7.,  7.,  7.,  7.]])  arr[[1,5,2]] In\u00a0[\u00a0]: Copied! <pre>arr = np.array([[ 0,  1,  2,  3],\n                [4, 5, 6, 7], \n                [8, 9,10,11], \n                [12, 13, 14, 15], \n                [16, 17, 18, 19], \n                [20, 21, 22, 23], \n                [24, 25, 26, 27], \n                [28, 29, 30, 31]])\n\narr[[1,4,7]]\n</pre> arr = np.array([[ 0,  1,  2,  3],                 [4, 5, 6, 7],                  [8, 9,10,11],                  [12, 13, 14, 15],                  [16, 17, 18, 19],                  [20, 21, 22, 23],                  [24, 25, 26, 27],                  [28, 29, 30, 31]])  arr[[1,4,7]] In\u00a0[\u00a0]: Copied! <pre>arr[[1,4, 7], [0, 0, 1]]\n</pre> arr[[1,4, 7], [0, 0, 1]] In\u00a0[\u00a0]: Copied! <pre>arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]] # ????\n</pre> arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]] # ???? In\u00a0[\u00a0]: Copied! <pre>#Reshape\narr = np.arange(0,27) #array([0, 1, 2, 3, 4, 5, 6, 7, 8,9, ....., 26])\narr.reshape(3,-1)\n</pre> #Reshape arr = np.arange(0,27) #array([0, 1, 2, 3, 4, 5, 6, 7, 8,9, ....., 26]) arr.reshape(3,-1)   In\u00a0[\u00a0]: Copied! <pre>x = np.zeros((8,8), dtype=np.float32)\nx[1::2,::2] = 1\nx[::2,1::2] = 1\nprint(x)\n</pre> x = np.zeros((8,8), dtype=np.float32) x[1::2,::2] = 1 x[::2,1::2] = 1 print(x) In\u00a0[\u00a0]: Copied! <pre>x = np.zeros((8,8))\nprint(x)\n</pre> x = np.zeros((8,8)) print(x) In\u00a0[\u00a0]: Copied! <pre>x = np.ones((5,5))\nprint(\"Original array:\")\nprint(x)\nprint(\"1 on the border and 0 inside in the array\")\nx[1:-1,1:-1] = 0\nprint(x)\n</pre> x = np.ones((5,5)) print(\"Original array:\") print(x) print(\"1 on the border and 0 inside in the array\") x[1:-1,1:-1] = 0 print(x)  In\u00a0[4]: Copied! <pre>import numpy as np \na = np.arange(16)\nprint(a)\na = a.reshape(4,-1)\nd = np.diag_indices(4)\nprint(d)\nprint(a)\n# a.diagonal()\na[d] = 100\nprint(a)\nprint(d)\n</pre> import numpy as np  a = np.arange(16) print(a) a = a.reshape(4,-1) d = np.diag_indices(4) print(d) print(a) # a.diagonal() a[d] = 100 print(a) print(d) <pre>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n(array([0, 1, 2, 3]), array([0, 1, 2, 3]))\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]]\n[[100   1   2   3]\n [  4 100   6   7]\n [  8   9 100  11]\n [ 12  13  14 100]]\n(array([0, 1, 2, 3]), array([0, 1, 2, 3]))\n</pre>"},{"location":"Learning/Python/Numpy_1/#numpy","title":"Numpy\u00b6","text":"<ul> <li>Multidimensional array</li> <li>Providing fast aray-oriented operations and flexible broadcasting capabilities</li> </ul>"},{"location":"Learning/Python/Numpy_1/#ex1-given-an-integer-matrix-of-nxm-set-negative-values-in-the-matrix-to-0","title":"Ex1. Given an integer matrix of NxM, set negative values in the matrix to 0\u00b6","text":""},{"location":"Learning/Python/Numpy_1/#ex2-given-an-integer-matrix-of-nxm-set-negative-values-in-even-rows-to-0","title":"Ex2. Given an integer matrix of NxM, set negative values in  even rows to 0\u00b6","text":""},{"location":"Learning/Python/Numpy_1/#ex1-create-a-checkboard-matrix-8x8-as-follows","title":"Ex1 Create a checkboard matrix 8x8 as follows:\u00b6","text":"<p>[[0 1 0 1 0 1 0 1] [1 0 1 0 1 0 1 0] [0 1 0 1 0 1 0 1] [1 0 1 0 1 0 1 0] [0 1 0 1 0 1 0 1] [1 0 1 0 1 0 1 0] [0 1 0 1 0 1 0 1] [1 0 1 0 1 0 1 0]]</p>"},{"location":"Learning/Python/Numpy_1/#ex2-create-a-matrix-with-1-in-borders-and-0-for-inside","title":"Ex2. Create a matrix with 1 in borders and 0 for inside\u00b6","text":"<p>[[ 1.  1.  1.  1.  1.] [ 1.  0.  0.  0.  1.] [ 1.  0.  0.  0.  1.] [ 1.  0.  0.  0.  1.] [ 1.  1.  1.  1.  1.]]</p>"},{"location":"Learning/Python/Numpy_1/#ex3-create-a-nxn-matrix-as-follows","title":"Ex3. Create a NxN matrix as follows:\u00b6","text":"<p>[[100,   1,   2,   3] ,</p> <p>[  4, 100,   6,   7],</p> <p>[  8,   9, 100,  11],</p> <p>[ 12,  13,  14, 100]]</p>"},{"location":"Learning/Python/Numpy_2/","title":"Broadcasting","text":"In\u00a0[1]: Copied! <pre>#Transposing Arrays and Swapping Axes\nimport numpy as np\n\narr = np.arange(15).reshape((3,5))\n\narr\n</pre> #Transposing Arrays and Swapping Axes import numpy as np  arr = np.arange(15).reshape((3,5))  arr Out[1]: <pre>array([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])</pre> In\u00a0[\u00a0]: Copied! <pre>arr.T #Transpose \n</pre> arr.T #Transpose  Out[\u00a0]: <pre>array([[ 0,  5, 10],\n       [ 1,  6, 11],\n       [ 2,  7, 12],\n       [ 3,  8, 13],\n       [ 4,  9, 14]])</pre> In\u00a0[\u00a0]: Copied! <pre>#Dot product\narr = np.random.randn(6, 3)\nnp.dot(arr.T,arr)\n</pre> #Dot product arr = np.random.randn(6, 3) np.dot(arr.T,arr) Out[\u00a0]: <pre>array([[6.1878891 , 2.92044961, 0.97504599],\n       [2.92044961, 6.54019254, 3.26760513],\n       [0.97504599, 3.26760513, 2.79437   ]])</pre> In\u00a0[\u00a0]: Copied! <pre>#Transpose with higher dimensional arrays\narr = np.arange(16).reshape(2,2,4)\narr\n</pre> #Transpose with higher dimensional arrays arr = np.arange(16).reshape(2,2,4) arr Out[\u00a0]: <pre>array([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7]],\n\n       [[ 8,  9, 10, 11],\n        [12, 13, 14, 15]]])</pre> In\u00a0[\u00a0]: Copied! <pre>arr.T\n</pre> arr.T Out[\u00a0]: <pre>array([[[ 0,  8],\n        [ 4, 12]],\n\n       [[ 1,  9],\n        [ 5, 13]],\n\n       [[ 2, 10],\n        [ 6, 14]],\n\n       [[ 3, 11],\n        [ 7, 15]]])</pre> In\u00a0[\u00a0]: Copied! <pre>arr.transpose((1,0,2))\n</pre> arr.transpose((1,0,2)) Out[\u00a0]: <pre>array([[[ 0,  1,  2,  3],\n        [ 8,  9, 10, 11]],\n\n       [[ 4,  5,  6,  7],\n        [12, 13, 14, 15]]])</pre> In\u00a0[\u00a0]: Copied! <pre>arr.strides #(64,32,8)\narr.transpose((1,0,2)).strides #(32,64,8)\n</pre> arr.strides #(64,32,8) arr.transpose((1,0,2)).strides #(32,64,8)  Out[\u00a0]: <pre>(32, 64, 8)</pre> In\u00a0[\u00a0]: Copied! <pre>arr.transpose((2,1,0)) #????\n</pre> arr.transpose((2,1,0)) #???? In\u00a0[\u00a0]: Copied! <pre>arr = np.arange(120).reshape(2,3,4,-1)\n\narr[1,2,3,3] #??\n\narr.transpose((1,3,2,0))[2,1,1,1] #??\n</pre> arr = np.arange(120).reshape(2,3,4,-1)  arr[1,2,3,3] #??  arr.transpose((1,3,2,0))[2,1,1,1] #??   Out[\u00a0]: <pre>106</pre> In\u00a0[\u00a0]: Copied! <pre>arr = np.arange(12).reshape(3,4)\narr.T.reshape(12)\n</pre> arr = np.arange(12).reshape(3,4) arr.T.reshape(12) Out[\u00a0]: <pre>array([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])</pre> In\u00a0[\u00a0]: Copied! <pre>#Swapaxes which similar to transpose\narr.swapaxes(0,1)\n</pre> #Swapaxes which similar to transpose arr.swapaxes(0,1) Out[\u00a0]: <pre>array([[[ 0,  1,  2,  3],\n        [ 8,  9, 10, 11]],\n\n       [[ 4,  5,  6,  7],\n        [12, 13, 14, 15]]])</pre> In\u00a0[\u00a0]: Copied! <pre>#Concatenating\narr1 = np.array([[1, 2, 3], [4, 5, 6]])\narr2 = np.array([[7, 8, 9], [10, 11, 12]])\n\nnp.concatenate([arr1, arr2]) #?\nnp.concatenate([arr1, arr2],axis=0) #?\nnp.concatenate([arr1, arr2],axis = 1) #?\n</pre> #Concatenating arr1 = np.array([[1, 2, 3], [4, 5, 6]]) arr2 = np.array([[7, 8, 9], [10, 11, 12]])  np.concatenate([arr1, arr2]) #? np.concatenate([arr1, arr2],axis=0) #? np.concatenate([arr1, arr2],axis = 1) #? Out[\u00a0]: <pre>array([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])</pre> In\u00a0[\u00a0]: Copied! <pre>#Tile &amp; repeat\n\narr = np.arange(3) # array([0, 1, 2])\n\narr.repeat(3) # array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n\narr.repeat([2, 3, 4]) # array([0, 0, 1, 1, 1, 2, 2, 2, 2])\n\narr = np.arange(4).reshape(2, 2)\narr\n</pre> #Tile &amp; repeat  arr = np.arange(3) # array([0, 1, 2])  arr.repeat(3) # array([0, 0, 0, 1, 1, 1, 2, 2, 2])  arr.repeat([2, 3, 4]) # array([0, 0, 1, 1, 1, 2, 2, 2, 2])  arr = np.arange(4).reshape(2, 2) arr Out[\u00a0]: <pre>array([[0, 1],\n       [2, 3]])</pre> In\u00a0[\u00a0]: Copied! <pre>arr.repeat(2, axis=0)\n</pre> arr.repeat(2, axis=0) Out[\u00a0]: <pre>array([[0, 1],\n       [0, 1],\n       [2, 3],\n       [2, 3]])</pre> In\u00a0[\u00a0]: Copied! <pre>arr.repeat([2, 3], axis=0)\n</pre> arr.repeat([2, 3], axis=0) Out[\u00a0]: <pre>array([[0, 1],\n       [0, 1],\n       [2, 3],\n       [2, 3],\n       [2, 3]])</pre> In\u00a0[\u00a0]: Copied! <pre>arr.repeat([2, 3], axis=1) #?\n</pre> arr.repeat([2, 3], axis=1) #? Out[\u00a0]: <pre>array([[0, 0, 1, 1, 1],\n       [2, 2, 3, 3, 3]])</pre> In\u00a0[\u00a0]: Copied! <pre>np.tile(arr,2)\n</pre> np.tile(arr,2) Out[\u00a0]: <pre>array([[0, 1, 0, 1],\n       [2, 3, 2, 3]])</pre> In\u00a0[\u00a0]: Copied! <pre>np.tile(arr,(2,1))\n</pre> np.tile(arr,(2,1)) Out[\u00a0]: <pre>array([[0, 1],\n       [2, 3],\n       [0, 1],\n       [2, 3]])</pre> In\u00a0[\u00a0]: Copied! <pre>np.tile(arr,(2,2)) #?\n</pre> np.tile(arr,(2,2)) #? Out[\u00a0]: <pre>array([[0, 1, 0, 1],\n       [2, 3, 2, 3],\n       [0, 1, 0, 1],\n       [2, 3, 2, 3]])</pre> In\u00a0[\u00a0]: Copied! <pre>#Universal functions which perform element-wise operations on ndarrays\n\narr = np.random.randn(10)\n\nnp.square(arr) \n</pre> #Universal functions which perform element-wise operations on ndarrays  arr = np.random.randn(10)  np.square(arr)   Out[\u00a0]: <pre>array([3.16274687e+00, 2.29088562e+00, 6.14659767e-06, 1.11834661e-01,\n       7.97263403e-02, 1.15980131e+00, 7.93531615e-01, 5.14354487e-01,\n       2.02893154e+00, 2.65375327e-01])</pre> In\u00a0[\u00a0]: Copied! <pre>np.exp(arr)\n</pre> np.exp(arr) Out[\u00a0]: <pre>array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n       5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n       2.98095799e+03, 8.10308393e+03])</pre> In\u00a0[\u00a0]: Copied! <pre>x = np.random.randn(8) #array([-0.0119,  1.0048,  1.3272, -0.9193, -1.5491,  0.0222,  0.7584,-0.6605])\ny = np.random.randn(8) #array([ 0.8626, -0.01  ,  0.05  ,  0.6702,  0.853 , -0.9559, -0.0235,-2.3042])\nnp.maximum(x,y) # array([ 0.8626,  1.0048,  1.3272,  0.6702,  0.853 ,  0.0222,  0.7584,-0.6605])\n</pre> x = np.random.randn(8) #array([-0.0119,  1.0048,  1.3272, -0.9193, -1.5491,  0.0222,  0.7584,-0.6605]) y = np.random.randn(8) #array([ 0.8626, -0.01  ,  0.05  ,  0.6702,  0.853 , -0.9559, -0.0235,-2.3042]) np.maximum(x,y) # array([ 0.8626,  1.0048,  1.3272,  0.6702,  0.853 ,  0.0222,  0.7584,-0.6605]) In\u00a0[\u00a0]: Copied! <pre>#Array-oriented Programming with Arrays\npoints = np.arange(-5, 5, 0.01)\n\nxs, ys = np.meshgrid(points, points)\n\nxs.shape #(1000,1000)\nys.shape #(1000,1000)\n\nxs\n</pre> #Array-oriented Programming with Arrays points = np.arange(-5, 5, 0.01)  xs, ys = np.meshgrid(points, points)  xs.shape #(1000,1000) ys.shape #(1000,1000)  xs Out[\u00a0]: <pre>array([[-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],\n       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],\n       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],\n       ...,\n       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],\n       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],\n       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99]])</pre> In\u00a0[\u00a0]: Copied! <pre>z=np.sqrt(xs**2+ys**2)\nz #(1000,1000)\n</pre> z=np.sqrt(xs**2+ys**2) z #(1000,1000) Out[\u00a0]: <pre>array([[7.07106781, 7.06400028, 7.05693985, ..., 7.04988652, 7.05693985,\n        7.06400028],\n       [7.06400028, 7.05692568, 7.04985815, ..., 7.04279774, 7.04985815,\n        7.05692568],\n       [7.05693985, 7.04985815, 7.04278354, ..., 7.03571603, 7.04278354,\n        7.04985815],\n       ...,\n       [7.04988652, 7.04279774, 7.03571603, ..., 7.0286414 , 7.03571603,\n        7.04279774],\n       [7.05693985, 7.04985815, 7.04278354, ..., 7.03571603, 7.04278354,\n        7.04985815],\n       [7.06400028, 7.05692568, 7.04985815, ..., 7.04279774, 7.04985815,\n        7.05692568]])</pre> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nplt.imshow(z, cmap=plt.cm.gray); plt.colorbar()\nplt.title(\"Image plot of $\\sqrt{x^2 + y^2}$ for a grid of values\")\n</pre> import matplotlib.pyplot as plt plt.imshow(z, cmap=plt.cm.gray); plt.colorbar() plt.title(\"Image plot of $\\sqrt{x^2 + y^2}$ for a grid of values\") Out[\u00a0]: <pre>Text(0.5, 1.0, 'Image plot of $\\\\sqrt{x^2 + y^2}$ for a grid of values')</pre> In\u00a0[\u00a0]: Copied! <pre>#Conditional logis as Array Operations\nxarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])\nyarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])\ncond = np.array([True, False, True, True, False])\n\nresult = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]\nresult\n</pre> #Conditional logis as Array Operations xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5]) yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5]) cond = np.array([True, False, True, True, False])  result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)] result Out[\u00a0]: <pre>[1.1, 2.2, 1.3, 1.4, 2.5]</pre> In\u00a0[\u00a0]: Copied! <pre>result = np.where(cond, xarr, yarr)\nresult\n</pre> result = np.where(cond, xarr, yarr) result Out[\u00a0]: <pre>array([1.1, 2.2, 1.3, 1.4, 2.5])</pre> <p>Ex. Create a random NxM matrix. Set all positive values x to x*x, and -2 for negative values</p> In\u00a0[\u00a0]: Copied! <pre>arr = np.random.randn(4, 4)\nprint(arr)\nnp.where(arr&gt;0,arr*arr,-2)\n</pre> arr = np.random.randn(4, 4) print(arr) np.where(arr&gt;0,arr*arr,-2) <pre>[[ 0.32795917 -0.50855939  0.70532204 -0.14252676]\n [-0.42429888 -1.22797053 -1.92360193  1.17996829]\n [ 1.73835008  0.25579915  0.8565595  -0.81707552]\n [ 0.41275343 -0.66089923  0.18677612  0.11249829]]\n</pre> Out[\u00a0]: <pre>array([[ 0.10755722, -2.        ,  0.49747919, -2.        ],\n       [-2.        , -2.        , -2.        ,  1.39232516],\n       [ 3.02186099,  0.06543321,  0.73369419, -2.        ],\n       [ 0.17036539, -2.        ,  0.03488532,  0.01265586]])</pre> In\u00a0[\u00a0]: Copied! <pre>#Mathematical and Statistical methods\narr = np.random.randn(5, 4)\narr\n</pre> #Mathematical and Statistical methods arr = np.random.randn(5, 4) arr Out[\u00a0]: <pre>array([[-0.54926229,  0.18278111, -0.83024367,  0.69999463],\n       [-0.1905132 , -0.34356363,  1.0181388 ,  0.24849718],\n       [ 0.46647855,  0.33590169,  0.44325881,  0.20612658],\n       [ 0.2589447 ,  0.29961133, -0.24995255,  0.49049097],\n       [-0.46239346, -0.30404133,  1.20203899, -1.21725287]])</pre> In\u00a0[\u00a0]: Copied! <pre>arr.mean()\narr.sum() \n</pre> arr.mean() arr.sum()  Out[\u00a0]: <pre>1.7050403500621718</pre> In\u00a0[\u00a0]: Copied! <pre>arr.mean(axis=1)\n</pre> arr.mean(axis=1) Out[\u00a0]: <pre>array([-0.12418255,  0.18313979,  0.36294141,  0.19977361, -0.19541217])</pre> In\u00a0[\u00a0]: Copied! <pre>arr.mean(axis=0)\n</pre> arr.mean(axis=0) Out[\u00a0]: <pre>array([-0.09534914,  0.03413784,  0.31664808,  0.0855713 ])</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>#File with numpy\n\narr = np.arange(10)\nnp.save('some_array', arr)\n</pre> #File with numpy  arr = np.arange(10) np.save('some_array', arr)  In\u00a0[\u00a0]: Copied! <pre>np.load('some_array.npy')\n</pre> np.load('some_array.npy') Out[\u00a0]: <pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> In\u00a0[\u00a0]: Copied! <pre>#Linear Algebra\nx = np.array([[1., 2., 3.], [4., 5., 6.]])\ny = np.array([[6., 23.], [-1, 7], [8, 9]])\nx.dot(y)\n</pre> #Linear Algebra x = np.array([[1., 2., 3.], [4., 5., 6.]]) y = np.array([[6., 23.], [-1, 7], [8, 9]]) x.dot(y) Out[\u00a0]: <pre>array([[ 28.,  64.],\n       [ 67., 181.]])</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>#Random in numpy\nnp.random.seed(124)\nnp.random.rand(2,3)\n</pre> #Random in numpy np.random.seed(124) np.random.rand(2,3) Out[\u00a0]: <pre>array([[0.10606491, 0.74547148, 0.57231354],\n       [0.45824118, 0.3847059 , 0.27398931]])</pre> In\u00a0[\u00a0]: Copied! <pre>#np.random.seed(124)\nnp.random.rand(2,3)\n</pre> #np.random.seed(124) np.random.rand(2,3) Out[\u00a0]: <pre>array([[0.38957403, 0.97498836, 0.4675989 ],\n       [0.14017141, 0.04206858, 0.72799627]])</pre> In\u00a0[\u00a0]: Copied! <pre>arr = np.arange(5) #array([0, 1, 2, 3, 4])\narr*4\n</pre> arr = np.arange(5) #array([0, 1, 2, 3, 4]) arr*4  Out[\u00a0]: <pre>array([ 0,  4,  8, 12, 16])</pre> In\u00a0[\u00a0]: Copied! <pre>arr = np.random.randn(4, 3)\narr.mean(0)\n</pre> arr = np.random.randn(4, 3) arr.mean(0)  Out[\u00a0]: <pre>array([0.48837945, 0.52888814, 0.08470346])</pre> In\u00a0[\u00a0]: Copied! <pre>de = arr - arr.mean(0)\nde\n</pre> de = arr - arr.mean(0) de Out[\u00a0]: <pre>array([[ 0.04160849,  0.38942007,  0.77749224],\n       [ 0.49715569, -1.44295791, -0.95987177],\n       [ 0.39977265,  0.2456237 ,  1.06945136],\n       [-0.93853683,  0.80791414, -0.88707184]])</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>arr.mean(1)\n</pre> arr.mean(1) Out[\u00a0]: <pre>array([ 0.77016395, -0.26790098,  0.93893959,  0.02809217])</pre> In\u00a0[\u00a0]: Copied! <pre>de = arr - arr.mean(1) #?\n</pre> de = arr - arr.mean(1) #?  <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-24-952255e5b562&gt; in &lt;module&gt;()\n----&gt; 1 de = arr - arr.mean(1) #?\n      2 \n\nValueError: operands could not be broadcast together with shapes (4,3) (4,) </pre> <p></p> <p></p> In\u00a0[\u00a0]: Copied! <pre>a = np.arange(10).reshape(2,-1)\nb = np.repeat(1, 10).reshape(2,-1)\n\nnp.concatenate([a, b], axis=1)\n</pre> a = np.arange(10).reshape(2,-1) b = np.repeat(1, 10).reshape(2,-1)  np.concatenate([a, b], axis=1)  In\u00a0[\u00a0]: Copied! <pre>import numpy as np\na = np.array([1,2,3])\nnp.repeat(a,3)\nnp.tile(a,3)\nnp.concatenate([np.repeat(a,3),np.tile(a,3)])\n</pre> import numpy as np a = np.array([1,2,3]) np.repeat(a,3) np.tile(a,3) np.concatenate([np.repeat(a,3),np.tile(a,3)]) Out[\u00a0]: <pre>array([1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3])</pre>"},{"location":"Learning/Python/Numpy_2/#numpy-2","title":"Numpy 2\u00b6","text":""},{"location":"Learning/Python/Numpy_2/#ex-1","title":"Ex 1\u00b6","text":"<p>Given an array array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 9, 10, 11])</p> <p>Let convert to array([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])</p>"},{"location":"Learning/Python/Numpy_2/#unary-ufuncs","title":"Unary ufuncs\u00b6","text":""},{"location":"Learning/Python/Numpy_2/#binary-ufuncs","title":"Binary ufuncs\u00b6","text":""},{"location":"Learning/Python/Numpy_2/#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"Learning/Python/Numpy_2/#ex-2","title":"Ex 2\u00b6","text":"<p>Create the following array:</p> <p>array([[0, 1, 2, 3, 4, 1, 1, 1, 1, 1], [5, 6, 7, 8, 9, 1, 1, 1, 1, 1]])</p>"},{"location":"Learning/Python/Numpy_2/#ex-3","title":"Ex 3\u00b6","text":"<p>Given a = np.array([1,2,3])</p> <p>Create a following array:</p> <p>array([1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3])</p>"},{"location":"Learning/Python/Numpy_2/#ex-4","title":"Ex 4\u00b6","text":"<p>arr = np.arange(9).reshape(3,3)</p> <p>arr[::-1] ??</p>"},{"location":"Learning/Python/Pandas/","title":"DataFrame","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[\u00a0]: Copied! <pre>obj = pd.Series([4, 7, -5, 3])\nobj\n</pre> obj = pd.Series([4, 7, -5, 3]) obj Out[\u00a0]: <pre>0    4\n1    7\n2   -5\n3    3\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>obj.index\n</pre> obj.index Out[\u00a0]: <pre>RangeIndex(start=0, stop=4, step=1)</pre> In\u00a0[\u00a0]: Copied! <pre>obj2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])\nobj2\n</pre> obj2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c']) obj2 Out[\u00a0]: <pre>d    4\nb    7\na   -5\nc    3\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>obj2['a']\n</pre> obj2['a'] Out[\u00a0]: <pre>-5</pre> In\u00a0[\u00a0]: Copied! <pre>obj2[['c', 'a', 'd']]\n</pre> obj2[['c', 'a', 'd']] Out[\u00a0]: <pre>c    3\na   -5\nd    4\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>obj2[obj2 &gt; 0]\n</pre> obj2[obj2 &gt; 0] Out[\u00a0]: <pre>d    4\nb    7\nc    3\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>obj2 * 2\n</pre> obj2 * 2 Out[\u00a0]: <pre>d     8\nb    14\na   -10\nc     6\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nnp.exp(obj2)\n</pre> import numpy as np np.exp(obj2) Out[\u00a0]: <pre>d      54.598150\nb    1096.633158\na       0.006738\nc      20.085537\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>'b' in obj2\n</pre> 'b' in obj2 Out[\u00a0]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\nobj3 = pd.Series(sdata)\nobj3\n</pre> sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000} obj3 = pd.Series(sdata) obj3 Out[\u00a0]: <pre>Ohio      35000\nTexas     71000\nOregon    16000\nUtah       5000\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>#Reorder key (index)\nstates = ['California', 'Ohio', 'Oregon', 'Texas']\nobj4 = pd.Series(sdata,index=states)\nobj4\n</pre> #Reorder key (index) states = ['California', 'Ohio', 'Oregon', 'Texas'] obj4 = pd.Series(sdata,index=states) obj4  Out[\u00a0]: <pre>California        NaN\nOhio          35000.0\nOregon        16000.0\nTexas         71000.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>obj4.isnull()\n</pre> obj4.isnull() Out[\u00a0]: <pre>California     True\nOhio          False\nOregon        False\nTexas         False\ndtype: bool</pre> In\u00a0[\u00a0]: Copied! <pre>#Aligns by index\n\nobj3 + obj4\n</pre> #Aligns by index  obj3 + obj4 Out[\u00a0]: <pre>California         NaN\nOhio           70000.0\nOregon         32000.0\nTexas         142000.0\nUtah               NaN\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n            'year': [2000, 2001, 2002, 2001, 2002, 2003],\n            'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\nframe = pd.DataFrame(data)\nframe\n</pre> data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],             'year': [2000, 2001, 2002, 2001, 2002, 2003],             'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]} frame = pd.DataFrame(data) frame Out[\u00a0]: state year pop 0 Ohio 2000 1.5 1 Ohio 2001 1.7 2 Ohio 2002 3.6 3 Nevada 2001 2.4 4 Nevada 2002 2.9 5 Nevada 2003 3.2 In\u00a0[\u00a0]: Copied! <pre>frame.head() #Get some top rows\n</pre> frame.head() #Get some top rows Out[\u00a0]: state year pop 0 Ohio 2000 1.5 1 Ohio 2001 1.7 2 Ohio 2002 3.6 3 Nevada 2001 2.4 4 Nevada 2002 2.9 In\u00a0[\u00a0]: Copied! <pre>#Re-arrange columns\npd.DataFrame(data, columns=['year', 'state', 'pop'])\n</pre> #Re-arrange columns pd.DataFrame(data, columns=['year', 'state', 'pop']) Out[\u00a0]: year state pop 0 2000 Ohio 1.5 1 2001 Ohio 1.7 2 2002 Ohio 3.6 3 2001 Nevada 2.4 4 2002 Nevada 2.9 5 2003 Nevada 3.2 In\u00a0[\u00a0]: Copied! <pre>frame2 = pd.DataFrame(data, columns=['year', 'state', 'pop', 'debt'],\n                      index=['one', 'two', 'three', 'four', 'five', 'six'])\nframe2\n</pre> frame2 = pd.DataFrame(data, columns=['year', 'state', 'pop', 'debt'],                       index=['one', 'two', 'three', 'four', 'five', 'six']) frame2 Out[\u00a0]: year state pop debt one 2000 Ohio 1.5 NaN two 2001 Ohio 1.7 NaN three 2002 Ohio 3.6 NaN four 2001 Nevada 2.4 NaN five 2002 Nevada 2.9 NaN six 2003 Nevada 3.2 NaN In\u00a0[\u00a0]: Copied! <pre>#Get one column\nframe2[\"state\"]\n</pre> #Get one column frame2[\"state\"]  Out[\u00a0]: <pre>one        Ohio\ntwo        Ohio\nthree      Ohio\nfour     Nevada\nfive     Nevada\nsix      Nevada\nName: state, dtype: object</pre> In\u00a0[\u00a0]: Copied! <pre>frame2.year\n</pre> frame2.year Out[\u00a0]: <pre>one      2000\ntwo      2001\nthree    2002\nfour     2001\nfive     2002\nsix      2003\nName: year, dtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>#Get a row\nframe2.loc['three']\n</pre> #Get a row frame2.loc['three'] Out[\u00a0]: <pre>year     2002\nstate    Ohio\npop       3.6\ndebt      NaN\nName: three, dtype: object</pre> In\u00a0[\u00a0]: Copied! <pre>#Assign values to a column\nframe2['debt'] = 16.5\nframe2\n</pre> #Assign values to a column frame2['debt'] = 16.5 frame2 Out[\u00a0]: year state pop debt one 2000 Ohio 1.5 16.5 two 2001 Ohio 1.7 16.5 three 2002 Ohio 3.6 16.5 four 2001 Nevada 2.4 16.5 five 2002 Nevada 2.9 16.5 six 2003 Nevada 3.2 16.5 In\u00a0[\u00a0]: Copied! <pre>frame2['debt'] = np.arange(6.) # The lens must match\nframe2\n</pre> frame2['debt'] = np.arange(6.) # The lens must match frame2 Out[\u00a0]: year state pop debt one 2000 Ohio 1.5 0.0 two 2001 Ohio 1.7 1.0 three 2002 Ohio 3.6 2.0 four 2001 Nevada 2.4 3.0 five 2002 Nevada 2.9 4.0 six 2003 Nevada 3.2 5.0 In\u00a0[\u00a0]: Copied! <pre>val = pd.Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])\n\n\nframe2['debt'] = val\nframe2\n</pre> val = pd.Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])   frame2['debt'] = val frame2  Out[\u00a0]: year state pop debt one 2000 Ohio 1.5 NaN two 2001 Ohio 1.7 -1.2 three 2002 Ohio 3.6 NaN four 2001 Nevada 2.4 -1.5 five 2002 Nevada 2.9 -1.7 six 2003 Nevada 3.2 NaN In\u00a0[\u00a0]: Copied! <pre>frame2['eastern'] = frame2.state == 'Ohio'\nframe2 #????\n</pre> frame2['eastern'] = frame2.state == 'Ohio' frame2 #???? In\u00a0[\u00a0]: Copied! <pre>del frame2['eastern']\nframe2\n</pre> del frame2['eastern'] frame2 Out[\u00a0]: year state pop debt one 2000 Ohio 1.5 0.0 two 2001 Ohio 1.7 1.0 three 2002 Ohio 3.6 2.0 four 2001 Nevada 2.4 3.0 five 2002 Nevada 2.9 4.0 six 2003 Nevada 3.2 5.0 In\u00a0[\u00a0]: Copied! <pre>#Create a dataframe via a nested dict of dicts\npop = {'Nevada': {2001: 2.4, 2002: 2.9},\n       'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}\n\nframe3 = pd.DataFrame(pop)\nframe3\n</pre> #Create a dataframe via a nested dict of dicts pop = {'Nevada': {2001: 2.4, 2002: 2.9},        'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}  frame3 = pd.DataFrame(pop) frame3 Out[\u00a0]: Nevada Ohio 2001 2.4 1.7 2002 2.9 3.6 2000 NaN 1.5 In\u00a0[\u00a0]: Copied! <pre>pd.DataFrame(pop, index=[2001, 2002, 2003])\n</pre> pd.DataFrame(pop, index=[2001, 2002, 2003]) Out[\u00a0]: Nevada Ohio 2001 2.4 1.7 2002 2.9 3.6 2003 NaN NaN In\u00a0[\u00a0]: Copied! <pre>frame3.T\n</pre> frame3.T Out[\u00a0]: 2001 2002 2000 Nevada 2.4 2.9 NaN Ohio 1.7 3.6 1.5 In\u00a0[\u00a0]: Copied! <pre>frame3.values #Get numpy array from dataframe\n</pre> frame3.values #Get numpy array from dataframe Out[\u00a0]: <pre>array([[2.4, 1.7],\n       [2.9, 3.6],\n       [nan, 1.5]])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>#Reindexing\nobj = pd.Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])\nobj\n</pre> #Reindexing obj = pd.Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c']) obj Out[\u00a0]: <pre>d    4.5\nb    7.2\na   -5.3\nc    3.6\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])\nobj2\n</pre> obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e']) obj2 Out[\u00a0]: <pre>a   -5.3\nb    7.2\nc    3.6\nd    4.5\ne    NaN\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>#Reindex with  'ffill' \nobj3 = pd.Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])\nobj3\n</pre> #Reindex with  'ffill'  obj3 = pd.Series(['blue', 'purple', 'yellow'], index=[0, 2, 4]) obj3 Out[\u00a0]: <pre>0      blue\n2    purple\n4    yellow\ndtype: object</pre> In\u00a0[\u00a0]: Copied! <pre>obj3.reindex(range(6), method='ffill') #Explain?\n</pre> obj3.reindex(range(6), method='ffill') #Explain? Out[\u00a0]: <pre>0      blue\n1      blue\n2    purple\n3    purple\n4    yellow\n5    yellow\ndtype: object</pre> In\u00a0[\u00a0]: Copied! <pre>frame = pd.DataFrame(np.arange(9).reshape((3, 3)),\n                     index=['a', 'c', 'd'],\n                     columns=['Ohio', 'Texas', 'California'])\nframe #?\n</pre> frame = pd.DataFrame(np.arange(9).reshape((3, 3)),                      index=['a', 'c', 'd'],                      columns=['Ohio', 'Texas', 'California']) frame #? Out[\u00a0]: Ohio Texas California a 0 1 2 c 3 4 5 d 6 7 8 In\u00a0[\u00a0]: Copied! <pre>frame2 = frame.reindex(['a', 'b', 'c', 'd'])\nframe2 #?\n</pre> frame2 = frame.reindex(['a', 'b', 'c', 'd']) frame2 #? Out[\u00a0]: Ohio Texas California a 0.0 1.0 2.0 b NaN NaN NaN c 3.0 4.0 5.0 d 6.0 7.0 8.0 In\u00a0[\u00a0]: Copied! <pre>#Reindex with columns\nstates = ['Texas', 'Utah', 'California']\nframe.reindex(columns=states)\n</pre> #Reindex with columns states = ['Texas', 'Utah', 'California'] frame.reindex(columns=states)   Out[\u00a0]: Texas Utah California a 1 NaN 2 c 4 NaN 5 d 7 NaN 8 In\u00a0[\u00a0]: Copied! <pre>#Dropping Entries from an Axis\nobj = pd.Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])\nobj\n</pre> #Dropping Entries from an Axis obj = pd.Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e']) obj Out[\u00a0]: <pre>a    0.0\nb    1.0\nc    2.0\nd    3.0\ne    4.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>new_obj = obj.drop('c')\nnew_obj\n</pre> new_obj = obj.drop('c') new_obj Out[\u00a0]: <pre>a    0.0\nb    1.0\nd    3.0\ne    4.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>obj.drop(['d', 'c'])\n</pre> obj.drop(['d', 'c']) Out[\u00a0]: <pre>a    0.0\nb    1.0\ne    4.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>data = pd.DataFrame(np.arange(16).reshape((4, 4)),\n                    index=['Ohio', 'Colorado', 'Utah', 'New York'],\n                    columns=['one', 'two', 'three', 'four'])\ndata\n</pre> data = pd.DataFrame(np.arange(16).reshape((4, 4)),                     index=['Ohio', 'Colorado', 'Utah', 'New York'],                     columns=['one', 'two', 'three', 'four']) data Out[\u00a0]: one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 In\u00a0[\u00a0]: Copied! <pre>data.drop(['Colorado', 'Ohio'])\n</pre> data.drop(['Colorado', 'Ohio']) Out[\u00a0]: one two three four Utah 8 9 10 11 New York 12 13 14 15 In\u00a0[\u00a0]: Copied! <pre>data.drop('two', axis=1) #or axis='columns'\n</pre> data.drop('two', axis=1) #or axis='columns' Out[\u00a0]: one three four Ohio 0 2 3 Colorado 4 6 7 Utah 8 10 11 New York 12 14 15 In\u00a0[\u00a0]: Copied! <pre>#Index, Selection and Filtering\nobj = pd.Series(np.arange(4.), index=['a', 'b', 'c', 'd'])\n\nobj\n</pre> #Index, Selection and Filtering obj = pd.Series(np.arange(4.), index=['a', 'b', 'c', 'd'])  obj Out[\u00a0]: <pre>a    0.0\nb    1.0\nc    2.0\nd    3.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>obj['b']\n</pre> obj['b'] Out[\u00a0]: <pre>1.0</pre> In\u00a0[\u00a0]: Copied! <pre>obj[2:4]\n</pre> obj[2:4] Out[\u00a0]: <pre>c    2.0\nd    3.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>obj[['b', 'a', 'd']]\n</pre> obj[['b', 'a', 'd']] Out[\u00a0]: <pre>b    1.0\na    0.0\nd    3.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>obj[[1, 3]]\n</pre> obj[[1, 3]] Out[\u00a0]: <pre>b    1.0\nd    3.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>obj[obj &lt; 2]\n</pre> obj[obj &lt; 2] Out[\u00a0]: <pre>a    0.0\nb    1.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>obj['b':'c'] #endpoint included\n</pre> obj['b':'c'] #endpoint included Out[\u00a0]: <pre>b    1.0\nc    2.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>obj['b':'c'] = 5\nobj\n</pre> obj['b':'c'] = 5 obj Out[\u00a0]: <pre>a    0.0\nb    5.0\nc    5.0\nd    3.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>#for dataframe\ndata = pd.DataFrame(np.arange(16).reshape((4, 4)),\n                    index=['Ohio', 'Colorado', 'Utah', 'New York'],\n                    columns=['one', 'two', 'three', 'four'])\n\ndata\n</pre> #for dataframe data = pd.DataFrame(np.arange(16).reshape((4, 4)),                     index=['Ohio', 'Colorado', 'Utah', 'New York'],                     columns=['one', 'two', 'three', 'four'])  data Out[\u00a0]: one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 In\u00a0[\u00a0]: Copied! <pre>data['two']\n</pre> data['two'] Out[\u00a0]: <pre>Ohio         1\nColorado     5\nUtah         9\nNew York    13\nName: two, dtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>data[['three', 'one']]\n</pre> data[['three', 'one']] Out[\u00a0]: three one Ohio 2 0 Colorado 6 4 Utah 10 8 New York 14 12 In\u00a0[\u00a0]: Copied! <pre>data[:2]\n</pre> data[:2] Out[\u00a0]: one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 In\u00a0[\u00a0]: Copied! <pre>data[data['three'] &gt; 5]\n</pre> data[data['three'] &gt; 5] Out[\u00a0]: one two three four Colorado 4 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 In\u00a0[\u00a0]: Copied! <pre>data[data &lt; 5] = 0\ndata\n</pre> data[data &lt; 5] = 0 data Out[\u00a0]: one two three four Ohio 0 0 0 0 Colorado 0 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 In\u00a0[\u00a0]: Copied! <pre>#Select with loc and iloc\n#loc for labels, iloc for integers\n\ndata.loc['Colorado', ['two', 'three']]\n</pre> #Select with loc and iloc #loc for labels, iloc for integers  data.loc['Colorado', ['two', 'three']]  Out[\u00a0]: <pre>two      5\nthree    6\nName: Colorado, dtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>data.iloc[2, [3, 0, 1]]\n</pre> data.iloc[2, [3, 0, 1]] Out[\u00a0]: <pre>four    11\none      8\ntwo      9\nName: Utah, dtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>data.iloc[2]\n</pre> data.iloc[2] Out[\u00a0]: <pre>one       8\ntwo       9\nthree    10\nfour     11\nName: Utah, dtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>data.iloc[[1, 2], [3, 0, 1]]\n</pre> data.iloc[[1, 2], [3, 0, 1]] Out[\u00a0]: four one two Colorado 7 0 5 Utah 11 8 9 In\u00a0[\u00a0]: Copied! <pre>data.loc[:'Utah', 'two']\n</pre> data.loc[:'Utah', 'two'] Out[\u00a0]: <pre>Ohio        0\nColorado    5\nUtah        9\nName: two, dtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>data.iloc[:, :3][data.three &gt; 5] #?\n</pre> data.iloc[:, :3][data.three &gt; 5] #? In\u00a0[\u00a0]: Copied! <pre>#Integer Indexes\nser2 = pd.Series(np.arange(3.), index=['a', 'b', 'c'])\nser2[-1]\n</pre> #Integer Indexes ser2 = pd.Series(np.arange(3.), index=['a', 'b', 'c']) ser2[-1] Out[\u00a0]: <pre>2.0</pre> In\u00a0[\u00a0]: Copied! <pre>ser2\n</pre> ser2 Out[\u00a0]: <pre>a    0.0\nb    1.0\nc    2.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>#Integer Indexes\nser = pd.Series(np.arange(3.))\nser\n</pre> #Integer Indexes ser = pd.Series(np.arange(3.)) ser Out[\u00a0]: <pre>0    0.0\n1    1.0\n2    2.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>ser[-1]\n</pre> ser[-1] <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py in get_loc(self, key, method, tolerance)\n    384                 try:\n--&gt; 385                     return self._range.index(new_key)\n    386                 except ValueError as err:\n\nValueError: -1 is not in range\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-54-44969a759c20&gt; in &lt;module&gt;()\n----&gt; 1 ser[-1]\n\n/usr/local/lib/python3.7/dist-packages/pandas/core/series.py in __getitem__(self, key)\n    940 \n    941         elif key_is_scalar:\n--&gt; 942             return self._get_value(key)\n    943 \n    944         if is_hashable(key):\n\n/usr/local/lib/python3.7/dist-packages/pandas/core/series.py in _get_value(self, label, takeable)\n   1049 \n   1050         # Similar to Index.get_value, but we do not fall back to positional\n-&gt; 1051         loc = self.index.get_loc(label)\n   1052         return self.index._get_values_for_loc(self, loc, label)\n   1053 \n\n/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py in get_loc(self, key, method, tolerance)\n    385                     return self._range.index(new_key)\n    386                 except ValueError as err:\n--&gt; 387                     raise KeyError(key) from err\n    388             raise KeyError(key)\n    389         return super().get_loc(key, method=method, tolerance=tolerance)\n\nKeyError: -1</pre> In\u00a0[\u00a0]: Copied! <pre>ser[:1]\n</pre> ser[:1] Out[\u00a0]: <pre>0    0.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>ser.loc[:1]\n</pre> ser.loc[:1] Out[\u00a0]: <pre>0    0.0\n1    1.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>ser.iloc[:1]\n</pre> ser.iloc[:1] Out[\u00a0]: <pre>0    0.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])\ns2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1],index=['a', 'c', 'e', 'f', 'g']) \ns1 + s2  \n</pre> s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e']) s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1],index=['a', 'c', 'e', 'f', 'g'])  s1 + s2   Out[\u00a0]: <pre>a    5.2\nc    1.1\nd    NaN\ne    0.0\nf    NaN\ng    NaN\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>df1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=['b','c','d'],\n                   index=['Ohio', 'Texas', 'Colorado'])\ndf2 = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=['b','d','e'],\n                   index=['Utah', 'Ohio', 'Texas', 'Oregon'])\ndf1 + df2\n</pre> df1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=['b','c','d'],                    index=['Ohio', 'Texas', 'Colorado']) df2 = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=['b','d','e'],                    index=['Utah', 'Ohio', 'Texas', 'Oregon']) df1 + df2 Out[\u00a0]: b c d e Colorado NaN NaN NaN NaN Ohio 3.0 NaN 6.0 NaN Oregon NaN NaN NaN NaN Texas 9.0 NaN 12.0 NaN Utah NaN NaN NaN NaN In\u00a0[\u00a0]: Copied! <pre>#Arithmetic methods with fill values\ndf1 = pd.DataFrame(np.arange(12.).reshape((3, 4)),\n                   columns=list('abcd'))\ndf2 = pd.DataFrame(np.arange(20.).reshape((4, 5)),\n                   columns=list('abcde'))\n\ndf2.loc[1,'b'] = np.nan\ndf1\n</pre> #Arithmetic methods with fill values df1 = pd.DataFrame(np.arange(12.).reshape((3, 4)),                    columns=list('abcd')) df2 = pd.DataFrame(np.arange(20.).reshape((4, 5)),                    columns=list('abcde'))  df2.loc[1,'b'] = np.nan df1 Out[\u00a0]: a b c d 0 0.0 1.0 2.0 3.0 1 4.0 5.0 6.0 7.0 2 8.0 9.0 10.0 11.0 In\u00a0[\u00a0]: Copied! <pre>df2\n</pre> df2 Out[\u00a0]: a b c d e 0 0.0 1.0 2.0 3.0 4.0 1 5.0 NaN 7.0 8.0 9.0 2 10.0 11.0 12.0 13.0 14.0 3 15.0 16.0 17.0 18.0 19.0 In\u00a0[\u00a0]: Copied! <pre>df1 + df2\n</pre> df1 + df2 Out[\u00a0]: a b c d e 0 0.0 2.0 4.0 6.0 NaN 1 9.0 NaN 13.0 15.0 NaN 2 18.0 20.0 22.0 24.0 NaN 3 NaN NaN NaN NaN NaN In\u00a0[\u00a0]: Copied! <pre>df1.add(df2,fill_value=0)\n</pre> df1.add(df2,fill_value=0) Out[\u00a0]: a b c d e 0 0.0 2.0 4.0 6.0 4.0 1 9.0 5.0 13.0 15.0 9.0 2 18.0 20.0 22.0 24.0 14.0 3 15.0 16.0 17.0 18.0 19.0 In\u00a0[\u00a0]: Copied! <pre>#Operations between DataFrame and Series\nframe = pd.DataFrame(np.arange(12.).reshape((4, 3)),\n                     columns=list('bde'),\n                     index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n\nseries = frame.iloc[0]\nframe\n</pre> #Operations between DataFrame and Series frame = pd.DataFrame(np.arange(12.).reshape((4, 3)),                      columns=list('bde'),                      index=['Utah', 'Ohio', 'Texas', 'Oregon'])  series = frame.iloc[0] frame Out[\u00a0]: b d e Utah 0.0 1.0 2.0 Ohio 3.0 4.0 5.0 Texas 6.0 7.0 8.0 Oregon 9.0 10.0 11.0 In\u00a0[\u00a0]: Copied! <pre>series\n</pre> series Out[\u00a0]: <pre>b    0.0\nd    1.0\ne    2.0\nName: Utah, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>frame - series\n</pre> frame - series Out[\u00a0]: b d e Utah 0.0 0.0 0.0 Ohio 3.0 3.0 3.0 Texas 6.0 6.0 6.0 Oregon 9.0 9.0 9.0 In\u00a0[\u00a0]: Copied! <pre>series2 = pd.Series(range(3), index=['b', 'e', 'f'])\nframe + series2\n</pre> series2 = pd.Series(range(3), index=['b', 'e', 'f']) frame + series2 Out[\u00a0]: b d e f Utah 0.0 NaN 3.0 NaN Ohio 3.0 NaN 6.0 NaN Texas 6.0 NaN 9.0 NaN Oregon 9.0 NaN 12.0 NaN In\u00a0[\u00a0]: Copied! <pre>series3 = frame['d']\nseries3\n</pre> series3 = frame['d'] series3 Out[\u00a0]: <pre>Utah       1.0\nOhio       4.0\nTexas      7.0\nOregon    10.0\nName: d, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>frame-series3\n</pre> frame-series3 Out[\u00a0]: Ohio Oregon Texas Utah b d e Utah NaN NaN NaN NaN NaN NaN NaN Ohio NaN NaN NaN NaN NaN NaN NaN Texas NaN NaN NaN NaN NaN NaN NaN Oregon NaN NaN NaN NaN NaN NaN NaN In\u00a0[\u00a0]: Copied! <pre>frame.sub(series3, axis='index')\n</pre> frame.sub(series3, axis='index') Out[\u00a0]: b d e Utah -1.0 0.0 1.0 Ohio -1.0 0.0 1.0 Texas -1.0 0.0 1.0 Oregon -1.0 0.0 1.0 In\u00a0[\u00a0]: Copied! <pre>#Function Application and Mapping\n\nframe = pd.DataFrame(np.random.randn(4, 3), columns=list('bde'),\n                     index=['Utah', 'Ohio', 'Texas', 'Oregon'])\nframe\n</pre> #Function Application and Mapping  frame = pd.DataFrame(np.random.randn(4, 3), columns=list('bde'),                      index=['Utah', 'Ohio', 'Texas', 'Oregon']) frame Out[\u00a0]: b d e Utah 0.212948 0.058307 -0.783999 Ohio -1.435137 0.733396 -0.794111 Texas -0.951841 0.448788 -0.635094 Oregon -0.444094 -0.497117 2.247839 In\u00a0[\u00a0]: Copied! <pre>np.abs(frame)\n</pre> np.abs(frame) Out[\u00a0]: b d e Utah 0.212948 0.058307 0.783999 Ohio 1.435137 0.733396 0.794111 Texas 0.951841 0.448788 0.635094 Oregon 0.444094 0.497117 2.247839 In\u00a0[\u00a0]: Copied! <pre>#Apply to row\nf = lambda x: x.max()-x.min()\nframe.apply(f)\n</pre> #Apply to row f = lambda x: x.max()-x.min() frame.apply(f) Out[\u00a0]: <pre>b    1.648085\nd    1.230513\ne    3.041950\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>#Apply to column\nframe.apply(f,axis='columns')\n</pre> #Apply to column frame.apply(f,axis='columns') Out[\u00a0]: <pre>Utah      0.996947\nOhio      2.168533\nTexas     1.400629\nOregon    2.744956\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>def f(x):\n  return pd.Series([x.min(),x.max()],index=['min','max'])\n\nframe.apply(f)\n</pre> def f(x):   return pd.Series([x.min(),x.max()],index=['min','max'])  frame.apply(f) Out[\u00a0]: b d e min -1.435137 -0.497117 -0.794111 max 0.212948 0.733396 2.247839 In\u00a0[\u00a0]: Copied! <pre>#Apply to each element\nformat = lambda x: '%.2f' % x\nframe.applymap(format)\n</pre> #Apply to each element format = lambda x: '%.2f' % x frame.applymap(format)   Out[\u00a0]: b d e Utah 0.21 0.06 -0.78 Ohio -1.44 0.73 -0.79 Texas -0.95 0.45 -0.64 Oregon -0.44 -0.50 2.25 In\u00a0[\u00a0]: Copied! <pre>#Sort and Ranking\nobj = pd.Series(range(4), index=['d', 'a', 'b', 'c'])\nobj.sort_index()\n</pre> #Sort and Ranking obj = pd.Series(range(4), index=['d', 'a', 'b', 'c']) obj.sort_index() Out[\u00a0]: <pre>a    1\nb    2\nc    3\nd    0\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>frame = pd.DataFrame(np.arange(8).reshape((2, 4)),\n                     index=['three', 'one'],\n                     columns=['d', 'a', 'b', 'c'])\n\nframe.sort_index()\n</pre> frame = pd.DataFrame(np.arange(8).reshape((2, 4)),                      index=['three', 'one'],                      columns=['d', 'a', 'b', 'c'])  frame.sort_index() Out[\u00a0]: d a b c one 4 5 6 7 three 0 1 2 3 In\u00a0[\u00a0]: Copied! <pre>frame.sort_index(axis=1)\n</pre> frame.sort_index(axis=1) Out[\u00a0]: a b c d three 1 2 3 0 one 5 6 7 4 In\u00a0[\u00a0]: Copied! <pre>#Sort by values\nobj = pd.Series([4, 7, -3, 2])\nobj.sort_values()\n</pre> #Sort by values obj = pd.Series([4, 7, -3, 2]) obj.sort_values()  Out[\u00a0]: <pre>2   -3\n3    2\n0    4\n1    7\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>frame = pd.DataFrame({'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]})\nframe.sort_values(by='b')\n</pre> frame = pd.DataFrame({'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]}) frame.sort_values(by='b') Out[\u00a0]: b a 2 -3 0 3 2 1 0 4 0 1 7 1 In\u00a0[\u00a0]: Copied! <pre>frame.sort_values(by=['a', 'b'])\n</pre> frame.sort_values(by=['a', 'b']) Out[\u00a0]: b a 2 -3 0 0 4 0 3 2 1 1 7 1 In\u00a0[\u00a0]: Copied! <pre>#Rank\nobj = pd.Series([7, -5, 7, 4, 2, 0, 4])\nobj.rank()\n</pre> #Rank obj = pd.Series([7, -5, 7, 4, 2, 0, 4]) obj.rank() Out[\u00a0]: <pre>0    6.5\n1    1.0\n2    6.5\n3    4.5\n4    3.0\n5    2.0\n6    4.5\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>obj.rank(method='first')\n</pre> obj.rank(method='first') Out[\u00a0]: <pre>0    6.0\n1    1.0\n2    7.0\n3    4.0\n4    3.0\n5    2.0\n6    5.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>frame = pd.DataFrame({'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1],\n                      'c': [-2, 5, 8, -2.5]})\nframe\n</pre> frame = pd.DataFrame({'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1],                       'c': [-2, 5, 8, -2.5]}) frame Out[\u00a0]: b a c 0 4.3 0 -2.0 1 7.0 1 5.0 2 -3.0 0 8.0 3 2.0 1 -2.5 In\u00a0[\u00a0]: Copied! <pre>frame.rank(axis='columns')\n</pre> frame.rank(axis='columns') Out[\u00a0]: b a c 0 3.0 2.0 1.0 1 3.0 1.0 2.0 2 1.0 2.0 3.0 3 3.0 2.0 1.0 In\u00a0[\u00a0]: Copied! <pre>frame.describe()\n</pre> frame.describe() Out[\u00a0]: b a c count 4.000000 4.00000 4.000000 mean 2.575000 0.50000 2.125000 std 4.241364 0.57735 5.202163 min -3.000000 0.00000 -2.500000 25% 0.750000 0.00000 -2.125000 50% 3.150000 0.50000 1.500000 75% 4.975000 1.00000 5.750000 max 7.000000 1.00000 8.000000 In\u00a0[3]: Copied! <pre>#unique values, Value counts, and Membership\nobj = pd.Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])\nuniques = obj.unique()\nuniques\n</pre> #unique values, Value counts, and Membership obj = pd.Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c']) uniques = obj.unique() uniques Out[3]: <pre>array(['c', 'a', 'd', 'b'], dtype=object)</pre> In\u00a0[4]: Copied! <pre>obj.value_counts()\n</pre> obj.value_counts() Out[4]: <pre>c    3\na    3\nb    2\nd    1\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>obj.isin(['b', 'c'])\n</pre> obj.isin(['b', 'c']) Out[\u00a0]: <pre>0     True\n1    False\n2    False\n3    False\n4    False\n5     True\n6     True\n7     True\n8     True\ndtype: bool</pre> In\u00a0[\u00a0]: Copied! <pre>obj[obj.isin(['b', 'c'])]\n</pre> obj[obj.isin(['b', 'c'])] Out[\u00a0]: <pre>0    c\n5    b\n6    b\n7    c\n8    c\ndtype: object</pre> In\u00a0[\u00a0]: Copied! <pre>data = pd.DataFrame({'Qu1': [1, 3, 4, 3, 4],\n                     'Qu2': [2, 3, 1, 2, 3],\n                     'Qu3': [1, 5, 2, 4, 4]})\n\nresult = data.apply(pd.value_counts).fillna(0)\nresult\n</pre> data = pd.DataFrame({'Qu1': [1, 3, 4, 3, 4],                      'Qu2': [2, 3, 1, 2, 3],                      'Qu3': [1, 5, 2, 4, 4]})  result = data.apply(pd.value_counts).fillna(0) result Out[\u00a0]: Qu1 Qu2 Qu3 1 1.0 1.0 1.0 2 0.0 2.0 1.0 3 2.0 2.0 0.0 4 2.0 0.0 2.0 5 0.0 0.0 1.0"},{"location":"Learning/Python/Pandas/#pandas-contains-data-structures-and-data-manipulation-tools-designed-to-make-data-cleaning-and-analysis-fast-and-easy-in-python","title":"Pandas contains data structures and data manipulation tools designed to make data cleaning and analysis fast and easy in Python\u00b6","text":""},{"location":"Learning/Python/Pandas/#series","title":"Series\u00b6","text":""},{"location":"Learning/Python/Pandas/#dataframe","title":"DataFrame\u00b6","text":"<p>DataFrame represents a rectangular table of data</p>"},{"location":"Learning/Python/Pandas/#essential-functionality","title":"Essential Functionality\u00b6","text":""},{"location":"Learning/Python/Pandas/#arithmetic-and-data-alignment","title":"Arithmetic and Data Alignment\u00b6","text":"<p>The behavior of arithmetic between objects with different indexes.</p>"},{"location":"Learning/Python/Pandas/#ex-given-a-following-data-frame","title":"Ex. Given a following data frame:\u00b6","text":"<p>data = pd.DataFrame({ 'Qu1': [1, 3, 4, 3, 4],</p> <p>'Qu2': [2, 3, 1, 2, 3],</p> <p>'Qu3': [1, 5, 2, 4, 4]})</p> <p>Compute a histogram on multiple related columns.</p> <p>Expected result: </p>"},{"location":"Learning/Python/Pandas2/","title":"Writing Data to Text Format","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('ex1.csv') #sep=','\ndf\n</pre> df = pd.read_csv('ex1.csv') #sep=',' df Out[\u00a0]: a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo In\u00a0[\u00a0]: Copied! <pre>pd.read_csv('ex2.csv')\n</pre> pd.read_csv('ex2.csv') Out[\u00a0]: 1 2 3 4 hello 0 5 6 7 8 world 1 9 10 11 12 foo In\u00a0[\u00a0]: Copied! <pre>pd.read_csv('ex2.csv', names=['a', 'b', 'c', 'd', 'message'])\n</pre> pd.read_csv('ex2.csv', names=['a', 'b', 'c', 'd', 'message']) Out[\u00a0]: a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo In\u00a0[\u00a0]: Copied! <pre>pd.read_csv('ex2.csv', names=['a', 'b', 'c', 'd', 'message'],index_col='message')\n</pre> pd.read_csv('ex2.csv', names=['a', 'b', 'c', 'd', 'message'],index_col='message') Out[\u00a0]: a b c d message hello 1 2 3 4 world 5 6 7 8 foo 9 10 11 12 In\u00a0[\u00a0]: Copied! <pre>#hierarchical index\nparsed = pd.read_csv('csv_mindex.csv',\n                     index_col=['key1', 'key2'])\nparsed\n</pre> #hierarchical index parsed = pd.read_csv('csv_mindex.csv',                      index_col=['key1', 'key2']) parsed Out[\u00a0]: value1 value2 key1 key2 one a 1 2 b 3 4 c 5 6 d 7 8 two a 9 10 b 11 12 c 13 14 d 15 16 In\u00a0[\u00a0]: Copied! <pre>result = pd.read_table('ex3.txt', sep='\\s+')\nresult\n</pre> result = pd.read_table('ex3.txt', sep='\\s+') result Out[\u00a0]: A B C aaa -0.264438 -1.026059 -0.619500 bbb 0.927272 0.302904 -0.032399 ccc -0.264273 -0.386314 -0.217601 ddd -0.871858 -0.348382 1.100491 In\u00a0[\u00a0]: Copied! <pre>pd.read_csv('ex4.csv', skiprows=[0, 2, 3])\n</pre> pd.read_csv('ex4.csv', skiprows=[0, 2, 3]) Out[\u00a0]: a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo In\u00a0[\u00a0]: Copied! <pre>result = pd.read_csv('ex5.csv')\nresult\n</pre> result = pd.read_csv('ex5.csv') result Out[\u00a0]: something a b c d message 0 one 1 2 3.0 4 NaN 1 two 5 6 NaN 8 world 2 three 9 10 11.0 12 foo In\u00a0[\u00a0]: Copied! <pre>pd.isnull(result)\n</pre> pd.isnull(result) Out[\u00a0]: something a b c d message 0 False False False False False True 1 False False False True False False 2 False False False False False False In\u00a0[\u00a0]: Copied! <pre>#How to show rows which having at least one null value ???\n</pre> #How to show rows which having at least one null value ??? In\u00a0[\u00a0]: Copied! <pre>#Set NA sentinels\nsentinels = {'message': ['foo'], 'something': ['two']]}\npd.read_csv('ex5.csv', na_values=sentinels)\n</pre> #Set NA sentinels sentinels = {'message': ['foo'], 'something': ['two']]} pd.read_csv('ex5.csv', na_values=sentinels)  Out[\u00a0]: something a b c d message 0 one 1 2 3.0 4 NaN 1 NaN 5 6 NaN 8 world 2 three 9 10 11.0 12 NaN In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\na = np.random.rand(10000,4)\na\n\nkey = np.array(['L','B','G','R','Q','H','E'])\nb = np.random.randint(0,6,(10000,))\n\nwith open(\"ex6.txt\",\"w\") as f:\n  for i in range(len(b)):\n    f.write('\\t'.join([\"%.2f\" % number for number in a[i]]) + '\\t' + key[b][i]+\"\\n\")\n</pre> import numpy as np  a = np.random.rand(10000,4) a  key = np.array(['L','B','G','R','Q','H','E']) b = np.random.randint(0,6,(10000,))  with open(\"ex6.txt\",\"w\") as f:   for i in range(len(b)):     f.write('\\t'.join([\"%.2f\" % number for number in a[i]]) + '\\t' + key[b][i]+\"\\n\") In\u00a0[\u00a0]: Copied! <pre>pd.read_csv('ex6.csv', nrows=5, sep=\"\\t\")\n</pre> pd.read_csv('ex6.csv', nrows=5, sep=\"\\t\") Out[\u00a0]: one two three four key 0 0.94 0.53 0.68 0.33 Q 1 0.20 0.41 0.70 0.26 L 2 0.19 0.44 0.51 0.95 R 3 0.06 0.09 0.66 0.31 Q 4 0.74 0.61 0.05 0.90 G In\u00a0[\u00a0]: Copied! <pre>chunker = pd.read_csv('ex6.csv', chunksize=1000,sep=\"\\t\")\nchunker\n</pre> chunker = pd.read_csv('ex6.csv', chunksize=1000,sep=\"\\t\") chunker Out[\u00a0]: <pre>&lt;pandas.io.parsers.readers.TextFileReader at 0x7f30cf8f99d0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>tot = pd.Series([])\nfor piece in chunker:\n  tot = tot.add(piece['key'].value_counts(), fill_value=0)\ntot = tot.sort_values(ascending=False)\ntot\n</pre> tot = pd.Series([]) for piece in chunker:   tot = tot.add(piece['key'].value_counts(), fill_value=0) tot = tot.sort_values(ascending=False) tot <pre>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n  \"\"\"Entry point for launching an IPython kernel.\n</pre> Out[\u00a0]: <pre>H    1703.0\nQ    1681.0\nB    1676.0\nG    1653.0\nL    1647.0\nR    1640.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>data = pd.read_csv('ex5.csv')\ndata\n</pre> data = pd.read_csv('ex5.csv') data Out[\u00a0]: something a b c d message 0 one 1 2 3.0 4 NaN 1 two 5 6 NaN 8 world 2 three 9 10 11.0 12 foo In\u00a0[\u00a0]: Copied! <pre>data.to_csv('out.csv',sep=',')\n</pre> data.to_csv('out.csv',sep=',') In\u00a0[\u00a0]: Copied! <pre>!cat out.csv\n</pre> !cat out.csv <pre>,something,a,b,c,d,message\n0,one,1,2,3.0,4,\n1,two,5,6,,8,world\n2,three,9,10,11.0,12,foo\n</pre> In\u00a0[\u00a0]: Copied! <pre>data.to_csv('out.csv',sep=',',na_rep='NA')\n</pre> data.to_csv('out.csv',sep=',',na_rep='NA') In\u00a0[\u00a0]: Copied! <pre>!cat out.csv\n</pre> !cat out.csv <pre>,something,a,b,c,d,message\n0,one,1,2,3.0,4,NA\n1,two,5,6,NA,8,world\n2,three,9,10,11.0,12,foo\n</pre> In\u00a0[\u00a0]: Copied! <pre>data.to_csv('out.csv', index=False, columns=['a', 'b', 'c'])\n!cat out.csv\n</pre> data.to_csv('out.csv', index=False, columns=['a', 'b', 'c']) !cat out.csv <pre>a,b,c\n1,2,3.0\n5,6,\n9,10,11.0\n</pre> In\u00a0[\u00a0]: Copied! <pre>obj = \"\"\"\n    {\"name\": \"Wes\",\n     \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n     \"pet\": null,\n     \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},\n                  {\"name\": \"Katie\", \"age\": 38,\n                   \"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}]\n} \"\"\"\n</pre> obj = \"\"\"     {\"name\": \"Wes\",      \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],      \"pet\": null,      \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},                   {\"name\": \"Katie\", \"age\": 38,                    \"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}] } \"\"\"  In\u00a0[\u00a0]: Copied! <pre>import json\nresult = json.loads(obj)\nresult\n</pre> import json result = json.loads(obj) result Out[\u00a0]: <pre>{'name': 'Wes',\n 'pet': None,\n 'places_lived': ['United States', 'Spain', 'Germany'],\n 'siblings': [{'age': 30, 'name': 'Scott', 'pets': ['Zeus', 'Zuko']},\n  {'age': 38, 'name': 'Katie', 'pets': ['Sixes', 'Stache', 'Cisco']}]}</pre> In\u00a0[\u00a0]: Copied! <pre>siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])\nsiblings\n</pre> siblings = pd.DataFrame(result['siblings'], columns=['name', 'age']) siblings Out[\u00a0]: name age 0 Scott 30 1 Katie 38 In\u00a0[\u00a0]: Copied! <pre>data = pd.read_json('example.json')\ndata\n</pre> data = pd.read_json('example.json') data Out[\u00a0]: a b c 0 1 2 3 1 4 5 6 2 7 8 9 In\u00a0[\u00a0]: Copied! <pre>print(data.to_json())\n</pre> print(data.to_json()) <pre>{\"a\":{\"0\":1,\"1\":4,\"2\":7},\"b\":{\"0\":2,\"1\":5,\"2\":8},\"c\":{\"0\":3,\"1\":6,\"2\":9}}\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(data.to_json(orient='records'))\n</pre> print(data.to_json(orient='records')) <pre>[{\"a\":1,\"b\":2,\"c\":3},{\"a\":4,\"b\":5,\"c\":6},{\"a\":7,\"b\":8,\"c\":9}]\n</pre> In\u00a0[4]: Copied! <pre>frame = pd.read_csv('ex1.csv')\nframe\n</pre> frame = pd.read_csv('ex1.csv') frame Out[4]: a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo In\u00a0[5]: Copied! <pre>frame.to_pickle('frame_pickle')\n</pre> frame.to_pickle('frame_pickle') In\u00a0[6]: Copied! <pre>pd.read_pickle('frame_pickle')\n</pre> pd.read_pickle('frame_pickle') Out[6]: a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo In\u00a0[\u00a0]: Copied! <pre>xlsx = pd.ExcelFile('ex1.xlsx')\npd.read_excel(xlsx, 'sheet1')\n</pre> xlsx = pd.ExcelFile('ex1.xlsx') pd.read_excel(xlsx, 'sheet1') Out[\u00a0]: a b c d message 0 1.0 2.0 3.0 4.0 hello 1 5.0 6.0 7.0 8.0 world 2 9.0 10.0 11.0 12.0 foo In\u00a0[\u00a0]: Copied! <pre>pd.read_excel('ex1.xlsx', 'sheet1')\n</pre> pd.read_excel('ex1.xlsx', 'sheet1') Out[\u00a0]: a b c d message 0 1.0 2.0 3.0 4.0 hello 1 5.0 6.0 7.0 8.0 world 2 9.0 10.0 11.0 12.0 foo In\u00a0[\u00a0]: Copied! <pre>frame.to_excel('ex2.xlsx','sheet1')\n</pre> frame.to_excel('ex2.xlsx','sheet1')"},{"location":"Learning/Python/Pandas2/#read-and-writing-data-in-text-format","title":"Read and Writing Data in Text Format\u00b6","text":""},{"location":"Learning/Python/Pandas2/#writing-data-to-text-format","title":"Writing Data to Text Format\u00b6","text":""},{"location":"Learning/Python/Pandas2/#json-data","title":"Json data\u00b6","text":""},{"location":"Learning/Python/Pandas2/#pickle","title":"Pickle\u00b6","text":"<p>To export data in binary format</p>"},{"location":"Learning/Python/Pandas2/#microsoft-excel-files","title":"Microsoft Excel Files\u00b6","text":""},{"location":"Learning/Python/Pandas_Join_Combine/","title":"Ex. How to select row with key1= \"a\" and key2 = 2","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\ndata = pd.Series(np.random.randn(9),\n                 index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'], \n                        [1,2,3,1,3,1,2,2,3]])\ndata\n</pre> import pandas as pd import numpy as np data = pd.Series(np.random.randn(9),                  index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'],                          [1,2,3,1,3,1,2,2,3]]) data Out[1]: <pre>a  1    0.066072\n   2    0.229214\n   3    0.430933\nb  1    0.092056\n   3   -1.164205\nc  1   -1.124371\n   2    0.231518\nd  2   -0.681320\n   3    1.154228\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>data['b']\n</pre> data['b'] Out[\u00a0]: <pre>1    1.257585\n3   -1.041106\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>data['b':'c']\n</pre> data['b':'c'] Out[\u00a0]: <pre>b  1    1.257585\n   3   -1.041106\nc  1    1.002684\n   2    0.752568\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>data.loc[['b', 'd']]\n</pre> data.loc[['b', 'd']] Out[\u00a0]: <pre>b  1    1.257585\n   3   -1.041106\nd  2    1.292383\n   3   -0.298235\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>data.loc[:, 2]\n</pre> data.loc[:, 2] Out[\u00a0]: <pre>a   -0.616969\nc    0.752568\nd    1.292383\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>data.unstack()\n</pre> data.unstack() Out[\u00a0]: 1 2 3 a 0.495643 -0.616969 -1.062689 b 1.257585 NaN -1.041106 c 1.002684 0.752568 NaN d NaN 1.292383 -0.298235 In\u00a0[\u00a0]: Copied! <pre>data.unstack().stack()\n</pre> data.unstack().stack() Out[\u00a0]: <pre>a  1    0.495643\n   2   -0.616969\n   3   -1.062689\nb  1    1.257585\n   3   -1.041106\nc  1    1.002684\n   2    0.752568\nd  2    1.292383\n   3   -0.298235\ndtype: float64</pre> In\u00a0[2]: Copied! <pre>frame = pd.DataFrame(np.arange(12).reshape((4, 3)),\n                     index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\n                     columns=[['Ohio', 'Ohio', 'Colorado'],\n                              ['Green', 'Red', 'Green']])\nframe\n</pre> frame = pd.DataFrame(np.arange(12).reshape((4, 3)),                      index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],                      columns=[['Ohio', 'Ohio', 'Colorado'],                               ['Green', 'Red', 'Green']]) frame Out[2]: Ohio Colorado Green Red Green a 1 0 1 2 2 3 4 5 b 1 6 7 8 2 9 10 11 In\u00a0[3]: Copied! <pre>frame.index.names = ['key1', 'key2']\nframe.columns.names = ['state', 'color']\nframe\n</pre> frame.index.names = ['key1', 'key2'] frame.columns.names = ['state', 'color'] frame  Out[3]: state Ohio Colorado color Green Red Green key1 key2 a 1 0 1 2 2 3 4 5 b 1 6 7 8 2 9 10 11 In\u00a0[\u00a0]: Copied! <pre>frame['Ohio']\n</pre> frame['Ohio']  Out[\u00a0]: color Green Red key1 key2 a 1 0 1 2 3 4 b 1 6 7 2 9 10 In\u00a0[4]: Copied! <pre>frame.loc['a',2]\n</pre> frame.loc['a',2] Out[4]: <pre>state     color\nOhio      Green    3\n          Red      4\nColorado  Green    5\nName: (a, 2), dtype: int64</pre> In\u00a0[7]: Copied! <pre>frame.iloc[1,1]\n</pre> frame.iloc[1,1] Out[7]: <pre>4</pre> In\u00a0[\u00a0]: Copied! <pre>frame.swaplevel('key1', 'key2')\n</pre> frame.swaplevel('key1', 'key2') Out[\u00a0]: state Ohio Colorado color Green Red Green key2 key1 1 a 0 1 2 2 a 3 4 5 1 b 6 7 8 2 b 9 10 11 In\u00a0[\u00a0]: Copied! <pre>df1 = pd.DataFrame({'key': ['b', 'a'],\n                    'data1': range(2)})\ndf1\n</pre> df1 = pd.DataFrame({'key': ['b', 'a'],                     'data1': range(2)}) df1 Out[\u00a0]: key data1 0 b 0 1 a 1 In\u00a0[\u00a0]: Copied! <pre>df2 = pd.DataFrame({'key': ['a', 'b', 'd'],\n                    'data2': range(3)})\ndf2\n</pre> df2 = pd.DataFrame({'key': ['a', 'b', 'd'],                     'data2': range(3)}) df2 Out[\u00a0]: key data2 0 a 0 1 b 1 2 d 2 In\u00a0[\u00a0]: Copied! <pre>pd.merge(df1, df2,on=\"key\")\n</pre> pd.merge(df1, df2,on=\"key\") Out[\u00a0]: key data1 data2 0 b 0 1 1 a 1 0 In\u00a0[\u00a0]: Copied! <pre>df1 = pd.DataFrame({'key': ['b','b', 'a'],\n                    'data1': range(3)})\ndf1\n</pre> df1 = pd.DataFrame({'key': ['b','b', 'a'],                     'data1': range(3)}) df1 Out[\u00a0]: key data1 0 b 0 1 b 1 2 a 2 In\u00a0[\u00a0]: Copied! <pre>pd.merge(df1, df2,on=\"key\")\n</pre> pd.merge(df1, df2,on=\"key\") Out[\u00a0]: key data1 data2 0 b 0 1 1 b 1 1 2 a 2 0 In\u00a0[\u00a0]: Copied! <pre>pd.merge(df1, df2,on=\"key\",how=\"right\")\n</pre> pd.merge(df1, df2,on=\"key\",how=\"right\") Out[\u00a0]: key data1 data2 0 a 2.0 0 1 b 0.0 1 2 b 1.0 1 3 d NaN 2 In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\n                   'key2' : ['one', 'two', 'one', 'two', 'one'],\n                   'data1' : np.random.randn(5),\n                   'data2' : np.random.randn(5)})\ndf\n</pre> df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],                    'key2' : ['one', 'two', 'one', 'two', 'one'],                    'data1' : np.random.randn(5),                    'data2' : np.random.randn(5)}) df Out[\u00a0]: key1 key2 data1 data2 0 a one -1.096775 -0.782163 1 a two -0.486617 -1.131576 2 b one 0.538019 -1.877360 3 b two -0.015299 1.119469 4 a one -0.306884 -0.866532 In\u00a0[\u00a0]: Copied! <pre>grouped = df['data1'].groupby(df['key1'])\ngrouped\n</pre> grouped = df['data1'].groupby(df['key1']) grouped Out[\u00a0]: <pre>&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x7fad05031b50&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>grouped.mean()\n</pre> grouped.mean() Out[\u00a0]: <pre>key1\na   -0.630092\nb    0.261360\nName: data1, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>means = df['data1'].groupby([df['key1'], df['key2']]).mean()\nmeans\n</pre> means = df['data1'].groupby([df['key1'], df['key2']]).mean() means Out[\u00a0]: <pre>key1  key2\na     one    -0.701829\n      two    -0.486617\nb     one     0.538019\n      two    -0.015299\nName: data1, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>means.unstack()\n</pre> means.unstack() Out[\u00a0]: key2 one two key1 a -0.701829 -0.486617 b 0.538019 -0.015299 In\u00a0[\u00a0]: Copied! <pre>df.groupby(['key1', 'key2']).size()\n</pre> df.groupby(['key1', 'key2']).size() Out[\u00a0]: <pre>key1  key2\na     one     2\n      two     1\nb     one     1\n      two     1\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>for name, group in df.groupby('key1'):\n  print(name)\n  print(group)\n</pre> for name, group in df.groupby('key1'):   print(name)   print(group) <pre>a\n  key1 key2     data1     data2\n0    a  one -1.096775 -0.782163\n1    a  two -0.486617 -1.131576\n4    a  one -0.306884 -0.866532\nb\n  key1 key2     data1     data2\n2    b  one  0.538019 -1.877360\n3    b  two -0.015299  1.119469\n</pre> In\u00a0[\u00a0]: Copied! <pre>for (k1, k2), group in df.groupby(['key1', 'key2']):\n  print((k1, k2))\n  print(group)\n</pre> for (k1, k2), group in df.groupby(['key1', 'key2']):   print((k1, k2))   print(group) <pre>('a', 'one')\n  key1 key2     data1     data2\n0    a  one -1.096775 -0.782163\n4    a  one -0.306884 -0.866532\n('a', 'two')\n  key1 key2     data1     data2\n1    a  two -0.486617 -1.131576\n('b', 'one')\n  key1 key2     data1    data2\n2    b  one  0.538019 -1.87736\n('b', 'two')\n  key1 key2     data1     data2\n3    b  two -0.015299  1.119469\n</pre> In\u00a0[\u00a0]: Copied! <pre>tips = pd.read_csv('tips.csv')\ntips\n</pre> tips = pd.read_csv('tips.csv') tips Out[\u00a0]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 ... ... ... ... ... ... ... ... 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 <p>244 rows \u00d7 7 columns</p> In\u00a0[\u00a0]: Copied! <pre>tips['tip_pct'] = tips['tip'] / tips['total_bill']\ntips\n</pre> tips['tip_pct'] = tips['tip'] / tips['total_bill'] tips Out[\u00a0]: total_bill tip sex smoker day time size tip_pct 0 16.99 1.01 Female No Sun Dinner 2 0.059447 1 10.34 1.66 Male No Sun Dinner 3 0.160542 2 21.01 3.50 Male No Sun Dinner 3 0.166587 3 23.68 3.31 Male No Sun Dinner 2 0.139780 4 24.59 3.61 Female No Sun Dinner 4 0.146808 ... ... ... ... ... ... ... ... ... 239 29.03 5.92 Male No Sat Dinner 3 0.203927 240 27.18 2.00 Female Yes Sat Dinner 2 0.073584 241 22.67 2.00 Male Yes Sat Dinner 2 0.088222 242 17.82 1.75 Male No Sat Dinner 2 0.098204 243 18.78 3.00 Female No Thur Dinner 2 0.159744 <p>244 rows \u00d7 8 columns</p> In\u00a0[\u00a0]: Copied! <pre>tips.groupby(['day', 'smoker'])['tip_pct'].mean()\n</pre> tips.groupby(['day', 'smoker'])['tip_pct'].mean() Out[\u00a0]: <pre>day   smoker\nFri   No        0.151650\n      Yes       0.174783\nSat   No        0.158048\n      Yes       0.147906\nSun   No        0.160113\n      Yes       0.187250\nThur  No        0.160298\n      Yes       0.163863\nName: tip_pct, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>tips.groupby(['day', 'smoker'])['tip_pct'].agg('mean')\n</pre> tips.groupby(['day', 'smoker'])['tip_pct'].agg('mean') Out[\u00a0]: <pre>day   smoker\nFri   No        0.151650\n      Yes       0.174783\nSat   No        0.158048\n      Yes       0.147906\nSun   No        0.160113\n      Yes       0.187250\nThur  No        0.160298\n      Yes       0.163863\nName: tip_pct, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>tips.groupby(['day', 'smoker'])['tip_pct'].agg(['mean','std'])\n</pre> tips.groupby(['day', 'smoker'])['tip_pct'].agg(['mean','std']) Out[\u00a0]: mean std day smoker Fri No 0.151650 0.028123 Yes 0.174783 0.051293 Sat No 0.158048 0.039767 Yes 0.147906 0.061375 Sun No 0.160113 0.042347 Yes 0.187250 0.154134 Thur No 0.160298 0.038774 Yes 0.163863 0.039389 In\u00a0[\u00a0]: Copied! <pre>def peak_to_peak(arr):\n  return arr.max() - arr.min()\ntips.groupby(['day', 'smoker'])['tip_pct'].agg(['mean','std',peak_to_peak])\n</pre> def peak_to_peak(arr):   return arr.max() - arr.min() tips.groupby(['day', 'smoker'])['tip_pct'].agg(['mean','std',peak_to_peak]) Out[\u00a0]: mean std peak_to_peak day smoker Fri No 0.151650 0.028123 0.067349 Yes 0.174783 0.051293 0.159925 Sat No 0.158048 0.039767 0.235193 Yes 0.147906 0.061375 0.290095 Sun No 0.160113 0.042347 0.193226 Yes 0.187250 0.154134 0.644685 Thur No 0.160298 0.038774 0.193350 Yes 0.163863 0.039389 0.151240 In\u00a0[\u00a0]: Copied! <pre>def top(df, n=5, column='tip_pct'):\n  return df.sort_values(by=column)[-n:]\ntop(tips, n=6)\n</pre> def top(df, n=5, column='tip_pct'):   return df.sort_values(by=column)[-n:] top(tips, n=6) Out[\u00a0]: total_bill tip sex smoker day time size tip_pct 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 232 11.61 3.39 Male No Sat Dinner 2 0.291990 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 In\u00a0[\u00a0]: Copied! <pre>tips.groupby('smoker').apply(top)\n</pre> tips.groupby('smoker').apply(top) Out[\u00a0]: total_bill tip sex smoker day time size tip_pct smoker No 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990 Yes 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 In\u00a0[\u00a0]: Copied! <pre>tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')\n</pre> tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill') Out[\u00a0]: total_bill tip sex smoker day time size tip_pct smoker day No Fri 94 22.75 3.25 Female No Fri Dinner 2 0.142857 Sat 212 48.33 9.00 Male No Sat Dinner 4 0.186220 Sun 156 48.17 5.00 Male No Sun Dinner 6 0.103799 Thur 142 41.19 5.00 Male No Thur Lunch 5 0.121389 Yes Fri 95 40.17 4.73 Male Yes Fri Dinner 4 0.117750 Sat 170 50.81 10.00 Male Yes Sat Dinner 3 0.196812 Sun 182 45.35 3.50 Male Yes Sun Dinner 3 0.077178 Thur 197 43.11 5.00 Female Yes Thur Lunch 4 0.115982"},{"location":"Learning/Python/Pandas_Join_Combine/#hierarchical-indexing","title":"Hierarchical Indexing\u00b6","text":""},{"location":"Learning/Python/Pandas_Join_Combine/#ex-how-to-select-row-with-key1-a-and-key2-2","title":"Ex. How to select row with key1= \"a\" and key2 = 2\u00b6","text":""},{"location":"Learning/Python/Pandas_Join_Combine/#swap-level","title":"Swap level\u00b6","text":""},{"location":"Learning/Python/Pandas_Join_Combine/#combine-and-merging-datasets","title":"Combine and Merging Datasets\u00b6","text":""},{"location":"Learning/Python/Pandas_Join_Combine/#data-aggregation-group","title":"Data Aggregation &amp; Group\u00b6","text":""},{"location":"Learning/Python/Pandas_Join_Combine/#data-aggregation","title":"Data Aggregation\u00b6","text":""},{"location":"Learning/Python/Pandas_Join_Combine/#iterating-over-groups","title":"Iterating Over Groups\u00b6","text":""},{"location":"Learning/Python/Pandas_Join_Combine/#column-wise-and-multiple-function-application","title":"Column-Wise and Multiple Function Application\u00b6","text":""},{"location":"Learning/Python/Pandas_Join_Combine/#apply","title":"Apply\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise1/","title":"Pandas exercise 1","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[2]: Copied! <pre>df = pd.read_csv('/content/pandas_project_data.csv')\ndf.head()\n</pre> df = pd.read_csv('/content/pandas_project_data.csv') df.head() Out[2]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms 0 20201701 1982 Graduation Single 58138.0 9/4/2012 58 8 4 0 3 11 1 20201702 1950 Graduation Married 46344.0 3/8/2014 38 1 2 0 3 11 2 20201703 1965 Graduation Divorced 71613.0 8/21/2013 26 8 10 0 3 11 3 20201704 1984 Graduation Relationship 26646.0 2/10/2014 26 2 4 0 3 11 4 20201705 1981 PhD Widowed 58293.0 1/19/2014 94 5 6 0 3 11 In\u00a0[3]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 499 entries, 0 to 498\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   customer_id        499 non-null    int64  \n 1   year_of_birth      499 non-null    int64  \n 2   educational_level  499 non-null    object \n 3   marital_status     499 non-null    object \n 4   annual_income      486 non-null    float64\n 5   purhcase_date      499 non-null    object \n 6   recency            499 non-null    int64  \n 7   online_purchases   499 non-null    int64  \n 8   store_purchases    499 non-null    int64  \n 9   complaints         499 non-null    int64  \n 10  calls              499 non-null    int64  \n 11  intercoms          499 non-null    int64  \ndtypes: float64(1), int64(8), object(3)\nmemory usage: 46.9+ KB\n</pre> In\u00a0[4]: Copied! <pre>df[df.isna().any(axis=1)] #Check null\n</pre> df[df.isna().any(axis=1)] #Check null Out[4]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms 10 20201711 1983 Graduation Single NaN 11/15/2013 11 1 2 0 3 11 27 20201728 1986 Graduation Single NaN 2/20/2013 19 27 0 4 3 11 43 20201744 1959 PhD Divorced NaN 11/5/2013 80 1 4 0 3 11 48 20201749 1951 Graduation Married NaN 1/1/2014 96 2 4 0 3 11 58 20201759 1982 Graduation Relationship NaN 6/17/2013 57 2 3 0 3 11 71 20201772 1988 High School Single NaN 9/14/2012 25 3 3 0 3 11 90 20201791 1957 PhD Relationship NaN 11/19/2012 4 7 8 0 3 7 91 20201792 1957 Graduation Married NaN 5/27/2014 45 1 2 0 3 7 92 20201793 1973 Master Relationship NaN 11/23/2013 87 2 8 0 3 7 128 20201829 1961 PhD Married NaN 7/11/2013 23 6 7 0 3 11 133 20201834 1980 Graduation Relationship NaN 8/11/2013 96 6 7 0 3 2 312 20202013 1990 Graduation Single NaN 6/3/2013 69 6 12 0 3 11 319 20202020 1997 Graduation Divorced NaN 8/23/2013 67 2 10 0 3 11 In\u00a0[\u00a0]: Copied! <pre>df.annual_income = df.annual_income.fillna(df.annual_income.mean())\ndf[df.isna().any(axis=1)] #Check null\n</pre> df.annual_income = df.annual_income.fillna(df.annual_income.mean()) df[df.isna().any(axis=1)] #Check null Out[\u00a0]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms In\u00a0[\u00a0]: Copied! <pre>df[df.annual_income &gt; 50000]\n</pre> df[df.annual_income &gt; 50000] Out[\u00a0]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms 0 20201701 1982 Graduation Single 58138.0 9/4/2012 58 8 4 0 3 11 2 20201703 1965 Graduation Divorced 71613.0 8/21/2013 26 8 10 0 3 11 4 20201705 1981 PhD Widowed 58293.0 1/19/2014 94 5 6 0 3 11 5 20201706 1967 Master Relationship 62000.0 9/9/2013 16 6 10 5 3 11 6 20201707 1971 Graduation Divorced 55635.0 11/13/2012 34 7 7 0 3 11 ... ... ... ... ... ... ... ... ... ... ... ... ... 490 20202191 1958 PhD Relationship 70991.0 9/24/2012 11 2 12 0 5 11 493 20202194 1964 Master Single 58308.0 1/12/2013 77 2 3 0 3 11 494 20202195 1944 PhD Divorced 55614.0 11/27/2013 85 9 6 0 3 11 495 20202196 1962 Master Divorced 59432.0 4/13/2013 88 5 11 0 3 11 496 20202197 1978 Graduation Divorced 55563.0 4/5/2014 22 2 3 0 3 11 <p>251 rows \u00d7 12 columns</p> In\u00a0[\u00a0]: Copied! <pre>df[(df['complaints'] &gt; 2) &amp; (df['annual_income'] &gt; 50000)][\"customer_id\"]\n</pre> df[(df['complaints'] &gt; 2) &amp; (df['annual_income'] &gt; 50000)][\"customer_id\"] Out[\u00a0]: <pre>5     20201706\n18    20201719\n22    20201723\n23    20201724\n29    20201730\n34    20201735\n45    20201746\n59    20201760\n68    20201769\n80    20201781\nName: customer_id, dtype: int64</pre> In\u00a0[6]: Copied! <pre>df['purhcase_date'] = pd.to_datetime(df.purhcase_date, infer_datetime_format=True) \n</pre> df['purhcase_date'] = pd.to_datetime(df.purhcase_date, infer_datetime_format=True)  In\u00a0[\u00a0]: Copied! <pre>df[df['purhcase_date'].dt.month==4]\n</pre> df[df['purhcase_date'].dt.month==4] In\u00a0[\u00a0]: Copied! <pre>df[df.online_purchases + df.store_purchases &gt; 5]\n</pre> df[df.online_purchases + df.store_purchases &gt; 5] Out[\u00a0]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms 0 20201701 1982 Graduation Single 58138.0 9/4/2012 58 8 4 0 3 11 2 20201703 1965 Graduation Divorced 71613.0 8/21/2013 26 8 10 0 3 11 3 20201704 1984 Graduation Relationship 26646.0 2/10/2014 26 2 4 0 3 11 4 20201705 1981 PhD Widowed 58293.0 1/19/2014 94 5 6 0 3 11 5 20201706 1967 Master Relationship 62000.0 9/9/2013 16 6 10 5 3 11 ... ... ... ... ... ... ... ... ... ... ... ... ... 492 20202193 1955 Graduation Relationship 23018.0 2/22/2013 72 3 13 0 3 11 494 20202195 1944 PhD Divorced 55614.0 11/27/2013 85 9 6 0 3 11 495 20202196 1962 Master Divorced 59432.0 4/13/2013 88 5 11 0 3 11 497 20202198 1971 PhD Relationship 43624.0 4/21/2013 83 4 4 0 6 11 498 20202199 1949 PhD Relationship 41461.0 5/22/2014 63 6 11 0 6 11 <p>345 rows \u00d7 12 columns</p> In\u00a0[\u00a0]: Copied! <pre>df[df.marital_status == 'Divorced'].mean()\n</pre> df[df.marital_status == 'Divorced'].mean() <pre>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  \"\"\"Entry point for launching an IPython kernel.\n</pre> Out[\u00a0]: <pre>customer_id         2.020195e+07\nyear_of_birth       1.977437e+03\nannual_income       5.131645e+04\nrecency             5.329885e+01\nonline_purchases    4.287356e+00\nstore_purchases     5.954023e+00\ncomplaints          1.264368e-01\ncalls               3.839080e+00\nintercoms           1.029885e+01\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>df['complaints'].value_counts()\n</pre> df['complaints'].value_counts() Out[\u00a0]: <pre>0     461\n1      14\n5       5\n3       5\n2       4\n7       3\n4       3\n6       2\n8       1\n11      1\nName: complaints, dtype: int64</pre> In\u00a0[8]: Copied! <pre>import datetime\n\ndf[\"age\"] = datetime.datetime.now().year - df[\"year_of_birth \"]\ndf[ (df[\"age\"] &gt;= 30) &amp; (df[\"age\"] &lt;= 40)].mean()[\"annual_income\"]\n</pre> import datetime  df[\"age\"] = datetime.datetime.now().year - df[\"year_of_birth \"] df[ (df[\"age\"] &gt;= 30) &amp; (df[\"age\"] &lt;= 40)].mean()[\"annual_income\"]  <pre>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n  after removing the cwd from sys.path.\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  after removing the cwd from sys.path.\n</pre> Out[8]: <pre>Index(['customer_id', 'year_of_birth ', 'educational_level', 'marital_status',\n       'annual_income', 'purhcase_date', 'recency', 'online_purchases',\n       'store_purchases', 'complaints', 'calls', 'intercoms', 'age'],\n      dtype='object')</pre> In\u00a0[\u00a0]: Copied! <pre>max = 0\nname = \"\"\nfor edu_level in df[\"educational_level\"].unique():\n  temp = df[df.educational_level == edu_level].mean()['annual_income']\n  if temp &gt; max:\n    max = temp\n    name = edu_level\nprint(name,max)\n</pre> max = 0 name = \"\" for edu_level in df[\"educational_level\"].unique():   temp = df[df.educational_level == edu_level].mean()['annual_income']   if temp &gt; max:     max = temp     name = edu_level print(name,max) <pre>PhD 55762.55712583929\n</pre> <pre>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  after removing the cwd from sys.path.\n</pre>"},{"location":"Learning/Python/Python_Pandas_exercise1/#introduction","title":"Introduction\u00b6","text":"<p>Let us consider you just joined a startup as a data analyst and you have been assigned to support the team to drive insights about the customers base. Your manager shared the dataset and here is your opportunity to showcase your python skills and use the Pandas package to perform the analysis.</p>"},{"location":"Learning/Python/Python_Pandas_exercise1/#ex1-fill-missing-in-annual_income-with-mean-of-incomme","title":"Ex1. Fill missing in annual_income with mean of incomme\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise1/#ex2-show-a-list-of-customers-having-annual-income-greater-than-50000","title":"Ex2. Show a list of customers having annual income greater than $50.000\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise1/#ex3-show-a-list-of-customer-ids-having-annual-income-50k-and-complaints-2","title":"Ex3. Show a list of customer ids having annual income &gt; $50k and complaints &gt; 2\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise1/#ex4-show-a-list-of-customers-purchasing-in-april","title":"Ex4. Show a list of customers purchasing in April\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise1/#ex5-show-a-list-of-customers-with-purchases-5","title":"Ex5. Show a list of customers with purchases &gt; 5\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise1/#ex6-average-income-of-customers-with-divorced-status","title":"Ex6. Average Income of customers with Divorced status\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise1/#ex7-show-a-frequency-of-complains-against-customers","title":"Ex7. Show a frequency of complains against customers.\u00b6","text":"<p>Example: There are 4 customers with 3 complaints, 15 customers with 1 complaints....</p>"},{"location":"Learning/Python/Python_Pandas_exercise1/#ex8-show-the-average-income-of-30-40-years-old-customers","title":"Ex8. Show the average income of 30-40 years old customers\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise1/#ex-9-which-educational-level-haves-the-highest-average-annual-income","title":"Ex 9. Which educational level haves the highest average annual income?\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/","title":"Pandas exercise 2","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[3]: Copied! <pre>df = pd.read_csv('/content/pandas_project_data.csv')\ndf.head()\n</pre> df = pd.read_csv('/content/pandas_project_data.csv') df.head() Out[3]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms 0 20201701 1982 Graduation Single 58138.0 9/4/2012 58 8 4 0 3 11 1 20201702 1950 Graduation Married 46344.0 3/8/2014 38 1 2 0 3 11 2 20201703 1965 Graduation Divorced 71613.0 8/21/2013 26 8 10 0 3 11 3 20201704 1984 Graduation Relationship 26646.0 2/10/2014 26 2 4 0 3 11 4 20201705 1981 PhD Widowed 58293.0 1/19/2014 94 5 6 0 3 11 In\u00a0[\u00a0]: Copied! <pre>df.info()\n</pre> df.info() In\u00a0[\u00a0]: Copied! <pre>df[df.isna().any(axis=1)] #Check null\n</pre> df[df.isna().any(axis=1)] #Check null In\u00a0[\u00a0]: Copied! <pre>df.annual_income = df.annual_income.fillna(df.annual_income.mean())\ndf[df.isna().any(axis=1)] #Check null\n</pre> df.annual_income = df.annual_income.fillna(df.annual_income.mean()) df[df.isna().any(axis=1)] #Check null Out[\u00a0]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms In\u00a0[\u00a0]: Copied! <pre>df[df.annual_income &gt; 50000]\n</pre> df[df.annual_income &gt; 50000] Out[\u00a0]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms 0 20201701 1982 Graduation Single 58138.0 9/4/2012 58 8 4 0 3 11 2 20201703 1965 Graduation Divorced 71613.0 8/21/2013 26 8 10 0 3 11 4 20201705 1981 PhD Widowed 58293.0 1/19/2014 94 5 6 0 3 11 5 20201706 1967 Master Relationship 62000.0 9/9/2013 16 6 10 5 3 11 6 20201707 1971 Graduation Divorced 55635.0 11/13/2012 34 7 7 0 3 11 ... ... ... ... ... ... ... ... ... ... ... ... ... 490 20202191 1958 PhD Relationship 70991.0 9/24/2012 11 2 12 0 5 11 493 20202194 1964 Master Single 58308.0 1/12/2013 77 2 3 0 3 11 494 20202195 1944 PhD Divorced 55614.0 11/27/2013 85 9 6 0 3 11 495 20202196 1962 Master Divorced 59432.0 4/13/2013 88 5 11 0 3 11 496 20202197 1978 Graduation Divorced 55563.0 4/5/2014 22 2 3 0 3 11 <p>251 rows \u00d7 12 columns</p> In\u00a0[\u00a0]: Copied! <pre>df[(df['complaints'] &gt; 2) &amp; (df['annual_income'] &gt; 50000)][\"customer_id\"]\n</pre> df[(df['complaints'] &gt; 2) &amp; (df['annual_income'] &gt; 50000)][\"customer_id\"] Out[\u00a0]: <pre>5     20201706\n18    20201719\n22    20201723\n23    20201724\n29    20201730\n34    20201735\n45    20201746\n59    20201760\n68    20201769\n80    20201781\nName: customer_id, dtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>df['purhcase_date'] = pd.to_datetime(df.purhcase_date, infer_datetime_format=True) \n</pre> df['purhcase_date'] = pd.to_datetime(df.purhcase_date, infer_datetime_format=True)  In\u00a0[\u00a0]: Copied! <pre>df[df['purhcase_date'].dt.month==4]\n</pre> df[df['purhcase_date'].dt.month==4] In\u00a0[\u00a0]: Copied! <pre>df[df.online_purchases + df.store_purchases &gt; 5]\n</pre> df[df.online_purchases + df.store_purchases &gt; 5] Out[\u00a0]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms 0 20201701 1982 Graduation Single 58138.0 9/4/2012 58 8 4 0 3 11 2 20201703 1965 Graduation Divorced 71613.0 8/21/2013 26 8 10 0 3 11 3 20201704 1984 Graduation Relationship 26646.0 2/10/2014 26 2 4 0 3 11 4 20201705 1981 PhD Widowed 58293.0 1/19/2014 94 5 6 0 3 11 5 20201706 1967 Master Relationship 62000.0 9/9/2013 16 6 10 5 3 11 ... ... ... ... ... ... ... ... ... ... ... ... ... 492 20202193 1955 Graduation Relationship 23018.0 2/22/2013 72 3 13 0 3 11 494 20202195 1944 PhD Divorced 55614.0 11/27/2013 85 9 6 0 3 11 495 20202196 1962 Master Divorced 59432.0 4/13/2013 88 5 11 0 3 11 497 20202198 1971 PhD Relationship 43624.0 4/21/2013 83 4 4 0 6 11 498 20202199 1949 PhD Relationship 41461.0 5/22/2014 63 6 11 0 6 11 <p>345 rows \u00d7 12 columns</p> In\u00a0[\u00a0]: Copied! <pre>df[df.marital_status == 'Divorced'].mean()\n</pre> df[df.marital_status == 'Divorced'].mean() <pre>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  \"\"\"Entry point for launching an IPython kernel.\n</pre> Out[\u00a0]: <pre>customer_id         2.020195e+07\nyear_of_birth       1.977437e+03\nannual_income       5.131645e+04\nrecency             5.329885e+01\nonline_purchases    4.287356e+00\nstore_purchases     5.954023e+00\ncomplaints          1.264368e-01\ncalls               3.839080e+00\nintercoms           1.029885e+01\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>df['complaints'].value_counts()\n</pre> df['complaints'].value_counts() Out[\u00a0]: <pre>0     461\n1      14\n5       5\n3       5\n2       4\n7       3\n4       3\n6       2\n8       1\n11      1\nName: complaints, dtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>import datetime\n\ndf[\"age\"] = datetime.datetime.now().year - df[\"year_of_birth \"]\ndf[ (df[\"age\"] &gt;= 30) &amp; (df[\"age\"] &lt;= 40)].mean()[\"annual_income\"]\n</pre> import datetime  df[\"age\"] = datetime.datetime.now().year - df[\"year_of_birth \"] df[ (df[\"age\"] &gt;= 30) &amp; (df[\"age\"] &lt;= 40)].mean()[\"annual_income\"] <pre>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  after removing the cwd from sys.path.\n</pre> Out[\u00a0]: <pre>50989.36708860759</pre> In\u00a0[\u00a0]: Copied! <pre>max = 0\nname = \"\"\nfor edu_level in df[\"educational_level\"].unique():\n  temp = df[df.educational_level == edu_level].mean()['annual_income']\n  if temp &gt; max:\n    max = temp\n    name = edu_level\nprint(name,max)\n</pre> max = 0 name = \"\" for edu_level in df[\"educational_level\"].unique():   temp = df[df.educational_level == edu_level].mean()['annual_income']   if temp &gt; max:     max = temp     name = edu_level print(name,max) <pre>PhD 55762.55712583929\n</pre> <pre>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  after removing the cwd from sys.path.\n</pre> In\u00a0[\u00a0]: Copied! <pre>df.groupby([\"educational_level\"]).max()\n</pre> df.groupby([\"educational_level\"]).max() Out[\u00a0]: customer_id year_of_birth marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms educational_level Basic 20202045 1999 Single 28249.0 9/9/2012 94 3 4 6 3 11 Graduation 20202197 2000 Widowed 102692.0 9/8/2012 99 27 13 11 88 44 High School 20202190 1999 Widowed 89572.0 9/4/2013 99 11 12 5 8 11 Master 20202196 2000 Widowed 92859.0 9/9/2013 93 11 13 8 11 11 PhD 20202199 2000 Widowed 157243.0 9/26/2013 98 11 13 6 9 11 In\u00a0[5]: Copied! <pre>df[\"purchases\"] = df[\"online_purchases\"] + df[\"store_purchases\"]\ndf.groupby([\"year_of_birth \",\"marital_status\"])[\"purchases\"].sum()\n#df.groupby([\"year_of_birth \",\"marital_status\"])[\"online_purchases\"].sum() + df.groupby([\"year_of_birth \",\"marital_status\"])[\"store_purchases\"].sum()\n</pre> df[\"purchases\"] = df[\"online_purchases\"] + df[\"store_purchases\"] df.groupby([\"year_of_birth \",\"marital_status\"])[\"purchases\"].sum() #df.groupby([\"year_of_birth \",\"marital_status\"])[\"online_purchases\"].sum() + df.groupby([\"year_of_birth \",\"marital_status\"])[\"store_purchases\"].sum()  Out[5]: <pre>year_of_birth   marital_status\n1899            Single             8\n1940            Married            6\n1943            Divorced          12\n                Married           10\n1944            Divorced          15\n                                  ..\n1999            Widowed            3\n2000            Married           10\n                Relationship      17\n                Single            25\n                Widowed           12\nLength: 207, dtype: int64</pre> In\u00a0[8]: Copied! <pre>mean_income = df[\"annual_income\"].mean()\ndf[\"annual_income_cate\"] = df[\"annual_income\"]\ndf[\"annual_income_cate\"] = np.nan\ndf.loc[df[\"annual_income\"] &gt; mean_income + 0.1*mean_income,[\"annual_income_cate\"]] = \"high\"\ndf.loc[df[\"annual_income\"] &lt; mean_income - 0.1*mean_income,[\"annual_income_cate\"]] = \"low\"\ndf[\"annual_income_cate\"].fillna(\"medium\",inplace=True)\ndf\n</pre> mean_income = df[\"annual_income\"].mean() df[\"annual_income_cate\"] = df[\"annual_income\"] df[\"annual_income_cate\"] = np.nan df.loc[df[\"annual_income\"] &gt; mean_income + 0.1*mean_income,[\"annual_income_cate\"]] = \"high\" df.loc[df[\"annual_income\"] &lt; mean_income - 0.1*mean_income,[\"annual_income_cate\"]] = \"low\" df[\"annual_income_cate\"].fillna(\"medium\",inplace=True) df Out[8]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms purchases annual_income_cate 0 20201701 1982 Graduation Single 58138.0 9/4/2012 58 8 4 0 3 11 12 high 1 20201702 1950 Graduation Married 46344.0 3/8/2014 38 1 2 0 3 11 3 medium 2 20201703 1965 Graduation Divorced 71613.0 8/21/2013 26 8 10 0 3 11 18 high 3 20201704 1984 Graduation Relationship 26646.0 2/10/2014 26 2 4 0 3 11 6 low 4 20201705 1981 PhD Widowed 58293.0 1/19/2014 94 5 6 0 3 11 11 high ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 494 20202195 1944 PhD Divorced 55614.0 11/27/2013 85 9 6 0 3 11 15 medium 495 20202196 1962 Master Divorced 59432.0 4/13/2013 88 5 11 0 3 11 16 high 496 20202197 1978 Graduation Divorced 55563.0 4/5/2014 22 2 3 0 3 11 5 medium 497 20202198 1971 PhD Relationship 43624.0 4/21/2013 83 4 4 0 6 11 8 low 498 20202199 1949 PhD Relationship 41461.0 5/22/2014 63 6 11 0 6 11 17 low <p>499 rows \u00d7 14 columns</p> In\u00a0[\u00a0]: Copied! <pre>df_price = pd.read_csv(\"/content/price.csv\")\ndf_price\n</pre> df_price = pd.read_csv(\"/content/price.csv\") df_price Out[\u00a0]: educational_level price 0 Graduation 150 1 PhD 180 2 Master 160 3 Basic 130 4 High School 100 In\u00a0[\u00a0]: Copied! <pre>merge_result = df.merge(df_price,left_on=\"educational_level\",right_on=\"educational_level\")\nmerge_result\n</pre> merge_result = df.merge(df_price,left_on=\"educational_level\",right_on=\"educational_level\") merge_result Out[\u00a0]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms purchases annual_income_cate price 0 20201701 1982 Graduation Single 58138.0 9/4/2012 58 8 4 0 3 11 12 high 150 1 20201702 1950 Graduation Married 46344.0 3/8/2014 38 1 2 0 3 11 3 medium 150 2 20201703 1965 Graduation Divorced 71613.0 8/21/2013 26 8 10 0 3 11 18 high 150 3 20201704 1984 Graduation Relationship 26646.0 2/10/2014 26 2 4 0 3 11 6 low 150 4 20201707 1971 Graduation Divorced 55635.0 11/13/2012 34 7 7 0 3 11 14 medium 150 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 494 20202145 1992 High School Married 20130.0 3/17/2014 99 1 3 0 3 11 4 low 100 495 20202154 1975 High School Married 70829.0 12/1/2013 87 4 8 0 3 11 12 high 100 496 20202157 1957 High School Single 89572.0 9/15/2012 44 7 9 0 3 11 16 high 100 497 20202160 1957 High School Relationship 58723.0 9/25/2012 78 5 6 0 0 11 11 high 100 498 20202190 1952 High School Widow 28457.0 10/28/2012 96 4 4 0 3 11 8 low 100 <p>499 rows \u00d7 15 columns</p> In\u00a0[\u00a0]: Copied! <pre>merge_result[\"Amount\"] = (merge_result[\"online_purchases\"]+merge_result[\"store_purchases\"])*merge_result[\"price\"]\nmerge_result\n</pre> merge_result[\"Amount\"] = (merge_result[\"online_purchases\"]+merge_result[\"store_purchases\"])*merge_result[\"price\"] merge_result Out[\u00a0]: customer_id year_of_birth educational_level marital_status annual_income purhcase_date recency online_purchases store_purchases complaints calls intercoms purchases annual_income_cate price Amount 0 20201701 1982 Graduation Single 58138.0 9/4/2012 58 8 4 0 3 11 12 high 150 1800 1 20201702 1950 Graduation Married 46344.0 3/8/2014 38 1 2 0 3 11 3 medium 150 450 2 20201703 1965 Graduation Divorced 71613.0 8/21/2013 26 8 10 0 3 11 18 high 150 2700 3 20201704 1984 Graduation Relationship 26646.0 2/10/2014 26 2 4 0 3 11 6 low 150 900 4 20201707 1971 Graduation Divorced 55635.0 11/13/2012 34 7 7 0 3 11 14 medium 150 2100 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 494 20202145 1992 High School Married 20130.0 3/17/2014 99 1 3 0 3 11 4 low 100 400 495 20202154 1975 High School Married 70829.0 12/1/2013 87 4 8 0 3 11 12 high 100 1200 496 20202157 1957 High School Single 89572.0 9/15/2012 44 7 9 0 3 11 16 high 100 1600 497 20202160 1957 High School Relationship 58723.0 9/25/2012 78 5 6 0 0 11 11 high 100 1100 498 20202190 1952 High School Widow 28457.0 10/28/2012 96 4 4 0 3 11 8 low 100 800 <p>499 rows \u00d7 16 columns</p>"},{"location":"Learning/Python/Python_Pandas_exercise2/#introduction","title":"Introduction\u00b6","text":"<p>Let us consider you just joined a startup as a data analyst and you have been assigned to support the team to drive insights about the customers base. Your manager shared the dataset and here is your opportunity to showcase your python skills and use the Pandas package to perform the analysis.</p>"},{"location":"Learning/Python/Python_Pandas_exercise2/#ex1-fill-missing-in-annual_income-with-mean-of-incomme","title":"Ex1. Fill missing in annual_income with mean of incomme\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex2-show-a-list-of-customers-having-annual-income-greater-than-50000","title":"Ex2. Show a list of customers having annual income greater than $50.000\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex3-show-a-list-of-customer-ids-having-annual-income-50k-and-complaints-2","title":"Ex3. Show a list of customer ids having annual income &gt; $50k and complaints &gt; 2\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex4-show-a-list-of-customers-purchasing-in-april","title":"Ex4. Show a list of customers purchasing in April\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex5-show-a-list-of-customers-with-purchases-5","title":"Ex5. Show a list of customers with purchases &gt; 5\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex6-average-income-of-customers-with-divorced-status","title":"Ex6. Average Income of customers with Divorced status\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex7-show-a-frequency-of-complains-against-customers","title":"Ex7. Show a frequency of complains against customers.\u00b6","text":"<p>Example: There are 4 customers with 3 complaints, 15 customers with 1 complaints....</p>"},{"location":"Learning/Python/Python_Pandas_exercise2/#ex8-show-the-average-income-of-30-40-years-old-customers","title":"Ex8. Show the average income of 30-40 years old customers\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex-9-which-educational-level-haves-the-highest-average-annual-income","title":"Ex 9. Which educational level haves the highest average annual income?\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex-10-show-max-income-in-each-group-of-educational_level","title":"Ex 10. Show max income in each group of educational_level\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex11-show-total-sale-of-each-group-of-year-of-birth-and-marital_status","title":"Ex11. Show total sale of each group of year of birth and marital_status\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex12-categorize-the-annual-income-into-three-values-low-medium-and-high-as-follow","title":"Ex12. Categorize the annual income into three values: low, medium and high as follow:\u00b6","text":"<p>low: income &lt; mean - 10% mean</p> <p>High: income &gt; mean + 10% mean</p> <p>medium: the remaining</p>"},{"location":"Learning/Python/Python_Pandas_exercise2/#to-each-group-of-educational_level-the-item-has-a-corresponding-price-provided-in-the-item-pricecsv-file","title":"To each group of educational_level, the item has a corresponding price provided in the item-price.csv file\u00b6","text":""},{"location":"Learning/Python/Python_Pandas_exercise2/#ex-13-add-an-amount-column-to-show-the-purchase-amount-for-each-customer","title":"Ex 13. Add an \"Amount\" column to show the purchase amount for each customer.\u00b6","text":""},{"location":"Learning/Pytorch/00_pytorch_fundamentals/","title":"00. PyTorch Fundamentals","text":"<p>View Source Code | View Slides | Watch Video Walkthrough</p> In\u00a0[1]: Copied! <pre>import torch\ntorch.__version__\n</pre> import torch torch.__version__ Out[1]: <pre>'2.2.2'</pre> <p>Wonderful, it looks like we've got PyTorch 1.10.0+.</p> <p>This means if you're going through these materials, you'll see most compatability with PyTorch 1.10.0+, however if your version number is far higher than that, you might notice some inconsistencies.</p> <p>And if you do have any issues, please post on the course GitHub Discussions page.</p> In\u00a0[2]: Copied! <pre># Scalar\nscalar = torch.tensor(7)\nscalar\n</pre> # Scalar scalar = torch.tensor(7) scalar Out[2]: <pre>tensor(7)</pre> <p>See how the above printed out <code>tensor(7)</code>?</p> <p>That means although <code>scalar</code> is a single number, it's of type <code>torch.Tensor</code>.</p> <p>We can check the dimensions of a tensor using the <code>ndim</code> attribute.</p> In\u00a0[3]: Copied! <pre>scalar.ndim\n</pre> scalar.ndim Out[3]: <pre>0</pre> <p>What if we wanted to retrieve the number from the tensor?</p> <p>As in, turn it from <code>torch.Tensor</code> to a Python integer?</p> <p>To do we can use the <code>item()</code> method.</p> In\u00a0[4]: Copied! <pre># Get the Python number within a tensor (only works with one-element tensors)\nscalar.item()\n</pre> # Get the Python number within a tensor (only works with one-element tensors) scalar.item() Out[4]: <pre>7</pre> <p>Okay, now let's see a vector.</p> <p>A vector is a single dimension tensor but can contain many numbers.</p> <p>As in, you could have a vector <code>[3, 2]</code> to describe <code>[bedrooms, bathrooms]</code> in your house. Or you could have <code>[3, 2, 2]</code> to describe <code>[bedrooms, bathrooms, car_parks]</code> in your house.</p> <p>The important trend here is that a vector is flexible in what it can represent (the same with tensors).</p> In\u00a0[5]: Copied! <pre># Vector\nvector = torch.tensor([7, 7])\nvector\n</pre> # Vector vector = torch.tensor([7, 7]) vector Out[5]: <pre>tensor([7, 7])</pre> <p>Wonderful, <code>vector</code> now contains two 7's, my favourite number.</p> <p>How many dimensions do you think it'll have?</p> In\u00a0[6]: Copied! <pre># Check the number of dimensions of vector\nvector.ndim\n</pre> # Check the number of dimensions of vector vector.ndim Out[6]: <pre>1</pre> <p>Hmm, that's strange, <code>vector</code> contains two numbers but only has a single dimension.</p> <p>I'll let you in on a trick.</p> <p>You can tell the number of dimensions a tensor in PyTorch has by the number of square brackets on the outside (<code>[</code>) and you only need to count one side.</p> <p>How many square brackets does <code>vector</code> have?</p> <p>Another important concept for tensors is their <code>shape</code> attribute. The shape tells you how the elements inside them are arranged.</p> <p>Let's check out the shape of <code>vector</code>.</p> In\u00a0[7]: Copied! <pre># Check shape of vector\nvector.shape\n</pre> # Check shape of vector vector.shape Out[7]: <pre>torch.Size([2])</pre> <p>The above returns <code>torch.Size([2])</code> which means our vector has a shape of <code>[2]</code>. This is because of the two elements we placed inside the square brackets (<code>[7, 7]</code>).</p> <p>Let's now see a matrix.</p> In\u00a0[8]: Copied! <pre># Matrix\nMATRIX = torch.tensor([[7, 8], \n                       [9, 10]])\nMATRIX\n</pre> # Matrix MATRIX = torch.tensor([[7, 8],                         [9, 10]]) MATRIX Out[8]: <pre>tensor([[ 7,  8],\n        [ 9, 10]])</pre> <p>Wow! More numbers! Matrices are as flexible as vectors, except they've got an extra dimension.</p> In\u00a0[9]: Copied! <pre># Check number of dimensions\nMATRIX.ndim\n</pre> # Check number of dimensions MATRIX.ndim Out[9]: <pre>2</pre> <p><code>MATRIX</code> has two dimensions (did you count the number of square brakcets on the outside of one side?).</p> <p>What <code>shape</code> do you think it will have?</p> In\u00a0[10]: Copied! <pre>MATRIX.shape\n</pre> MATRIX.shape Out[10]: <pre>torch.Size([2, 2])</pre> <p>We get the output <code>torch.Size([2, 2])</code> because <code>MATRIX</code> is two elements deep and two elements wide.</p> <p>How about we create a tensor?</p> In\u00a0[11]: Copied! <pre># Tensor\nTENSOR = torch.tensor([[[1, 2, 3],\n                        [3, 6, 9],\n                        [2, 4, 5]]])\nTENSOR\n</pre> # Tensor TENSOR = torch.tensor([[[1, 2, 3],                         [3, 6, 9],                         [2, 4, 5]]]) TENSOR Out[11]: <pre>tensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])</pre> <p>Woah! What a nice looking tensor.</p> <p>I want to stress that tensors can represent almost anything.</p> <p>The one we just created could be the sales numbers for a steak and almond butter store (two of my favourite foods).</p> <p></p> <p>How many dimensions do you think it has? (hint: use the square bracket counting trick)</p> In\u00a0[12]: Copied! <pre># Check number of dimensions for TENSOR\nTENSOR.ndim\n</pre> # Check number of dimensions for TENSOR TENSOR.ndim Out[12]: <pre>3</pre> <p>And what about its shape?</p> In\u00a0[13]: Copied! <pre># Check shape of TENSOR\nTENSOR.shape\n</pre> # Check shape of TENSOR TENSOR.shape Out[13]: <pre>torch.Size([1, 3, 3])</pre> <p>Alright, it outputs <code>torch.Size([1, 3, 3])</code>.</p> <p>The dimensions go outer to inner.</p> <p>That means there's 1 dimension of 3 by 3.</p> <p></p> <p>Note: You might've noticed me using lowercase letters for <code>scalar</code> and <code>vector</code> and uppercase letters for <code>MATRIX</code> and <code>TENSOR</code>. This was on purpose. In practice, you'll often see scalars and vectors denoted as lowercase letters such as <code>y</code> or <code>a</code>. And matrices and tensors denoted as uppercase letters such as <code>X</code> or <code>W</code>.</p> <p>You also might notice the names martrix and tensor used interchangably. This is common. Since in PyTorch you're often dealing with <code>torch.Tensor</code>s (hence the tensor name), however, the shape and dimensions of what's inside will dictate what it actually is.</p> <p>Let's summarise.</p> Name What is it? Number of dimensions Lower or upper (usually/example) scalar a single number 0 Lower (<code>a</code>) vector a number with direction (e.g. wind speed with direction) but can also have many other numbers 1 Lower (<code>y</code>) matrix a 2-dimensional array of numbers 2 Upper (<code>Q</code>) tensor an n-dimensional array of numbers can be any number, a 0-dimension tensor is a scalar, a 1-dimension tensor is a vector Upper (<code>X</code>) <p></p> In\u00a0[14]: Copied! <pre># Create a random tensor of size (3, 4)\nrandom_tensor = torch.rand(size=(3, 4))\nrandom_tensor, random_tensor.dtype\n</pre> # Create a random tensor of size (3, 4) random_tensor = torch.rand(size=(3, 4)) random_tensor, random_tensor.dtype Out[14]: <pre>(tensor([[0.2488, 0.7382, 0.2682, 0.8594],\n         [0.9544, 0.7761, 0.6800, 0.1087],\n         [0.5898, 0.8385, 0.1554, 0.3773]]),\n torch.float32)</pre> <p>The flexibility of <code>torch.rand()</code> is that we can adjust the <code>size</code> to be whatever we want.</p> <p>For example, say you wanted a random tensor in the common image shape of <code>[224, 224, 3]</code> (<code>[height, width, color_channels</code>]).</p> In\u00a0[15]: Copied! <pre># Create a random tensor of size (224, 224, 3)\nrandom_image_size_tensor = torch.rand(size=(224, 224, 3))\nrandom_image_size_tensor.shape, random_image_size_tensor.ndim\n</pre> # Create a random tensor of size (224, 224, 3) random_image_size_tensor = torch.rand(size=(224, 224, 3)) random_image_size_tensor.shape, random_image_size_tensor.ndim Out[15]: <pre>(torch.Size([224, 224, 3]), 3)</pre> In\u00a0[16]: Copied! <pre># Create a tensor of all zeros\nzeros = torch.zeros(size=(3, 4))\nzeros, zeros.dtype\n</pre> # Create a tensor of all zeros zeros = torch.zeros(size=(3, 4)) zeros, zeros.dtype Out[16]: <pre>(tensor([[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]),\n torch.float32)</pre> <p>We can do the same to create a tensor of all ones except using <code>torch.ones()</code>  instead.</p> In\u00a0[17]: Copied! <pre># Create a tensor of all ones\nones = torch.ones(size=(3, 4))\nones, ones.dtype\n</pre> # Create a tensor of all ones ones = torch.ones(size=(3, 4)) ones, ones.dtype Out[17]: <pre>(tensor([[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]),\n torch.float32)</pre> In\u00a0[18]: Copied! <pre># Use torch.arange(), torch.range() is deprecated \nzero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n\n# Create a range of values 0 to 10\nzero_to_ten = torch.arange(start=0, end=10, step=1)\nzero_to_ten\n</pre> # Use torch.arange(), torch.range() is deprecated  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future  # Create a range of values 0 to 10 zero_to_ten = torch.arange(start=0, end=10, step=1) zero_to_ten <pre>/tmp/ipykernel_1201490/193451495.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n</pre> Out[18]: <pre>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> <p>Sometimes you might want one tensor of a certain type with the same shape as another tensor.</p> <p>For example, a tensor of all zeros with the same shape as a previous tensor.</p> <p>To do so you can use <code>torch.zeros_like(input)</code> or <code>torch.ones_like(input)</code> which return a tensor filled with zeros or ones in the same shape as the <code>input</code> respectively.</p> In\u00a0[19]: Copied! <pre># Can also create a tensor of zeros similar to another tensor\nten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\nten_zeros\n</pre> # Can also create a tensor of zeros similar to another tensor ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape ten_zeros Out[19]: <pre>tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</pre> In\u00a0[20]: Copied! <pre># Default datatype for tensors is float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=None, # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations performed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n</pre> # Default datatype for tensors is float32 float_32_tensor = torch.tensor([3.0, 6.0, 9.0],                                dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed                                device=None, # defaults to None, which uses the default tensor type                                requires_grad=False) # if True, operations performed on the tensor are recorded   float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device Out[20]: <pre>(torch.Size([3]), torch.float32, device(type='cpu'))</pre> <p>Aside from shape issues (tensor shapes don't match up), two of the other most common issues you'll come across in PyTorch are datatype and device issues.</p> <p>For example, one of tensors is <code>torch.float32</code> and the other is <code>torch.float16</code> (PyTorch often likes tensors to be the same format).</p> <p>Or one of your tensors is on the CPU and the other is on the GPU (PyTorch likes calculations between tensors to be on the same device).</p> <p>We'll see more of this device talk later on.</p> <p>For now let's create a tensor with <code>dtype=torch.float16</code>.</p> In\u00a0[21]: Copied! <pre>float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=torch.float16) # torch.half would also work\n\nfloat_16_tensor.dtype\n</pre> float_16_tensor = torch.tensor([3.0, 6.0, 9.0],                                dtype=torch.float16) # torch.half would also work  float_16_tensor.dtype Out[21]: <pre>torch.float16</pre> In\u00a0[22]: Copied! <pre># Create a tensor\nsome_tensor = torch.rand(3, 4)\n\n# Find out details about it\nprint(some_tensor)\nprint(f\"Shape of tensor: {some_tensor.shape}\")\nprint(f\"Datatype of tensor: {some_tensor.dtype}\")\nprint(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU\n</pre> # Create a tensor some_tensor = torch.rand(3, 4)  # Find out details about it print(some_tensor) print(f\"Shape of tensor: {some_tensor.shape}\") print(f\"Datatype of tensor: {some_tensor.dtype}\") print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU <pre>tensor([[0.8524, 0.8941, 0.5894, 0.7621],\n        [0.4638, 0.4476, 0.6611, 0.9728],\n        [0.9525, 0.0924, 0.9369, 0.1373]])\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n</pre> <p>Note: When you run into issues in PyTorch, it's very often one to do with one of the three attributes above. So when the error messages show up, sing yourself a little song called \"what, what, where\":</p> <ul> <li>\"what shape are my tensors? what datatype are they and where are they stored? what shape, what datatype, where where where\"</li> </ul> In\u00a0[23]: Copied! <pre># Create a tensor of values and add a number to it\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n</pre> # Create a tensor of values and add a number to it tensor = torch.tensor([1, 2, 3]) tensor + 10 Out[23]: <pre>tensor([11, 12, 13])</pre> In\u00a0[24]: Copied! <pre># Multiply it by 10\ntensor * 10\n</pre> # Multiply it by 10 tensor * 10 Out[24]: <pre>tensor([10, 20, 30])</pre> <p>Notice how the tensor values above didn't end up being <code>tensor([110, 120, 130])</code>, this is because the values inside the tensor don't change unless they're reassigned.</p> In\u00a0[25]: Copied! <pre># Tensors don't change unless reassigned\ntensor\n</pre> # Tensors don't change unless reassigned tensor Out[25]: <pre>tensor([1, 2, 3])</pre> <p>Let's subtract a number and this time we'll reassign the <code>tensor</code> variable.</p> In\u00a0[26]: Copied! <pre># Subtract and reassign\ntensor = tensor - 10\ntensor\n</pre> # Subtract and reassign tensor = tensor - 10 tensor Out[26]: <pre>tensor([-9, -8, -7])</pre> In\u00a0[27]: Copied! <pre># Add and reassign\ntensor = tensor + 10\ntensor\n</pre> # Add and reassign tensor = tensor + 10 tensor Out[27]: <pre>tensor([1, 2, 3])</pre> <p>PyTorch also has a bunch of built-in functions like <code>torch.mul()</code> (short for multiplication) and <code>torch.add()</code> to perform basic operations.</p> In\u00a0[28]: Copied! <pre># Can also use torch functions\ntorch.multiply(tensor, 10)\n</pre> # Can also use torch functions torch.multiply(tensor, 10) Out[28]: <pre>tensor([10, 20, 30])</pre> In\u00a0[29]: Copied! <pre># Original tensor is still unchanged \ntensor\n</pre> # Original tensor is still unchanged  tensor Out[29]: <pre>tensor([1, 2, 3])</pre> <p>However, it's more common to use the operator symbols like <code>*</code> instead of <code>torch.mul()</code></p> In\u00a0[30]: Copied! <pre># Element-wise multiplication (each element multiplies its equivalent, index 0-&gt;0, 1-&gt;1, 2-&gt;2)\nprint(tensor, \"*\", tensor)\nprint(\"Equals:\", tensor * tensor)\n</pre> # Element-wise multiplication (each element multiplies its equivalent, index 0-&gt;0, 1-&gt;1, 2-&gt;2) print(tensor, \"*\", tensor) print(\"Equals:\", tensor * tensor) <pre>tensor([1, 2, 3]) * tensor([1, 2, 3])\nEquals: tensor([1, 4, 9])\n</pre> In\u00a0[31]: Copied! <pre>import torch\ntensor = torch.tensor([1, 2, 3])\ntensor.shape\n</pre> import torch tensor = torch.tensor([1, 2, 3]) tensor.shape Out[31]: <pre>torch.Size([3])</pre> <p>The difference between element-wise multiplication and matrix multiplication is the addition of values.</p> <p>For our <code>tensor</code> variable with values <code>[1, 2, 3]</code>:</p> Operation Calculation Code Element-wise multiplication <code>[1*1, 2*2, 3*3]</code> = <code>[1, 4, 9]</code> <code>tensor * tensor</code> Matrix multiplication <code>[1*1 + 2*2 + 3*3]</code> = <code>[14]</code> <code>tensor.matmul(tensor)</code> In\u00a0[32]: Copied! <pre># Element-wise matrix multiplication\ntensor * tensor\n</pre> # Element-wise matrix multiplication tensor * tensor Out[32]: <pre>tensor([1, 4, 9])</pre> In\u00a0[33]: Copied! <pre># Matrix multiplication\ntorch.matmul(tensor, tensor)\n</pre> # Matrix multiplication torch.matmul(tensor, tensor) Out[33]: <pre>tensor(14)</pre> In\u00a0[34]: Copied! <pre># Can also use the \"@\" symbol for matrix multiplication, though not recommended\ntensor @ tensor\n</pre> # Can also use the \"@\" symbol for matrix multiplication, though not recommended tensor @ tensor Out[34]: <pre>tensor(14)</pre> <p>You can do matrix multiplication by hand but it's not recommended.</p> <p>The in-built <code>torch.matmul()</code> method is faster.</p> In\u00a0[35]: Copied! <pre>%%time\n# Matrix multiplication by hand \n# (avoid doing operations with for loops at all cost, they are computationally expensive)\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\nvalue\n</pre> %%time # Matrix multiplication by hand  # (avoid doing operations with for loops at all cost, they are computationally expensive) value = 0 for i in range(len(tensor)):   value += tensor[i] * tensor[i] value <pre>CPU times: user 695 \u00b5s, sys: 0 ns, total: 695 \u00b5s\nWall time: 537 \u00b5s\n</pre> Out[35]: <pre>tensor(14)</pre> In\u00a0[36]: Copied! <pre>%%time\ntorch.matmul(tensor, tensor)\n</pre> %%time torch.matmul(tensor, tensor) <pre>CPU times: user 190 \u00b5s, sys: 0 ns, total: 190 \u00b5s\nWall time: 244 \u00b5s\n</pre> Out[36]: <pre>tensor(14)</pre> In\u00a0[37]: Copied! <pre># Shapes need to be in the right way  \ntensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11], \n                         [9, 12]], dtype=torch.float32)\n\ntorch.matmul(tensor_A, tensor_B) # (this will error)\n</pre> # Shapes need to be in the right way   tensor_A = torch.tensor([[1, 2],                          [3, 4],                          [5, 6]], dtype=torch.float32)  tensor_B = torch.tensor([[7, 10],                          [8, 11],                           [9, 12]], dtype=torch.float32)  torch.matmul(tensor_A, tensor_B) # (this will error) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[37], line 10\n      2 tensor_A = torch.tensor([[1, 2],\n      3                          [3, 4],\n      4                          [5, 6]], dtype=torch.float32)\n      6 tensor_B = torch.tensor([[7, 10],\n      7                          [8, 11], \n      8                          [9, 12]], dtype=torch.float32)\n---&gt; 10 torch.matmul(tensor_A, tensor_B) # (this will error)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)</pre> <p>We can make matrix multiplication work between <code>tensor_A</code> and <code>tensor_B</code> by making their inner dimensions match.</p> <p>One of the ways to do this is with a transpose (switch the dimensions of a given tensor).</p> <p>You can perform transposes in PyTorch using either:</p> <ul> <li><code>torch.transpose(input, dim0, dim1)</code> - where <code>input</code> is the desired tensor to transpose and <code>dim0</code> and <code>dim1</code> are the dimensions to be swapped.</li> <li><code>tensor.T</code> - where <code>tensor</code> is the desired tensor to transpose.</li> </ul> <p>Let's try the latter.</p> In\u00a0[38]: Copied! <pre># View tensor_A and tensor_B\nprint(tensor_A)\nprint(tensor_B)\n</pre> # View tensor_A and tensor_B print(tensor_A) print(tensor_B) <pre>tensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7., 10.],\n        [ 8., 11.],\n        [ 9., 12.]])\n</pre> In\u00a0[39]: Copied! <pre># View tensor_A and tensor_B.T\nprint(tensor_A)\nprint(tensor_B.T)\n</pre> # View tensor_A and tensor_B.T print(tensor_A) print(tensor_B.T) <pre>tensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]])\n</pre> In\u00a0[40]: Copied! <pre># The operation works when tensor_B is transposed\nprint(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\nprint(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\nprint(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\")\nprint(\"Output:\\n\")\noutput = torch.matmul(tensor_A, tensor_B.T)\nprint(output) \nprint(f\"\\nOutput shape: {output.shape}\")\n</pre> # The operation works when tensor_B is transposed print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\") print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\") print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\") print(\"Output:\\n\") output = torch.matmul(tensor_A, tensor_B.T) print(output)  print(f\"\\nOutput shape: {output.shape}\") <pre>Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n\nNew shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n\nMultiplying: torch.Size([3, 2]) * torch.Size([2, 3]) &lt;- inner dimensions match\n\nOutput:\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\nOutput shape: torch.Size([3, 3])\n</pre> <p>You can also use <code>torch.mm()</code> which is a short for <code>torch.matmul()</code>.</p> In\u00a0[41]: Copied! <pre># torch.mm is a shortcut for matmul\ntorch.mm(tensor_A, tensor_B.T)\n</pre> # torch.mm is a shortcut for matmul torch.mm(tensor_A, tensor_B.T) Out[41]: <pre>tensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])</pre> <p>Without the transpose, the rules of matrix mulitplication aren't fulfilled and we get an error like above.</p> <p>How about a visual?</p> <p></p> <p>You can create your own matrix multiplication visuals like this at http://matrixmultiplication.xyz/.</p> <p>Note: A matrix multiplication like this is also referred to as the dot product of two matrices.</p> <p>Neural networks are full of matrix multiplications and dot products.</p> <p>The <code>torch.nn.Linear()</code> module (we'll see this in action later on), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input <code>x</code> and a weights matrix <code>A</code>.</p> <p>$$ y = x\\cdot{A^T} + b $$</p> <p>Where:</p> <ul> <li><code>x</code> is the input to the layer (deep learning is a stack of layers like <code>torch.nn.Linear()</code> and others on top of each other).</li> <li><code>A</code> is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the \"<code>T</code>\", that's because the weights matrix gets transposed).<ul> <li>Note: You might also often see <code>W</code> or another letter like <code>X</code> used to showcase the weights matrix.</li> </ul> </li> <li><code>b</code> is the bias term used to slightly offset the weights and inputs.</li> <li><code>y</code> is the output (a manipulation of the input in the hopes to discover patterns in it).</li> </ul> <p>This is a linear function (you may have seen something like $y = mx+b$ in high school or elsewhere), and can be used to draw a straight line!</p> <p>Let's play around with a linear layer.</p> <p>Try changing the values of <code>in_features</code> and <code>out_features</code> below and see what happens.</p> <p>Do you notice anything to do with the shapes?</p> In\u00a0[42]: Copied! <pre># Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\ntorch.manual_seed(42)\n# This uses matrix multiplication\nlinear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input \n                         out_features=6) # out_features = describes outer value \nx = tensor_A\noutput = linear(x)\nprint(f\"Input shape: {x.shape}\\n\")\nprint(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n</pre> # Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later) torch.manual_seed(42) # This uses matrix multiplication linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input                           out_features=6) # out_features = describes outer value  x = tensor_A output = linear(x) print(f\"Input shape: {x.shape}\\n\") print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\") <pre>Input shape: torch.Size([3, 2])\n\nOutput:\ntensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\nOutput shape: torch.Size([3, 6])\n</pre> <p>Question: What happens if you change <code>in_features</code> from 2 to 3 above? Does it error? How could you change the shape of the input (<code>x</code>) to accomodate to the error? Hint: what did we have to do to <code>tensor_B</code> above?</p> <p>If you've never done it before, matrix multiplication can be a confusing topic at first.</p> <p>But after you've played around with it a few times and even cracked open a few neural networks, you'll notice it's everywhere.</p> <p>Remember, matrix multiplication is all you need.</p> <p></p> <p>When you start digging into neural network layers and building your own, you'll find matrix multiplications everywhere. Source: https://marksaroufim.substack.com/p/working-class-deep-learner</p> In\u00a0[43]: Copied! <pre># Create a tensor\nx = torch.arange(0, 100, 10)\nx\n</pre> # Create a tensor x = torch.arange(0, 100, 10) x Out[43]: <pre>tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])</pre> <p>Now let's perform some aggregation.</p> In\u00a0[44]: Copied! <pre>print(f\"Minimum: {x.min()}\")\nprint(f\"Maximum: {x.max()}\")\n# print(f\"Mean: {x.mean()}\") # this will error\nprint(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\nprint(f\"Sum: {x.sum()}\")\n</pre> print(f\"Minimum: {x.min()}\") print(f\"Maximum: {x.max()}\") # print(f\"Mean: {x.mean()}\") # this will error print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype print(f\"Sum: {x.sum()}\") <pre>Minimum: 0\nMaximum: 90\nMean: 45.0\nSum: 450\n</pre> <p>Note: You may find some methods such as <code>torch.mean()</code> require tensors to be in <code>torch.float32</code> (the most common) or another specific datatype, otherwise the operation will fail.</p> <p>You can also do the same as above with <code>torch</code> methods.</p> In\u00a0[45]: Copied! <pre>torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)\n</pre> torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x) Out[45]: <pre>(tensor(90), tensor(0), tensor(45.), tensor(450))</pre> In\u00a0[46]: Copied! <pre># Create a tensor\ntensor = torch.arange(10, 100, 10)\nprint(f\"Tensor: {tensor}\")\n\n# Returns index of max and min values\nprint(f\"Index where max value occurs: {tensor.argmax()}\")\nprint(f\"Index where min value occurs: {tensor.argmin()}\")\n</pre> # Create a tensor tensor = torch.arange(10, 100, 10) print(f\"Tensor: {tensor}\")  # Returns index of max and min values print(f\"Index where max value occurs: {tensor.argmax()}\") print(f\"Index where min value occurs: {tensor.argmin()}\") <pre>Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\nIndex where max value occurs: 8\nIndex where min value occurs: 0\n</pre> In\u00a0[47]: Copied! <pre># Create a tensor and check its datatype\ntensor = torch.arange(10., 100., 10.)\ntensor.dtype\n</pre> # Create a tensor and check its datatype tensor = torch.arange(10., 100., 10.) tensor.dtype Out[47]: <pre>torch.float32</pre> <p>Now we'll create another tensor the same as before but change its datatype to <code>torch.float16</code>.</p> In\u00a0[48]: Copied! <pre># Create a float16 tensor\ntensor_float16 = tensor.type(torch.float16)\ntensor_float16\n</pre> # Create a float16 tensor tensor_float16 = tensor.type(torch.float16) tensor_float16 Out[48]: <pre>tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)</pre> <p>And we can do something similar to make a <code>torch.int8</code> tensor.</p> In\u00a0[49]: Copied! <pre># Create a int8 tensor\ntensor_int8 = tensor.type(torch.int8)\ntensor_int8\n</pre> # Create a int8 tensor tensor_int8 = tensor.type(torch.int8) tensor_int8 Out[49]: <pre>tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)</pre> <p>Note: Different datatypes can be confusing to begin with. But think of it like this, the lower the number (e.g. 32, 16, 8), the less precise a computer stores the value. And with a lower amount of storage, this generally results in faster computation and a smaller overall model. Mobile-based neural networks often operate with 8-bit integers, smaller and faster to run but less accurate than their float32 counterparts. For more on this, I'd read up about precision in computing).</p> <p>Exercise: So far we've covered a fair few tensor methods but there's a bunch more in the <code>torch.Tensor</code> documentation, I'd recommend spending 10-minutes scrolling through and looking into any that catch your eye. Click on them and then write them out in code yourself to see what happens.</p> In\u00a0[50]: Copied! <pre># Create a tensor\nimport torch\nx = torch.arange(1., 8.)\nx, x.shape\n</pre> # Create a tensor import torch x = torch.arange(1., 8.) x, x.shape Out[50]: <pre>(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))</pre> <p>Now let's add an extra dimension with <code>torch.reshape()</code>.</p> In\u00a0[51]: Copied! <pre># Add an extra dimension\nx_reshaped = x.reshape(1, 7)\nx_reshaped, x_reshaped.shape\n</pre> # Add an extra dimension x_reshaped = x.reshape(1, 7) x_reshaped, x_reshaped.shape Out[51]: <pre>(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</pre> <p>We can also change the view with <code>torch.view()</code>.</p> In\u00a0[52]: Copied! <pre># Change view (keeps same data as original but changes view)\n# See more: https://stackoverflow.com/a/54507446/7900723\nz = x.view(1, 7)\nz, z.shape\n</pre> # Change view (keeps same data as original but changes view) # See more: https://stackoverflow.com/a/54507446/7900723 z = x.view(1, 7) z, z.shape Out[52]: <pre>(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</pre> <p>Remember though, changing the view of a tensor with <code>torch.view()</code> really only creates a new view of the same tensor.</p> <p>So changing the view changes the original tensor too.</p> In\u00a0[53]: Copied! <pre># Changing z changes x\nz[:, 0] = 5\nz, x\n</pre> # Changing z changes x z[:, 0] = 5 z, x Out[53]: <pre>(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))</pre> <p>If we wanted to stack our new tensor on top of itself five times, we could do so with <code>torch.stack()</code>.</p> In\u00a0[54]: Copied! <pre># Stack tensors on top of each other\nx_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\nx_stacked\n</pre> # Stack tensors on top of each other x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens x_stacked Out[54]: <pre>tensor([[5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.]])</pre> <p>How about removing all single dimensions from a tensor?</p> <p>To do so you can use <code>torch.squeeze()</code> (I remember this as squeezing the tensor to only have dimensions over 1).</p> In\u00a0[55]: Copied! <pre>print(f\"Previous tensor: {x_reshaped}\")\nprint(f\"Previous shape: {x_reshaped.shape}\")\n\n# Remove extra dimension from x_reshaped\nx_squeezed = x_reshaped.squeeze()\nprint(f\"\\nNew tensor: {x_squeezed}\")\nprint(f\"New shape: {x_squeezed.shape}\")\n</pre> print(f\"Previous tensor: {x_reshaped}\") print(f\"Previous shape: {x_reshaped.shape}\")  # Remove extra dimension from x_reshaped x_squeezed = x_reshaped.squeeze() print(f\"\\nNew tensor: {x_squeezed}\") print(f\"New shape: {x_squeezed.shape}\") <pre>Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nPrevious shape: torch.Size([1, 7])\n\nNew tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nNew shape: torch.Size([7])\n</pre> <p>And to do the reverse of <code>torch.squeeze()</code> you can use <code>torch.unsqueeze()</code> to add a dimension value of 1 at a specific index.</p> In\u00a0[56]: Copied! <pre>print(f\"Previous tensor: {x_squeezed}\")\nprint(f\"Previous shape: {x_squeezed.shape}\")\n\n## Add an extra dimension with unsqueeze\nx_unsqueezed = x_squeezed.unsqueeze(dim=0)\nprint(f\"\\nNew tensor: {x_unsqueezed}\")\nprint(f\"New shape: {x_unsqueezed.shape}\")\n</pre> print(f\"Previous tensor: {x_squeezed}\") print(f\"Previous shape: {x_squeezed.shape}\")  ## Add an extra dimension with unsqueeze x_unsqueezed = x_squeezed.unsqueeze(dim=0) print(f\"\\nNew tensor: {x_unsqueezed}\") print(f\"New shape: {x_unsqueezed.shape}\") <pre>Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nPrevious shape: torch.Size([7])\n\nNew tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nNew shape: torch.Size([1, 7])\n</pre> <p>You can also rearrange the order of axes values with <code>torch.permute(input, dims)</code>, where the <code>input</code> gets turned into a view with new <code>dims</code>.</p> In\u00a0[57]: Copied! <pre># Create tensor with specific shape\nx_original = torch.rand(size=(224, 224, 3))\n\n# Permute the original tensor to rearrange the axis order\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0\n\nprint(f\"Previous shape: {x_original.shape}\")\nprint(f\"New shape: {x_permuted.shape}\")\n</pre> # Create tensor with specific shape x_original = torch.rand(size=(224, 224, 3))  # Permute the original tensor to rearrange the axis order x_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0  print(f\"Previous shape: {x_original.shape}\") print(f\"New shape: {x_permuted.shape}\") <pre>Previous shape: torch.Size([224, 224, 3])\nNew shape: torch.Size([3, 224, 224])\n</pre> <p>Note: Because permuting returns a view (shares the same data as the original), the values in the permuted tensor will be the same as the original tensor and if you change the values in the view, it will change the values of the original.</p> In\u00a0[58]: Copied! <pre># Create a tensor \nimport torch\nx = torch.arange(1, 10).reshape(1, 3, 3)\nx, x.shape\n</pre> # Create a tensor  import torch x = torch.arange(1, 10).reshape(1, 3, 3) x, x.shape Out[58]: <pre>(tensor([[[1, 2, 3],\n          [4, 5, 6],\n          [7, 8, 9]]]),\n torch.Size([1, 3, 3]))</pre> <p>Indexing values goes outer dimension -&gt; inner dimension (check out the square brackets).</p> In\u00a0[59]: Copied! <pre># Let's index bracket by bracket\nprint(f\"First square bracket:\\n{x[0]}\") \nprint(f\"Second square bracket: {x[0][0]}\") \nprint(f\"Third square bracket: {x[0][0][0]}\")\n</pre> # Let's index bracket by bracket print(f\"First square bracket:\\n{x[0]}\")  print(f\"Second square bracket: {x[0][0]}\")  print(f\"Third square bracket: {x[0][0][0]}\") <pre>First square bracket:\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\nSecond square bracket: tensor([1, 2, 3])\nThird square bracket: 1\n</pre> <p>You can also use <code>:</code> to specify \"all values in this dimension\" and then use a comma (<code>,</code>) to add another dimension.</p> In\u00a0[60]: Copied! <pre># Get all values of 0th dimension and the 0 index of 1st dimension\nx[:, 0]\n</pre> # Get all values of 0th dimension and the 0 index of 1st dimension x[:, 0] Out[60]: <pre>tensor([[1, 2, 3]])</pre> In\u00a0[61]: Copied! <pre># Get all values of 0th &amp; 1st dimensions but only index 1 of 2nd dimension\nx[:, :, 1]\n</pre> # Get all values of 0th &amp; 1st dimensions but only index 1 of 2nd dimension x[:, :, 1] Out[61]: <pre>tensor([[2, 5, 8]])</pre> In\u00a0[62]: Copied! <pre># Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\nx[:, 1, 1]\n</pre> # Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension x[:, 1, 1] Out[62]: <pre>tensor([5])</pre> In\u00a0[63]: Copied! <pre># Get index 0 of 0th and 1st dimension and all values of 2nd dimension \nx[0, 0, :] # same as x[0][0]\n</pre> # Get index 0 of 0th and 1st dimension and all values of 2nd dimension  x[0, 0, :] # same as x[0][0] Out[63]: <pre>tensor([1, 2, 3])</pre> <p>Indexing can be quite confusing to begin with, especially with larger tensors (I still have to try indexing multiple times to get it right). But with a bit of practice and following the data explorer's motto (visualize, visualize, visualize), you'll start to get the hang of it.</p> In\u00a0[65]: Copied! <pre># NumPy array to tensor\nimport torch\nimport numpy as np\narray = np.arange(1.0, 8.0)\ntensor = torch.from_numpy(array)\narray, tensor\n</pre> # NumPy array to tensor import torch import numpy as np array = np.arange(1.0, 8.0) tensor = torch.from_numpy(array) array, tensor Out[65]: <pre>(array([1., 2., 3., 4., 5., 6., 7.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))</pre> <p>Note: By default, NumPy arrays are created with the datatype <code>float64</code> and if you convert it to a PyTorch tensor, it'll keep the same datatype (as above).</p> <p>However, many PyTorch calculations default to using <code>float32</code>.</p> <p>So if you want to convert your NumPy array (float64) -&gt; PyTorch tensor (float64) -&gt; PyTorch tensor (float32), you can use <code>tensor = torch.from_numpy(array).type(torch.float32)</code>.</p> <p>Because we reassigned <code>tensor</code> above, if you change the tensor, the array stays the same.</p> In\u00a0[66]: Copied! <pre># Change the array, keep the tensor\narray = array + 1\narray, tensor\n</pre> # Change the array, keep the tensor array = array + 1 array, tensor Out[66]: <pre>(array([2., 3., 4., 5., 6., 7., 8.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))</pre> <p>And if you want to go from PyTorch tensor to NumPy array, you can call <code>tensor.numpy()</code>.</p> In\u00a0[67]: Copied! <pre># Tensor to NumPy array\ntensor = torch.ones(7) # create a tensor of ones with dtype=float32\nnumpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\ntensor, numpy_tensor\n</pre> # Tensor to NumPy array tensor = torch.ones(7) # create a tensor of ones with dtype=float32 numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed tensor, numpy_tensor Out[67]: <pre>(tensor([1., 1., 1., 1., 1., 1., 1.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))</pre> <p>And the same rule applies as above, if you change the original <code>tensor</code>, the new <code>numpy_tensor</code> stays the same.</p> In\u00a0[64]: Copied! <pre># Change the tensor, keep the array the same\ntensor = tensor + 1\ntensor, numpy_tensor\n</pre> # Change the tensor, keep the array the same tensor = tensor + 1 tensor, numpy_tensor Out[64]: <pre>(tensor([2., 2., 2., 2., 2., 2., 2.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))</pre> In\u00a0[68]: Copied! <pre>import torch\n\n# Create two random tensors\nrandom_tensor_A = torch.rand(3, 4)\nrandom_tensor_B = torch.rand(3, 4)\n\nprint(f\"Tensor A:\\n{random_tensor_A}\\n\")\nprint(f\"Tensor B:\\n{random_tensor_B}\\n\")\nprint(f\"Does Tensor A equal Tensor B? (anywhere)\")\nrandom_tensor_A == random_tensor_B\n</pre> import torch  # Create two random tensors random_tensor_A = torch.rand(3, 4) random_tensor_B = torch.rand(3, 4)  print(f\"Tensor A:\\n{random_tensor_A}\\n\") print(f\"Tensor B:\\n{random_tensor_B}\\n\") print(f\"Does Tensor A equal Tensor B? (anywhere)\") random_tensor_A == random_tensor_B <pre>Tensor A:\ntensor([[0.8016, 0.3649, 0.6286, 0.9663],\n        [0.7687, 0.4566, 0.5745, 0.9200],\n        [0.3230, 0.8613, 0.0919, 0.3102]])\n\nTensor B:\ntensor([[0.9536, 0.6002, 0.0351, 0.6826],\n        [0.3743, 0.5220, 0.1336, 0.9666],\n        [0.9754, 0.8474, 0.8988, 0.1105]])\n\nDoes Tensor A equal Tensor B? (anywhere)\n</pre> Out[68]: <pre>tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])</pre> <p>Just as you might've expected, the tensors come out with different values.</p> <p>But what if you wanted to created two random tensors with the same values.</p> <p>As in, the tensors would still contain random values but they would be of the same flavour.</p> <p>That's where <code>torch.manual_seed(seed)</code> comes in, where <code>seed</code> is an integer (like <code>42</code> but it could be anything) that flavours the randomness.</p> <p>Let's try it out by creating some more flavoured random tensors.</p> In\u00a0[69]: Copied! <pre>import torch\nimport random\n\n# # Set the random seed\nRANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\ntorch.manual_seed(seed=RANDOM_SEED) \nrandom_tensor_C = torch.rand(3, 4)\n\n# Have to reset the seed every time a new rand() is called \n# Without this, tensor_D would be different to tensor_C \ntorch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\nrandom_tensor_D = torch.rand(3, 4)\n\nprint(f\"Tensor C:\\n{random_tensor_C}\\n\")\nprint(f\"Tensor D:\\n{random_tensor_D}\\n\")\nprint(f\"Does Tensor C equal Tensor D? (anywhere)\")\nrandom_tensor_C == random_tensor_D\n</pre> import torch import random  # # Set the random seed RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below torch.manual_seed(seed=RANDOM_SEED)  random_tensor_C = torch.rand(3, 4)  # Have to reset the seed every time a new rand() is called  # Without this, tensor_D would be different to tensor_C  torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens random_tensor_D = torch.rand(3, 4)  print(f\"Tensor C:\\n{random_tensor_C}\\n\") print(f\"Tensor D:\\n{random_tensor_D}\\n\") print(f\"Does Tensor C equal Tensor D? (anywhere)\") random_tensor_C == random_tensor_D <pre>Tensor C:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor D:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor C equal Tensor D? (anywhere)\n</pre> Out[69]: <pre>tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])</pre> <p>Nice!</p> <p>It looks like setting the seed worked.</p> <p>Resource: What we've just covered only scratches the surface of reproducibility in PyTorch. For more, on reproducbility in general and random seeds, I'd checkout:</p> <ul> <li>The PyTorch reproducibility documentation (a good exericse would be to read through this for 10-minutes and even if you don't understand it now, being aware of it is important).</li> <li>The Wikipedia random seed page (this'll give a good overview of random seeds and pseudorandomness in general).</li> </ul> In\u00a0[70]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Fri Nov  1 08:25:53 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n|  0%   38C    P2             82W /  480W |   13068MiB /  24564MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A   1198496      C   python                                      13056MiB |\n+-----------------------------------------------------------------------------------------+\n</pre> <p>If you don't have a Nvidia GPU accessible, the above will output something like:</p> <pre><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n</code></pre> <p>In that case, go back up and follow the install steps.</p> <p>If you do have a GPU, the line above will output something like:</p> <pre><code>Wed Jan 19 22:09:08 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> In\u00a0[71]: Copied! <pre># Check for GPU\nimport torch\ntorch.cuda.is_available()\n</pre> # Check for GPU import torch torch.cuda.is_available() Out[71]: <pre>True</pre> <p>If the above outputs <code>True</code>, PyTorch can see and use the GPU, if it outputs <code>False</code>, it can't see the GPU and in that case, you'll have to go back through the installation steps.</p> <p>Now, let's say you wanted to setup your code so it ran on CPU or the GPU if it was available.</p> <p>That way, if you or someone decides to run your code, it'll work regardless of the computing device they're using.</p> <p>Let's create a <code>device</code> variable to store what kind of device is available.</p> In\u00a0[72]: Copied! <pre># Set device type\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Set device type device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[72]: <pre>'cuda'</pre> <p>If the above output <code>\"cuda\"</code> it means we can set all of our PyTorch code to use the available CUDA device (a GPU) and if it output <code>\"cpu\"</code>, our PyTorch code will stick with the CPU.</p> <p>Note: In PyTorch, it's best practice to write device agnostic code. This means code that'll run on CPU (always available) or GPU (if available).</p> <p>If you want to do faster computing you can use a GPU but if you want to do much faster computing, you can use multiple GPUs.</p> <p>You can count the number of GPUs PyTorch has access to using <code>torch.cuda.device_count()</code>.</p> In\u00a0[73]: Copied! <pre># Count number of devices\ntorch.cuda.device_count()\n</pre> # Count number of devices torch.cuda.device_count() Out[73]: <pre>1</pre> <p>Knowing the number of GPUs PyTorch has access to is helpful incase you wanted to run a specific process on one GPU and another process on another (PyTorch also has features to let you run a process across all GPUs).</p> In\u00a0[74]: Copied! <pre># Check for Apple Silicon GPU\nimport torch\ntorch.backends.mps.is_available() # Note this will print false if you're not running on a Mac\n</pre> # Check for Apple Silicon GPU import torch torch.backends.mps.is_available() # Note this will print false if you're not running on a Mac Out[74]: <pre>False</pre> In\u00a0[75]: Copied! <pre># Set device type\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\ndevice\n</pre> # Set device type device = \"mps\" if torch.backends.mps.is_available() else \"cpu\" device Out[75]: <pre>'cpu'</pre> <p>As before, if the above output <code>\"mps\"</code> it means we can set all of our PyTorch code to use the available Apple Silicon GPU.</p> In\u00a0[76]: Copied! <pre>if torch.cuda.is_available():\n    device = \"cuda\" # Use NVIDIA GPU (if available)\nelif torch.backends.mps.is_available():\n    device = \"mps\" # Use Apple Silicon GPU (if available)\nelse:\n    device = \"cpu\" # Default to CPU if no GPU is available\n</pre> if torch.cuda.is_available():     device = \"cuda\" # Use NVIDIA GPU (if available) elif torch.backends.mps.is_available():     device = \"mps\" # Use Apple Silicon GPU (if available) else:     device = \"cpu\" # Default to CPU if no GPU is available In\u00a0[77]: Copied! <pre># Create tensor (default on CPU)\ntensor = torch.tensor([1, 2, 3])\n\n# Tensor not on GPU\nprint(tensor, tensor.device)\n\n# Move tensor to GPU (if available)\ntensor_on_gpu = tensor.to(device)\ntensor_on_gpu\n</pre> # Create tensor (default on CPU) tensor = torch.tensor([1, 2, 3])  # Tensor not on GPU print(tensor, tensor.device)  # Move tensor to GPU (if available) tensor_on_gpu = tensor.to(device) tensor_on_gpu <pre>tensor([1, 2, 3]) cpu\n</pre> Out[77]: <pre>tensor([1, 2, 3], device='cuda:0')</pre> <p>If you have a GPU available, the above code will output something like:</p> <pre><code>tensor([1, 2, 3]) cpu\ntensor([1, 2, 3], device='cuda:0')\n</code></pre> <p>Notice the second tensor has <code>device='cuda:0'</code>, this means it's stored on the 0th GPU available (GPUs are 0 indexed, if two GPUs were available, they'd be <code>'cuda:0'</code> and <code>'cuda:1'</code> respectively, up to <code>'cuda:n'</code>).</p> In\u00a0[78]: Copied! <pre># If tensor is on GPU, can't transform it to NumPy (this will error)\ntensor_on_gpu.numpy()\n</pre> # If tensor is on GPU, can't transform it to NumPy (this will error) tensor_on_gpu.numpy() <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[78], line 2\n      1 # If tensor is on GPU, can't transform it to NumPy (this will error)\n----&gt; 2 tensor_on_gpu.numpy()\n\nTypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.</pre> <p>Instead, to get a tensor back to CPU and usable with NumPy we can use <code>Tensor.cpu()</code>.</p> <p>This copies the tensor to CPU memory so it's usable with CPUs.</p> In\u00a0[79]: Copied! <pre># Instead, copy the tensor back to cpu\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\ntensor_back_on_cpu\n</pre> # Instead, copy the tensor back to cpu tensor_back_on_cpu = tensor_on_gpu.cpu().numpy() tensor_back_on_cpu Out[79]: <pre>array([1, 2, 3])</pre> <p>The above returns a copy of the GPU tensor in CPU memory so the original tensor is still on GPU.</p> In\u00a0[77]: Copied! <pre>tensor_on_gpu\n</pre> tensor_on_gpu Out[77]: <pre>tensor([1, 2, 3], device='cuda:0')</pre>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#00-pytorch-fundamentals","title":"00. PyTorch Fundamentals\u00b6","text":""},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#what-is-pytorch","title":"What is PyTorch?\u00b6","text":"<p>PyTorch is an open source machine learning and deep learning framework.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#what-can-pytorch-be-used-for","title":"What can PyTorch be used for?\u00b6","text":"<p>PyTorch allows you to manipulate and process data and write machine learning algorithms using Python code.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#who-uses-pytorch","title":"Who uses PyTorch?\u00b6","text":"<p>Many of the worlds largest technology companies such as Meta (Facebook), Tesla and Microsoft as well as artificial intelligence research companies such as OpenAI use PyTorch to power research and bring machine learning to their products.</p> <p></p> <p>For example, Andrej Karpathy (head of AI at Tesla) has given several talks (PyTorch DevCon 2019, Tesla AI Day 2021) about how Tesla use PyTorch to power their self-driving computer vision models.</p> <p>PyTorch is also used in other industries such as agriculture to power computer vision on tractors.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#why-use-pytorch","title":"Why use PyTorch?\u00b6","text":"<p>Machine learning researchers love using PyTorch. And as of February 2022, PyTorch is the most used deep learning framework on Papers With Code, a website for tracking machine learning research papers and the code repositories attached with them.</p> <p>PyTorch also helps take care of many things such as GPU acceleration (making your code run faster) behind the scenes.</p> <p>So you can focus on manipulating data and writing algorithms and PyTorch will make sure it runs fast.</p> <p>And if companies such as Tesla and Meta (Facebook) use it to build models they deploy to power hundreds of applications, drive thousands of cars and deliver content to billions of people, it's clearly capable on the development front too.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#what-were-going-to-cover-in-this-module","title":"What we're going to cover in this module\u00b6","text":"<p>This course is broken down into different sections (notebooks).</p> <p>Each notebook covers important ideas and concepts within PyTorch.</p> <p>Subsequent notebooks build upon knowledge from the previous one (numbering starts at 00, 01, 02 and goes to whatever it ends up going to).</p> <p>This notebook deals with the basic building block of machine learning and deep learning, the tensor.</p> <p>Specifically, we're going to cover:</p> Topic Contents Introduction to tensors Tensors are the basic building block of all of machine learning and deep learning. Creating tensors Tensors can represent almost any kind of data (images, words, tables of numbers). Getting information from tensors If you can put information into a tensor, you'll want to get it out too. Manipulating tensors Machine learning algorithms (like neural networks) involve manipulating tensors in many different ways such as adding, multiplying, combining. Dealing with tensor shapes One of the most common issues in machine learning is dealing with shape mismatches (trying to mixed wrong shaped tensors with other tensors). Indexing on tensors If you've indexed on a Python list or NumPy array, it's very similar with tensors, except they can have far more dimensions. Mixing PyTorch tensors and NumPy PyTorch plays with tensors (<code>torch.Tensor</code>), NumPy likes arrays (<code>np.ndarray</code>) sometimes you'll want to mix and match these. Reproducibility Machine learning is very experimental and since it uses a lot of randomness to work, sometimes you'll want that randomness to not be so random. Running tensors on GPU GPUs (Graphics Processing Units) make your code faster, PyTorch makes it easy to run your code on GPUs."},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>And if you run into trouble, you can ask a question on the Discussions page there too.</p> <p>There's also the PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#importing-pytorch","title":"Importing PyTorch\u00b6","text":"<p>Note: Before running any of the code in this notebook, you should have gone through the PyTorch setup steps.</p> <p>However, if you're running on Google Colab, everything should work (Google Colab comes with PyTorch and other libraries installed).</p> <p>Let's start by importing PyTorch and checking the version we're using.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#introduction-to-tensors","title":"Introduction to tensors\u00b6","text":"<p>Now we've got PyTorch imported, it's time to learn about tensors.</p> <p>Tensors are the fundamental building block of machine learning.</p> <p>Their job is to represent data in a numerical way.</p> <p>For example, you could represent an image as a tensor with shape <code>[3, 224, 224]</code> which would mean <code>[colour_channels, height, width]</code>, as in the image has <code>3</code> colour channels (red, green, blue), a height of <code>224</code> pixels and a width of <code>224</code> pixels.</p> <p></p> <p>In tensor-speak (the language used to describe tensors), the tensor would have three dimensions, one for <code>colour_channels</code>, <code>height</code> and <code>width</code>.</p> <p>But we're getting ahead of ourselves.</p> <p>Let's learn more about tensors by coding them.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#creating-tensors","title":"Creating tensors\u00b6","text":"<p>PyTorch loves tensors. So much so there's a whole documentation page dedicated to the <code>torch.Tensor</code> class.</p> <p>Your first piece of homework is to read through the documentation on <code>torch.Tensor</code> for 10-minutes. But you can get to that later.</p> <p>Let's code.</p> <p>The first thing we're going to create is a scalar.</p> <p>A scalar is a single number and in tensor-speak it's a zero dimension tensor.</p> <p>Note: That's a trend for this course. We'll focus on writing specific code. But often I'll set exercises which involve reading and getting familiar with the PyTorch documentation. Because after all, once you're finished this course, you'll no doubt want to learn more. And the documentation is somewhere you'll be finding yourself quite often.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#random-tensors","title":"Random tensors\u00b6","text":"<p>We've established tensors represent some form of data.</p> <p>And machine learning models such as neural networks manipulate and seek patterns within tensors.</p> <p>But when building machine learning models with PyTorch, it's rare you'll create tensors by hand (like what we've being doing).</p> <p>Instead, a machine learning model often starts out with large random tensors of numbers and adjusts these random numbers as it works through data to better represent it.</p> <p>In essence:</p> <p><code>Start with random numbers -&gt; look at data -&gt; update random numbers -&gt; look at data -&gt; update random numbers...</code></p> <p>As a data scientist, you can define how the machine learning model starts (initialization), looks at data (representation) and updates (optimization) its random numbers.</p> <p>We'll get hands on with these steps later on.</p> <p>For now, let's see how to create a tensor of random numbers.</p> <p>We can do so using <code>torch.rand()</code> and passing in the <code>size</code> parameter.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#zeros-and-ones","title":"Zeros and ones\u00b6","text":"<p>Sometimes you'll just want to fill tensors with zeros or ones.</p> <p>This happens a lot with masking (like masking some of the values in one tensor with zeros to let a model know not to learn them).</p> <p>Let's create a tensor full of zeros with <code>torch.zeros()</code></p> <p>Again, the <code>size</code> parameter comes into play.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#creating-a-range-and-tensors-like","title":"Creating a range and tensors like\u00b6","text":"<p>Sometimes you might want a range of numbers, such as 1 to 10 or 0 to 100.</p> <p>You can use <code>torch.arange(start, end, step)</code> to do so.</p> <p>Where:</p> <ul> <li><code>start</code> = start of range (e.g. 0)</li> <li><code>end</code> = end of range (e.g. 10)</li> <li><code>step</code> = how many steps in between each value (e.g. 1)</li> </ul> <p>Note: In Python, you can use <code>range()</code> to create a range. However in PyTorch, <code>torch.range()</code> is deprecated and may show an error in the future.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#tensor-datatypes","title":"Tensor datatypes\u00b6","text":"<p>There are many different tensor datatypes available in PyTorch.</p> <p>Some are specific for CPU and some are better for GPU.</p> <p>Getting to know which is which can take some time.</p> <p>Generally if you see <code>torch.cuda</code> anywhere, the tensor is being used for GPU (since Nvidia GPUs use a computing toolkit called CUDA).</p> <p>The most common type (and generally the default) is <code>torch.float32</code> or <code>torch.float</code>.</p> <p>This is referred to as \"32-bit floating point\".</p> <p>But there's also 16-bit floating point (<code>torch.float16</code> or <code>torch.half</code>) and 64-bit floating point (<code>torch.float64</code> or <code>torch.double</code>).</p> <p>And to confuse things even more there's also 8-bit, 16-bit, 32-bit and 64-bit integers.</p> <p>Plus more!</p> <p>Note: An integer is a flat round number like <code>7</code> whereas a float has a decimal <code>7.0</code>.</p> <p>The reason for all of these is to do with precision in computing.</p> <p>Precision is the amount of detail used to describe a number.</p> <p>The higher the precision value (8, 16, 32), the more detail and hence data used to express a number.</p> <p>This matters in deep learning and numerical computing because you're making so many operations, the more detail you have to calculate on, the more compute you have to use.</p> <p>So lower precision datatypes are generally faster to compute on but sacrifice some performance on evaluation metrics like accuracy (faster to compute but less accurate).</p> <p>Resources:</p> <ul> <li>See the PyTorch documentation for a list of all available tensor datatypes.</li> <li>Read the Wikipedia page for an overview of what precision in computing) is.</li> </ul> <p>Let's see how to create some tensors with specific datatypes. We can do so using the <code>dtype</code> parameter.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#getting-information-from-tensors","title":"Getting information from tensors\u00b6","text":"<p>Once you've created tensors (or someone else or a PyTorch module has created them for you), you might want to get some information from them.</p> <p>We've seen these before but three of the most common attributes you'll want to find out about tensors are:</p> <ul> <li><code>shape</code> - what shape is the tensor? (some operations require specific shape rules)</li> <li><code>dtype</code> - what datatype are the elements within the tensor stored in?</li> <li><code>device</code> - what device is the tensor stored on? (usually GPU or CPU)</li> </ul> <p>Let's create a random tensor and find out details about it.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#manipulating-tensors-tensor-operations","title":"Manipulating tensors (tensor operations)\u00b6","text":"<p>In deep learning, data (images, text, video, audio, protein structures, etc) gets represented as tensors.</p> <p>A model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a representation of the patterns in the input data.</p> <p>These operations are often a wonderful dance between:</p> <ul> <li>Addition</li> <li>Substraction</li> <li>Multiplication (element-wise)</li> <li>Division</li> <li>Matrix multiplication</li> </ul> <p>And that's it. Sure there are a few more here and there but these are the basic building blocks of neural networks.</p> <p>Stacking these building blocks in the right way, you can create the most sophisticated of neural networks (just like lego!).</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#basic-operations","title":"Basic operations\u00b6","text":"<p>Let's start with a few of the fundamental operations, addition (<code>+</code>), subtraction (<code>-</code>), mutliplication (<code>*</code>).</p> <p>They work just as you think they would.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#matrix-multiplication-is-all-you-need","title":"Matrix multiplication (is all you need)\u00b6","text":"<p>One of the most common operations in machine learning and deep learning algorithms (like neural networks) is matrix multiplication.</p> <p>PyTorch implements matrix multiplication functionality in the <code>torch.matmul()</code> method.</p> <p>The main two rules for matrix multiplication to remember are:</p> <ol> <li>The inner dimensions must match:</li> </ol> <ul> <li><code>(3, 2) @ (3, 2)</code> won't work</li> <li><code>(2, 3) @ (3, 2)</code> will work</li> <li><code>(3, 2) @ (2, 3)</code> will work</li> </ul> <ol> <li>The resulting matrix has the shape of the outer dimensions:</li> </ol> <ul> <li><code>(2, 3) @ (3, 2)</code> -&gt; <code>(2, 2)</code></li> <li><code>(3, 2) @ (2, 3)</code> -&gt; <code>(3, 3)</code></li> </ul> <p>Note: \"<code>@</code>\" in Python is the symbol for matrix multiplication.</p> <p>Resource: You can see all of the rules for matrix multiplication using <code>torch.matmul()</code> in the PyTorch documentation.</p> <p>Let's create a tensor and perform element-wise multiplication and matrix multiplication on it.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#one-of-the-most-common-errors-in-deep-learning-shape-errors","title":"One of the most common errors in deep learning (shape errors)\u00b6","text":"<p>Because much of deep learning is multiplying and performing operations on matrices and matrices have a strict rule about what shapes and sizes can be combined, one of the most common errors you'll run into in deep learning is shape mismatches.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#finding-the-min-max-mean-sum-etc-aggregation","title":"Finding the min, max, mean, sum, etc (aggregation)\u00b6","text":"<p>Now we've seen a few ways to manipulate tensors, let's run through a few ways to aggregate them (go from more values to less values).</p> <p>First we'll create a tensor and then find the max, min, mean and sum of it.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#positional-minmax","title":"Positional min/max\u00b6","text":"<p>You can also find the index of a tensor where the max or minimum occurs with <code>torch.argmax()</code> and <code>torch.argmin()</code> respectively.</p> <p>This is helpful incase you just want the position where the highest (or lowest) value is and not the actual value itself (we'll see this in a later section when using the softmax activation function).</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#change-tensor-datatype","title":"Change tensor datatype\u00b6","text":"<p>As mentioned, a common issue with deep learning operations is having your tensors in different datatypes.</p> <p>If one tensor is in <code>torch.float64</code> and another is in <code>torch.float32</code>, you might run into some errors.</p> <p>But there's a fix.</p> <p>You can change the datatypes of tensors using <code>torch.Tensor.type(dtype=None)</code> where the <code>dtype</code> parameter is the datatype you'd like to use.</p> <p>First we'll create a tensor and check it's datatype (the default is <code>torch.float32</code>).</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#reshaping-stacking-squeezing-and-unsqueezing","title":"Reshaping, stacking, squeezing and unsqueezing\u00b6","text":"<p>Often times you'll want to reshape or change the dimensions of your tensors without actually changing the values inside them.</p> <p>To do so, some popular methods are:</p> Method One-line description <code>torch.reshape(input, shape)</code> Reshapes <code>input</code> to <code>shape</code> (if compatible), can also use <code>torch.Tensor.reshape()</code>. <code>Tensor.view(shape)</code> Returns a view of the original tensor in a different <code>shape</code> but shares the same data as the original tensor. <code>torch.stack(tensors, dim=0)</code> Concatenates a sequence of <code>tensors</code> along a new dimension (<code>dim</code>), all <code>tensors</code> must be same size. <code>torch.squeeze(input)</code> Squeezes <code>input</code> to remove all the dimenions with value <code>1</code>. <code>torch.unsqueeze(input, dim)</code> Returns <code>input</code> with a dimension value of <code>1</code> added at <code>dim</code>. <code>torch.permute(input, dims)</code> Returns a view of the original <code>input</code> with its dimensions permuted (rearranged) to <code>dims</code>. <p>Why do any of these?</p> <p>Because deep learning models (neural networks) are all about manipulating tensors in some way. And because of the rules of matrix multiplication, if you've got shape mismatches, you'll run into errors. These methods help you make sure the right elements of your tensors are mixing with the right elements of other tensors.</p> <p>Let's try them out.</p> <p>First, we'll create a tensor.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#indexing-selecting-data-from-tensors","title":"Indexing (selecting data from tensors)\u00b6","text":"<p>Sometimes you'll want to select specific data from tensors (for example, only the first column or second row).</p> <p>To do so, you can use indexing.</p> <p>If you've ever done indexing on Python lists or NumPy arrays, indexing in PyTorch with tensors is very similar.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#pytorch-tensors-numpy","title":"PyTorch tensors &amp; NumPy\u00b6","text":"<p>Since NumPy is a popular Python numerical computing library, PyTorch has functionality to interact with it nicely.</p> <p>The two main methods you'll want to use for NumPy to PyTorch (and back again) are:</p> <ul> <li><code>torch.from_numpy(ndarray)</code> - NumPy array -&gt; PyTorch tensor.</li> <li><code>torch.Tensor.numpy()</code> - PyTorch tensor -&gt; NumPy array.</li> </ul> <p>Let's try them out.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#reproducibility-trying-to-take-the-random-out-of-random","title":"Reproducibility (trying to take the random out of random)\u00b6","text":"<p>As you learn more about neural networks and machine learning, you'll start to discover how much randomness plays a part.</p> <p>Well, pseudorandomness that is. Because after all, as they're designed, a computer is fundamentally deterministic (each step is predictable) so the randomness they create are simulated randomness (though there is debate on this too, but since I'm not a computer scientist, I'll let you find out more yourself).</p> <p>How does this relate to neural networks and deep learning then?</p> <p>We've discussed neural networks start with random numbers to describe patterns in data (these numbers are poor descriptions) and try to improve those random numbers using tensor operations (and a few other things we haven't discussed yet) to better describe patterns in data.</p> <p>In short:</p> <p><code>start with random numbers -&gt; tensor operations -&gt; try to make better (again and again and again)</code></p> <p>Although randomness is nice and powerful, sometimes you'd like there to be a little less randomness.</p> <p>Why?</p> <p>So you can perform repeatable experiments.</p> <p>For example, you create an algorithm capable of achieving X performance.</p> <p>And then your friend tries it out to verify you're not crazy.</p> <p>How could they do such a thing?</p> <p>That's where reproducibility comes in.</p> <p>In other words, can you get the same (or very similar) results on your computer running the same code as I get on mine?</p> <p>Let's see a brief example of reproducibility in PyTorch.</p> <p>We'll start by creating two random tensors, since they're random, you'd expect them to be different right?</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#running-tensors-on-gpus-and-making-faster-computations","title":"Running tensors on GPUs (and making faster computations)\u00b6","text":"<p>Deep learning algorithms require a lot of numerical operations.</p> <p>And by default these operations are often done on a CPU (computer processing unit).</p> <p>However, there's another common piece of hardware called a GPU (graphics processing unit), which is often much faster at performing the specific types of operations neural networks need (matrix multiplications) than CPUs.</p> <p>Your computer might have one.</p> <p>If so, you should look to use it whenever you can to train neural networks because chances are it'll speed up the training time dramatically.</p> <p>There are a few ways to first get access to a GPU and secondly get PyTorch to use the GPU.</p> <p>Note: When I reference \"GPU\" throughout this course, I'm referencing a Nvidia GPU with CUDA enabled (CUDA is a computing platform and API that helps allow GPUs be used for general purpose computing &amp; not just graphics) unless otherwise specified.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#1-getting-a-gpu","title":"1. Getting a GPU\u00b6","text":"<p>You may already know what's going on when I say GPU. But if not, there are a few ways to get access to one.</p> Method Difficulty to setup Pros Cons How to setup Google Colab Easy Free to use, almost zero setup required, can share work with others as easy as a link Doesn't save your data outputs, limited compute, subject to timeouts Follow the Google Colab Guide Use your own Medium Run everything locally on your own machine GPUs aren't free, require upfront cost Follow the PyTorch installation guidelines Cloud computing (AWS, GCP, Azure) Medium-Hard Small upfront cost, access to almost infinite compute Can get expensive if running continually, takes some time to setup right Follow the PyTorch installation guidelines <p>There are more options for using GPUs but the above three will suffice for now.</p> <p>Personally, I use a combination of Google Colab and my own personal computer for small scale experiments (and creating this course) and go to cloud resources when I need more compute power.</p> <p>Resource: If you're looking to purchase a GPU of your own but not sure what to get, Tim Dettmers has an excellent guide.</p> <p>To check if you've got access to a Nvidia GPU, you can run <code>!nvidia-smi</code> where the <code>!</code> (also called bang) means \"run this on the command line\".</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#2-getting-pytorch-to-run-on-the-gpu","title":"2. Getting PyTorch to run on the GPU\u00b6","text":"<p>Once you've got a GPU ready to access, the next step is getting PyTorch to use for storing data (tensors) and computing on data (performing operations on tensors).</p> <p>To do so, you can use the <code>torch.cuda</code> package.</p> <p>Rather than talk about it, let's try it out.</p> <p>You can test if PyTorch has access to a GPU using <code>torch.cuda.is_available()</code>.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#21-getting-pytorch-to-run-on-apple-silicon","title":"2.1 Getting PyTorch to run on Apple Silicon\u00b6","text":"<p>In order to run PyTorch on Apple's M1/M2/M3 GPUs you can use the <code>torch.backends.mps</code> module.</p> <p>Be sure that the versions of the macOS and Pytorch are updated.</p> <p>You can test if PyTorch has access to a GPU using <code>torch.backends.mps.is_available()</code>.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#3-putting-tensors-and-models-on-the-gpu","title":"3. Putting tensors (and models) on the GPU\u00b6","text":"<p>You can put tensors (and models, we'll see this later) on a specific device by calling <code>to(device)</code> on them. Where <code>device</code> is the target device you'd like the tensor (or model) to go to.</p> <p>Why do this?</p> <p>GPUs offer far faster numerical computing than CPUs do and if a GPU isn't available, because of our device agnostic code (see above), it'll run on the CPU.</p> <p>Note: Putting a tensor on GPU using <code>to(device)</code> (e.g. <code>some_tensor.to(device)</code>) returns a copy of that tensor, e.g. the same tensor will be on CPU and GPU. To overwrite tensors, reassign them:</p> <p><code>some_tensor = some_tensor.to(device)</code></p> <p>Let's try creating a tensor and putting it on the GPU (if it's available).</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#4-moving-tensors-back-to-the-cpu","title":"4. Moving tensors back to the CPU\u00b6","text":"<p>What if we wanted to move the tensor back to CPU?</p> <p>For example, you'll want to do this if you want to interact with your tensors with NumPy (NumPy does not leverage the GPU).</p> <p>Let's try using the <code>torch.Tensor.numpy()</code> method on our <code>tensor_on_gpu</code>.</p>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 00.</li> <li>Example solutions notebook for 00 (try the exercises before looking at this).</li> </ul> <ol> <li>Documentation reading - A big part of deep learning (and learning to code in general) is getting familiar with the documentation of a certain framework you're using. We'll be using the PyTorch documentation a lot throughout the rest of this course. So I'd recommend spending 10-minutes reading the following (it's okay if you don't get some things for now, the focus is not yet full understanding, it's awareness). See the documentation on <code>torch.Tensor</code> and for <code>torch.cuda</code>.</li> <li>Create a random tensor with shape <code>(7, 7)</code>.</li> <li>Perform a matrix multiplication on the tensor from 2 with another random tensor with shape <code>(1, 7)</code> (hint: you may have to transpose the second tensor).</li> <li>Set the random seed to <code>0</code> and do exercises 2 &amp; 3 over again.</li> <li>Speaking of random seeds, we saw how to set it with <code>torch.manual_seed()</code> but is there a GPU equivalent? (hint: you'll need to look into the documentation for <code>torch.cuda</code> for this one). If there is, set the GPU random seed to <code>1234</code>.</li> <li>Create two random tensors of shape <code>(2, 3)</code> and send them both to the GPU (you'll need access to a GPU for this). Set <code>torch.manual_seed(1234)</code> when creating the tensors (this doesn't have to be the GPU random seed).</li> <li>Perform a matrix multiplication on the tensors you created in 6 (again, you may have to adjust the shapes of one of the tensors).</li> <li>Find the maximum and minimum values of the output of 7.</li> <li>Find the maximum and minimum index values of the output of 7.</li> <li>Make a random tensor with shape <code>(1, 1, 1, 10)</code> and then create a new tensor with all the <code>1</code> dimensions removed to be left with a tensor of shape <code>(10)</code>. Set the seed to <code>7</code> when you create it and print out the first tensor and it's shape as well as the second tensor and it's shape.</li> </ol>"},{"location":"Learning/Pytorch/00_pytorch_fundamentals/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Spend 1-hour going through the PyTorch basics tutorial (I'd recommend the Quickstart and Tensors sections).</li> <li>To learn more on how a tensor can represent data, see this video: What's a tensor?</li> </ul>"},{"location":"Learning/Pytorch/01_pytorch_workflow/","title":"01. PyTorch Workflow Fundamentals","text":"<p>View Source Code | View Slides | Watch Video Walkthrough</p> In\u00a0[1]: Copied! <pre>what_were_covering = {1: \"data (prepare and load)\",\n    2: \"build model\",\n    3: \"fitting the model to data (training)\",\n    4: \"making predictions and evaluating a model (inference)\",\n    5: \"saving and loading a model\",\n    6: \"putting it all together\"\n}\n</pre> what_were_covering = {1: \"data (prepare and load)\",     2: \"build model\",     3: \"fitting the model to data (training)\",     4: \"making predictions and evaluating a model (inference)\",     5: \"saving and loading a model\",     6: \"putting it all together\" } <p>And now let's import what we'll need for this module.</p> <p>We're going to get <code>torch</code>, <code>torch.nn</code> (<code>nn</code> stands for neural network and this package contains the building blocks for creating neural networks in PyTorch) and <code>matplotlib</code>.</p> In\u00a0[2]: Copied! <pre>import torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n# Check PyTorch version\ntorch.__version__\n</pre> import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import matplotlib.pyplot as plt # Check PyTorch version torch.__version__ Out[2]: <pre>'2.2.2'</pre> In\u00a0[3]: Copied! <pre># Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n</pre> # Create *known* parameters weight = 0.7 bias = 0.3  # Create data start = 0 end = 1 step = 0.02 X = torch.arange(start, end, step).unsqueeze(dim=1) y = weight * X + bias  X[:10], y[:10] Out[3]: <pre>(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))</pre> <p>Beautiful! Now we're going to move towards building a model that can learn the relationship between <code>X</code> (features) and <code>y</code> (labels).</p> In\u00a0[4]: Copied! <pre># Create train/test split\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Create train/test split train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing  X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) Out[4]: <pre>(40, 40, 10, 10)</pre> <p>Wonderful, we've got 40 samples for training (<code>X_train</code> &amp; <code>y_train</code>) and 10 samples for testing (<code>X_test</code> &amp; <code>y_test</code>).</p> <p>The model we create is going to try and learn the relationship between <code>X_train</code> &amp; <code>y_train</code> and then we will evaluate what it learns on <code>X_test</code> and <code>y_test</code>.</p> <p>But right now our data is just numbers on a page.</p> <p>Let's create a function to visualize it.</p> In\u00a0[5]: Copied! <pre>def plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n  \"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n  \n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n  if predictions is not None:\n    # Plot the predictions in red (predictions were made on the test data)\n    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n  # Show the legend\n  plt.legend(prop={\"size\": 14});\n</pre> def plot_predictions(train_data=X_train,                       train_labels=y_train,                       test_data=X_test,                       test_labels=y_test,                       predictions=None):   \"\"\"   Plots training data, test data and compares predictions.   \"\"\"   plt.figure(figsize=(10, 7))    # Plot training data in blue   plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")      # Plot test data in green   plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")    if predictions is not None:     # Plot the predictions in red (predictions were made on the test data)     plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")    # Show the legend   plt.legend(prop={\"size\": 14}); In\u00a0[6]: Copied! <pre>plot_predictions();\n</pre> plot_predictions(); <p>Epic!</p> <p>Now instead of just being numbers on a page, our data is a straight line.</p> <p>Note: Now's a good time to introduce you to the data explorer's motto... \"visualize, visualize, visualize!\"</p> <p>Think of this whenever you're working with data and turning it into numbers, if you can visualize something, it can do wonders for understanding.</p> <p>Machines love numbers and we humans like numbers too but we also like to look at things.</p> In\u00a0[7]: Copied! <pre># Create a Linear Regression model class\nclass LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)\n                                                dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                   requires_grad=True) # &lt;- can we update this value with gradient descent?)\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)\n                                            dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                requires_grad=True) # &lt;- can we update this value with gradient descent?))\n\n    # Forward defines the computation in the model\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)\n        return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b)\n</pre> # Create a Linear Regression model class class LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)     def __init__(self):         super().__init__()          self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)                                                 dtype=torch.float), # &lt;- PyTorch loves float32 by default                                    requires_grad=True) # &lt;- can we update this value with gradient descent?)          self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)                                             dtype=torch.float), # &lt;- PyTorch loves float32 by default                                 requires_grad=True) # &lt;- can we update this value with gradient descent?))      # Forward defines the computation in the model     def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)         return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b) <p>Alright there's a fair bit going on above but let's break it down bit by bit.</p> <p>Resource: We'll be using Python classes to create bits and pieces for building neural networks. If you're unfamiliar with Python class notation, I'd recommend reading Real Python's Object Orientating programming in Python 3 guide a few times.</p> In\u00a0[8]: Copied! <pre># Set manual seed since nn.Parameter are randomly initialzied\ntorch.manual_seed(42)\n\n# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\nmodel_0 = LinearRegressionModel()\n\n# Check the nn.Parameter(s) within the nn.Module subclass we created\nlist(model_0.parameters())\n</pre> # Set manual seed since nn.Parameter are randomly initialzied torch.manual_seed(42)  # Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s)) model_0 = LinearRegressionModel()  # Check the nn.Parameter(s) within the nn.Module subclass we created list(model_0.parameters()) Out[8]: <pre>[Parameter containing:\n tensor([0.3367], requires_grad=True),\n Parameter containing:\n tensor([0.1288], requires_grad=True)]</pre> <p>We can also get the state (what the model contains) of the model using <code>.state_dict()</code>.</p> In\u00a0[9]: Copied! <pre># List named parameters \nmodel_0.state_dict()\n</pre> # List named parameters  model_0.state_dict() Out[9]: <pre>OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])</pre> <p>Notice how the values for <code>weights</code> and <code>bias</code> from <code>model_0.state_dict()</code> come out as random float tensors?</p> <p>This is because we initialized them above using <code>torch.randn()</code>.</p> <p>Essentially we want to start from random parameters and get the model to update them towards parameters that fit our data best (the hardcoded <code>weight</code> and <code>bias</code> values we set when creating our straight line data).</p> <p>Exercise: Try changing the <code>torch.manual_seed()</code> value two cells above, see what happens to the weights and bias values.</p> <p>Because our model starts with random values, right now it'll have poor predictive power.</p> In\u00a0[10]: Copied! <pre># Make predictions with model\nwith torch.inference_mode(): \n    y_preds = model_0(X_test)\n\n# Note: in older PyTorch code you might also see torch.no_grad()\n# with torch.no_grad():\n#   y_preds = model_0(X_test)\n</pre> # Make predictions with model with torch.inference_mode():      y_preds = model_0(X_test)  # Note: in older PyTorch code you might also see torch.no_grad() # with torch.no_grad(): #   y_preds = model_0(X_test) <p>Hmm?</p> <p>You probably noticed we used <code>torch.inference_mode()</code> as a context manager (that's what the <code>with torch.inference_mode():</code> is) to make the predictions.</p> <p>As the name suggests, <code>torch.inference_mode()</code> is used when using a model for inference (making predictions).</p> <p><code>torch.inference_mode()</code> turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make forward-passes (data going through the <code>forward()</code> method) faster.</p> <p>Note: In older PyTorch code, you may also see <code>torch.no_grad()</code> being used for inference. While <code>torch.inference_mode()</code> and <code>torch.no_grad()</code> do similar things, <code>torch.inference_mode()</code> is newer, potentially faster and preferred. See this Tweet from PyTorch for more.</p> <p>We've made some predictions, let's see what they look like.</p> In\u00a0[11]: Copied! <pre># Check the predictions\nprint(f\"Number of testing samples: {len(X_test)}\") \nprint(f\"Number of predictions made: {len(y_preds)}\")\nprint(f\"Predicted values:\\n{y_preds}\")\n</pre> # Check the predictions print(f\"Number of testing samples: {len(X_test)}\")  print(f\"Number of predictions made: {len(y_preds)}\") print(f\"Predicted values:\\n{y_preds}\") <pre>Number of testing samples: 10\nNumber of predictions made: 10\nPredicted values:\ntensor([[0.3982],\n        [0.4049],\n        [0.4116],\n        [0.4184],\n        [0.4251],\n        [0.4318],\n        [0.4386],\n        [0.4453],\n        [0.4520],\n        [0.4588]])\n</pre> <p>Notice how there's one prediction value per testing sample.</p> <p>This is because of the kind of data we're using. For our straight line, one <code>X</code> value maps to one <code>y</code> value.</p> <p>However, machine learning models are very flexible. You could have 100 <code>X</code> values mapping to one, two, three or 10 <code>y</code> values. It all depends on what you're working on.</p> <p>Our predictions are still numbers on a page, let's visualize them with our <code>plot_predictions()</code> function we created above.</p> In\u00a0[12]: Copied! <pre>plot_predictions(predictions=y_preds)\n</pre> plot_predictions(predictions=y_preds) In\u00a0[13]: Copied! <pre>y_test - y_preds\n</pre> y_test - y_preds Out[13]: <pre>tensor([[0.4618],\n        [0.4691],\n        [0.4764],\n        [0.4836],\n        [0.4909],\n        [0.4982],\n        [0.5054],\n        [0.5127],\n        [0.5200],\n        [0.5272]])</pre> <p>Woah! Those predictions look pretty bad...</p> <p>This make sense though when you remember our model is just using random parameter values to make predictions.</p> <p>It hasn't even looked at the blue dots to try to predict the green dots.</p> <p>Time to change that.</p> In\u00a0[14]: Copied! <pre># Create the loss function\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Create the optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))\n</pre> # Create the loss function loss_fn = nn.L1Loss() # MAE loss is same as L1Loss  # Create the optimizer optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize                             lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time)) <pre>/home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[15]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs (how many times the model will pass over the training data)\nepochs = 100\n\n# Create empty loss lists to track values\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model)\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside \n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Progress the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode\n    model_0.eval()\n\n    with torch.inference_mode():\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening\n      if epoch % 10 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy())\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n</pre> torch.manual_seed(42)  # Set the number of epochs (how many times the model will pass over the training data) epochs = 100  # Create empty loss lists to track values train_loss_values = [] test_loss_values = [] epoch_count = []  for epoch in range(epochs):     ### Training      # Put model in training mode (this is the default state of a model)     model_0.train()      # 1. Forward pass on train data using the forward() method inside      y_pred = model_0(X_train)     # print(y_pred)      # 2. Calculate the loss (how different are our models predictions to the ground truth)     loss = loss_fn(y_pred, y_train)      # 3. Zero grad of the optimizer     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Progress the optimizer     optimizer.step()      ### Testing      # Put the model in evaluation mode     model_0.eval()      with torch.inference_mode():       # 1. Forward pass on test data       test_pred = model_0(X_test)        # 2. Caculate loss on test data       test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type        # Print out what's happening       if epoch % 10 == 0:             epoch_count.append(epoch)             train_loss_values.append(loss.detach().numpy())             test_loss_values.append(test_loss.detach().numpy())             print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \") <pre>Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 \nEpoch: 10 | MAE Train Loss: 0.1976713240146637 | MAE Test Loss: 0.3463551998138428 \nEpoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 \nEpoch: 30 | MAE Train Loss: 0.053148526698350906 | MAE Test Loss: 0.14464017748832703 \nEpoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 \nEpoch: 50 | MAE Train Loss: 0.04167863354086876 | MAE Test Loss: 0.09919948130846024 \nEpoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 \nEpoch: 70 | MAE Train Loss: 0.03476089984178543 | MAE Test Loss: 0.0805937647819519 \nEpoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 \nEpoch: 90 | MAE Train Loss: 0.02788739837706089 | MAE Test Loss: 0.06473556160926819 \n</pre> <p>Oh would you look at that! Looks like our loss is going down with every epoch, let's plot it to find out.</p> In\u00a0[16]: Copied! <pre># Plot the loss curves\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</pre> # Plot the loss curves plt.plot(epoch_count, train_loss_values, label=\"Train loss\") plt.plot(epoch_count, test_loss_values, label=\"Test loss\") plt.title(\"Training and test loss curves\") plt.ylabel(\"Loss\") plt.xlabel(\"Epochs\") plt.legend(); <p>Nice! The loss curves show the loss going down over time. Remember, loss is the measure of how wrong your model is, so the lower the better.</p> <p>But why did the loss go down?</p> <p>Well, thanks to our loss function and optimizer, the model's internal parameters (<code>weights</code> and <code>bias</code>) were updated to better reflect the underlying patterns in the data.</p> <p>Let's inspect our model's <code>.state_dict()</code> to see see how close our model gets to the original values we set for weights and bias.</p> In\u00a0[17]: Copied! <pre># Find our model's learned parameters\nprint(\"The model learned the following values for weights and bias:\")\nprint(model_0.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</pre> # Find our model's learned parameters print(\"The model learned the following values for weights and bias:\") print(model_0.state_dict()) print(\"\\nAnd the original values for weights and bias are:\") print(f\"weights: {weight}, bias: {bias}\") <pre>The model learned the following values for weights and bias:\nOrderedDict([('weights', tensor([0.5784])), ('bias', tensor([0.3513]))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n</pre> <p>Wow! How cool is that?</p> <p>Our model got very close to calculate the exact original values for <code>weight</code> and <code>bias</code> (and it would probably get even closer if we trained it for longer).</p> <p>Exercise: Try changing the <code>epochs</code> value above to 200, what happens to the loss curves and the weights and bias parameter values of the model?</p> <p>It'd likely never guess them perfectly (especially when using more complicated datasets) but that's okay, often you can do very cool things with a close approximation.</p> <p>This is the whole idea of machine learning and deep learning, there are some ideal values that describe our data and rather than figuring them out by hand, we can train a model to figure them out programmatically.</p> In\u00a0[18]: Copied! <pre># 1. Set the model in evaluation mode\nmodel_0.eval()\n\n# 2. Setup the inference mode context manager\nwith torch.inference_mode():\n  # 3. Make sure the calculations are done with the model and data on the same device\n  # in our case, we haven't setup device-agnostic code yet so our data and model are\n  # on the CPU by default.\n  # model_0.to(device)\n  # X_test = X_test.to(device)\n  y_preds = model_0(X_test)\ny_preds\n</pre> # 1. Set the model in evaluation mode model_0.eval()  # 2. Setup the inference mode context manager with torch.inference_mode():   # 3. Make sure the calculations are done with the model and data on the same device   # in our case, we haven't setup device-agnostic code yet so our data and model are   # on the CPU by default.   # model_0.to(device)   # X_test = X_test.to(device)   y_preds = model_0(X_test) y_preds Out[18]: <pre>tensor([[0.8141],\n        [0.8256],\n        [0.8372],\n        [0.8488],\n        [0.8603],\n        [0.8719],\n        [0.8835],\n        [0.8950],\n        [0.9066],\n        [0.9182]])</pre> <p>Nice! We've made some predictions with our trained model, now how do they look?</p> In\u00a0[19]: Copied! <pre>plot_predictions(predictions=y_preds)\n</pre> plot_predictions(predictions=y_preds) <p>Woohoo! Those red dots are looking far closer than they were before!</p> <p>Let's get onto saving an reloading a model in PyTorch.</p> In\u00a0[20]: Copied! <pre>from pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n</pre> from pathlib import Path  # 1. Create models directory  MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, exist_ok=True)  # 2. Create model save path  MODEL_NAME = \"01_pytorch_workflow_model_0.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # 3. Save the model state dict  print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters            f=MODEL_SAVE_PATH)  <pre>Saving model to: models/01_pytorch_workflow_model_0.pth\n</pre> In\u00a0[21]: Copied! <pre># Check the saved file path\n!ls -l models/01_pytorch_workflow_model_0.pth\n</pre> # Check the saved file path !ls -l models/01_pytorch_workflow_model_0.pth <pre>-rw-rw-r-- 1 jupyter-trunglph jupyter-trunglph 1680 Nov  4 05:08 models/01_pytorch_workflow_model_0.pth\n</pre> In\u00a0[22]: Copied! <pre># Instantiate a new instance of our model (this will be instantiated with random weights)\nloaded_model_0 = LinearRegressionModel()\n\n# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\nloaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n</pre> # Instantiate a new instance of our model (this will be instantiated with random weights) loaded_model_0 = LinearRegressionModel()  # Load the state_dict of our saved model (this will update the new instance of our model with trained weights) loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH)) Out[22]: <pre>&lt;All keys matched successfully&gt;</pre> <p>Excellent! It looks like things matched up.</p> <p>Now to test our loaded model, let's perform inference with it (make predictions) on the test data.</p> <p>Remember the rules for performing inference with PyTorch models?</p> <p>If not, here's a refresher:</p> PyTorch inference rules <ol> <li> Set the model in evaluation mode (<code>model.eval()</code>). </li> <li> Make the predictions using the inference mode context manager (<code>with torch.inference_mode(): ...</code>). </li> <li> All predictions should be made with objects on the same device (e.g. data and model on GPU only or data and model on CPU only).</li> </ol> In\u00a0[23]: Copied! <pre># 1. Put the loaded model into evaluation mode\nloaded_model_0.eval()\n\n# 2. Use the inference mode context manager to make predictions\nwith torch.inference_mode():\n    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model\n</pre> # 1. Put the loaded model into evaluation mode loaded_model_0.eval()  # 2. Use the inference mode context manager to make predictions with torch.inference_mode():     loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model <p>Now we've made some predictions with the loaded model, let's see if they're the same as the previous predictions.</p> In\u00a0[24]: Copied! <pre># Compare previous model predictions with loaded model predictions (these should be the same)\ny_preds == loaded_model_preds\n</pre> # Compare previous model predictions with loaded model predictions (these should be the same) y_preds == loaded_model_preds Out[24]: <pre>tensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]])</pre> <p>Nice!</p> <p>It looks like the loaded model predictions are the same as the previous model predictions (predictions made prior to saving). This indicates our model is saving and loading as expected.</p> <p>Note: There are more methods to save and load PyTorch models but I'll leave these for extra-curriculum and further reading. See the PyTorch guide for saving and loading models for more.</p> In\u00a0[25]: Copied! <pre># Import PyTorch and matplotlib\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n</pre> # Import PyTorch and matplotlib import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import matplotlib.pyplot as plt  # Check PyTorch version torch.__version__ Out[25]: <pre>'2.2.2'</pre> <p>Now let's start making our code device agnostic by setting <code>device=\"cuda\"</code> if it's available, otherwise it'll default to <code>device=\"cpu\"</code>.</p> In\u00a0[26]: Copied! <pre># Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n</pre> # Setup device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Using device: {device}\") <pre>Using device: cuda\n</pre> <p>If you've got access to a GPU, the above should've printed out:</p> <pre><code>Using device: cuda\n</code></pre> <p>Otherwise, you'll be using a CPU for the following computations. This is fine for our small dataset but it will take longer for larger datasets.</p> In\u00a0[27]: Copied! <pre># Create weight and bias\nweight = 0.7\nbias = 0.3\n\n# Create range values\nstart = 0\nend = 1\nstep = 0.02\n\n# Create X and y (features and labels)\nX = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\ny = weight * X + bias \nX[:10], y[:10]\n</pre> # Create weight and bias weight = 0.7 bias = 0.3  # Create range values start = 0 end = 1 step = 0.02  # Create X and y (features and labels) X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers) y = weight * X + bias  X[:10], y[:10] Out[27]: <pre>(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))</pre> <p>Wonderful!</p> <p>Now we've got some data, let's split it into training and test sets.</p> <p>We'll use an 80/20 split with 80% training data and 20% testing data.</p> In\u00a0[28]: Copied! <pre># Split data\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Split data train_split = int(0.8 * len(X)) X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) Out[28]: <pre>(40, 40, 10, 10)</pre> <p>Excellent, let's visualize them to make sure they look okay.</p> In\u00a0[29]: Copied! <pre># Note: If you've reset your runtime, this function won't work, \n# you'll have to rerun the cell above where it's instantiated.\nplot_predictions(X_train, y_train, X_test, y_test)\n</pre> # Note: If you've reset your runtime, this function won't work,  # you'll have to rerun the cell above where it's instantiated. plot_predictions(X_train, y_train, X_test, y_test) In\u00a0[30]: Copied! <pre># Subclass nn.Module to make our model\nclass LinearRegressionModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\n# Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens)\ntorch.manual_seed(42)\nmodel_1 = LinearRegressionModelV2()\nmodel_1, model_1.state_dict()\n</pre> # Subclass nn.Module to make our model class LinearRegressionModelV2(nn.Module):     def __init__(self):         super().__init__()         # Use nn.Linear() for creating the model parameters         self.linear_layer = nn.Linear(in_features=1,                                        out_features=1)          # Define the forward computation (input data x flows through nn.Linear())     def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         return self.linear_layer(x)  # Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens) torch.manual_seed(42) model_1 = LinearRegressionModelV2() model_1, model_1.state_dict() Out[30]: <pre>(LinearRegressionModelV2(\n   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n              ('linear_layer.bias', tensor([0.8300]))]))</pre> <p>Notice the outputs of <code>model_1.state_dict()</code>, the <code>nn.Linear()</code> layer created a random <code>weight</code> and <code>bias</code> parameter for us.</p> <p>Now let's put our model on the GPU (if it's available).</p> <p>We can change the device our PyTorch objects are on using <code>.to(device)</code>.</p> <p>First let's check the model's current device.</p> In\u00a0[31]: Copied! <pre># Check model device\nnext(model_1.parameters()).device\n</pre> # Check model device next(model_1.parameters()).device Out[31]: <pre>device(type='cpu')</pre> <p>Wonderful, looks like the model's on the CPU by default.</p> <p>Let's change it to be on the GPU (if it's available).</p> In\u00a0[32]: Copied! <pre># Set model to GPU if it's availalble, otherwise it'll default to CPU\nmodel_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\nnext(model_1.parameters()).device\n</pre> # Set model to GPU if it's availalble, otherwise it'll default to CPU model_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not next(model_1.parameters()).device Out[32]: <pre>device(type='cuda', index=0)</pre> <p>Nice! Because of our device agnostic code, the above cell will work regardless of whether a GPU is available or not.</p> <p>If you do have access to a CUDA-enabled GPU, you should see an output of something like:</p> <pre><code>device(type='cuda', index=0)\n</code></pre> <p>Time to build a training and testing loop.</p> <p>First we'll need a loss function and an optimizer.</p> <p>Let's use the same functions we used earlier, <code>nn.L1Loss()</code> and <code>torch.optim.SGD()</code>.</p> <p>We'll have to pass the new model's parameters (<code>model.parameters()</code>) to the optimizer for it to adjust them during training.</p> <p>The learning rate of <code>0.01</code> worked well before too so let's use that again.</p> In\u00a0[33]: Copied! <pre># Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n</pre> # Create loss function loss_fn = nn.L1Loss()  # Create optimizer optimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters                             lr=0.01) <p>Beautiful, loss function and optimizer ready, now let's train and evaluate our model using a training and testing loop.</p> <p>The only different thing we'll be doing in this step compared to the previous training loop is putting the data on the target <code>device</code>.</p> <p>We've already put our model on the target <code>device</code> using <code>model_1.to(device)</code>.</p> <p>And we can do the same with the data.</p> <p>That way if the model is on the GPU, the data is on the GPU (and vice versa).</p> <p>Let's step things up a notch this time and set <code>epochs=1000</code>.</p> <p>If you need a reminder of the PyTorch training loop steps, see below.</p> PyTorch training loop steps <ol> <li>Forward pass - The model goes through all of the training data once, performing its             <code>forward()</code> function             calculations (<code>model(x_train)</code>).         </li> <li>Calculate the loss - The model's outputs (predictions) are compared to the ground truth and evaluated             to see how             wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li> <li>Zero gradients - The optimizers gradients are set to zero (they are accumulated by default) so they             can be             recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li> <li>Perform backpropagation on the loss - Computes the gradient of the loss with respect for every model             parameter to             be updated (each parameter             with <code>requires_grad=True</code>). This is known as backpropagation, hence \"backwards\"             (<code>loss.backward()</code>).</li> <li>Step the optimizer (gradient descent) - Update the parameters with <code>requires_grad=True</code>             with respect to the loss             gradients in order to improve them (<code>optimizer.step()</code>).</li> </ol> In\u00a0[34]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, error will happen (not all model/data on device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n</pre> torch.manual_seed(42)  # Set the number of epochs  epochs = 1000   # Put data on the available device # Without this, error will happen (not all model/data on device) X_train = X_train.to(device) X_test = X_test.to(device) y_train = y_train.to(device) y_test = y_test.to(device)  for epoch in range(epochs):     ### Training     model_1.train() # train mode is on by default after construction      # 1. Forward pass     y_pred = model_1(X_train)      # 2. Calculate loss     loss = loss_fn(y_pred, y_train)      # 3. Zero grad optimizer     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Step the optimizer     optimizer.step()      ### Testing     model_1.eval() # put the model in evaluation mode for testing (inference)     # 1. Forward pass     with torch.inference_mode():         test_pred = model_1(X_test)              # 2. Calculate the loss         test_loss = loss_fn(test_pred, y_test)      if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\") <pre>Epoch: 0 | Train loss: 0.5551779866218567 | Test loss: 0.5739762187004089\nEpoch: 100 | Train loss: 0.006215683650225401 | Test loss: 0.014086711220443249\nEpoch: 200 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 300 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 400 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 500 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 600 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 700 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 800 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 900 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\n</pre> <p>Note: Due to the random nature of machine learning, you will likely get slightly different results (different loss and prediction values) depending on whether your model was trained on CPU or GPU. This is true even if you use the same random seed on either device. If the difference is large, you may want to look for errors, however, if it is small (ideally it is), you can ignore it.</p> <p>Nice! That loss looks pretty low.</p> <p>Let's check the parameters our model has learned and compare them to the original parameters we hard-coded.</p> In\u00a0[35]: Copied! <pre># Find our model's learned parameters\nfrom pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \nprint(\"The model learned the following values for weights and bias:\")\npprint(model_1.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</pre> # Find our model's learned parameters from pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html  print(\"The model learned the following values for weights and bias:\") pprint(model_1.state_dict()) print(\"\\nAnd the original values for weights and bias are:\") print(f\"weights: {weight}, bias: {bias}\") <pre>The model learned the following values for weights and bias:\nOrderedDict([('linear_layer.weight', tensor([[0.6968]], device='cuda:0')),\n             ('linear_layer.bias', tensor([0.3025], device='cuda:0'))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n</pre> <p>Ho ho! Now that's pretty darn close to a perfect model.</p> <p>Remember though, in practice, it's rare that you'll know the perfect parameters ahead of time.</p> <p>And if you knew the parameters your model had to learn ahead of time, what would be the fun of machine learning?</p> <p>Plus, in many real-world machine learning problems, the number of parameters can well exceed tens of millions.</p> <p>I don't know about you but I'd rather write code for a computer to figure those out rather than doing it by hand.</p> In\u00a0[36]: Copied! <pre># Turn model into evaluation mode\nmodel_1.eval()\n\n# Make predictions on the test data\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\ny_preds\n</pre> # Turn model into evaluation mode model_1.eval()  # Make predictions on the test data with torch.inference_mode():     y_preds = model_1(X_test) y_preds Out[36]: <pre>tensor([[0.8600],\n        [0.8739],\n        [0.8878],\n        [0.9018],\n        [0.9157],\n        [0.9296],\n        [0.9436],\n        [0.9575],\n        [0.9714],\n        [0.9854]], device='cuda:0')</pre> <p>If you're making predictions with data on the GPU, you might notice the output of the above has <code>device='cuda:0'</code> towards the end. That means the data is on CUDA device 0 (the first GPU your system has access to due to zero-indexing), if you end up using multiple GPUs in the future, this number may be higher.</p> <p>Now let's plot our model's predictions.</p> <p>Note: Many data science libraries such as pandas, matplotlib and NumPy aren't capable of using data that is stored on GPU. So you might run into some issues when trying to use a function from one of these libraries with tensor data not stored on the CPU. To fix this, you can call <code>.cpu()</code> on your target tensor to return a copy of your target tensor on the CPU.</p> In\u00a0[37]: Copied! <pre># plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU\n\n# Put data on the CPU and plot it\nplot_predictions(predictions=y_preds.cpu())\n</pre> # plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU  # Put data on the CPU and plot it plot_predictions(predictions=y_preds.cpu()) <p>Woah! Look at those red dots, they line up almost perfectly with the green dots. I guess the extra epochs helped.</p> In\u00a0[38]: Copied! <pre>from pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n</pre> from pathlib import Path  # 1. Create models directory  MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, exist_ok=True)  # 2. Create model save path  MODEL_NAME = \"01_pytorch_workflow_model_1.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # 3. Save the model state dict  print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters            f=MODEL_SAVE_PATH)  <pre>Saving model to: models/01_pytorch_workflow_model_1.pth\n</pre> <p>And just to make sure everything worked well, let's load it back in.</p> <p>We'll:</p> <ul> <li>Create a new instance of the <code>LinearRegressionModelV2()</code> class</li> <li>Load in the model state dict using <code>torch.nn.Module.load_state_dict()</code></li> <li>Send the new instance of the model to the target device (to ensure our code is device-agnostic)</li> </ul> In\u00a0[39]: Copied! <pre># Instantiate a fresh instance of LinearRegressionModelV2\nloaded_model_1 = LinearRegressionModelV2()\n\n# Load model state dict \nloaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\nloaded_model_1.to(device)\n\nprint(f\"Loaded model:\\n{loaded_model_1}\")\nprint(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")\n</pre> # Instantiate a fresh instance of LinearRegressionModelV2 loaded_model_1 = LinearRegressionModelV2()  # Load model state dict  loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))  # Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions) loaded_model_1.to(device)  print(f\"Loaded model:\\n{loaded_model_1}\") print(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\") <pre>Loaded model:\nLinearRegressionModelV2(\n  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n)\nModel on device:\ncuda:0\n</pre> <p>Now we can evaluate the loaded model to see if its predictions line up with the predictions made prior to saving.</p> In\u00a0[40]: Copied! <pre># Evaluate loaded model\nloaded_model_1.eval()\nwith torch.inference_mode():\n    loaded_model_1_preds = loaded_model_1(X_test)\ny_preds == loaded_model_1_preds\n</pre> # Evaluate loaded model loaded_model_1.eval() with torch.inference_mode():     loaded_model_1_preds = loaded_model_1(X_test) y_preds == loaded_model_1_preds Out[40]: <pre>tensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]], device='cuda:0')</pre> <p>Everything adds up! Nice!</p> <p>Well, we've come a long way. You've now built and trained your first two neural network models in PyTorch!</p> <p>Time to practice your skills.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#01-pytorch-workflow-fundamentals","title":"01. PyTorch Workflow Fundamentals\u00b6","text":"<p>The essence of machine learning and deep learning is to take some data from the past, build an algorithm (like a neural network) to discover patterns in it and use the discoverd patterns to predict the future.</p> <p>There are many ways to do this and many new ways are being discovered all the time.</p> <p>But let's start small.</p> <p>How about we start with a straight line?</p> <p>And we see if we can build a PyTorch model that learns the pattern of the straight line and matches it.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>In this module we're going to cover a standard PyTorch workflow (it can be chopped and changed as necessary but it covers the main outline of steps).</p> <p></p> <p>For now, we'll use this workflow to predict a simple straight line but the workflow steps can be repeated and changed depending on the problem you're working on.</p> <p>Specifically, we're going to cover:</p> Topic Contents 1. Getting data ready Data can be almost anything but to get started we're going to create a simple straight line 2. Building a model Here we'll create a model to learn patterns in the data, we'll also choose a loss function, optimizer and build a training loop. 3. Fitting the model to data (training) We've got data and a model, now let's let the model (try to) find patterns in the (training) data. 4. Making predictions and evaluating a model (inference) Our model's found patterns in the data, let's compare its findings to the actual (testing) data. 5. Saving and loading a model You may want to use your model elsewhere, or come back to it later, here we'll cover that. 6. Putting it all together Let's take all of the above and combine it."},{"location":"Learning/Pytorch/01_pytorch_workflow/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>And if you run into trouble, you can ask a question on the Discussions page there too.</p> <p>There's also the PyTorch developer forums, a very helpful place for all things PyTorch.</p> <p>Let's start by putting what we're covering into a dictionary to reference later.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#1-data-preparing-and-loading","title":"1. Data (preparing and loading)\u00b6","text":"<p>I want to stress that \"data\" in machine learning can be almost anything you can imagine. A table of numbers (like a big Excel spreadsheet), images of any kind, videos (YouTube has lots of data!), audio files like songs or podcasts, protein structures, text and more.</p> <p></p> <p>Machine learning is a game of two parts:</p> <ol> <li>Turn your data, whatever it is, into numbers (a representation).</li> <li>Pick or build a model to learn the representation as best as possible.</li> </ol> <p>Sometimes one and two can be done at the same time.</p> <p>But what if you don't have data?</p> <p>Well, that's where we're at now.</p> <p>No data.</p> <p>But we can create some.</p> <p>Let's create our data as a straight line.</p> <p>We'll use linear regression to create the data with known parameters (things that can be learned by a model) and then we'll use PyTorch to see if we can build model to estimate these parameters using gradient descent.</p> <p>Don't worry if the terms above don't mean much now, we'll see them in action and I'll put extra resources below where you can learn more.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#split-data-into-training-and-test-sets","title":"Split data into training and test sets\u00b6","text":"<p>We've got some data.</p> <p>But before we build a model we need to split it up.</p> <p>One of most important steps in a machine learning project is creating a training and test set (and when required, a validation set).</p> <p>Each split of the dataset serves a specific purpose:</p> Split Purpose Amount of total data How often is it used? Training set The model learns from this data (like the course materials you study during the semester). ~60-80% Always Validation set The model gets tuned on this data (like the practice exam you take before the final exam). ~10-20% Often but not always Testing set The model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester). ~10-20% Always <p>For now, we'll just use a training and test set, this means we'll have a dataset for our model to learn on as well as be evaluated on.</p> <p>We can create them by splitting our <code>X</code> and <code>y</code> tensors.</p> <p>Note: When dealing with real-world data, this step is typically done right at the start of a project (the test set should always be kept separate from all other data). We want our model to learn on training data and then evaluate it on test data to get an indication of how well it generalizes to unseen examples.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#2-build-model","title":"2. Build model\u00b6","text":"<p>Now we've got some data, let's build a model to use the blue dots to predict the green dots.</p> <p>We're going to jump right in.</p> <p>We'll write the code first and then explain everything.</p> <p>Let's replicate a standard linear regression model using pure PyTorch.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#pytorch-model-building-essentials","title":"PyTorch model building essentials\u00b6","text":"<p>PyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine.</p> <p>They are <code>torch.nn</code>, <code>torch.optim</code>, <code>torch.utils.data.Dataset</code> and <code>torch.utils.data.DataLoader</code>. For now, we'll focus on the first two and get to the other two later (though you may be able to guess what they do).</p> PyTorch module What does it do? <code>torch.nn</code> Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way). <code>torch.nn.Parameter</code> Stores tensors that can be used with <code>nn.Module</code>. If <code>requires_grad=True</code> gradients (used for updating model parameters via gradient descent)  are calculated automatically, this is often referred to as \"autograd\". <code>torch.nn.Module</code> The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass <code>nn.Module</code>. Requires a <code>forward()</code> method be implemented. <code>torch.optim</code> Contains various optimization algorithms (these tell the model parameters stored in <code>nn.Parameter</code> how to best change to improve gradient descent and in turn reduce the loss). <code>def forward()</code> All <code>nn.Module</code> subclasses require a <code>forward()</code> method, this defines the computation that will take place on the data passed to the particular <code>nn.Module</code> (e.g. the linear regression formula above). <p>If the above sounds complex, think of like this, almost everything in a PyTorch neural network comes from <code>torch.nn</code>,</p> <ul> <li><code>nn.Module</code> contains the larger building blocks (layers)</li> <li><code>nn.Parameter</code> contains the smaller parameters like weights and biases (put these together to make <code>nn.Module</code>(s))</li> <li><code>forward()</code> tells the larger blocks how to make calculations on inputs (tensors full of data) within  <code>nn.Module</code>(s)</li> <li><code>torch.optim</code> contains optimization methods on how to improve the parameters within <code>nn.Parameter</code> to better represent input data</li> </ul> <p> Basic building blocks of creating a PyTorch model by subclassing <code>nn.Module</code>. For objects that subclass <code>nn.Module</code>, the <code>forward()</code> method must be defined.</p> <p>Resource: See more of these essential modules and their uses cases in the PyTorch Cheat Sheet.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#checking-the-contents-of-a-pytorch-model","title":"Checking the contents of a PyTorch model\u00b6","text":"<p>Now we've got these out of the way, let's create a model instance with the class we've made and check its parameters using <code>.parameters()</code>.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#making-predictions-using-torchinference_mode","title":"Making predictions using <code>torch.inference_mode()</code>\u00b6","text":"<p>To check this we can pass it the test data <code>X_test</code> to see how closely it predicts <code>y_test</code>.</p> <p>When we pass data to our model, it'll go through the model's <code>forward()</code> method and produce a result using the computation we've defined.</p> <p>Let's make some predictions.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#3-train-model","title":"3. Train model\u00b6","text":"<p>Right now our model is making predictions using random parameters to make calculations, it's basically guessing (randomly).</p> <p>To fix that, we can update its internal parameters (I also refer to parameters as patterns), the <code>weights</code> and <code>bias</code> values we set randomly using <code>nn.Parameter()</code> and <code>torch.randn()</code> to be something that better represents the data.</p> <p>We could hard code this (since we know the default values <code>weight=0.7</code> and <code>bias=0.3</code>) but where's the fun in that?</p> <p>Much of the time you won't know what the ideal parameters are for a model.</p> <p>Instead, it's much more fun to write code to see if the model can try and figure them out itself.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#creating-a-loss-function-and-optimizer-in-pytorch","title":"Creating a loss function and optimizer in PyTorch\u00b6","text":"<p>For our model to update its parameters on its own, we'll need to add a few more things to our recipe.</p> <p>And that's a loss function as well as an optimizer.</p> <p>The rolls of these are:</p> Function What does it do? Where does it live in PyTorch? Common values Loss function Measures how wrong your models predictions (e.g. <code>y_preds</code>) are compared to the truth labels (e.g. <code>y_test</code>). Lower the better. PyTorch has plenty of built-in loss functions in <code>torch.nn</code>. Mean absolute error (MAE) for regression problems (<code>torch.nn.L1Loss()</code>). Binary cross entropy for binary classification problems (<code>torch.nn.BCELoss()</code>). Optimizer Tells your model how to update its internal parameters to best lower the loss. You can find various optimization function implementations in <code>torch.optim</code>. Stochastic gradient descent (<code>torch.optim.SGD()</code>). Adam optimizer (<code>torch.optim.Adam()</code>). <p>Let's create a loss function and an optimizer we can use to help improve our model.</p> <p>Depending on what kind of problem you're working on will depend on what loss function and what optimizer you use.</p> <p>However, there are some common values, that are known to work well such as the SGD (stochastic gradient descent) or Adam optimizer. And the MAE (mean absolute error) loss function for regression problems (predicting a number) or binary cross entropy loss function for classification problems (predicting one thing or another).</p> <p>For our problem, since we're predicting a number, let's use MAE (which is under <code>torch.nn.L1Loss()</code>) in PyTorch as our loss function.</p> <p> Mean absolute error (MAE, in PyTorch: <code>torch.nn.L1Loss</code>) measures the absolute difference between two points (predictions and labels) and then takes the mean across all examples.</p> <p>And we'll use SGD, <code>torch.optim.SGD(params, lr)</code> where:</p> <ul> <li><code>params</code> is the target model parameters you'd like to optimize (e.g. the <code>weights</code> and <code>bias</code> values we randomly set before).</li> <li><code>lr</code> is the learning rate you'd like the optimizer to update the parameters at, higher means the optimizer will try larger updates (these can sometimes be too large and the optimizer will fail to work), lower means the optimizer will try smaller updates (these can sometimes be too small and the optimizer will take too long to find the ideal values). The learning rate is considered a hyperparameter (because it's set by a machine learning engineer). Common starting values for the learning rate are <code>0.01</code>, <code>0.001</code>, <code>0.0001</code>, however, these can also be adjusted over time (this is called learning rate scheduling).</li> </ul> <p>Woah, that's a lot, let's see it in code.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#creating-an-optimization-loop-in-pytorch","title":"Creating an optimization loop in PyTorch\u00b6","text":"<p>Woohoo! Now we've got a loss function and an optimizer, it's now time to create a training loop (and testing loop).</p> <p>The training loop involves the model going through the training data and learning the relationships between the <code>features</code> and <code>labels</code>.</p> <p>The testing loop involves going through the testing data and evaluating how good the patterns are that the model learned on the training data (the model never see's the testing data during training).</p> <p>Each of these is called a \"loop\" because we want our model to look (loop through) at each sample in each dataset.</p> <p>To create these we're going to write a Python <code>for</code> loop in the theme of the unofficial PyTorch optimization loop song (there's a video version too).</p> <p> The unoffical PyTorch optimization loops song, a fun way to remember the steps in a PyTorch training (and testing) loop.</p> <p>There will be a fair bit of code but nothing we can't handle.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#pytorch-training-loop","title":"PyTorch training loop\u00b6","text":"<p>For the training loop, we'll build the following steps:</p> Number Step name What does it do? Code example 1 Forward pass The model goes through all of the training data once, performing its <code>forward()</code> function calculations. <code>model(x_train)</code> 2 Calculate the loss The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. <code>loss = loss_fn(y_pred, y_train)</code> 3 Zero gradients The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step. <code>optimizer.zero_grad()</code> 4 Perform backpropagation on the loss Computes the gradient of the loss with respect for every model parameter to be updated  (each parameter with <code>requires_grad=True</code>). This is known as backpropagation, hence \"backwards\". <code>loss.backward()</code> 5 Update the optimizer (gradient descent) Update the parameters with <code>requires_grad=True</code> with respect to the loss gradients in order to improve them. <code>optimizer.step()</code> <p></p> <p>Note: The above is just one example of how the steps could be ordered or described. With experience you'll find making PyTorch training loops can be quite flexible.</p> <p>And on the ordering of things, the above is a good default order but you may see slightly different orders. Some rules of thumb:</p> <ul> <li>Calculate the loss (<code>loss = ...</code>) before performing backpropagation on it (<code>loss.backward()</code>).</li> <li>Zero gradients (<code>optimizer.zero_grad()</code>) before stepping them (<code>optimizer.step()</code>).</li> <li>Step the optimizer (<code>optimizer.step()</code>) after performing backpropagation on the loss (<code>loss.backward()</code>).</li> </ul> <p>For resources to help understand what's happening behind the scenes with backpropagation and gradient descent, see the extra-curriculum section.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#pytorch-testing-loop","title":"PyTorch testing loop\u00b6","text":"<p>As for the testing loop (evaluating our model), the typical steps include:</p> Number Step name What does it do? Code example 1 Forward pass The model goes through all of the training data once, performing its <code>forward()</code> function calculations. <code>model(x_test)</code> 2 Calculate the loss The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. <code>loss = loss_fn(y_pred, y_test)</code> 3 Calulate evaluation metrics (optional) Alongisde the loss value you may want to calculate other evaluation metrics such as accuracy on the test set. Custom functions <p>Notice the testing loop doesn't contain performing backpropagation (<code>loss.backward()</code>) or stepping the optimizer (<code>optimizer.step()</code>), this is because no parameters in the model are being changed during testing, they've already been calculated. For testing, we're only interested in the output of the forward pass through the model.</p> <p></p> <p>Let's put all of the above together and train our model for 100 epochs (forward passes through the data) and we'll evaluate it every 10 epochs.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#4-making-predictions-with-a-trained-pytorch-model-inference","title":"4. Making predictions with a trained PyTorch model (inference)\u00b6","text":"<p>Once you've trained a model, you'll likely want to make predictions with it.</p> <p>We've already seen a glimpse of this in the training and testing code above, the steps to do it outside of the training/testing loop are similar.</p> <p>There are three things to remember when making predictions (also called performing inference) with a PyTorch model:</p> <ol> <li>Set the model in evaluation mode (<code>model.eval()</code>).</li> <li>Make the predictions using the inference mode context manager (<code>with torch.inference_mode(): ...</code>).</li> <li>All predictions should be made with objects on the same device (e.g. data and model on GPU only or data and model on CPU only).</li> </ol> <p>The first two items make sure all helpful calculations and settings PyTorch uses behind the scenes during training but aren't necessary for inference are turned off (this results in faster computation). And the third ensures that you won't run into cross-device errors.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#5-saving-and-loading-a-pytorch-model","title":"5. Saving and loading a PyTorch model\u00b6","text":"<p>If you've trained a PyTorch model, chances are you'll want to save it and export it somewhere.</p> <p>As in, you might train it on Google Colab or your local machine with a GPU but you'd like to now export it to some sort of application where others can use it.</p> <p>Or maybe you'd like to save your progress on a model and come back and load it back later.</p> <p>For saving and loading models in PyTorch, there are three main methods you should be aware of (all of below have been taken from the PyTorch saving and loading models guide):</p> PyTorch method What does it do? <code>torch.save</code> Saves a serialized object to disk using Python's <code>pickle</code> utility. Models, tensors and various other Python objects like dictionaries can be saved using <code>torch.save</code>. <code>torch.load</code> Uses <code>pickle</code>'s unpickling features to deserialize and load pickled Python object files (like models, tensors or dictionaries) into memory. You can also set which device to load the object to (CPU, GPU etc). <code>torch.nn.Module.load_state_dict</code> Loads a model's parameter dictionary (<code>model.state_dict()</code>) using a saved <code>state_dict()</code> object. <p>Note: As stated in Python's <code>pickle</code> documentation, the <code>pickle</code> module is not secure. That means you should only ever unpickle (load) data you trust. That goes for loading PyTorch models as well. Only ever use saved PyTorch models from sources you trust.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#saving-a-pytorch-models-state_dict","title":"Saving a PyTorch model's <code>state_dict()</code>\u00b6","text":"<p>The recommended way for saving and loading a model for inference (making predictions) is by saving and loading a model's <code>state_dict()</code>.</p> <p>Let's see how we can do that in a few steps:</p> <ol> <li>We'll create a directory for saving models to called <code>models</code> using Python's <code>pathlib</code> module.</li> <li>We'll create a file path to save the model to.</li> <li>We'll call <code>torch.save(obj, f)</code> where <code>obj</code> is the target model's <code>state_dict()</code> and <code>f</code> is the filename of where to save the model.</li> </ol> <p>Note: It's common convention for PyTorch saved models or objects to end with <code>.pt</code> or <code>.pth</code>, like <code>saved_model_01.pth</code>.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#loading-a-saved-pytorch-models-state_dict","title":"Loading a saved PyTorch model's <code>state_dict()</code>\u00b6","text":"<p>Since we've now got a saved model <code>state_dict()</code> at <code>models/01_pytorch_workflow_model_0.pth</code> we can now load it in using <code>torch.nn.Module.load_state_dict(torch.load(f))</code> where <code>f</code> is the filepath of our saved model <code>state_dict()</code>.</p> <p>Why call <code>torch.load()</code> inside <code>torch.nn.Module.load_state_dict()</code>?</p> <p>Because we only saved the model's <code>state_dict()</code> which is a dictionary of learned parameters and not the entire model, we first have to load the <code>state_dict()</code> with <code>torch.load()</code> and then pass that <code>state_dict()</code> to a new instance of our model (which is a subclass of <code>nn.Module</code>).</p> <p>Why not save the entire model?</p> <p>Saving the entire model rather than just the <code>state_dict()</code> is more intuitive, however, to quote the PyTorch documentation (italics mine):</p> <p>The disadvantage of this approach (saving the whole model) is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved...</p> <p>Because of this, your code can break in various ways when used in other projects or after refactors.</p> <p>So instead, we're using the flexible method of saving and loading just the <code>state_dict()</code>, which again is basically a dictionary of model parameters.</p> <p>Let's test it out by created another instance of <code>LinearRegressionModel()</code>, which is a subclass of <code>torch.nn.Module</code> and will hence have the in-built method <code>load_state_dict()</code>.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#6-putting-it-all-together","title":"6. Putting it all together\u00b6","text":"<p>We've covered a fair bit of ground so far.</p> <p>But once you've had some practice, you'll be performing the above steps like dancing down the street.</p> <p>Speaking of practice, let's put everything we've done so far together.</p> <p>Except this time we'll make our code device agnostic (so if there's a GPU available, it'll use it and if not, it will default to the CPU).</p> <p>There'll be far less commentary in this section than above since what we're going to go through has already been covered.</p> <p>We'll start by importing the standard libraries we need.</p> <p>Note: If you're using Google Colab, to setup a GPU, go to Runtime -&gt; Change runtime type -&gt; Hardware acceleration -&gt; GPU. If you do this, it will reset the Colab runtime and you will lose saved variables.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#61-data","title":"6.1 Data\u00b6","text":"<p>Let's create some data just like before.</p> <p>First, we'll hard-code some <code>weight</code> and <code>bias</code> values.</p> <p>Then we'll make a range of numbers between 0 and 1, these will be our <code>X</code> values.</p> <p>Finally, we'll use the <code>X</code> values, as well as the <code>weight</code> and <code>bias</code> values to create <code>y</code> using the linear regression formula (<code>y = weight * X + bias</code>).</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#62-building-a-pytorch-linear-model","title":"6.2 Building a PyTorch linear model\u00b6","text":"<p>We've got some data, now it's time to make a model.</p> <p>We'll create the same style of model as before except this time, instead of defining the weight and bias parameters of our model manually using <code>nn.Parameter()</code>, we'll use <code>nn.Linear(in_features, out_features)</code> to do it for us.</p> <p>Where <code>in_features</code> is the number of dimensions your input data has and <code>out_features</code> is the number of dimensions you'd like it to be output to.</p> <p>In our case, both of these are <code>1</code> since our data has <code>1</code> input feature (<code>X</code>) per label (<code>y</code>).</p> <p> Creating a linear regression model using <code>nn.Parameter</code> versus using <code>nn.Linear</code>. There are plenty more examples of where the <code>torch.nn</code> module has pre-built computations, including many popular and useful neural network layers.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#63-training","title":"6.3 Training\u00b6","text":""},{"location":"Learning/Pytorch/01_pytorch_workflow/#64-making-predictions","title":"6.4 Making predictions\u00b6","text":"<p>Now we've got a trained model, let's turn on it's evaluation mode and make some predictions.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#65-saving-and-loading-a-model","title":"6.5 Saving and loading a model\u00b6","text":"<p>We're happy with our models predictions, so let's save it to file so it can be used later.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#exercises","title":"Exercises\u00b6","text":"<p>All exercises have been inspired from code throughout the notebook.</p> <p>There is one exercise per major section.</p> <p>You should be able to complete them by referencing their specific section.</p> <p>Note: For all exercises, your code should be device agnostic (meaning it could run on CPU or GPU if it's available).</p> <ol> <li>Create a straight line dataset using the linear regression formula (<code>weight * X + bias</code>).</li> </ol> <ul> <li>Set <code>weight=0.3</code> and <code>bias=0.9</code> there should be at least 100 datapoints total.</li> <li>Split the data into 80% training, 20% testing.</li> <li>Plot the training and testing data so it becomes visual.</li> </ul> <ol> <li>Build a PyTorch model by subclassing <code>nn.Module</code>.</li> </ol> <ul> <li>Inside should be a randomly initialized <code>nn.Parameter()</code> with <code>requires_grad=True</code>, one for <code>weights</code> and one for <code>bias</code>.</li> <li>Implement the <code>forward()</code> method to compute the linear regression function you used to create the dataset in 1.</li> <li>Once you've constructed the model, make an instance of it and check its <code>state_dict()</code>.</li> <li>Note: If you'd like to use <code>nn.Linear()</code> instead of <code>nn.Parameter()</code> you can.</li> </ul> <ol> <li>Create a loss function and optimizer using <code>nn.L1Loss()</code> and <code>torch.optim.SGD(params, lr)</code> respectively.</li> </ol> <ul> <li>Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.</li> <li>Write a training loop to perform the appropriate training steps for 300 epochs.</li> <li>The training loop should test the model on the test dataset every 20 epochs.</li> </ul> <ol> <li>Make predictions with the trained model on the test data.</li> </ol> <ul> <li>Visualize these predictions against the original training and testing data (note: you may need to make sure the predictions are not on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).</li> </ul> <ol> <li>Save your trained model's <code>state_dict()</code> to file.</li> </ol> <ul> <li>Create a new instance of your model class you made in 2. and load in the <code>state_dict()</code> you just saved to it.</li> <li>Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.</li> </ul> <p>Resource: See the exercises notebooks templates and solutions on the course GitHub.</p>"},{"location":"Learning/Pytorch/01_pytorch_workflow/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Listen to The Unofficial PyTorch Optimization Loop Song (to help remember the steps in a PyTorch training/testing loop).</li> <li>Read What is <code>torch.nn</code>, really? by Jeremy Howard for a deeper understanding of how one of the most important modules in PyTorch works.</li> <li>Spend 10-minutes scrolling through and checking out the PyTorch documentation cheatsheet for all of the different PyTorch modules you might come across.</li> <li>Spend 10-minutes reading the loading and saving documentation on the PyTorch website to become more familiar with the different saving and loading options in PyTorch.</li> <li>Spend 1-2 hours read/watching the following for an overview of the internals of gradient descent and backpropagation, the two main algorithms that have been working in the background to help our model learn.</li> <li>Wikipedia page for gradient descent</li> <li>Gradient Descent Algorithm \u2014 a deep dive by Robert Kwiatkowski</li> <li>Gradient descent, how neural networks learn video by 3Blue1Brown</li> <li>What is backpropagation really doing? video by 3Blue1Brown</li> <li>Backpropagation Wikipedia Page</li> </ul>"},{"location":"Learning/Pytorch/02_pytorch_classification/","title":"02. PyTorch Neural Network Classification","text":"<p>View Source Code | View Slides | Watch Video Walkthrough</p> In\u00a0[1]: Copied! <pre>from sklearn.datasets import make_circles\n\n\n# Make 1000 samples \nn_samples = 1000\n\n# Create circles\nX, y = make_circles(n_samples,\n                    noise=0.03, # a little bit of noise to the dots\n                    random_state=42) # keep random state so we get the same values\n</pre> from sklearn.datasets import make_circles   # Make 1000 samples  n_samples = 1000  # Create circles X, y = make_circles(n_samples,                     noise=0.03, # a little bit of noise to the dots                     random_state=42) # keep random state so we get the same values <p>Alright, now let's view the first 5 <code>X</code> and <code>y</code> values.</p> In\u00a0[2]: Copied! <pre>print(f\"First 5 X features:\\n{X[:5]}\")\nprint(f\"\\nFirst 5 y labels:\\n{y[:5]}\")\n</pre> print(f\"First 5 X features:\\n{X[:5]}\") print(f\"\\nFirst 5 y labels:\\n{y[:5]}\") <pre>First 5 X features:\n[[ 0.75424625  0.23148074]\n [-0.75615888  0.15325888]\n [-0.81539193  0.17328203]\n [-0.39373073  0.69288277]\n [ 0.44220765 -0.89672343]]\n\nFirst 5 y labels:\n[1 1 1 1 0]\n</pre> <p>Looks like there's two <code>X</code> values per one <code>y</code> value.</p> <p>Let's keep following the data explorer's motto of visualize, visualize, visualize and put them into a pandas DataFrame.</p> In\u00a0[3]: Copied! <pre># Make DataFrame of circle data\nimport pandas as pd\ncircles = pd.DataFrame({\"X1\": X[:, 0],\n    \"X2\": X[:, 1],\n    \"label\": y\n})\ncircles.head(10)\n</pre> # Make DataFrame of circle data import pandas as pd circles = pd.DataFrame({\"X1\": X[:, 0],     \"X2\": X[:, 1],     \"label\": y }) circles.head(10) Out[3]: X1 X2 label 0 0.754246 0.231481 1 1 -0.756159 0.153259 1 2 -0.815392 0.173282 1 3 -0.393731 0.692883 1 4 0.442208 -0.896723 0 5 -0.479646 0.676435 1 6 -0.013648 0.803349 1 7 0.771513 0.147760 1 8 -0.169322 -0.793456 1 9 -0.121486 1.021509 0 <p>It looks like each pair of <code>X</code> features (<code>X1</code> and <code>X2</code>) has a label (<code>y</code>) value of either 0 or 1.</p> <p>This tells us that our problem is binary classification since there's only two options (0 or 1).</p> <p>How many values of each class is there?</p> In\u00a0[4]: Copied! <pre># Check different labels\ncircles.label.value_counts()\n</pre> # Check different labels circles.label.value_counts() Out[4]: <pre>label\n1    500\n0    500\nName: count, dtype: int64</pre> <p>500 each, nice and balanced.</p> <p>Let's plot them.</p> In\u00a0[5]: Copied! <pre># Visualize with a plot\nimport matplotlib.pyplot as plt\nplt.scatter(x=X[:, 0], \n            y=X[:, 1], \n            c=y, \n            cmap=plt.cm.RdYlBu);\n</pre> # Visualize with a plot import matplotlib.pyplot as plt plt.scatter(x=X[:, 0],              y=X[:, 1],              c=y,              cmap=plt.cm.RdYlBu); <p>Alrighty, looks like we've got a problem to solve.</p> <p>Let's find out how we could build a PyTorch neural network to classify dots into red (0) or blue (1).</p> <p>Note: This dataset is often what's considered a toy problem (a problem that's used to try and test things out on) in machine learning.</p> <p>But it represents the major key of classification, you have some kind of data represented as numerical values and you'd like to build a model that's able to classify it, in our case, separate it into red or blue dots.</p> In\u00a0[6]: Copied! <pre># Check the shapes of our features and labels\nX.shape, y.shape\n</pre> # Check the shapes of our features and labels X.shape, y.shape Out[6]: <pre>((1000, 2), (1000,))</pre> <p>Looks like we've got a match on the first dimension of each.</p> <p>There's 1000 <code>X</code> and 1000 <code>y</code>.</p> <p>But what's the second dimension on <code>X</code>?</p> <p>It often helps to view the values and shapes of a single sample (features and labels).</p> <p>Doing so will help you understand what input and output shapes you'd be expecting from your model.</p> In\u00a0[7]: Copied! <pre># View the first example of features and labels\nX_sample = X[0]\ny_sample = y[0]\nprint(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\nprint(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")\n</pre> # View the first example of features and labels X_sample = X[0] y_sample = y[0] print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\") print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\") <pre>Values for one sample of X: [0.75424625 0.23148074] and the same for y: 1\nShapes for one sample of X: (2,) and the same for y: ()\n</pre> <p>This tells us the second dimension for <code>X</code> means it has two features (vector) where as <code>y</code> has a single feature (scalar).</p> <p>We have two inputs for one output.</p> In\u00a0[8]: Copied! <pre># Turn data into tensors\n# Otherwise this causes issues with computations later on\nimport torch\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# View the first five samples\nX[:5], y[:5]\n</pre> # Turn data into tensors # Otherwise this causes issues with computations later on import torch X = torch.from_numpy(X).type(torch.float) y = torch.from_numpy(y).type(torch.float)  # View the first five samples X[:5], y[:5] Out[8]: <pre>(tensor([[ 0.7542,  0.2315],\n         [-0.7562,  0.1533],\n         [-0.8154,  0.1733],\n         [-0.3937,  0.6929],\n         [ 0.4422, -0.8967]]),\n tensor([1., 1., 1., 1., 0.]))</pre> <p>Now our data is in tensor format, let's split it into training and test sets.</p> <p>To do so, let's use the helpful function <code>train_test_split()</code> from Scikit-Learn.</p> <p>We'll use <code>test_size=0.2</code> (80% training, 20% testing) and because the split happens randomly across the data, let's use <code>random_state=42</code> so the split is reproducible.</p> In\u00a0[9]: Copied! <pre># Split data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2, # 20% test, 80% train\n                                                    random_state=42) # make the random split reproducible\n\nlen(X_train), len(X_test), len(y_train), len(y_test)\n</pre> # Split data into train and test sets from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2, # 20% test, 80% train                                                     random_state=42) # make the random split reproducible  len(X_train), len(X_test), len(y_train), len(y_test) Out[9]: <pre>(800, 200, 800, 200)</pre> <p>Nice! Looks like we've now got 800 training samples and 200 testing samples.</p> In\u00a0[10]: Copied! <pre># Standard PyTorch imports\nimport torch\nfrom torch import nn\n\n# Make device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Standard PyTorch imports import torch from torch import nn  # Make device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[10]: <pre>'cuda'</pre> <p>Excellent, now <code>device</code> is setup, we can use it for any data or models we create and PyTorch will handle it on the CPU (default) or GPU if it's available.</p> <p>How about we create a model?</p> <p>We'll want a model capable of handling our <code>X</code> data as inputs and producing something in the shape of our <code>y</code> data as ouputs.</p> <p>In other words, given <code>X</code> (features) we want our model to predict <code>y</code> (label).</p> <p>This setup where you have features and labels is referred to as supervised learning. Because your data is telling your model what the outputs should be given a certain input.</p> <p>To create such a model it'll need to handle the input and output shapes of <code>X</code> and <code>y</code>.</p> <p>Remember how I said input and output shapes are important? Here we'll see why.</p> <p>Let's create a model class that:</p> <ol> <li>Subclasses <code>nn.Module</code> (almost all PyTorch models are subclasses of <code>nn.Module</code>).</li> <li>Creates 2 <code>nn.Linear</code> layers in the constructor capable of handling the input and output shapes of <code>X</code> and <code>y</code>.</li> <li>Defines a <code>forward()</code> method containing the forward pass computation of the model.</li> <li>Instantiates the model class and sends it to the target <code>device</code>.</li> </ol> In\u00a0[11]: Copied! <pre># 1. Construct a model class that subclasses nn.Module\nclass CircleModelV0(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n    \n    # 3. Define a forward method containing the forward pass computation\n    def forward(self, x):\n        # Return the output of layer_2, a single feature, the same shape as y\n        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n\n# 4. Create an instance of the model and send it to target device\nmodel_0 = CircleModelV0().to(device)\nmodel_0\n</pre> # 1. Construct a model class that subclasses nn.Module class CircleModelV0(nn.Module):     def __init__(self):         super().__init__()         # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes         self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features         self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)          # 3. Define a forward method containing the forward pass computation     def forward(self, x):         # Return the output of layer_2, a single feature, the same shape as y         return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2  # 4. Create an instance of the model and send it to target device model_0 = CircleModelV0().to(device) model_0 Out[11]: <pre>CircleModelV0(\n  (layer_1): Linear(in_features=2, out_features=5, bias=True)\n  (layer_2): Linear(in_features=5, out_features=1, bias=True)\n)</pre> <p>What's going on here?</p> <p>We've seen a few of these steps before.</p> <p>The only major change is what's happening between <code>self.layer_1</code> and <code>self.layer_2</code>.</p> <p><code>self.layer_1</code> takes 2 input features <code>in_features=2</code> and produces 5 output features <code>out_features=5</code>.</p> <p>This is known as having 5 hidden units or neurons.</p> <p>This layer turns the input data from having 2 features to 5 features.</p> <p>Why do this?</p> <p>This allows the model to learn patterns from 5 numbers rather than just 2 numbers, potentially leading to better outputs.</p> <p>I say potentially because sometimes it doesn't work.</p> <p>The number of hidden units you can use in neural network layers is a hyperparameter (a value you can set yourself) and there's no set in stone value you have to use.</p> <p>Generally more is better but there's also such a thing as too much. The amount you choose will depend on your model type and dataset you're working with.</p> <p>Since our dataset is small and simple, we'll keep it small.</p> <p>The only rule with hidden units is that the next layer, in our case, <code>self.layer_2</code> has to take the same <code>in_features</code> as the previous layer <code>out_features</code>.</p> <p>That's why <code>self.layer_2</code> has <code>in_features=5</code>, it takes the <code>out_features=5</code> from <code>self.layer_1</code> and performs a linear computation on them, turning them into <code>out_features=1</code> (the same shape as <code>y</code>).</p> <p> A visual example of what a similar classificiation neural network to the one we've just built looks like. Try create one of your own on the TensorFlow Playground website.</p> <p>You can also do the same as above using <code>nn.Sequential</code>.</p> <p><code>nn.Sequential</code> performs a forward pass computation of the input data through the layers in the order they appear.</p> In\u00a0[12]: Copied! <pre># Replicate CircleModelV0 with nn.Sequential\nmodel_0 = nn.Sequential(\n    nn.Linear(in_features=2, out_features=5),\n    nn.Linear(in_features=5, out_features=1)\n).to(device)\n\nmodel_0\n</pre> # Replicate CircleModelV0 with nn.Sequential model_0 = nn.Sequential(     nn.Linear(in_features=2, out_features=5),     nn.Linear(in_features=5, out_features=1) ).to(device)  model_0 Out[12]: <pre>Sequential(\n  (0): Linear(in_features=2, out_features=5, bias=True)\n  (1): Linear(in_features=5, out_features=1, bias=True)\n)</pre> <p>Woah, that looks much simpler than subclassing <code>nn.Module</code>, why not just always use <code>nn.Sequential</code>?</p> <p><code>nn.Sequential</code> is fantastic for straight-forward computations, however, as the namespace says, it always runs in sequential order.</p> <p>So if you'd something else to happen (rather than just straight-forward sequential computation) you'll want to define your own custom <code>nn.Module</code> subclass.</p> <p>Now we've got a model, let's see what happens when we pass some data through it.</p> In\u00a0[13]: Copied! <pre># Make predictions with the model\nuntrained_preds = model_0(X_test.to(device))\nprint(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\nprint(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\nprint(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\nprint(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")\n</pre> # Make predictions with the model untrained_preds = model_0(X_test.to(device)) print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\") print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\") print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\") print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\") <pre>Length of predictions: 200, Shape: torch.Size([200, 1])\nLength of test samples: 200, Shape: torch.Size([200])\n\nFirst 10 predictions:\ntensor([[-0.1517],\n        [-0.0126],\n        [-0.5548],\n        [-0.0373],\n        [-0.6159],\n        [-0.5305],\n        [-0.0904],\n        [-0.2119],\n        [-0.5516],\n        [-0.0039]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nFirst 10 test labels:\ntensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])\n</pre> <p>Hmm, it seems there's the same amount of predictions as there is test labels but the predictions don't look like they're in the same form or shape as the test labels.</p> <p>We've got a couple steps we can do to fix this, we'll see these later on.</p> In\u00a0[14]: Copied! <pre># Create a loss function\n# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\nloss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n\n# Create an optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), \n                            lr=0.1)\n</pre> # Create a loss function # loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in  # Create an optimizer optimizer = torch.optim.SGD(params=model_0.parameters(),                              lr=0.1) <pre>/home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Now let's also create an evaluation metric.</p> <p>An evaluation metric can be used to offer another perspective on how your model is going.</p> <p>If a loss function measures how wrong your model is, I like to think of evaluation metrics as measuring how right it is.</p> <p>Of course, you could argue both of these are doing the same thing but evaluation metrics offer a different perspective.</p> <p>After all, when evaluating your models it's good to look at things from multiple points of view.</p> <p>There are several evaluation metrics that can be used for classification problems but let's start out with accuracy.</p> <p>Accuracy can be measured by dividing the total number of correct predictions over the total number of predictions.</p> <p>For example, a model that makes 99 correct predictions out of 100 will have an accuracy of 99%.</p> <p>Let's write a function to do so.</p> In\u00a0[15]: Copied! <pre># Calculate accuracy (a classification metric)\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n    acc = (correct / len(y_pred)) * 100 \n    return acc\n</pre> # Calculate accuracy (a classification metric) def accuracy_fn(y_true, y_pred):     correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal     acc = (correct / len(y_pred)) * 100      return acc <p>Excellent! We can now use this function whilst training our model to measure it's performance alongside the loss.</p> In\u00a0[16]: Copied! <pre># View the frist 5 outputs of the forward pass on the test data\ny_logits = model_0(X_test.to(device))[:5]\ny_logits\n</pre> # View the frist 5 outputs of the forward pass on the test data y_logits = model_0(X_test.to(device))[:5] y_logits Out[16]: <pre>tensor([[-0.1517],\n        [-0.0126],\n        [-0.5548],\n        [-0.0373],\n        [-0.6159]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)</pre> <p>Since our model hasn't been trained, these outputs are basically random.</p> <p>But what are they?</p> <p>They're the output of our <code>forward()</code> method.</p> <p>Which implements two layers of <code>nn.Linear()</code> which internally calls the following equation:</p> <p>$$ \\mathbf{y} = x \\cdot \\mathbf{Weights}^T  + \\mathbf{bias} $$</p> <p>The raw outputs (unmodified) of this equation ($\\mathbf{y}$) and in turn, the raw outputs of our model are often referred to as logits.</p> <p>That's what our model is outputing above when it takes in the input data ($x$ in the equation or <code>X_test</code> in the code), logits.</p> <p>However, these numbers are hard to interpret.</p> <p>We'd like some numbers that are comparable to our truth labels.</p> <p>To get our model's raw outputs (logits) into such a form, we can use the sigmoid activation function.</p> <p>Let's try it out.</p> In\u00a0[17]: Copied! <pre># Use sigmoid on model logits\ny_pred_probs = torch.sigmoid(y_logits)\ny_pred_probs\n</pre> # Use sigmoid on model logits y_pred_probs = torch.sigmoid(y_logits) y_pred_probs Out[17]: <pre>tensor([[0.4622],\n        [0.4969],\n        [0.3647],\n        [0.4907],\n        [0.3507]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)</pre> <p>Okay, it seems like the outputs now have some kind of consistency (even though they're still random).</p> <p>They're now in the form of prediction probabilities (I usually refer to these as <code>y_pred_probs</code>), in other words, the values are now how much the model thinks the data point belongs to one class or another.</p> <p>In our case, since we're dealing with binary classification, our ideal outputs are 0 or 1.</p> <p>So these values can be viewed as a decision boundary.</p> <p>The closer to 0, the more the model thinks the sample belongs to class 0, the closer to 1, the more the model thinks the sample belongs to class 1.</p> <p>More specificially:</p> <ul> <li>If <code>y_pred_probs</code> &gt;= 0.5, <code>y=1</code> (class 1)</li> <li>If <code>y_pred_probs</code> &lt; 0.5, <code>y=0</code> (class 0)</li> </ul> <p>To turn our prediction probabilities in prediction labels, we can round the outputs of the sigmoid activation function.</p> In\u00a0[18]: Copied! <pre># Find the predicted labels (round the prediction probabilities)\ny_preds = torch.round(y_pred_probs)\n\n# In full\ny_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n\n# Check for equality\nprint(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n\n# Get rid of extra dimension\ny_preds.squeeze()\n</pre> # Find the predicted labels (round the prediction probabilities) y_preds = torch.round(y_pred_probs)  # In full y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))  # Check for equality print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))  # Get rid of extra dimension y_preds.squeeze() <pre>tensor([True, True, True, True, True], device='cuda:0')\n</pre> Out[18]: <pre>tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=&lt;SqueezeBackward0&gt;)</pre> <p>Excellent! Now it looks like our model's predictions are in the same form as our truth labels (<code>y_test</code>).</p> In\u00a0[19]: Copied! <pre>y_test[:5]\n</pre> y_test[:5] Out[19]: <pre>tensor([1., 0., 1., 0., 1.])</pre> <p>This means we'll be able to compare our models predictions to the test labels to see how well it's going.</p> <p>To recap, we converted our model's raw outputs (logits) to predicition probabilities using a sigmoid activation function.</p> <p>And then converted the prediction probabilities to prediction labels by rounding them.</p> <p>Note: The use of the sigmoid activation function is often only for binary classification logits. For multi-class classification, we'll be looking at using the softmax activation function (this will come later on).</p> <p>And the use of the sigmoid activation function is not required when passing our model's raw outputs to the <code>nn.BCEWithLogitsLoss</code> (the \"logits\" in logits loss is because it works on the model's raw logits output), this is because it has a sigmoid function built-in.</p> In\u00a0[20]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 100\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\n# Build training and evaluation loop\nfor epoch in range(epochs):\n    ### Training\n    model_0.train()\n\n    # 1. Forward pass (model outputs raw logits)\n    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -&gt; pred probs -&gt; pred labls\n  \n    # 2. Calculate loss/accuracy\n    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n    #                y_train) \n    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n                   y_train) \n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred) \n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_0.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_0(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n</pre> torch.manual_seed(42)  # Set the number of epochs epochs = 100  # Put data to target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  # Build training and evaluation loop for epoch in range(epochs):     ### Training     model_0.train()      # 1. Forward pass (model outputs raw logits)     y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device      y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -&gt; pred probs -&gt; pred labls        # 2. Calculate loss/accuracy     # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()     #                y_train)      loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits                    y_train)      acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)       # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_0.eval()     with torch.inference_mode():         # 1. Forward pass         test_logits = model_0(X_test).squeeze()          test_pred = torch.round(torch.sigmoid(test_logits))         # 2. Caculate loss/accuracy         test_loss = loss_fn(test_logits,                             y_test)         test_acc = accuracy_fn(y_true=y_test,                                y_pred=test_pred)      # Print out what's happening every 10 epochs     if epoch % 10 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\") <pre>Epoch: 0 | Loss: 0.71614, Accuracy: 46.38% | Test loss: 0.72283, Test acc: 43.00%\nEpoch: 10 | Loss: 0.70364, Accuracy: 46.75% | Test loss: 0.71151, Test acc: 47.00%\nEpoch: 20 | Loss: 0.69892, Accuracy: 49.50% | Test loss: 0.70698, Test acc: 44.50%\nEpoch: 30 | Loss: 0.69692, Accuracy: 49.75% | Test loss: 0.70482, Test acc: 45.50%\nEpoch: 40 | Loss: 0.69593, Accuracy: 50.38% | Test loss: 0.70354, Test acc: 45.50%\nEpoch: 50 | Loss: 0.69534, Accuracy: 50.62% | Test loss: 0.70262, Test acc: 47.00%\nEpoch: 60 | Loss: 0.69494, Accuracy: 50.25% | Test loss: 0.70189, Test acc: 46.00%\nEpoch: 70 | Loss: 0.69463, Accuracy: 50.75% | Test loss: 0.70127, Test acc: 47.50%\nEpoch: 80 | Loss: 0.69439, Accuracy: 50.88% | Test loss: 0.70073, Test acc: 47.50%\nEpoch: 90 | Loss: 0.69420, Accuracy: 50.88% | Test loss: 0.70026, Test acc: 47.00%\n</pre> <p>Hmm, what do you notice about the performance of our model?</p> <p>It looks like it went through the training and testing steps fine but the results don't seem to have moved too much.</p> <p>The accuracy barely moves above 50% on each data split.</p> <p>And because we're working with a balanced binary classification problem, it means our model is performing as good as random guessing (with 500 samples of class 0 and class 1 a model predicting class 1 every single time would achieve 50% accuracy).</p> In\u00a0[21]: Copied! <pre>import requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n\nfrom helper_functions import plot_predictions, plot_decision_boundary\n</pre> import requests from pathlib import Path   # Download helper functions from Learn PyTorch repo (if not already downloaded) if Path(\"helper_functions.py\").is_file():   print(\"helper_functions.py already exists, skipping download\") else:   print(\"Downloading helper_functions.py\")   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")   with open(\"helper_functions.py\", \"wb\") as f:     f.write(request.content)  from helper_functions import plot_predictions, plot_decision_boundary <pre>helper_functions.py already exists, skipping download\n</pre> In\u00a0[22]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_0, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_0, X_test, y_test)\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_0, X_train, y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_0, X_test, y_test) <p>Oh wow, it seems like we've found the cause of model's performance issue.</p> <p>It's currently trying to split the red and blue dots using a straight line...</p> <p>That explains the 50% accuracy. Since our data is circular, drawing a straight line can at best cut it down the middle.</p> <p>In machine learning terms, our model is underfitting, meaning it's not learning predictive patterns from the data.</p> <p>How could we improve this?</p> In\u00a0[23]: Copied! <pre>class CircleModelV1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        \n    def forward(self, x): # note: always make sure forward is spelt correctly!\n        # Creating a model like this is the same as below, though below\n        # generally benefits from speedups where possible.\n        # z = self.layer_1(x)\n        # z = self.layer_2(z)\n        # z = self.layer_3(z)\n        # return z\n        return self.layer_3(self.layer_2(self.layer_1(x)))\n\nmodel_1 = CircleModelV1().to(device)\nmodel_1\n</pre> class CircleModelV1(nn.Module):     def __init__(self):         super().__init__()         self.layer_1 = nn.Linear(in_features=2, out_features=10)         self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer         self.layer_3 = nn.Linear(in_features=10, out_features=1)              def forward(self, x): # note: always make sure forward is spelt correctly!         # Creating a model like this is the same as below, though below         # generally benefits from speedups where possible.         # z = self.layer_1(x)         # z = self.layer_2(z)         # z = self.layer_3(z)         # return z         return self.layer_3(self.layer_2(self.layer_1(x)))  model_1 = CircleModelV1().to(device) model_1 Out[23]: <pre>CircleModelV1(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n)</pre> <p>Now we've got a model, we'll recreate a loss function and optimizer instance, using the same settings as before.</p> In\u00a0[24]: Copied! <pre># loss_fn = nn.BCELoss() # Requires sigmoid on input\nloss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input\noptimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)\n</pre> # loss_fn = nn.BCELoss() # Requires sigmoid on input loss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input optimizer = torch.optim.SGD(model_1.parameters(), lr=0.1) <p>Beautiful, model, optimizer and loss function ready, let's make a training loop.</p> <p>This time we'll train for longer (<code>epochs=1000</code> vs <code>epochs=100</code>) and see if it improves our model.</p> In\u00a0[25]: Copied! <pre>torch.manual_seed(42)\n\nepochs = 1000 # Train for longer\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    # 1. Forward pass\n    y_logits = model_1(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; predicition probabilities -&gt; prediction labels\n\n    # 2. Calculate loss/accuracy\n    loss = loss_fn(y_logits, y_train)\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_1.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_1(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n</pre> torch.manual_seed(42)  epochs = 1000 # Train for longer  # Put data to target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  for epoch in range(epochs):     ### Training     # 1. Forward pass     y_logits = model_1(X_train).squeeze()     y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; predicition probabilities -&gt; prediction labels      # 2. Calculate loss/accuracy     loss = loss_fn(y_logits, y_train)     acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_1.eval()     with torch.inference_mode():         # 1. Forward pass         test_logits = model_1(X_test).squeeze()          test_pred = torch.round(torch.sigmoid(test_logits))         # 2. Caculate loss/accuracy         test_loss = loss_fn(test_logits,                             y_test)         test_acc = accuracy_fn(y_true=y_test,                                y_pred=test_pred)      # Print out what's happening every 10 epochs     if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")  <pre>Epoch: 0 | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%\nEpoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%\nEpoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%\nEpoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%\nEpoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%\nEpoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%\nEpoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\n</pre> <p>What? Our model trained for longer and with an extra layer but it still looks like it didn't learn any patterns better than random guessing.</p> <p>Let's visualize.</p> In\u00a0[26]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_1, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_1, X_test, y_test)\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_1, X_train, y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_1, X_test, y_test) <p>Hmmm.</p> <p>Our model is still drawing a straight line between the red and blue dots.</p> <p>If our model is drawing a straight line, could it model linear data? Like we did in notebook 01?</p> In\u00a0[27]: Copied! <pre># Create some data (same as notebook 01)\nweight = 0.7\nbias = 0.3\nstart = 0\nend = 1\nstep = 0.01\n\n# Create data\nX_regression = torch.arange(start, end, step).unsqueeze(dim=1)\ny_regression = weight * X_regression + bias # linear regression formula\n\n# Check the data\nprint(len(X_regression))\nX_regression[:5], y_regression[:5]\n</pre> # Create some data (same as notebook 01) weight = 0.7 bias = 0.3 start = 0 end = 1 step = 0.01  # Create data X_regression = torch.arange(start, end, step).unsqueeze(dim=1) y_regression = weight * X_regression + bias # linear regression formula  # Check the data print(len(X_regression)) X_regression[:5], y_regression[:5] <pre>100\n</pre> Out[27]: <pre>(tensor([[0.0000],\n         [0.0100],\n         [0.0200],\n         [0.0300],\n         [0.0400]]),\n tensor([[0.3000],\n         [0.3070],\n         [0.3140],\n         [0.3210],\n         [0.3280]]))</pre> <p>Wonderful, now let's split our data into training and test sets.</p> In\u00a0[28]: Copied! <pre># Create train and test splits\ntrain_split = int(0.8 * len(X_regression)) # 80% of data used for training set\nX_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\nX_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n\n# Check the lengths of each split\nprint(len(X_train_regression), \n    len(y_train_regression), \n    len(X_test_regression), \n    len(y_test_regression))\n</pre> # Create train and test splits train_split = int(0.8 * len(X_regression)) # 80% of data used for training set X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split] X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]  # Check the lengths of each split print(len(X_train_regression),      len(y_train_regression),      len(X_test_regression),      len(y_test_regression)) <pre>80 80 20 20\n</pre> <p>Beautiful, let's see how the data looks.</p> <p>To do so, we'll use the <code>plot_predictions()</code> function we created in notebook 01.</p> <p>It's contained within the <code>helper_functions.py</code> script on the Learn PyTorch for Deep Learning repo which we downloaded above.</p> In\u00a0[29]: Copied! <pre>plot_predictions(train_data=X_train_regression,\n    train_labels=y_train_regression,\n    test_data=X_test_regression,\n    test_labels=y_test_regression\n);\n</pre> plot_predictions(train_data=X_train_regression,     train_labels=y_train_regression,     test_data=X_test_regression,     test_labels=y_test_regression ); In\u00a0[30]: Copied! <pre># Same architecture as model_1 (but using nn.Sequential)\nmodel_2 = nn.Sequential(\n    nn.Linear(in_features=1, out_features=10),\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=1)\n).to(device)\n\nmodel_2\n</pre> # Same architecture as model_1 (but using nn.Sequential) model_2 = nn.Sequential(     nn.Linear(in_features=1, out_features=10),     nn.Linear(in_features=10, out_features=10),     nn.Linear(in_features=10, out_features=1) ).to(device)  model_2 Out[30]: <pre>Sequential(\n  (0): Linear(in_features=1, out_features=10, bias=True)\n  (1): Linear(in_features=10, out_features=10, bias=True)\n  (2): Linear(in_features=10, out_features=1, bias=True)\n)</pre> <p>We'll setup the loss function to be <code>nn.L1Loss()</code> (the same as mean absolute error) and the optimizer to be <code>torch.optim.SGD()</code>.</p> In\u00a0[31]: Copied! <pre># Loss and optimizer\nloss_fn = nn.L1Loss()\noptimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)\n</pre> # Loss and optimizer loss_fn = nn.L1Loss() optimizer = torch.optim.SGD(model_2.parameters(), lr=0.1) <p>Now let's train the model using the regular training loop steps for <code>epochs=1000</code> (just like <code>model_1</code>).</p> <p>Note: We've been writing similar training loop code over and over again. I've made it that way on purpose though, to keep practicing. However, do you have ideas how we could functionize this? That would save a fair bit of coding in the future. Potentially there could be a function for training and a function for testing.</p> In\u00a0[32]: Copied! <pre># Train the model\ntorch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 1000\n\n# Put data to target device\nX_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\nX_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n\nfor epoch in range(epochs):\n    ### Training \n    # 1. Forward pass\n    y_pred = model_2(X_train_regression)\n    \n    # 2. Calculate loss (no accuracy since it's a regression problem, not classification)\n    loss = loss_fn(y_pred, y_train_regression)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_2.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_pred = model_2(X_test_regression)\n      # 2. Calculate the loss \n      test_loss = loss_fn(test_pred, y_test_regression)\n\n    # Print out what's happening\n    if epoch % 100 == 0: \n        print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\")\n</pre> # Train the model torch.manual_seed(42)  # Set the number of epochs epochs = 1000  # Put data to target device X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device) X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)  for epoch in range(epochs):     ### Training      # 1. Forward pass     y_pred = model_2(X_train_regression)          # 2. Calculate loss (no accuracy since it's a regression problem, not classification)     loss = loss_fn(y_pred, y_train_regression)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_2.eval()     with torch.inference_mode():       # 1. Forward pass       test_pred = model_2(X_test_regression)       # 2. Calculate the loss        test_loss = loss_fn(test_pred, y_test_regression)      # Print out what's happening     if epoch % 100 == 0:          print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\") <pre>Epoch: 0 | Train loss: 0.75986, Test loss: 0.54143\nEpoch: 100 | Train loss: 0.09309, Test loss: 0.02901\nEpoch: 200 | Train loss: 0.07376, Test loss: 0.02850\nEpoch: 300 | Train loss: 0.06745, Test loss: 0.00615\nEpoch: 400 | Train loss: 0.06107, Test loss: 0.02004\nEpoch: 500 | Train loss: 0.05698, Test loss: 0.01061\nEpoch: 600 | Train loss: 0.04857, Test loss: 0.01326\nEpoch: 700 | Train loss: 0.06109, Test loss: 0.02127\nEpoch: 800 | Train loss: 0.05599, Test loss: 0.01426\nEpoch: 900 | Train loss: 0.05571, Test loss: 0.00603\n</pre> <p>Okay, unlike <code>model_1</code> on the classification data, it looks like <code>model_2</code>'s loss is actually going down.</p> <p>Let's plot its predictions to see if that's so.</p> <p>And remember, since our model and data are using the target <code>device</code>, and this device may be a GPU, however, our plotting function uses matplotlib and matplotlib can't handle data on the GPU.</p> <p>To handle that, we'll send all of our data to the CPU using <code>.cpu()</code> when we pass it to <code>plot_predictions()</code>.</p> In\u00a0[33]: Copied! <pre># Turn on evaluation mode\nmodel_2.eval()\n\n# Make predictions (inference)\nwith torch.inference_mode():\n    y_preds = model_2(X_test_regression)\n\n# Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)\n# (try removing .cpu() from one of the below and see what happens)\nplot_predictions(train_data=X_train_regression.cpu(),\n                 train_labels=y_train_regression.cpu(),\n                 test_data=X_test_regression.cpu(),\n                 test_labels=y_test_regression.cpu(),\n                 predictions=y_preds.cpu());\n</pre> # Turn on evaluation mode model_2.eval()  # Make predictions (inference) with torch.inference_mode():     y_preds = model_2(X_test_regression)  # Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU) # (try removing .cpu() from one of the below and see what happens) plot_predictions(train_data=X_train_regression.cpu(),                  train_labels=y_train_regression.cpu(),                  test_data=X_test_regression.cpu(),                  test_labels=y_test_regression.cpu(),                  predictions=y_preds.cpu()); <p>Alright, it looks like our model is able to do far better than random guessing on straight lines.</p> <p>This is a good thing.</p> <p>It means our model at least has some capacity to learn.</p> <p>Note: A helpful troubleshooting step when building deep learning models is to start as small as possible to see if the model works before scaling it up.</p> <p>This could mean starting with a simple neural network (not many layers, not many hidden neurons) and a small dataset (like the one we've made) and then overfitting (making the model perform too well) on that small example before increasing the amount data or the model size/design to reduce overfitting.</p> <p>So what could it be?</p> <p>Let's find out.</p> In\u00a0[34]: Copied! <pre># Make and plot data\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_circles\n\nn_samples = 1000\n\nX, y = make_circles(n_samples=1000,\n    noise=0.03,\n    random_state=42,\n)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu);\n</pre> # Make and plot data import matplotlib.pyplot as plt from sklearn.datasets import make_circles  n_samples = 1000  X, y = make_circles(n_samples=1000,     noise=0.03,     random_state=42, )  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu); <p>Nice! Now let's split it into training and test sets using 80% of the data for training and 20% for testing.</p> In\u00a0[35]: Copied! <pre># Convert to tensors and split into train and test sets\nimport torch\nfrom sklearn.model_selection import train_test_split\n\n# Turn data into tensors\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2,\n                                                    random_state=42\n)\n\nX_train[:5], y_train[:5]\n</pre> # Convert to tensors and split into train and test sets import torch from sklearn.model_selection import train_test_split  # Turn data into tensors X = torch.from_numpy(X).type(torch.float) y = torch.from_numpy(y).type(torch.float)  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2,                                                     random_state=42 )  X_train[:5], y_train[:5] Out[35]: <pre>(tensor([[ 0.6579, -0.4651],\n         [ 0.6319, -0.7347],\n         [-1.0086, -0.1240],\n         [-0.9666, -0.2256],\n         [-0.1666,  0.7994]]),\n tensor([1., 0., 0., 0., 1.]))</pre> In\u00a0[36]: Copied! <pre># Build model with non-linear activation function\nfrom torch import nn\nclass CircleModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        self.relu = nn.ReLU() # &lt;- add in ReLU activation function\n        # Can also put sigmoid in the model \n        # This would mean you don't need to use it on the predictions\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n      # Intersperse the ReLU activation function between layers\n       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n\nmodel_3 = CircleModelV2().to(device)\nprint(model_3)\n</pre> # Build model with non-linear activation function from torch import nn class CircleModelV2(nn.Module):     def __init__(self):         super().__init__()         self.layer_1 = nn.Linear(in_features=2, out_features=10)         self.layer_2 = nn.Linear(in_features=10, out_features=10)         self.layer_3 = nn.Linear(in_features=10, out_features=1)         self.relu = nn.ReLU() # &lt;- add in ReLU activation function         # Can also put sigmoid in the model          # This would mean you don't need to use it on the predictions         # self.sigmoid = nn.Sigmoid()      def forward(self, x):       # Intersperse the ReLU activation function between layers        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))  model_3 = CircleModelV2().to(device) print(model_3) <pre>CircleModelV2(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n  (relu): ReLU()\n)\n</pre> <p> A visual example of what a similar classificiation neural network to the one we've just built (using ReLU activation) looks like. Try create one of your own on the TensorFlow Playground website.</p> <p>Question: Where should I put the non-linear activation functions when constructing a neural network?</p> <p>A rule of thumb is to put them in between hidden layers and just after the output layer, however, there is no set in stone option. As you learn more about neural networks and deep learning you'll find a bunch of different ways of putting things together. In the meantine, best to experiment, experiment, experiment.</p> <p>Now we've got a model ready to go, let's create a binary classification loss function as well as an optimizer.</p> In\u00a0[37]: Copied! <pre># Setup loss and optimizer \nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)\n</pre> # Setup loss and optimizer  loss_fn = nn.BCEWithLogitsLoss() optimizer = torch.optim.SGD(model_3.parameters(), lr=0.1) <p>Wonderful!</p> In\u00a0[38]: Copied! <pre># Fit the model\ntorch.manual_seed(42)\nepochs = 1000\n\n# Put all data on target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    # 1. Forward pass\n    y_logits = model_3(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n    \n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n    \n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_3.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_3(X_test).squeeze()\n      test_pred = torch.round(torch.sigmoid(test_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n      # 2. Calcuate loss and accuracy\n      test_loss = loss_fn(test_logits, y_test)\n      test_acc = accuracy_fn(y_true=y_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n</pre> # Fit the model torch.manual_seed(42) epochs = 1000  # Put all data on target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  for epoch in range(epochs):     # 1. Forward pass     y_logits = model_3(X_train).squeeze()     y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels          # 2. Calculate loss and accuracy     loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits     acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)          # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_3.eval()     with torch.inference_mode():       # 1. Forward pass       test_logits = model_3(X_test).squeeze()       test_pred = torch.round(torch.sigmoid(test_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels       # 2. Calcuate loss and accuracy       test_loss = loss_fn(test_logits, y_test)       test_acc = accuracy_fn(y_true=y_test,                              y_pred=test_pred)      # Print out what's happening     if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\") <pre>Epoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%\nEpoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%\nEpoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%\nEpoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%\nEpoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%\nEpoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%\nEpoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%\nEpoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%\nEpoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%\nEpoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%\n</pre> <p>Ho ho! That's looking far better!</p> In\u00a0[39]: Copied! <pre># Make predictions\nmodel_3.eval()\nwith torch.inference_mode():\n    y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\ny_preds[:10], y[:10] # want preds in same format as truth labels\n</pre> # Make predictions model_3.eval() with torch.inference_mode():     y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze() y_preds[:10], y[:10] # want preds in same format as truth labels Out[39]: <pre>(tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0'),\n tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]))</pre> In\u00a0[40]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity <p>Nice! Not perfect but still far better than before.</p> <p>Potentially you could try a few tricks to improve the test accuracy of the model? (hint: head back to section 5 for tips on improving the model)</p> In\u00a0[41]: Copied! <pre># Create a toy tensor (similar to the data going into our model(s))\nA = torch.arange(-10, 10, 1, dtype=torch.float32)\nA\n</pre> # Create a toy tensor (similar to the data going into our model(s)) A = torch.arange(-10, 10, 1, dtype=torch.float32) A Out[41]: <pre>tensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,\n          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])</pre> <p>Wonderful, now let's plot it.</p> In\u00a0[42]: Copied! <pre># Visualize the toy tensor\nplt.plot(A);\n</pre> # Visualize the toy tensor plt.plot(A); <p>A straight line, nice.</p> <p>Now let's see how the ReLU activation function influences it.</p> <p>And instead of using PyTorch's ReLU (<code>torch.nn.ReLU</code>), we'll recreate it ourselves.</p> <p>The ReLU function turns all negatives to 0 and leaves the positive values as they are.</p> In\u00a0[43]: Copied! <pre># Create ReLU function by hand \ndef relu(x):\n  return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n\n# Pass toy tensor through ReLU function\nrelu(A)\n</pre> # Create ReLU function by hand  def relu(x):   return torch.maximum(torch.tensor(0), x) # inputs must be tensors  # Pass toy tensor through ReLU function relu(A) Out[43]: <pre>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,\n        8., 9.])</pre> <p>It looks like our ReLU function worked, all of the negative values are zeros.</p> <p>Let's plot them.</p> In\u00a0[44]: Copied! <pre># Plot ReLU activated toy tensor\nplt.plot(relu(A));\n</pre> # Plot ReLU activated toy tensor plt.plot(relu(A)); <p>Nice! That looks exactly like the shape of the ReLU function on the Wikipedia page for ReLU).</p> <p>How about we try the sigmoid function we've been using?</p> <p>The sigmoid function formula goes like so:</p> <p>$$ out_i = \\frac{1}{1+e^{-input_i}} $$</p> <p>Or using $x$ as input:</p> <p>$$ S(x) = \\frac{1}{1+e^{-x_i}} $$</p> <p>Where $S$ stands for sigmoid, $e$ stands for exponential (<code>torch.exp()</code>) and $i$ stands for a particular element in a tensor.</p> <p>Let's build a function to replicate the sigmoid function with PyTorch.</p> In\u00a0[45]: Copied! <pre># Create a custom sigmoid function\ndef sigmoid(x):\n  return 1 / (1 + torch.exp(-x))\n\n# Test custom sigmoid on toy tensor\nsigmoid(A)\n</pre> # Create a custom sigmoid function def sigmoid(x):   return 1 / (1 + torch.exp(-x))  # Test custom sigmoid on toy tensor sigmoid(A) Out[45]: <pre>tensor([4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,\n        1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,\n        8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,\n        9.9966e-01, 9.9988e-01])</pre> <p>Woah, those values look a lot like prediction probabilities we've seen earlier, let's see what they look like visualized.</p> In\u00a0[46]: Copied! <pre># Plot sigmoid activated toy tensor\nplt.plot(sigmoid(A));\n</pre> # Plot sigmoid activated toy tensor plt.plot(sigmoid(A)); <p>Looking good! We've gone from a straight line to a curved line.</p> <p>Now there's plenty more non-linear activation functions that exist in PyTorch that we haven't tried.</p> <p>But these two are two of the most common.</p> <p>And the point remains, what patterns could you draw using an unlimited amount of linear (straight) and non-linear (not straight) lines?</p> <p>Almost anything right?</p> <p>That's exactly what our model is doing when we combine linear and non-linear functions.</p> <p>Instead of telling our model what to do, we give it tools to figure out how to best discover patterns in the data.</p> <p>And those tools are linear and non-linear functions.</p> In\u00a0[47]: Copied! <pre># Import dependencies\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\n# Set the hyperparameters for data creation\nNUM_CLASSES = 4\nNUM_FEATURES = 2\nRANDOM_SEED = 42\n\n# 1. Create multi-class data\nX_blob, y_blob = make_blobs(n_samples=1000,\n    n_features=NUM_FEATURES, # X features\n    centers=NUM_CLASSES, # y labels \n    cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n    random_state=RANDOM_SEED\n)\n\n# 2. Turn data into tensors\nX_blob = torch.from_numpy(X_blob).type(torch.float)\ny_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\nprint(X_blob[:5], y_blob[:5])\n\n# 3. Split into train and test sets\nX_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n    y_blob,\n    test_size=0.2,\n    random_state=RANDOM_SEED\n)\n\n# 4. Plot data\nplt.figure(figsize=(10, 7))\nplt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);\n</pre> # Import dependencies import torch import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split  # Set the hyperparameters for data creation NUM_CLASSES = 4 NUM_FEATURES = 2 RANDOM_SEED = 42  # 1. Create multi-class data X_blob, y_blob = make_blobs(n_samples=1000,     n_features=NUM_FEATURES, # X features     centers=NUM_CLASSES, # y labels      cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)     random_state=RANDOM_SEED )  # 2. Turn data into tensors X_blob = torch.from_numpy(X_blob).type(torch.float) y_blob = torch.from_numpy(y_blob).type(torch.LongTensor) print(X_blob[:5], y_blob[:5])  # 3. Split into train and test sets X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,     y_blob,     test_size=0.2,     random_state=RANDOM_SEED )  # 4. Plot data plt.figure(figsize=(10, 7)) plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu); <pre>tensor([[-8.4134,  6.9352],\n        [-5.7665, -6.4312],\n        [-6.0421, -6.7661],\n        [ 3.9508,  0.6984],\n        [ 4.2505, -0.2815]]) tensor([3, 2, 2, 1, 1])\n</pre> <p>Nice! Looks like we've got some multi-class data ready to go.</p> <p>Let's build a model to separate the coloured blobs.</p> <p>Question: Does this dataset need non-linearity? Or could you draw a succession of straight lines to separate it?</p> In\u00a0[48]: Copied! <pre># Create device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Create device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[48]: <pre>'cuda'</pre> In\u00a0[49]: Copied! <pre>from torch import nn\n\n# Build model\nclass BlobModel(nn.Module):\n    def __init__(self, input_features, output_features, hidden_units=8):\n        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n\n        Args:\n            input_features (int): Number of input features to the model.\n            out_features (int): Number of output features of the model\n              (how many classes there are).\n            hidden_units (int): Number of hidden units between layers, default 8.\n        \"\"\"\n        super().__init__()\n        self.linear_layer_stack = nn.Sequential(\n            nn.Linear(in_features=input_features, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n        )\n    \n    def forward(self, x):\n        return self.linear_layer_stack(x)\n\n# Create an instance of BlobModel and send it to the target device\nmodel_4 = BlobModel(input_features=NUM_FEATURES, \n                    output_features=NUM_CLASSES, \n                    hidden_units=8).to(device)\nmodel_4\n</pre> from torch import nn  # Build model class BlobModel(nn.Module):     def __init__(self, input_features, output_features, hidden_units=8):         \"\"\"Initializes all required hyperparameters for a multi-class classification model.          Args:             input_features (int): Number of input features to the model.             out_features (int): Number of output features of the model               (how many classes there are).             hidden_units (int): Number of hidden units between layers, default 8.         \"\"\"         super().__init__()         self.linear_layer_stack = nn.Sequential(             nn.Linear(in_features=input_features, out_features=hidden_units),             # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)             nn.Linear(in_features=hidden_units, out_features=hidden_units),             # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)             nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?         )          def forward(self, x):         return self.linear_layer_stack(x)  # Create an instance of BlobModel and send it to the target device model_4 = BlobModel(input_features=NUM_FEATURES,                      output_features=NUM_CLASSES,                      hidden_units=8).to(device) model_4 Out[49]: <pre>BlobModel(\n  (linear_layer_stack): Sequential(\n    (0): Linear(in_features=2, out_features=8, bias=True)\n    (1): Linear(in_features=8, out_features=8, bias=True)\n    (2): Linear(in_features=8, out_features=4, bias=True)\n  )\n)</pre> <p>Excellent! Our multi-class model is ready to go, let's create a loss function and optimizer for it.</p> In\u00a0[50]: Copied! <pre># Create loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model_4.parameters(), \n                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance\n</pre> # Create loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model_4.parameters(),                              lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance In\u00a0[51]: Copied! <pre># Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\nmodel_4(X_blob_train.to(device))[:5]\n</pre> # Perform a single forward pass on the data (we'll need to put it to the target device for it to work) model_4(X_blob_train.to(device))[:5] Out[51]: <pre>tensor([[-1.2711, -0.6494, -1.4740, -0.7044],\n        [ 0.2210, -1.5439,  0.0420,  1.1531],\n        [ 2.8698,  0.9143,  3.3169,  1.4027],\n        [ 1.9576,  0.3125,  2.2244,  1.1324],\n        [ 0.5458, -1.2381,  0.4441,  1.1804]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)</pre> <p>What's coming out here?</p> <p>It looks like we get one value per feature of each sample.</p> <p>Let's check the shape to confirm.</p> In\u00a0[52]: Copied! <pre># How many elements in a single prediction sample?\nmodel_4(X_blob_train.to(device))[0].shape, NUM_CLASSES \n</pre> # How many elements in a single prediction sample? model_4(X_blob_train.to(device))[0].shape, NUM_CLASSES  Out[52]: <pre>(torch.Size([4]), 4)</pre> <p>Wonderful, our model is predicting one value for each class that we have.</p> <p>Do you remember what the raw outputs of our model are called?</p> <p>Hint: it rhymes with \"frog splits\" (no animals were harmed in the creation of these materials).</p> <p>If you guessed logits, you'd be correct.</p> <p>So right now our model is outputing logits but what if we wanted to figure out exactly which label is was giving the sample?</p> <p>As in, how do we go from <code>logits -&gt; prediction probabilities -&gt; prediction labels</code> just like we did with the binary classification problem?</p> <p>That's where the softmax activation function comes into play.</p> <p>The softmax function calculates the probability of each prediction class being the actual predicted class compared to all other possible classes.</p> <p>If this doesn't make sense, let's see in code.</p> In\u00a0[53]: Copied! <pre># Make prediction logits with model\ny_logits = model_4(X_blob_test.to(device))\n\n# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1) \nprint(y_logits[:5])\nprint(y_pred_probs[:5])\n</pre> # Make prediction logits with model y_logits = model_4(X_blob_test.to(device))  # Perform softmax calculation on logits across dimension 1 to get prediction probabilities y_pred_probs = torch.softmax(y_logits, dim=1)  print(y_logits[:5]) print(y_pred_probs[:5]) <pre>tensor([[-1.2549, -0.8112, -1.4795, -0.5696],\n        [ 1.7168, -1.2270,  1.7367,  2.1010],\n        [ 2.2400,  0.7714,  2.6020,  1.0107],\n        [-0.7993, -0.3723, -0.9138, -0.5388],\n        [-0.4332, -1.6117, -0.6891,  0.6852]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\ntensor([[0.1872, 0.2918, 0.1495, 0.3715],\n        [0.2824, 0.0149, 0.2881, 0.4147],\n        [0.3380, 0.0778, 0.4854, 0.0989],\n        [0.2118, 0.3246, 0.1889, 0.2748],\n        [0.1945, 0.0598, 0.1506, 0.5951]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\n</pre> <p>Hmm, what's happened here?</p> <p>It may still look like the outputs of the softmax function are jumbled numbers (and they are, since our model hasn't been trained and is predicting using random patterns) but there's a very specific thing different about each sample.</p> <p>After passing the logits through the softmax function, each individual sample now adds to 1 (or very close to).</p> <p>Let's check.</p> In\u00a0[54]: Copied! <pre># Sum the first sample output of the softmax activation function \ntorch.sum(y_pred_probs[0])\n</pre> # Sum the first sample output of the softmax activation function  torch.sum(y_pred_probs[0]) Out[54]: <pre>tensor(1., device='cuda:0', grad_fn=&lt;SumBackward0&gt;)</pre> <p>These prediction probablities are essentially saying how much the model thinks the target <code>X</code> sample (the input) maps to each class.</p> <p>Since there's one value for each class in <code>y_pred_probs</code>, the index of the highest value is the class the model thinks the specific data sample most belongs to.</p> <p>We can check which index has the highest value using <code>torch.argmax()</code>.</p> In\u00a0[55]: Copied! <pre># Which class does the model think is *most* likely at the index 0 sample?\nprint(y_pred_probs[0])\nprint(torch.argmax(y_pred_probs[0]))\n</pre> # Which class does the model think is *most* likely at the index 0 sample? print(y_pred_probs[0]) print(torch.argmax(y_pred_probs[0])) <pre>tensor([0.1872, 0.2918, 0.1495, 0.3715], device='cuda:0',\n       grad_fn=&lt;SelectBackward0&gt;)\ntensor(3, device='cuda:0')\n</pre> <p>You can see the output of <code>torch.argmax()</code> returns 3, so for the features (<code>X</code>) of the sample at index 0, the model is predicting that the most likely class value (<code>y</code>) is 3.</p> <p>Of course, right now this is just random guessing so it's got a 25% chance of being right (since there's four classes). But we can improve those chances by training the model.</p> <p>Note: To summarize the above, a model's raw output is referred to as logits.</p> <p>For a multi-class classification problem, to turn the logits into prediction probabilities, you use the softmax activation function (<code>torch.softmax</code>).</p> <p>The index of the value with the highest prediction probability is the class number the model thinks is most likely given the input features for that sample (although this is a prediction, it doesn't mean it will be correct).</p> In\u00a0[56]: Copied! <pre># Fit the model\ntorch.manual_seed(42)\n\n# Set number of epochs\nepochs = 100\n\n# Put data to target device\nX_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\nX_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_4.train()\n\n    # 1. Forward pass\n    y_logits = model_4(X_blob_train) # model outputs raw logits \n    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -&gt; prediction probabilities -&gt; prediction labels\n    # print(y_logits)\n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_blob_train) \n    acc = accuracy_fn(y_true=y_blob_train,\n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_4.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_4(X_blob_test)\n      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n      # 2. Calculate test loss and accuracy\n      test_loss = loss_fn(test_logits, y_blob_test)\n      test_acc = accuracy_fn(y_true=y_blob_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\") \n</pre> # Fit the model torch.manual_seed(42)  # Set number of epochs epochs = 100  # Put data to target device X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device) X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)  for epoch in range(epochs):     ### Training     model_4.train()      # 1. Forward pass     y_logits = model_4(X_blob_train) # model outputs raw logits      y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -&gt; prediction probabilities -&gt; prediction labels     # print(y_logits)     # 2. Calculate loss and accuracy     loss = loss_fn(y_logits, y_blob_train)      acc = accuracy_fn(y_true=y_blob_train,                       y_pred=y_pred)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_4.eval()     with torch.inference_mode():       # 1. Forward pass       test_logits = model_4(X_blob_test)       test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)       # 2. Calculate test loss and accuracy       test_loss = loss_fn(test_logits, y_blob_test)       test_acc = accuracy_fn(y_true=y_blob_test,                              y_pred=test_pred)      # Print out what's happening     if epoch % 10 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")  <pre>Epoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%\nEpoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%\nEpoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%\nEpoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%\nEpoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%\nEpoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%\nEpoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%\nEpoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%\nEpoch: 80 | Loss: 0.03448, Acc: 99.00% | Test Loss: 0.02587, Test Acc: 99.50%\nEpoch: 90 | Loss: 0.03300, Acc: 99.12% | Test Loss: 0.02423, Test Acc: 99.50%\n</pre> In\u00a0[57]: Copied! <pre># Make predictions\nmodel_4.eval()\nwith torch.inference_mode():\n    y_logits = model_4(X_blob_test)\n\n# View the first 10 predictions\ny_logits[:10]\n</pre> # Make predictions model_4.eval() with torch.inference_mode():     y_logits = model_4(X_blob_test)  # View the first 10 predictions y_logits[:10] Out[57]: <pre>tensor([[  4.3377,  10.3539, -14.8948,  -9.7642],\n        [  5.0142, -12.0371,   3.3860,  10.6699],\n        [ -5.5885, -13.3448,  20.9894,  12.7711],\n        [  1.8400,   7.5599,  -8.6016,  -6.9942],\n        [  8.0727,   3.2906, -14.5998,  -3.6186],\n        [  5.5844, -14.9521,   5.0168,  13.2890],\n        [ -5.9739, -10.1913,  18.8655,   9.9179],\n        [  7.0755,  -0.7601,  -9.5531,   0.1736],\n        [ -5.5918, -18.5990,  25.5309,  17.5799],\n        [  7.3142,   0.7197, -11.2017,  -1.2011]], device='cuda:0')</pre> <p>Alright, looks like our model's predictions are still in logit form.</p> <p>Though to evaluate them, they'll have to be in the same form as our labels (<code>y_blob_test</code>) which are in integer form.</p> <p>Let's convert our model's prediction logits to prediction probabilities (using <code>torch.softmax()</code>) then to prediction labels (by taking the <code>argmax()</code> of each sample).</p> <p>Note: It's possible to skip the <code>torch.softmax()</code> function and go straight from <code>predicted logits -&gt; predicted labels</code> by calling <code>torch.argmax()</code> directly on the logits.</p> <p>For example, <code>y_preds = torch.argmax(y_logits, dim=1)</code>, this saves a computation step (no <code>torch.softmax()</code>) but results in no prediction probabilities being available to use.</p> In\u00a0[58]: Copied! <pre># Turn predicted logits in prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1)\n\n# Turn prediction probabilities into prediction labels\ny_preds = y_pred_probs.argmax(dim=1)\n\n# Compare first 10 model preds and test labels\nprint(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\")\nprint(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")\n</pre> # Turn predicted logits in prediction probabilities y_pred_probs = torch.softmax(y_logits, dim=1)  # Turn prediction probabilities into prediction labels y_preds = y_pred_probs.argmax(dim=1)  # Compare first 10 model preds and test labels print(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\") print(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\") <pre>Predictions: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nLabels: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nTest accuracy: 99.5%\n</pre> <p>Nice! Our model predictions are now in the same form as our test labels.</p> <p>Let's visualize them with <code>plot_decision_boundary()</code>, remember because our data is on the GPU, we'll have to move it to the CPU for use with matplotlib (<code>plot_decision_boundary()</code> does this automatically for us).</p> In\u00a0[59]: Copied! <pre>plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_4, X_blob_train, y_blob_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_4, X_blob_test, y_blob_test)\n</pre> plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_4, X_blob_train, y_blob_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_4, X_blob_test, y_blob_test) In\u00a0[60]: Copied! <pre>try:\n    from torchmetrics import Accuracy\nexcept:\n    !pip install torchmetrics==0.9.3 # this is the version we're using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)\n    from torchmetrics import Accuracy\n\n# Setup metric and make sure it's on the target device\ntorchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)\n\n# Calculate accuracy\ntorchmetrics_accuracy(y_preds, y_blob_test)\n</pre> try:     from torchmetrics import Accuracy except:     !pip install torchmetrics==0.9.3 # this is the version we're using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)     from torchmetrics import Accuracy  # Setup metric and make sure it's on the target device torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)  # Calculate accuracy torchmetrics_accuracy(y_preds, y_blob_test) <pre>Collecting torchmetrics==0.9.3\n  Downloading torchmetrics-0.9.3-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: numpy&gt;=1.17.2 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torchmetrics==0.9.3) (1.24.4)\nRequirement already satisfied: torch&gt;=1.3.1 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torchmetrics==0.9.3) (2.2.2)\nRequirement already satisfied: packaging in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torchmetrics==0.9.3) (24.1)\nRequirement already satisfied: filelock in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch&gt;=1.3.1-&gt;torchmetrics==0.9.3) (3.16.1)\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch&gt;=1.3.1-&gt;torchmetrics==0.9.3) (4.12.2)\nRequirement already satisfied: sympy in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch&gt;=1.3.1-&gt;torchmetrics==0.9.3) (1.13.2)\nRequirement already satisfied: networkx in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch&gt;=1.3.1-&gt;torchmetrics==0.9.3) (3.1)\nRequirement already satisfied: jinja2 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch&gt;=1.3.1-&gt;torchmetrics==0.9.3) (3.1.4)\nRequirement already satisfied: fsspec in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch&gt;=1.3.1-&gt;torchmetrics==0.9.3) (2024.6.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from jinja2-&gt;torch&gt;=1.3.1-&gt;torchmetrics==0.9.3) (2.1.5)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from sympy-&gt;torch&gt;=1.3.1-&gt;torchmetrics==0.9.3) (1.3.0)\nDownloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\nInstalling collected packages: torchmetrics\nSuccessfully installed torchmetrics-0.9.3\n</pre> Out[60]: <pre>tensor(0.9950, device='cuda:0')</pre>"},{"location":"Learning/Pytorch/02_pytorch_classification/#02-pytorch-neural-network-classification","title":"02. PyTorch Neural Network Classification\u00b6","text":""},{"location":"Learning/Pytorch/02_pytorch_classification/#what-is-a-classification-problem","title":"What is a classification problem?\u00b6","text":"<p>A classification problem involves predicting whether something is one thing or another.</p> <p>For example, you might want to:</p> Problem type What is it? Example Binary classification Target can be one of two options, e.g. yes or no Predict whether or not someone has heart disease based on their health parameters. Multi-class classification Target can be one of more than two options Decide whether a photo of is of food, a person or a dog. Multi-label classification Target can be assigned more than one option Predict what categories should be assigned to a Wikipedia article (e.g. mathematics, science &amp; philosohpy). <p>Classification, along with regression (predicting a number, covered in notebook 01) is one of the most common types of machine learning problems.</p> <p>In this notebook, we're going to work through a couple of different classification problems with PyTorch.</p> <p>In other words, taking a set of inputs and predicting what class those set of inputs belong to.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>In this notebook we're going to reiterate over the PyTorch workflow we coverd in 01. PyTorch Workflow.</p> <p>Except instead of trying to predict a straight line (predicting a number, also called a regression problem), we'll be working on a classification problem.</p> <p>Specifically, we're going to cover:</p> Topic Contents 0. Architecture of a classification neural network Neural networks can come in almost any shape or size, but they typically follow a similar floor plan. 1. Getting binary classification data ready Data can be almost anything but to get started we're going to create a simple binary classification dataset. 2. Building a PyTorch classification model Here we'll create a model to learn patterns in the data, we'll also choose a loss function, optimizer and build a training loop specific to classification. 3. Fitting the model to data (training) We've got data and a model, now let's let the model (try to) find patterns in the (training) data. 4. Making predictions and evaluating a model (inference) Our model's found patterns in the data, let's compare its findings to the actual (testing) data. 5. Improving a model (from a model perspective) We've trained an evaluated a model but it's not working, let's try a few things to improve it. 6. Non-linearity So far our model has only had the ability to model straight lines, what about non-linear (non-straight) lines? 7. Replicating non-linear functions We used non-linear functions to help model non-linear data, but what do these look like? 8. Putting it all together with multi-class classification Let's put everything we've done so far for binary classification together with a multi-class classification problem."},{"location":"Learning/Pytorch/02_pytorch_classification/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>And if you run into trouble, you can ask a question on the Discussions page there too.</p> <p>There's also the PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#0-architecture-of-a-classification-neural-network","title":"0. Architecture of a classification neural network\u00b6","text":"<p>Before we get into writing code, let's look at the general architecture of a classification neural network.</p> Hyperparameter Binary Classification Multiclass classification Input layer shape (<code>in_features</code>) Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) Same as binary classification Hidden layer(s) Problem specific, minimum = 1, maximum = unlimited Same as binary classification Neurons per hidden layer Problem specific, generally 10 to 512 Same as binary classification Output layer shape (<code>out_features</code>) 1 (one class or the other) 1 per class (e.g. 3 for food, person or dog photo) Hidden layer activation Usually ReLU (rectified linear unit) but can be many others Same as binary classification Output activation Sigmoid (<code>torch.sigmoid</code> in PyTorch) Softmax (<code>torch.softmax</code> in PyTorch) Loss function Binary crossentropy (<code>torch.nn.BCELoss</code> in PyTorch) Cross entropy (<code>torch.nn.CrossEntropyLoss</code> in PyTorch) Optimizer SGD (stochastic gradient descent), Adam (see <code>torch.optim</code> for more options) Same as binary classification <p>Of course, this ingredient list of classification neural network components will vary depending on the problem you're working on.</p> <p>But it's more than enough to get started.</p> <p>We're going to gets hands-on with this setup throughout this notebook.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#1-make-classification-data-and-get-it-ready","title":"1. Make classification data and get it ready\u00b6","text":"<p>Let's begin by making some data.</p> <p>We'll use the <code>make_circles()</code> method from Scikit-Learn to generate two circles with different coloured dots.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#11-input-and-output-shapes","title":"1.1 Input and output shapes\u00b6","text":"<p>One of the most common errors in deep learning is shape errors.</p> <p>Mismatching the shapes of tensors and tensor operations with result in errors in your models.</p> <p>We're going to see plenty of these throughout the course.</p> <p>And there's no surefire way to making sure they won't happen, they will.</p> <p>What you can do instead is continually familiarize yourself with the shape of the data you're working with.</p> <p>I like referring to it as input and output shapes.</p> <p>Ask yourself:</p> <p>\"What shapes are my inputs and what shapes are my outputs?\"</p> <p>Let's find out.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#12-turn-data-into-tensors-and-create-train-and-test-splits","title":"1.2 Turn data into tensors and create train and test splits\u00b6","text":"<p>We've investigated the input and output shapes of our data, now let's prepare it for being used with PyTorch and for modelling.</p> <p>Specifically, we'll need to:</p> <ol> <li>Turn our data into tensors (right now our data is in NumPy arrays and PyTorch prefers to work with PyTorch tensors).</li> <li>Split our data into training and test sets (we'll train a model on the training set to learn the patterns between <code>X</code> and <code>y</code> and then evaluate those learned patterns on the test dataset).</li> </ol>"},{"location":"Learning/Pytorch/02_pytorch_classification/#2-building-a-model","title":"2. Building a model\u00b6","text":"<p>We've got some data ready, now it's time to build a model.</p> <p>We'll break it down into a few parts.</p> <ol> <li>Setting up device agnostic code (so our model can run on CPU or GPU if it's available).</li> <li>Constructing a model by subclassing <code>nn.Module</code>.</li> <li>Defining a loss function and optimizer.</li> <li>Creating a training loop (this'll be in the next section).</li> </ol> <p>The good news is we've been through all of the above steps before in notebook 01.</p> <p>Except now we'll be adjusting them so they work with a classification dataset.</p> <p>Let's start by importing PyTorch and <code>torch.nn</code> as well as setting up device agnostic code.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#21-setup-loss-function-and-optimizer","title":"2.1 Setup loss function and optimizer\u00b6","text":"<p>We've setup a loss (also called a criterion or cost function) and optimizer before in notebook 01.</p> <p>But different problem types require different loss functions.</p> <p>For example, for a regression problem (predicting a number) you might used mean absolute error (MAE) loss.</p> <p>And for a binary classification problem (like ours), you'll often use binary cross entropy as the loss function.</p> <p>However, the same optimizer function can often be used across different problem spaces.</p> <p>For example, the stochastic gradient descent optimizer (SGD, <code>torch.optim.SGD()</code>) can be used for a range of problems, and the same applies to the Adam optimizer (<code>torch.optim.Adam()</code>).</p> Loss function/Optimizer Problem type PyTorch Code Stochastic Gradient Descent (SGD) optimizer Classification, regression, many others. <code>torch.optim.SGD()</code> Adam Optimizer Classification, regression, many others. <code>torch.optim.Adam()</code> Binary cross entropy loss Binary classification <code>torch.nn.BCELossWithLogits</code> or <code>torch.nn.BCELoss</code> Cross entropy loss Mutli-class classification <code>torch.nn.CrossEntropyLoss</code> Mean absolute error (MAE) or L1 Loss Regression <code>torch.nn.L1Loss</code> Mean squared error (MSE) or L2 Loss Regression <code>torch.nn.MSELoss</code> <p>Table of various loss functions and optimizers, there are more but these some common ones you'll see.</p> <p>Since we're working with a binary classification problem, let's use a binary cross entropy loss function.</p> <p>Note: Recall a loss function is what measures how wrong your model predictions are, the higher the loss, the worse your model.</p> <p>Also, PyTorch documentation often refers to loss functions as \"loss criterion\" or \"criterion\", these are all different ways of describing the same thing.</p> <p>PyTorch has two binary cross entropy implementations:</p> <ol> <li><code>torch.nn.BCELoss()</code> - Creates a loss function that measures the binary cross entropy between the target (label) and input (features).</li> <li><code>torch.nn.BCEWithLogitsLoss()</code> - This is the same as above except it has a sigmoid layer (<code>nn.Sigmoid</code>) built-in (we'll see what this means soon).</li> </ol> <p>Which one should you use?</p> <p>The documentation for <code>torch.nn.BCEWithLogitsLoss()</code> states that it's more numerically stable than using <code>torch.nn.BCELoss()</code> after a <code>nn.Sigmoid</code> layer.</p> <p>So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of <code>nn.Sigmoid</code> and <code>torch.nn.BCELoss()</code> but that is beyond the scope of this notebook.</p> <p>Knowing this, let's create a loss function and an optimizer.</p> <p>For the optimizer we'll use <code>torch.optim.SGD()</code> to optimize the model parameters with learning rate 0.1.</p> <p>Note: There's a discussion on the PyTorch forums about the use of <code>nn.BCELoss</code> vs. <code>nn.BCEWithLogitsLoss</code>. It can be confusing at first but as with many things, it becomes easier with practice.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#3-train-model","title":"3. Train model\u00b6","text":"<p>Okay, now we've got a loss function and optimizer ready to go, let's train a model.</p> <p>Do you remember the steps in a PyTorch training loop?</p> <p>If not, here's a reminder.</p> <p>Steps in training:</p> PyTorch training loop steps <ol> <li>Forward pass - The model goes through all of the training data once, performing its             <code>forward()</code> function             calculations (<code>model(x_train)</code>).         </li> <li>Calculate the loss - The model's outputs (predictions) are compared to the ground truth and evaluated             to see how             wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li> <li>Zero gradients - The optimizers gradients are set to zero (they are accumulated by default) so they             can be             recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li> <li>Perform backpropagation on the loss - Computes the gradient of the loss with respect for every model             parameter to             be updated (each parameter             with <code>requires_grad=True</code>). This is known as backpropagation, hence \"backwards\"             (<code>loss.backward()</code>).</li> <li>Step the optimizer (gradient descent) - Update the parameters with <code>requires_grad=True</code>             with respect to the loss             gradients in order to improve them (<code>optimizer.step()</code>).</li> </ol>"},{"location":"Learning/Pytorch/02_pytorch_classification/#31-going-from-raw-model-outputs-to-predicted-labels-logits-prediction-probabilities-prediction-labels","title":"3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels)\u00b6","text":"<p>Before the training loop steps, let's see what comes out of our model during the forward pass (the forward pass is defined by the <code>forward()</code> method).</p> <p>To do so, let's pass the model some data.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#32-building-a-training-and-testing-loop","title":"3.2 Building a training and testing loop\u00b6","text":"<p>Alright, we've discussed how to take our raw model outputs and convert them to prediction labels, now let's build a training loop.</p> <p>Let's start by training for 100 epochs and outputing the model's progress every 10 epochs.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#4-make-predictions-and-evaluate-the-model","title":"4. Make predictions and evaluate the model\u00b6","text":"<p>From the metrics it looks like our model is random guessing.</p> <p>How could we investigate this further?</p> <p>I've got an idea.</p> <p>The data explorer's motto!</p> <p>\"Visualize, visualize, visualize!\"</p> <p>Let's make a plot of our model's predictions, the data it's trying to predict on and the decision boundary it's creating for whether something is class 0 or class 1.</p> <p>To do so, we'll write some code to download and import the <code>helper_functions.py</code> script from the Learn PyTorch for Deep Learning repo.</p> <p>It contains a helpful function called <code>plot_decision_boundary()</code> which creates a NumPy meshgrid to visually plot the different points where our model is predicting certain classes.</p> <p>We'll also import <code>plot_predictions()</code> which we wrote in notebook 01 to use later.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#5-improving-a-model-from-a-model-perspective","title":"5. Improving a model (from a model perspective)\u00b6","text":"<p>Let's try to fix our model's underfitting problem.</p> <p>Focusing specifically on the model (not the data), there are a few ways we could do this.</p> Model improvement technique* What does it do? Add more layers Each layer potentially increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data, more layers is often referred to as making your neural network deeper. Add more hidden units Similar to the above, more hidden units per layer means a potential increase in learning capabilities of the model, more hidden units is often referred to as making your neural network wider. Fitting for longer (more epochs) Your model might learn more if it had more opportunities to look at the data. Changing the activation functions Some data just can't be fit with only straight lines (like what we've seen), using non-linear activation functions can help with this (hint, hint). Change the learning rate Less model specific, but still related, the learning rate of the optimizer decides how much a model should change its parameters each step, too much and the model overcorrects, too little and it doesn't learn enough. Change the loss function Again, less model specific but still important, different problems require different loss functions. For example, a binary cross entropy loss function won't work with a multi-class classification problem. Use transfer learning Take a pretrained model from a problem domain similar to yours and adjust it to your own problem. We cover transfer learning in notebook 06. <p>Note: *because you can adjust all of these by hand, they're referred to as hyperparameters.</p> <p>And this is also where machine learning's half art half science comes in, there's no real way to know here what the best combination of values is for your project, best to follow the data scientist's motto of \"experiment, experiment, experiment\".</p> <p>Let's see what happens if we add an extra layer to our model, fit for longer (<code>epochs=1000</code> instead of <code>epochs=100</code>) and increase the number of hidden units from <code>5</code> to <code>10</code>.</p> <p>We'll follow the same steps we did above but with a few changed hyperparameters.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#51-preparing-data-to-see-if-our-model-can-model-a-straight-line","title":"5.1 Preparing data to see if our model can model a straight line\u00b6","text":"<p>Let's create some linear data to see if our model's able to model it and we're not just using a model that can't learn anything.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#52-adjusting-model_1-to-fit-a-straight-line","title":"5.2 Adjusting <code>model_1</code> to fit a straight line\u00b6","text":"<p>Now we've got some data, let's recreate <code>model_1</code> but with a loss function suited to our regression data.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#6-the-missing-piece-non-linearity","title":"6. The missing piece: non-linearity\u00b6","text":"<p>We've seen our model can draw straight (linear) lines, thanks to its linear layers.</p> <p>But how about we give it the capacity to draw non-straight (non-linear) lines?</p> <p>How?</p> <p>Let's find out.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#61-recreating-non-linear-data-red-and-blue-circles","title":"6.1 Recreating non-linear data (red and blue circles)\u00b6","text":"<p>First, let's recreate the data to start off fresh. We'll use the same setup as before.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#62-building-a-model-with-non-linearity","title":"6.2 Building a model with non-linearity\u00b6","text":"<p>Now here comes the fun part.</p> <p>What kind of pattern do you think you could draw with unlimited straight (linear) and non-straight (non-linear) lines?</p> <p>I bet you could get pretty creative.</p> <p>So far our neural networks have only been using linear (straight) line functions.</p> <p>But the data we've been working with is non-linear (circles).</p> <p>What do you think will happen when we introduce the capability for our model to use non-linear actviation functions?</p> <p>Well let's see.</p> <p>PyTorch has a bunch of ready-made non-linear activation functions that do similiar but different things.</p> <p>One of the most common and best performing is ReLU (rectified linear-unit, <code>torch.nn.ReLU()</code>).</p> <p>Rather than talk about it, let's put it in our neural network between the hidden layers in the forward pass and see what happens.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#63-training-a-model-with-non-linearity","title":"6.3 Training a model with non-linearity\u00b6","text":"<p>You know the drill, model, loss function, optimizer ready to go, let's create a training and testing loop.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#64-evaluating-a-model-trained-with-non-linear-activation-functions","title":"6.4 Evaluating a model trained with non-linear activation functions\u00b6","text":"<p>Remember how our circle data is non-linear? Well, let's see how our models predictions look now the model's been trained with non-linear activation functions.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#7-replicating-non-linear-activation-functions","title":"7. Replicating non-linear activation functions\u00b6","text":"<p>We saw before how adding non-linear activation functions to our model can help it to model non-linear data.</p> <p>Note: Much of the data you'll encounter in the wild is non-linear (or a combination of linear and non-linear). Right now we've been working with dots on a 2D plot. But imagine if you had images of plants you'd like to classify, there's a lot of different plant shapes. Or text from Wikipedia you'd like to summarize, there's lots of different ways words can be put together (linear and non-linear patterns).</p> <p>But what does a non-linear activation look like?</p> <p>How about we replicate some and what they do?</p> <p>Let's start by creating a small amount of data.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#8-putting-things-together-by-building-a-multi-class-pytorch-model","title":"8. Putting things together by building a multi-class PyTorch model\u00b6","text":"<p>We've covered a fair bit.</p> <p>But now let's put it all together using a multi-class classification problem.</p> <p>Recall a binary classification problem deals with classifying something as one of two options (e.g. a photo as a cat photo or a dog photo) where as a multi-class classification problem deals with classifying something from a list of more than two options (e.g. classifying a photo as a cat a dog or a chicken).</p> <p> Example of binary vs. multi-class classification. Binary deals with two classes (one thing or another), where as multi-class classification can deal with any number of classes over two, for example, the popular ImageNet-1k dataset is used as a computer vision benchmark and has 1000 classes.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#81-creating-mutli-class-classification-data","title":"8.1 Creating mutli-class classification data\u00b6","text":"<p>To begin a multi-class classification problem, let's create some multi-class data.</p> <p>To do so, we can leverage Scikit-Learn's <code>make_blobs()</code> method.</p> <p>This method will create however many classes (using the <code>centers</code> parameter) we want.</p> <p>Specifically, let's do the following:</p> <ol> <li>Create some multi-class data with <code>make_blobs()</code>.</li> <li>Turn the data into tensors (the default of <code>make_blobs()</code> is to use NumPy arrays).</li> <li>Split the data into training and test sets using <code>train_test_split()</code>.</li> <li>Visualize the data.</li> </ol>"},{"location":"Learning/Pytorch/02_pytorch_classification/#82-building-a-multi-class-classification-model-in-pytorch","title":"8.2 Building a multi-class classification model in PyTorch\u00b6","text":"<p>We've created a few models in PyTorch so far.</p> <p>You might also be starting to get an idea of how flexible neural networks are.</p> <p>How about we build one similar to <code>model_3</code> but this still capable of handling multi-class data?</p> <p>To do so, let's create a subclass of <code>nn.Module</code> that takes in three hyperparameters:</p> <ul> <li><code>input_features</code> - the number of <code>X</code> features coming into the model.</li> <li><code>output_features</code> - the ideal numbers of output features we'd like (this will be equivalent to <code>NUM_CLASSES</code> or the number of classes in your multi-class classification problem).</li> <li><code>hidden_units</code> - the number of hidden neurons we'd like each hidden layer to use.</li> </ul> <p>Since we're putting things together, let's setup some device agnostic code (we don't have to do this again in the same notebook, it's only a reminder).</p> <p>Then we'll create the model class using the hyperparameters above.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#83-creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model","title":"8.3 Creating a loss function and optimizer for a multi-class PyTorch model\u00b6","text":"<p>Since we're working on a multi-class classification problem, we'll use the <code>nn.CrossEntropyLoss()</code> method as our loss function.</p> <p>And we'll stick with using SGD with a learning rate of 0.1 for optimizing our <code>model_4</code> parameters.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#84-getting-prediction-probabilities-for-a-multi-class-pytorch-model","title":"8.4 Getting prediction probabilities for a multi-class PyTorch model\u00b6","text":"<p>Alright, we've got a loss function and optimizer ready, and we're ready to train our model but before we do let's do a single forward pass with our model to see if it works.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#85-creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model","title":"8.5 Creating a training and testing loop for a multi-class PyTorch model\u00b6","text":"<p>Alright, now we've got all of the preparation steps out of the way, let's write a training and testing loop to improve and evaluate our model.</p> <p>We've done many of these steps before so much of this will be practice.</p> <p>The only difference is that we'll be adjusting the steps to turn the model outputs (logits) to prediction probabilities (using the softmax activation function) and then to prediction labels (by taking the argmax of the output of the softmax activation function).</p> <p>Let's train the model for <code>epochs=100</code> and evaluate it every 10 epochs.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#86-making-and-evaluating-predictions-with-a-pytorch-multi-class-model","title":"8.6 Making and evaluating predictions with a PyTorch multi-class model\u00b6","text":"<p>It looks like our trained model is performaning pretty well.</p> <p>But to make sure of this, let's make some predictions and visualize them.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#9-more-classification-evaluation-metrics","title":"9. More classification evaluation metrics\u00b6","text":"<p>So far we've only covered a couple of ways of evaluating a classification model (accuracy, loss and visualizing predictions).</p> <p>These are some of the most common methods you'll come across and are a good starting point.</p> <p>However, you may want to evaluate your classification model using more metrics such as the following:</p> Metric name/Evaluation method Defintion Code Accuracy Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. <code>torchmetrics.Accuracy()</code> or <code>sklearn.metrics.accuracy_score()</code> Precision Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). <code>torchmetrics.Precision()</code> or <code>sklearn.metrics.precision_score()</code> Recall Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. <code>torchmetrics.Recall()</code> or <code>sklearn.metrics.recall_score()</code> F1-score Combines precision and recall into one metric. 1 is best, 0 is worst. <code>torchmetrics.F1Score()</code> or <code>sklearn.metrics.f1_score()</code> Confusion matrix Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). <code>torchmetrics.ConfusionMatrix</code> or <code>sklearn.metrics.plot_confusion_matrix()</code> Classification report Collection of some of the main classification metrics such as precision, recall and f1-score. <code>sklearn.metrics.classification_report()</code> <p>Scikit-Learn (a popular and world-class machine learning library) has many implementations of the above metrics and you're looking for a PyTorch-like version, check out TorchMetrics, especially the TorchMetrics classification section.</p> <p>Let's try the <code>torchmetrics.Accuracy</code> metric out.</p>"},{"location":"Learning/Pytorch/02_pytorch_classification/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code in the sections above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agonistic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 02</li> <li>Example solutions notebook for 02 (try the exercises before looking at this)</li> </ul> <ol> <li>Make a binary classification dataset with Scikit-Learn's <code>make_moons()</code> function.<ul> <li>For consistency, the dataset should have 1000 samples and a <code>random_state=42</code>.</li> <li>Turn the data into PyTorch tensors. Split the data into training and test sets using <code>train_test_split</code> with 80% training and 20% testing.</li> </ul> </li> <li>Build a model by subclassing <code>nn.Module</code> that incorporates non-linear activation functions and is capable of fitting the data you created in 1.<ul> <li>Feel free to use any combination of PyTorch layers (linear and non-linear) you want.</li> </ul> </li> <li>Setup a binary classification compatible loss function and optimizer to use when training the model.</li> <li>Create a training and testing loop to fit the model you created in 2 to the data you created in 1.<ul> <li>To measure model accuray, you can create your own accuracy function or use the accuracy function in TorchMetrics.</li> <li>Train the model for long enough for it to reach over 96% accuracy.</li> <li>The training loop should output progress every 10 epochs of the model's training and test set loss and accuracy.</li> </ul> </li> <li>Make predictions with your trained model and plot them using the <code>plot_decision_boundary()</code> function created in this notebook.</li> <li>Replicate the Tanh (hyperbolic tangent) activation function in pure PyTorch.<ul> <li>Feel free to reference the ML cheatsheet website for the formula.</li> </ul> </li> <li>Create a multi-class dataset using the spirals data creation function from CS231n (see below for the code).<ul> <li>Construct a model capable of fitting the data (you may need a combination of linear and non-linear layers).</li> <li>Build a loss function and optimizer capable of handling multi-class data (optional extension: use the Adam optimizer instead of SGD, you may have to experiment with different values of the learning rate to get it working).</li> <li>Make a training and testing loop for the multi-class data and train a model on it to reach over 95% testing accuracy (you can use any accuracy measuring function here that you like).</li> <li>Plot the decision boundaries on the spirals dataset from your model predictions, the <code>plot_decision_boundary()</code> function should work for this dataset too.</li> </ul> </li> </ol> <pre># Code for creating a spiral dataset from CS231n\nimport numpy as np\nN = 100 # number of points per class\nD = 2 # dimensionality\nK = 3 # number of classes\nX = np.zeros((N*K,D)) # data matrix (each row = single example)\ny = np.zeros(N*K, dtype='uint8') # class labels\nfor j in range(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N) # radius\n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n  y[ix] = j\n# lets visualize the data\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.show()\n</pre>"},{"location":"Learning/Pytorch/02_pytorch_classification/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Write down 3 problems where you think machine classification could be useful (these can be anything, get creative as you like, for example, classifying credit card transactions as fraud or not fraud based on the purchase amount and purchase location features).</li> <li>Research the concept of \"momentum\" in gradient-based optimizers (like SGD or Adam), what does it mean?</li> <li>Spend 10-minutes reading the Wikipedia page for different activation functions, how many of these can you line up with PyTorch's activation functions?</li> <li>Research when accuracy might be a poor metric to use (hint: read \"Beyond Accuracy\" by by Will Koehrsen for ideas).</li> <li>Watch: For an idea of what's happening within our neural networks and what they're doing to learn, watch MIT's Introduction to Deep Learning video.</li> </ul>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/","title":"03. PyTorch Computer Vision","text":"<p>View Source Code | View Slides | Watch Video Walkthrough</p> In\u00a0[1]: Copied! <pre># Import PyTorch\nimport torch\nfrom torch import nn\n\n# Import torchvision \nimport torchvision\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Import matplotlib for visualization\nimport matplotlib.pyplot as plt\n\n# Check versions\n# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\nprint(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")\n</pre> # Import PyTorch import torch from torch import nn  # Import torchvision  import torchvision from torchvision import datasets from torchvision.transforms import ToTensor  # Import matplotlib for visualization import matplotlib.pyplot as plt  # Check versions # Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11 print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\") <pre>/home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <pre>PyTorch version: 2.2.2\ntorchvision version: 0.17.2\n</pre> In\u00a0[2]: Copied! <pre># Setup training data\ntrain_data = datasets.FashionMNIST(\n    root=\"data\", # where to download data to?\n    train=True, # get training data\n    download=True, # download data if it doesn't exist on disk\n    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n    target_transform=None # you can transform labels as well\n)\n\n# Setup testing data\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False, # get test data\n    download=True,\n    transform=ToTensor()\n)\n</pre> # Setup training data train_data = datasets.FashionMNIST(     root=\"data\", # where to download data to?     train=True, # get training data     download=True, # download data if it doesn't exist on disk     transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors     target_transform=None # you can transform labels as well )  # Setup testing data test_data = datasets.FashionMNIST(     root=\"data\",     train=False, # get test data     download=True,     transform=ToTensor() ) <pre>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26421880/26421880 [00:04&lt;00:00, 6256983.59it/s] \n</pre> <pre>Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29515/29515 [00:00&lt;00:00, 156422.41it/s]\n</pre> <pre>Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4422102/4422102 [00:02&lt;00:00, 1537035.93it/s]\n</pre> <pre>Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5148/5148 [00:00&lt;00:00, 12776495.26it/s]</pre> <pre>Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\n</pre> <pre>\n</pre> <p>Let's check out the first sample of the training data.</p> In\u00a0[3]: Copied! <pre># See first training sample\nimage, label = train_data[0]\nimage, label\n</pre> # See first training sample image, label = train_data[0] image, label Out[3]: <pre>(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n           0.0157, 0.0000, 0.0000, 0.0118],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0471, 0.0392, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n           0.3020, 0.5098, 0.2824, 0.0588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n           0.5529, 0.3451, 0.6745, 0.2588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n           0.4824, 0.7686, 0.8980, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n           0.8745, 0.9608, 0.6784, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n           0.8627, 0.9529, 0.7922, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n           0.8863, 0.7725, 0.8196, 0.2039],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n           0.9608, 0.4667, 0.6549, 0.2196],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n           0.8510, 0.8196, 0.3608, 0.0000],\n          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n           0.8549, 1.0000, 0.3020, 0.0000],\n          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n           0.8784, 0.9569, 0.6235, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n           0.9137, 0.9333, 0.8431, 0.0000],\n          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n           0.8627, 0.9098, 0.9647, 0.0000],\n          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n           0.8706, 0.8941, 0.8824, 0.0000],\n          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n           0.8745, 0.8784, 0.8980, 0.1137],\n          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n           0.8627, 0.8667, 0.9020, 0.2627],\n          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n           0.7098, 0.8039, 0.8078, 0.4510],\n          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n           0.6549, 0.6941, 0.8235, 0.3608],\n          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n           0.7529, 0.8471, 0.6667, 0.0000],\n          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n           0.3882, 0.2275, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000]]]),\n 9)</pre> In\u00a0[4]: Copied! <pre># What's the shape of the image?\nimage.shape\n</pre> # What's the shape of the image? image.shape Out[4]: <pre>torch.Size([1, 28, 28])</pre> <p>The shape of the image tensor is <code>[1, 28, 28]</code> or more specifically:</p> <pre><code>[color_channels=1, height=28, width=28]\n</code></pre> <p>Having <code>color_channels=1</code> means the image is grayscale.</p> <p> Various problems will have various input and output shapes. But the premise remains: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.</p> <p>If <code>color_channels=3</code>, the image comes in pixel values for red, green and blue (this is also known a the RGB color model).</p> <p>The order of our current tensor is often referred to as <code>CHW</code> (Color Channels, Height, Width).</p> <p>There's debate on whether images should be represented as <code>CHW</code> (color channels first) or <code>HWC</code> (color channels last).</p> <p>Note: You'll also see <code>NCHW</code> and <code>NHWC</code> formats where <code>N</code> stands for number of images. For example if you have a <code>batch_size=32</code>, your tensor shape may be <code>[32, 1, 28, 28]</code>. We'll cover batch sizes later.</p> <p>PyTorch generally accepts <code>NCHW</code> (channels first) as the default for many operators.</p> <p>However, PyTorch also explains that <code>NHWC</code> (channels last) performs better and is considered best practice.</p> <p>For now, since our dataset and models are relatively small, this won't make too much of a difference.</p> <p>But keep it in mind for when you're working on larger image datasets and using convolutional neural networks (we'll see these later).</p> <p>Let's check out more shapes of our data.</p> In\u00a0[5]: Copied! <pre># How many samples are there? \nlen(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)\n</pre> # How many samples are there?  len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets) Out[5]: <pre>(60000, 60000, 10000, 10000)</pre> <p>So we've got 60,000 training samples and 10,000 testing samples.</p> <p>What classes are there?</p> <p>We can find these via the <code>.classes</code> attribute.</p> In\u00a0[6]: Copied! <pre># See classes\nclass_names = train_data.classes\nclass_names\n</pre> # See classes class_names = train_data.classes class_names Out[6]: <pre>['T-shirt/top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']</pre> <p>Sweet! It looks like we're dealing with 10 different kinds of clothes.</p> <p>Because we're working with 10 different classes, it means our problem is multi-class classification.</p> <p>Let's get visual.</p> In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\nimage, label = train_data[0]\nprint(f\"Image shape: {image.shape}\")\nplt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\nplt.title(label);\n</pre> import matplotlib.pyplot as plt image, label = train_data[0] print(f\"Image shape: {image.shape}\") plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width) plt.title(label); <pre>Image shape: torch.Size([1, 28, 28])\n</pre> <p>We can turn the image into grayscale using the <code>cmap</code> parameter of <code>plt.imshow()</code>.</p> In\u00a0[8]: Copied! <pre>plt.imshow(image.squeeze(), cmap=\"gray\")\nplt.title(class_names[label]);\n</pre> plt.imshow(image.squeeze(), cmap=\"gray\") plt.title(class_names[label]); <p>Beautiful, well as beautiful as a pixelated grayscale ankle boot can get.</p> <p>Let's view a few more.</p> In\u00a0[9]: Copied! <pre># Plot more images\ntorch.manual_seed(42)\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 4, 4\nfor i in range(1, rows * cols + 1):\n    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n    img, label = train_data[random_idx]\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(img.squeeze(), cmap=\"gray\")\n    plt.title(class_names[label])\n    plt.axis(False);\n</pre> # Plot more images torch.manual_seed(42) fig = plt.figure(figsize=(9, 9)) rows, cols = 4, 4 for i in range(1, rows * cols + 1):     random_idx = torch.randint(0, len(train_data), size=[1]).item()     img, label = train_data[random_idx]     fig.add_subplot(rows, cols, i)     plt.imshow(img.squeeze(), cmap=\"gray\")     plt.title(class_names[label])     plt.axis(False); <p>Hmmm, this dataset doesn't look too aesthetic.</p> <p>But the principles we're going to learn on how to build a model for it will be similar across a wide range of computer vision problems.</p> <p>In essence, taking pixel values and building a model to find patterns in them to use on future pixel values.</p> <p>Plus, even for this small dataset (yes, even 60,000 images in deep learning is considered quite small), could you write a program to classify each one of them?</p> <p>You probably could.</p> <p>But I think coding a model in PyTorch would be faster.</p> <p>Question: Do you think the above data can be model with only straight (linear) lines? Or do you think you'd also need non-straight (non-linear) lines?</p> In\u00a0[10]: Copied! <pre>from torch.utils.data import DataLoader\n\n# Setup the batch size hyperparameter\nBATCH_SIZE = 32\n\n# Turn datasets into iterables (batches)\ntrain_dataloader = DataLoader(train_data, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle=True # shuffle data every epoch?\n)\n\ntest_dataloader = DataLoader(test_data,\n    batch_size=BATCH_SIZE,\n    shuffle=False # don't necessarily have to shuffle the testing data\n)\n\n# Let's check out what we've created\nprint(f\"Dataloaders: {train_dataloader, test_dataloader}\") \nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n</pre> from torch.utils.data import DataLoader  # Setup the batch size hyperparameter BATCH_SIZE = 32  # Turn datasets into iterables (batches) train_dataloader = DataLoader(train_data, # dataset to turn into iterable     batch_size=BATCH_SIZE, # how many samples per batch?      shuffle=True # shuffle data every epoch? )  test_dataloader = DataLoader(test_data,     batch_size=BATCH_SIZE,     shuffle=False # don't necessarily have to shuffle the testing data )  # Let's check out what we've created print(f\"Dataloaders: {train_dataloader, test_dataloader}\")  print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\") print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\") <pre>Dataloaders: (&lt;torch.utils.data.dataloader.DataLoader object at 0x7f15a41e1f40&gt;, &lt;torch.utils.data.dataloader.DataLoader object at 0x7f15a41e1ee0&gt;)\nLength of train dataloader: 1875 batches of 32\nLength of test dataloader: 313 batches of 32\n</pre> In\u00a0[11]: Copied! <pre># Check out what's inside the training dataloader\ntrain_features_batch, train_labels_batch = next(iter(train_dataloader))\ntrain_features_batch.shape, train_labels_batch.shape\n</pre> # Check out what's inside the training dataloader train_features_batch, train_labels_batch = next(iter(train_dataloader)) train_features_batch.shape, train_labels_batch.shape Out[11]: <pre>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</pre> <p>And we can see that the data remains unchanged by checking a single sample.</p> In\u00a0[12]: Copied! <pre># Show a sample\ntorch.manual_seed(42)\nrandom_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\nimg, label = train_features_batch[random_idx], train_labels_batch[random_idx]\nplt.imshow(img.squeeze(), cmap=\"gray\")\nplt.title(class_names[label])\nplt.axis(\"Off\");\nprint(f\"Image size: {img.shape}\")\nprint(f\"Label: {label}, label size: {label.shape}\")\n</pre> # Show a sample torch.manual_seed(42) random_idx = torch.randint(0, len(train_features_batch), size=[1]).item() img, label = train_features_batch[random_idx], train_labels_batch[random_idx] plt.imshow(img.squeeze(), cmap=\"gray\") plt.title(class_names[label]) plt.axis(\"Off\"); print(f\"Image size: {img.shape}\") print(f\"Label: {label}, label size: {label.shape}\") <pre>Image size: torch.Size([1, 28, 28])\nLabel: 6, label size: torch.Size([])\n</pre> In\u00a0[13]: Copied! <pre># Create a flatten layer\nflatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n\n# Get a single sample\nx = train_features_batch[0]\n\n# Flatten the sample\noutput = flatten_model(x) # perform forward pass\n\n# Print out what happened\nprint(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")\n\n# Try uncommenting below and see what happens\n#print(x)\n#print(output)\n</pre> # Create a flatten layer flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)  # Get a single sample x = train_features_batch[0]  # Flatten the sample output = flatten_model(x) # perform forward pass  # Print out what happened print(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\") print(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")  # Try uncommenting below and see what happens #print(x) #print(output) <pre>Shape before flattening: torch.Size([1, 28, 28]) -&gt; [color_channels, height, width]\nShape after flattening: torch.Size([1, 784]) -&gt; [color_channels, height*width]\n</pre> <p>The <code>nn.Flatten()</code> layer took our shape from <code>[color_channels, height, width]</code> to <code>[color_channels, height*width]</code>.</p> <p>Why do this?</p> <p>Because we've now turned our pixel data from height and width dimensions into one long feature vector.</p> <p>And <code>nn.Linear()</code> layers like their inputs to be in the form of feature vectors.</p> <p>Let's create our first model using <code>nn.Flatten()</code> as the first layer.</p> In\u00a0[14]: Copied! <pre>from torch import nn\nclass FashionMNISTModelV0(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # neural networks like their inputs in vector form\n            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n            nn.Linear(in_features=hidden_units, out_features=output_shape)\n        )\n    \n    def forward(self, x):\n        return self.layer_stack(x)\n</pre> from torch import nn class FashionMNISTModelV0(nn.Module):     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.layer_stack = nn.Sequential(             nn.Flatten(), # neural networks like their inputs in vector form             nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)             nn.Linear(in_features=hidden_units, out_features=output_shape)         )          def forward(self, x):         return self.layer_stack(x) <p>Wonderful!</p> <p>We've got a baseline model class we can use, now let's instantiate a model.</p> <p>We'll need to set the following parameters:</p> <ul> <li><code>input_shape=784</code> - this is how many features you've got going in the model, in our case, it's one for every pixel in the target image (28 pixels high by 28 pixels wide = 784 features).</li> <li><code>hidden_units=10</code> - number of units/neurons in the hidden layer(s), this number could be whatever you want but to keep the model small we'll start with <code>10</code>.</li> <li><code>output_shape=len(class_names)</code> - since we're working with a multi-class classification problem, we need an output neuron per class in our dataset.</li> </ul> <p>Let's create an instance of our model and send to the CPU for now (we'll run a small test for running <code>model_0</code> on CPU vs. a similar model on GPU soon).</p> In\u00a0[15]: Copied! <pre>torch.manual_seed(42)\n\n# Need to setup model with input parameters\nmodel_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n    hidden_units=10, # how many units in the hiden layer\n    output_shape=len(class_names) # one for every class\n)\nmodel_0.to(\"cpu\") # keep model on CPU to begin with \n</pre> torch.manual_seed(42)  # Need to setup model with input parameters model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)     hidden_units=10, # how many units in the hiden layer     output_shape=len(class_names) # one for every class ) model_0.to(\"cpu\") # keep model on CPU to begin with  Out[15]: <pre>FashionMNISTModelV0(\n  (layer_stack): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=10, bias=True)\n    (2): Linear(in_features=10, out_features=10, bias=True)\n  )\n)</pre> In\u00a0[16]: Copied! <pre>import requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  # Note: you need the \"raw\" GitHub URL for this to work\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n</pre> import requests from pathlib import Path   # Download helper functions from Learn PyTorch repo (if not already downloaded) if Path(\"helper_functions.py\").is_file():   print(\"helper_functions.py already exists, skipping download\") else:   print(\"Downloading helper_functions.py\")   # Note: you need the \"raw\" GitHub URL for this to work   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")   with open(\"helper_functions.py\", \"wb\") as f:     f.write(request.content) <pre>helper_functions.py already exists, skipping download\n</pre> In\u00a0[17]: Copied! <pre># Import accuracy metric\nfrom helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\noptimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n</pre> # Import accuracy metric from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1) In\u00a0[18]: Copied! <pre>from timeit import default_timer as timer \ndef print_train_time(start: float, end: float, device: torch.device = None):\n    \"\"\"Prints difference between start and end time.\n\n    Args:\n        start (float): Start time of computation (preferred in timeit format). \n        end (float): End time of computation.\n        device ([type], optional): Device that compute is running on. Defaults to None.\n\n    Returns:\n        float: time between start and end in seconds (higher is longer).\n    \"\"\"\n    total_time = end - start\n    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n    return total_time\n</pre> from timeit import default_timer as timer  def print_train_time(start: float, end: float, device: torch.device = None):     \"\"\"Prints difference between start and end time.      Args:         start (float): Start time of computation (preferred in timeit format).          end (float): End time of computation.         device ([type], optional): Device that compute is running on. Defaults to None.      Returns:         float: time between start and end in seconds (higher is longer).     \"\"\"     total_time = end - start     print(f\"Train time on {device}: {total_time:.3f} seconds\")     return total_time In\u00a0[19]: Copied! <pre># Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# Set the seed and start the timer\ntorch.manual_seed(42)\ntrain_time_start_on_cpu = timer()\n\n# Set the number of epochs (we'll keep this small for faster training times)\nepochs = 3\n\n# Create training and testing loop\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n-------\")\n    ### Training\n    train_loss = 0\n    # Add a loop to loop through training batches\n    for batch, (X, y) in enumerate(train_dataloader):\n        model_0.train() \n        # 1. Forward pass\n        y_pred = model_0(X)\n\n        # 2. Calculate loss (per batch)\n        loss = loss_fn(y_pred, y)\n        train_loss += loss # accumulatively add up the loss per epoch \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Print out how many samples have been seen\n        if batch % 400 == 0:\n            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n\n    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n    train_loss /= len(train_dataloader)\n    \n    ### Testing\n    # Setup variables for accumulatively adding up loss and accuracy \n    test_loss, test_acc = 0, 0 \n    model_0.eval()\n    with torch.inference_mode():\n        for X, y in test_dataloader:\n            # 1. Forward pass\n            test_pred = model_0(X)\n           \n            # 2. Calculate loss (accumatively)\n            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n\n            # 3. Calculate accuracy (preds need to be same as y_true)\n            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n        \n        # Calculations on test metrics need to happen inside torch.inference_mode()\n        # Divide total test loss by length of test dataloader (per batch)\n        test_loss /= len(test_dataloader)\n\n        # Divide total accuracy by length of test dataloader (per batch)\n        test_acc /= len(test_dataloader)\n\n    ## Print out what's happening\n    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n\n# Calculate training time      \ntrain_time_end_on_cpu = timer()\ntotal_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n                                           end=train_time_end_on_cpu,\n                                           device=str(next(model_0.parameters()).device))\n</pre> # Import tqdm for progress bar from tqdm.auto import tqdm  # Set the seed and start the timer torch.manual_seed(42) train_time_start_on_cpu = timer()  # Set the number of epochs (we'll keep this small for faster training times) epochs = 3  # Create training and testing loop for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n-------\")     ### Training     train_loss = 0     # Add a loop to loop through training batches     for batch, (X, y) in enumerate(train_dataloader):         model_0.train()          # 1. Forward pass         y_pred = model_0(X)          # 2. Calculate loss (per batch)         loss = loss_fn(y_pred, y)         train_loss += loss # accumulatively add up the loss per epoch           # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()          # Print out how many samples have been seen         if batch % 400 == 0:             print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")      # Divide total train loss by length of train dataloader (average loss per batch per epoch)     train_loss /= len(train_dataloader)          ### Testing     # Setup variables for accumulatively adding up loss and accuracy      test_loss, test_acc = 0, 0      model_0.eval()     with torch.inference_mode():         for X, y in test_dataloader:             # 1. Forward pass             test_pred = model_0(X)                         # 2. Calculate loss (accumatively)             test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch              # 3. Calculate accuracy (preds need to be same as y_true)             test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))                  # Calculations on test metrics need to happen inside torch.inference_mode()         # Divide total test loss by length of test dataloader (per batch)         test_loss /= len(test_dataloader)          # Divide total accuracy by length of test dataloader (per batch)         test_acc /= len(test_dataloader)      ## Print out what's happening     print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")  # Calculate training time       train_time_end_on_cpu = timer() total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,                                             end=train_time_end_on_cpu,                                            device=str(next(model_0.parameters()).device)) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n</pre> <pre> 33%|\u2588\u2588\u2588\u258e      | 1/3 [00:01&lt;00:03,  1.88s/it]</pre> <pre>\nTrain loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n\nEpoch: 1\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n</pre> <pre> 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:03&lt;00:01,  1.87s/it]</pre> <pre>\nTrain loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n\nEpoch: 2\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:05&lt;00:00,  1.87s/it]</pre> <pre>\nTrain loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n\nTrain time on cpu: 5.613 seconds\n</pre> <pre>\n</pre> <p>Nice! Looks like our baseline model did fairly well.</p> <p>It didn't take too long to train either, even just on the CPU, I wonder if it'll speed up on the GPU?</p> <p>Let's write some code to evaluate our model.</p> In\u00a0[20]: Copied! <pre>torch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn):\n    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Make predictions with the model\n            y_pred = model(X)\n            \n            # Accumulate the loss and accuracy values per batch\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, \n                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)\n        \n        # Scale loss and acc to find the average loss/acc per batch\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n        \n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 0 results on test dataset\nmodel_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n)\nmodel_0_results\n</pre> torch.manual_seed(42) def eval_model(model: torch.nn.Module,                 data_loader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 accuracy_fn):     \"\"\"Returns a dictionary containing the results of model predicting on data_loader.      Args:         model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.         data_loader (torch.utils.data.DataLoader): The target dataset to predict on.         loss_fn (torch.nn.Module): The loss function of model.         accuracy_fn: An accuracy function to compare the models predictions to the truth labels.      Returns:         (dict): Results of model making predictions on data_loader.     \"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in data_loader:             # Make predictions with the model             y_pred = model(X)                          # Accumulate the loss and accuracy values per batch             loss += loss_fn(y_pred, y)             acc += accuracy_fn(y_true=y,                                  y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)                  # Scale loss and acc to find the average loss/acc per batch         loss /= len(data_loader)         acc /= len(data_loader)              return {\"model_name\": model.__class__.__name__, # only works when model was created with a class             \"model_loss\": loss.item(),             \"model_acc\": acc}  # Calculate model 0 results on test dataset model_0_results = eval_model(model=model_0, data_loader=test_dataloader,     loss_fn=loss_fn, accuracy_fn=accuracy_fn ) model_0_results Out[20]: <pre>{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.4766390025615692,\n 'model_acc': 83.42651757188499}</pre> <p>Looking good!</p> <p>We can use this dictionary to compare the baseline model results to other models later on.</p> <p>Beautiful!</p> <p>Let's build another model.</p> In\u00a0[21]: Copied! <pre># Setup device agnostic code\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device agnostic code import torch device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[21]: <pre>'cuda'</pre> In\u00a0[22]: Copied! <pre># Create a model with non-linear and linear layers\nclass FashionMNISTModelV1(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # flatten inputs into single vector\n            nn.Linear(in_features=input_shape, out_features=hidden_units),\n            nn.ReLU(),\n            nn.Linear(in_features=hidden_units, out_features=output_shape),\n            nn.ReLU()\n        )\n    \n    def forward(self, x: torch.Tensor):\n        return self.layer_stack(x)\n</pre> # Create a model with non-linear and linear layers class FashionMNISTModelV1(nn.Module):     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.layer_stack = nn.Sequential(             nn.Flatten(), # flatten inputs into single vector             nn.Linear(in_features=input_shape, out_features=hidden_units),             nn.ReLU(),             nn.Linear(in_features=hidden_units, out_features=output_shape),             nn.ReLU()         )          def forward(self, x: torch.Tensor):         return self.layer_stack(x) <p>That looks good.</p> <p>Now let's instantiate it with the same settings we used before.</p> <p>We'll need <code>input_shape=784</code> (equal to the number of features of our image data), <code>hidden_units=10</code> (starting small and the same as our baseline model) and <code>output_shape=len(class_names)</code> (one output unit per class).</p> <p>Note: Notice how we kept most of the settings of our model the same except for one change: adding non-linear layers. This is a standard practice for running a series of machine learning experiments, change one thing and see what happens, then do it again, again, again.</p> In\u00a0[23]: Copied! <pre>torch.manual_seed(42)\nmodel_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n    hidden_units=10,\n    output_shape=len(class_names) # number of output classes desired\n).to(device) # send model to GPU if it's available\nnext(model_1.parameters()).device # check model device\n</pre> torch.manual_seed(42) model_1 = FashionMNISTModelV1(input_shape=784, # number of input features     hidden_units=10,     output_shape=len(class_names) # number of output classes desired ).to(device) # send model to GPU if it's available next(model_1.parameters()).device # check model device Out[23]: <pre>device(type='cuda', index=0)</pre> In\u00a0[24]: Copied! <pre>from helper_functions import accuracy_fn\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_1.parameters(), \n                            lr=0.1)\n</pre> from helper_functions import accuracy_fn loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_1.parameters(),                              lr=0.1) In\u00a0[25]: Copied! <pre>def train_step(model: torch.nn.Module,\n               data_loader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               accuracy_fn,\n               device: torch.device = device):\n    train_loss, train_acc = 0, 0\n    model.to(device)\n    for batch, (X, y) in enumerate(data_loader):\n        # Send data to GPU\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n        train_acc += accuracy_fn(y_true=y,\n                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels\n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n    # Calculate loss and accuracy per epoch and print out what's happening\n    train_loss /= len(data_loader)\n    train_acc /= len(data_loader)\n    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n\ndef test_step(data_loader: torch.utils.data.DataLoader,\n              model: torch.nn.Module,\n              loss_fn: torch.nn.Module,\n              accuracy_fn,\n              device: torch.device = device):\n    test_loss, test_acc = 0, 0\n    model.to(device)\n    model.eval() # put model in eval mode\n    # Turn on inference context manager\n    with torch.inference_mode(): \n        for X, y in data_loader:\n            # Send data to GPU\n            X, y = X.to(device), y.to(device)\n            \n            # 1. Forward pass\n            test_pred = model(X)\n            \n            # 2. Calculate loss and accuracy\n            test_loss += loss_fn(test_pred, y)\n            test_acc += accuracy_fn(y_true=y,\n                y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels\n            )\n        \n        # Adjust metrics and print out\n        test_loss /= len(data_loader)\n        test_acc /= len(data_loader)\n        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n</pre> def train_step(model: torch.nn.Module,                data_loader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module,                optimizer: torch.optim.Optimizer,                accuracy_fn,                device: torch.device = device):     train_loss, train_acc = 0, 0     model.to(device)     for batch, (X, y) in enumerate(data_loader):         # Send data to GPU         X, y = X.to(device), y.to(device)          # 1. Forward pass         y_pred = model(X)          # 2. Calculate loss         loss = loss_fn(y_pred, y)         train_loss += loss         train_acc += accuracy_fn(y_true=y,                                  y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels          # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()      # Calculate loss and accuracy per epoch and print out what's happening     train_loss /= len(data_loader)     train_acc /= len(data_loader)     print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")  def test_step(data_loader: torch.utils.data.DataLoader,               model: torch.nn.Module,               loss_fn: torch.nn.Module,               accuracy_fn,               device: torch.device = device):     test_loss, test_acc = 0, 0     model.to(device)     model.eval() # put model in eval mode     # Turn on inference context manager     with torch.inference_mode():          for X, y in data_loader:             # Send data to GPU             X, y = X.to(device), y.to(device)                          # 1. Forward pass             test_pred = model(X)                          # 2. Calculate loss and accuracy             test_loss += loss_fn(test_pred, y)             test_acc += accuracy_fn(y_true=y,                 y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels             )                  # Adjust metrics and print out         test_loss /= len(data_loader)         test_acc /= len(data_loader)         print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\") <p>Woohoo!</p> <p>Now we've got some functions for training and testing our model, let's run them.</p> <p>We'll do so inside another loop for each epoch.</p> <p>That way for each epoch we're going a training and a testing step.</p> <p>Note: You can customize how often you do a testing step. Sometimes people do them every five epochs or 10 epochs or in our case, every epoch.</p> <p>Let's also time things to see how long our code takes to run on the GPU.</p> In\u00a0[26]: Copied! <pre>torch.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start_on_gpu = timer()\n\nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_1, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_1,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn\n    )\n\ntrain_time_end_on_gpu = timer()\ntotal_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n                                            end=train_time_end_on_gpu,\n                                            device=device)\n</pre> torch.manual_seed(42)  # Measure time from timeit import default_timer as timer train_time_start_on_gpu = timer()  epochs = 3 for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(data_loader=train_dataloader,          model=model_1,          loss_fn=loss_fn,         optimizer=optimizer,         accuracy_fn=accuracy_fn     )     test_step(data_loader=test_dataloader,         model=model_1,         loss_fn=loss_fn,         accuracy_fn=accuracy_fn     )  train_time_end_on_gpu = timer() total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,                                             end=train_time_end_on_gpu,                                             device=device) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n---------\nTrain loss: 1.09199 | Train accuracy: 61.34%\n</pre> <pre> 33%|\u2588\u2588\u2588\u258e      | 1/3 [00:02&lt;00:04,  2.07s/it]</pre> <pre>Test loss: 0.95636 | Test accuracy: 65.00%\n\nEpoch: 1\n---------\nTrain loss: 0.78101 | Train accuracy: 71.93%\n</pre> <pre> 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:04&lt;00:02,  2.05s/it]</pre> <pre>Test loss: 0.72227 | Test accuracy: 73.91%\n\nEpoch: 2\n---------\nTrain loss: 0.67027 | Train accuracy: 75.94%\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:06&lt;00:00,  2.02s/it]</pre> <pre>Test loss: 0.68500 | Test accuracy: 75.02%\n\nTrain time on cuda: 6.060 seconds\n</pre> <pre>\n</pre> <p>Excellent!</p> <p>Our model trained but the training time took longer?</p> <p>Note: The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you're using. Read on for a more explained answer.</p> <p>Question: \"I used a a GPU but my model didn't train faster, why might that be?\"</p> <p>Answer: Well, one reason could be because your dataset and model are both so small (like the dataset and model we're working with) the benefits of using a GPU are outweighed by the time it actually takes to transfer the data there.</p> <p>There's a small bottleneck between copying data from the CPU memory (default) to the GPU memory.</p> <p>So for smaller models and datasets, the CPU might actually be the optimal place to compute on.</p> <p>But for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.</p> <p>However, this is largely dependant on the hardware you're using. With practice, you will get used to where the best place to train your models is.</p> <p>Let's evaluate our trained <code>model_1</code> using our <code>eval_model()</code> function and see how it went.</p> In\u00a0[27]: Copied! <pre>torch.manual_seed(42)\n\n# Note: This will error due to `eval_model()` not using device agnostic code \nmodel_1_results = eval_model(model=model_1, \n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn) \nmodel_1_results \n</pre> torch.manual_seed(42)  # Note: This will error due to `eval_model()` not using device agnostic code  model_1_results = eval_model(model=model_1,      data_loader=test_dataloader,     loss_fn=loss_fn,      accuracy_fn=accuracy_fn)  model_1_results  <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[27], line 4\n      1 torch.manual_seed(42)\n      3 # Note: This will error due to `eval_model()` not using device agnostic code \n----&gt; 4 model_1_results = eval_model(model=model_1, \n      5     data_loader=test_dataloader,\n      6     loss_fn=loss_fn, \n      7     accuracy_fn=accuracy_fn) \n      8 model_1_results \n\nCell In[20], line 22, in eval_model(model, data_loader, loss_fn, accuracy_fn)\n     19 with torch.inference_mode():\n     20     for X, y in data_loader:\n     21         # Make predictions with the model\n---&gt; 22         y_pred = model(X)\n     24         # Accumulate the loss and accuracy values per batch\n     25         loss += loss_fn(y_pred, y)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nCell In[22], line 14, in FashionMNISTModelV1.forward(self, x)\n     13 def forward(self, x: torch.Tensor):\n---&gt; 14     return self.layer_stack(x)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/container.py:217, in Sequential.forward(self, input)\n    215 def forward(self, input):\n    216     for module in self:\n--&gt; 217         input = module(input)\n    218     return input\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/linear.py:116, in Linear.forward(self, input)\n    115 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 116     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)</pre> <p>Oh no!</p> <p>It looks like our <code>eval_model()</code> function errors out with:</p> <p><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</code></p> <p>It's because we've setup our data and model to use device-agnostic code but not our evaluation function.</p> <p>How about we fix that by passing a target <code>device</code> parameter to our <code>eval_model()</code> function?</p> <p>Then we'll try calculating the results again.</p> In\u00a0[28]: Copied! <pre># Move values to device\ntorch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn, \n               device: torch.device = device):\n    \"\"\"Evaluates a given model on a given dataset.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n        device (str, optional): Target device to compute on. Defaults to device.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Send data to the target device\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n        \n        # Scale loss and acc\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 1 results with device-agnostic code \nmodel_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n    device=device\n)\nmodel_1_results\n</pre> # Move values to device torch.manual_seed(42) def eval_model(model: torch.nn.Module,                 data_loader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 accuracy_fn,                 device: torch.device = device):     \"\"\"Evaluates a given model on a given dataset.      Args:         model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.         data_loader (torch.utils.data.DataLoader): The target dataset to predict on.         loss_fn (torch.nn.Module): The loss function of model.         accuracy_fn: An accuracy function to compare the models predictions to the truth labels.         device (str, optional): Target device to compute on. Defaults to device.      Returns:         (dict): Results of model making predictions on data_loader.     \"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in data_loader:             # Send data to the target device             X, y = X.to(device), y.to(device)             y_pred = model(X)             loss += loss_fn(y_pred, y)             acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))                  # Scale loss and acc         loss /= len(data_loader)         acc /= len(data_loader)     return {\"model_name\": model.__class__.__name__, # only works when model was created with a class             \"model_loss\": loss.item(),             \"model_acc\": acc}  # Calculate model 1 results with device-agnostic code  model_1_results = eval_model(model=model_1, data_loader=test_dataloader,     loss_fn=loss_fn, accuracy_fn=accuracy_fn,     device=device ) model_1_results Out[28]: <pre>{'model_name': 'FashionMNISTModelV1',\n 'model_loss': 0.6850008368492126,\n 'model_acc': 75.01996805111821}</pre> In\u00a0[29]: Copied! <pre># Check baseline results\nmodel_0_results\n</pre> # Check baseline results model_0_results Out[29]: <pre>{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.4766390025615692,\n 'model_acc': 83.42651757188499}</pre> <p>Woah, in this case, it looks like adding non-linearities to our model made it perform worse than the baseline.</p> <p>That's a thing to note in machine learning, sometimes the thing you thought should work doesn't.</p> <p>And then the thing you thought might not work does.</p> <p>It's part science, part art.</p> <p>From the looks of things, it seems like our model is overfitting on the training data.</p> <p>Overfitting means our model is learning the training data well but those patterns aren't generalizing to the testing data.</p> <p>Two of the main to fix overfitting include:</p> <ol> <li>Using a smaller or different model (some models fit certain kinds of data better than others).</li> <li>Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns).</li> </ol> <p>There are more, but I'm going to leave that as a challenge for you to explore.</p> <p>Try searching online, \"ways to prevent overfitting in machine learning\" and see what comes up.</p> <p>In the meantime, let's take a look at number 1: using a different model.</p> In\u00a0[30]: Copied! <pre># Create a convolutional neural network \nclass FashionMNISTModelV2(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*7*7, \n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.block_1(x)\n        # print(x.shape)\n        x = self.block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n\ntorch.manual_seed(42)\nmodel_2 = FashionMNISTModelV2(input_shape=1, \n    hidden_units=10, \n    output_shape=len(class_names)).to(device)\nmodel_2\n</pre> # Create a convolutional neural network  class FashionMNISTModelV2(nn.Module):     \"\"\"     Model architecture copying TinyVGG from:      https://poloclub.github.io/cnn-explainer/     \"\"\"     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.block_1 = nn.Sequential(             nn.Conv2d(in_channels=input_shape,                        out_channels=hidden_units,                        kernel_size=3, # how big is the square that's going over the image?                       stride=1, # default                       padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number              nn.ReLU(),             nn.Conv2d(in_channels=hidden_units,                        out_channels=hidden_units,                       kernel_size=3,                       stride=1,                       padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2,                          stride=2) # default stride value is same as kernel_size         )         self.block_2 = nn.Sequential(             nn.Conv2d(hidden_units, hidden_units, 3, padding=1),             nn.ReLU(),             nn.Conv2d(hidden_units, hidden_units, 3, padding=1),             nn.ReLU(),             nn.MaxPool2d(2)         )         self.classifier = nn.Sequential(             nn.Flatten(),             # Where did this in_features shape come from?              # It's because each layer of our network compresses and changes the shape of our inputs data.             nn.Linear(in_features=hidden_units*7*7,                        out_features=output_shape)         )          def forward(self, x: torch.Tensor):         x = self.block_1(x)         # print(x.shape)         x = self.block_2(x)         # print(x.shape)         x = self.classifier(x)         # print(x.shape)         return x  torch.manual_seed(42) model_2 = FashionMNISTModelV2(input_shape=1,      hidden_units=10,      output_shape=len(class_names)).to(device) model_2 Out[30]: <pre>FashionMNISTModelV2(\n  (block_1): Sequential(\n    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=490, out_features=10, bias=True)\n  )\n)</pre> <p>Nice!</p> <p>Our biggest model yet!</p> <p>What we've done is a common practice in machine learning.</p> <p>Find a model architecture somewhere and replicate it with code.</p> In\u00a0[31]: Copied! <pre>torch.manual_seed(42)\n\n# Create sample batch of random numbers with same size as image batch\nimages = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\ntest_image = images[0] # get a single image for testing\nprint(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\") \nprint(f\"Single image pixel values:\\n{test_image}\")\n</pre> torch.manual_seed(42)  # Create sample batch of random numbers with same size as image batch images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width] test_image = images[0] # get a single image for testing print(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\")  print(f\"Single image pixel values:\\n{test_image}\") <pre>Image batch shape: torch.Size([32, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nSingle image shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nSingle image pixel values:\ntensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n         ...,\n         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n\n        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n         ...,\n         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n\n        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n         ...,\n         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n</pre> <p>Let's create an example <code>nn.Conv2d()</code> with various parameters:</p> <ul> <li><code>in_channels</code> (int) - Number of channels in the input image.</li> <li><code>out_channels</code> (int) - Number of channels produced by the convolution.</li> <li><code>kernel_size</code> (int or tuple) - Size of the convolving kernel/filter.</li> <li><code>stride</code> (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1.</li> <li><code>padding</code> (int, tuple, str) - Padding added to all four sides of input. Default: 0.</li> </ul> <p></p> <p>Example of what happens when you change the hyperparameters of a <code>nn.Conv2d()</code> layer.</p> In\u00a0[32]: Copied! <pre>torch.manual_seed(42)\n\n# Create a convolutional layer with same dimensions as TinyVGG \n# (try changing any of the parameters and see what happens)\nconv_layer = nn.Conv2d(in_channels=3,\n                       out_channels=10,\n                       kernel_size=3,\n                       stride=1,\n                       padding=0) # also try using \"valid\" or \"same\" here \n\n# Pass the data through the convolutional layer\nconv_layer(test_image) # Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input) \n</pre> torch.manual_seed(42)  # Create a convolutional layer with same dimensions as TinyVGG  # (try changing any of the parameters and see what happens) conv_layer = nn.Conv2d(in_channels=3,                        out_channels=10,                        kernel_size=3,                        stride=1,                        padding=0) # also try using \"valid\" or \"same\" here   # Pass the data through the convolutional layer conv_layer(test_image) # Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)  Out[32]: <pre>tensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],\n         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],\n         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],\n         ...,\n         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],\n         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],\n         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],\n\n        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],\n         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],\n         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],\n         ...,\n         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],\n         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],\n         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],\n\n        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],\n         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],\n         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],\n         ...,\n         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],\n         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],\n         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],\n\n        ...,\n\n        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],\n         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],\n         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],\n         ...,\n         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],\n         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],\n         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],\n\n        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],\n         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],\n         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],\n         ...,\n         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],\n         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],\n         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],\n\n        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],\n         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],\n         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],\n         ...,\n         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],\n         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],\n         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],\n       grad_fn=&lt;SqueezeBackward1&gt;)</pre> <p>If we try to pass a single image in, we get a shape mismatch error:</p> <p><code>RuntimeError: Expected 4-dimensional input for 4-dimensional weight [10, 3, 3, 3], but got 3-dimensional input of size [3, 64, 64] instead</code></p> <p>Note: If you're running PyTorch 1.11.0+, this error won't occur.</p> <p>This is because our <code>nn.Conv2d()</code> layer expects a 4-dimensional tensor as input with size <code>(N, C, H, W)</code> or <code>[batch_size, color_channels, height, width]</code>.</p> <p>Right now our single image <code>test_image</code> only has a shape of <code>[color_channels, height, width]</code> or <code>[3, 64, 64]</code>.</p> <p>We can fix this for a single image using <code>test_image.unsqueeze(dim=0)</code> to add an extra dimension for <code>N</code>.</p> In\u00a0[33]: Copied! <pre># Add extra dimension to test image\ntest_image.unsqueeze(dim=0).shape\n</pre> # Add extra dimension to test image test_image.unsqueeze(dim=0).shape Out[33]: <pre>torch.Size([1, 3, 64, 64])</pre> In\u00a0[34]: Copied! <pre># Pass test image with extra dimension through conv_layer\nconv_layer(test_image.unsqueeze(dim=0)).shape\n</pre> # Pass test image with extra dimension through conv_layer conv_layer(test_image.unsqueeze(dim=0)).shape Out[34]: <pre>torch.Size([1, 10, 62, 62])</pre> <p>Hmm, notice what happens to our shape (the same shape as the first layer of TinyVGG on CNN Explainer), we get different channel sizes as well as different pixel sizes.</p> <p>What if we changed the values of <code>conv_layer</code>?</p> In\u00a0[35]: Copied! <pre>torch.manual_seed(42)\n# Create a new conv_layer with different values (try setting these to whatever you like)\nconv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n                         out_channels=10,\n                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n                         stride=2,\n                         padding=0)\n\n# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\nconv_layer_2(test_image.unsqueeze(dim=0)).shape\n</pre> torch.manual_seed(42) # Create a new conv_layer with different values (try setting these to whatever you like) conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image                          out_channels=10,                          kernel_size=(5, 5), # kernel is usually a square so a tuple also works                          stride=2,                          padding=0)  # Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input) conv_layer_2(test_image.unsqueeze(dim=0)).shape Out[35]: <pre>torch.Size([1, 10, 30, 30])</pre> <p>Woah, we get another shape change.</p> <p>Now our image is of shape <code>[1, 10, 30, 30]</code> (it will be different if you use different values) or <code>[batch_size=1, color_channels=10, height=30, width=30]</code>.</p> <p>What's going on here?</p> <p>Behind the scenes, our <code>nn.Conv2d()</code> is compressing the information stored in the image.</p> <p>It does this by performing operations on the input (our test image) against its internal parameters.</p> <p>The goal of this is similar to all of the other neural networks we've been building.</p> <p>Data goes in and the layers try to update their internal parameters (patterns) to lower the loss function thanks to some help of the optimizer.</p> <p>The only difference is how the different layers calculate their parameter updates or in PyTorch terms, the operation present in the layer <code>forward()</code> method.</p> <p>If we check out our <code>conv_layer_2.state_dict()</code> we'll find a similar weight and bias setup as we've seen before.</p> In\u00a0[36]: Copied! <pre># Check out the conv_layer_2 internal parameters\nprint(conv_layer_2.state_dict())\n</pre> # Check out the conv_layer_2 internal parameters print(conv_layer_2.state_dict()) <pre>OrderedDict([('weight', tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],\n          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],\n          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],\n          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],\n          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],\n\n         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],\n          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],\n          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],\n          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],\n          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],\n\n         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],\n          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],\n          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],\n          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],\n          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],\n\n\n        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],\n          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],\n          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],\n          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],\n          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],\n\n         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],\n          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],\n          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],\n          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],\n          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],\n\n         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],\n          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],\n          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],\n          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],\n          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],\n\n\n        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],\n          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],\n          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],\n          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],\n          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],\n\n         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],\n          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],\n          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],\n          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],\n          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],\n\n         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],\n          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],\n          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],\n          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],\n          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],\n\n\n        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],\n          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],\n          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],\n          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],\n          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],\n\n         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],\n          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],\n          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],\n          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],\n          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],\n\n         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],\n          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],\n          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],\n          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],\n          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],\n\n\n        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],\n          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],\n          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],\n          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],\n          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],\n\n         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],\n          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],\n          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],\n          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],\n          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],\n\n         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],\n          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],\n          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],\n          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],\n          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],\n\n\n        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],\n          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],\n          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],\n          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],\n          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],\n\n         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],\n          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],\n          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],\n          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],\n          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],\n\n         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],\n          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],\n          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],\n          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],\n          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],\n\n\n        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],\n          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],\n          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],\n          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],\n          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],\n\n         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],\n          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],\n          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],\n          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],\n          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],\n\n         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],\n          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],\n          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],\n          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],\n          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],\n\n\n        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],\n          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],\n          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],\n          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],\n          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],\n\n         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],\n          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],\n          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],\n          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],\n          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],\n\n         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],\n          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],\n          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],\n          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],\n          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],\n\n\n        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],\n          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],\n          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],\n          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],\n          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],\n\n         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],\n          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],\n          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],\n          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],\n          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],\n\n         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],\n          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],\n          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],\n          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],\n          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],\n\n\n        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],\n          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],\n          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],\n          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],\n          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],\n\n         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],\n          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],\n          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],\n          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],\n          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],\n\n         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],\n          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],\n          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],\n          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],\n          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), ('bias', tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,\n        -0.0797,  0.1121]))])\n</pre> <p>Look at that! A bunch of random numbers for a weight and bias tensor.</p> <p>The shapes of these are manipulated by the inputs we passed to <code>nn.Conv2d()</code> when we set it up.</p> <p>Let's check them out.</p> In\u00a0[37]: Copied! <pre># Get shapes of weight and bias tensors within conv_layer_2\nprint(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\nprint(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\")\n</pre> # Get shapes of weight and bias tensors within conv_layer_2 print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\") print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\") <pre>conv_layer_2 weight shape: \ntorch.Size([10, 3, 5, 5]) -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\n\nconv_layer_2 bias shape: \ntorch.Size([10]) -&gt; [out_channels=10]\n</pre> <p>Question: What should we set the parameters of our <code>nn.Conv2d()</code> layers?</p> <p>That's a good one. But similar to many other things in machine learning, the values of these aren't set in stone (and recall, because these values are ones we can set ourselves, they're referred to as \"hyperparameters\").</p> <p>The best way to find out is to try out different values and see how they effect your model's performance.</p> <p>Or even better, find a working example on a problem similar to yours (like we've done with TinyVGG) and copy it.</p> <p>We're working with a different of layer here to what we've seen before.</p> <p>But the premise remains the same: start with random numbers and update them to better represent the data.</p> In\u00a0[38]: Copied! <pre># Print out original image shape without and with unsqueezed dimension\nprint(f\"Test image original shape: {test_image.shape}\")\nprint(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n\n# Create a sample nn.MaxPoo2d() layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2)\n\n# Pass data through just the conv_layer\ntest_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\nprint(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n\n# Pass data through the max pool layer\ntest_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\nprint(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")\n</pre> # Print out original image shape without and with unsqueezed dimension print(f\"Test image original shape: {test_image.shape}\") print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")  # Create a sample nn.MaxPoo2d() layer max_pool_layer = nn.MaxPool2d(kernel_size=2)  # Pass data through just the conv_layer test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0)) print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")  # Pass data through the max pool layer test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv) print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\") <pre>Test image original shape: torch.Size([3, 64, 64])\nTest image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\nShape after going through conv_layer(): torch.Size([1, 10, 62, 62])\nShape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])\n</pre> <p>Notice the change in the shapes of what's happening in and out of a <code>nn.MaxPool2d()</code> layer.</p> <p>The <code>kernel_size</code> of the <code>nn.MaxPool2d()</code> layer will effects the size of the output shape.</p> <p>In our case, the shape halves from a <code>62x62</code> image to <code>31x31</code> image.</p> <p>Let's see this work with a smaller tensor.</p> In\u00a0[39]: Copied! <pre>torch.manual_seed(42)\n# Create a random tensor with a similiar number of dimensions to our images\nrandom_tensor = torch.randn(size=(1, 1, 2, 2))\nprint(f\"Random tensor:\\n{random_tensor}\")\nprint(f\"Random tensor shape: {random_tensor.shape}\")\n\n# Create a max pool layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n\n# Pass the random tensor through the max pool layer\nmax_pool_tensor = max_pool_layer(random_tensor)\nprint(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\")\nprint(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n</pre> torch.manual_seed(42) # Create a random tensor with a similiar number of dimensions to our images random_tensor = torch.randn(size=(1, 1, 2, 2)) print(f\"Random tensor:\\n{random_tensor}\") print(f\"Random tensor shape: {random_tensor.shape}\")  # Create a max pool layer max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value   # Pass the random tensor through the max pool layer max_pool_tensor = max_pool_layer(random_tensor) print(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\") print(f\"Max pool tensor shape: {max_pool_tensor.shape}\") <pre>Random tensor:\ntensor([[[[0.3367, 0.1288],\n          [0.2345, 0.2303]]]])\nRandom tensor shape: torch.Size([1, 1, 2, 2])\n\nMax pool tensor:\ntensor([[[[0.3367]]]]) &lt;- this is the maximum value from random_tensor\nMax pool tensor shape: torch.Size([1, 1, 1, 1])\n</pre> <p>Notice the final two dimensions between <code>random_tensor</code> and <code>max_pool_tensor</code>, they go from <code>[2, 2]</code> to <code>[1, 1]</code>.</p> <p>In essence, they get halved.</p> <p>And the change would be different for different values of <code>kernel_size</code> for <code>nn.MaxPool2d()</code>.</p> <p>Also notice the value leftover in <code>max_pool_tensor</code> is the maximum value from <code>random_tensor</code>.</p> <p>What's happening here?</p> <p>This is another important piece of the puzzle of neural networks.</p> <p>Essentially, every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space.</p> <p>In other words, take a lot of numbers (raw data) and learn patterns in those numbers, patterns that are predictive whilst also being smaller in size than the original values.</p> <p>From an artificial intelligence perspective, you could consider the whole goal of a neural network to compress information.</p> <p></p> <p>This means, that from the point of view of a neural network, intelligence is compression.</p> <p>This is the idea of the use of a <code>nn.MaxPool2d()</code> layer: take the maximum value from a portion of a tensor and disregard the rest.</p> <p>In essence, lowering the dimensionality of a tensor whilst still retaining a (hopefully) significant portion of the information.</p> <p>It is the same story for a <code>nn.Conv2d()</code> layer.</p> <p>Except instead of just taking the maximum, the <code>nn.Conv2d()</code> performs a convolutional operation on the data (see this in action on the CNN Explainer webpage).</p> <p>Exercise: What do you think the <code>nn.AvgPool2d()</code> layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values.</p> <p>Extra-curriculum: Lookup \"most common convolutional neural networks\", what architectures do you find? Are any of them contained within the <code>torchvision.models</code> library? What do you think you could do with these?</p> In\u00a0[40]: Copied! <pre># Setup loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_2.parameters(), \n                             lr=0.1)\n</pre> # Setup loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_2.parameters(),                               lr=0.1) In\u00a0[41]: Copied! <pre>torch.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start_model_2 = timer()\n\n# Train and test model \nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_2, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_2,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n\ntrain_time_end_model_2 = timer()\ntotal_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n                                           end=train_time_end_model_2,\n                                           device=device)\n</pre> torch.manual_seed(42)  # Measure time from timeit import default_timer as timer train_time_start_model_2 = timer()  # Train and test model  epochs = 3 for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(data_loader=train_dataloader,          model=model_2,          loss_fn=loss_fn,         optimizer=optimizer,         accuracy_fn=accuracy_fn,         device=device     )     test_step(data_loader=test_dataloader,         model=model_2,         loss_fn=loss_fn,         accuracy_fn=accuracy_fn,         device=device     )  train_time_end_model_2 = timer() total_train_time_model_2 = print_train_time(start=train_time_start_model_2,                                            end=train_time_end_model_2,                                            device=device) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n---------\nTrain loss: 0.59461 | Train accuracy: 78.45%\n</pre> <pre> 33%|\u2588\u2588\u2588\u258e      | 1/3 [00:02&lt;00:05,  2.86s/it]</pre> <pre>Test loss: 0.39558 | Test accuracy: 85.72%\n\nEpoch: 1\n---------\nTrain loss: 0.35893 | Train accuracy: 87.03%\n</pre> <pre> 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:05&lt;00:02,  2.69s/it]</pre> <pre>Test loss: 0.35471 | Test accuracy: 86.87%\n\nEpoch: 2\n---------\nTrain loss: 0.32232 | Train accuracy: 88.37%\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:08&lt;00:00,  2.69s/it]</pre> <pre>Test loss: 0.32805 | Test accuracy: 88.03%\n\nTrain time on cuda: 8.063 seconds\n</pre> <pre>\n</pre> <p>Woah! Looks like the convolutional and max pooling layers helped improve performance a little.</p> <p>Let's evaluate <code>model_2</code>'s results with our <code>eval_model()</code> function.</p> In\u00a0[42]: Copied! <pre># Get model_2 results \nmodel_2_results = eval_model(\n    model=model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn,\n    accuracy_fn=accuracy_fn\n)\nmodel_2_results\n</pre> # Get model_2 results  model_2_results = eval_model(     model=model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,     accuracy_fn=accuracy_fn ) model_2_results Out[42]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.328047513961792,\n 'model_acc': 88.02915335463258}</pre> In\u00a0[43]: Copied! <pre>import pandas as pd\ncompare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\ncompare_results\n</pre> import pandas as pd compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results]) compare_results Out[43]: model_name model_loss model_acc 0 FashionMNISTModelV0 0.476639 83.426518 1 FashionMNISTModelV1 0.685001 75.019968 2 FashionMNISTModelV2 0.328048 88.029153 <p>Nice!</p> <p>We can add the training time values too.</p> In\u00a0[44]: Copied! <pre># Add training times to results comparison\ncompare_results[\"training_time\"] = [total_train_time_model_0,\n                                    total_train_time_model_1,\n                                    total_train_time_model_2]\ncompare_results\n</pre> # Add training times to results comparison compare_results[\"training_time\"] = [total_train_time_model_0,                                     total_train_time_model_1,                                     total_train_time_model_2] compare_results Out[44]: model_name model_loss model_acc training_time 0 FashionMNISTModelV0 0.476639 83.426518 5.613171 1 FashionMNISTModelV1 0.685001 75.019968 6.060330 2 FashionMNISTModelV2 0.328048 88.029153 8.062894 <p>It looks like our CNN (<code>FashionMNISTModelV2</code>) model performed the best (lowest loss, highest accuracy) but had the longest training time.</p> <p>And our baseline model (<code>FashionMNISTModelV0</code>) performed better than <code>model_1</code> (<code>FashionMNISTModelV1</code>).</p> In\u00a0[45]: Copied! <pre># Visualize our model results\ncompare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\nplt.xlabel(\"accuracy (%)\")\nplt.ylabel(\"model\");\n</pre> # Visualize our model results compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\") plt.xlabel(\"accuracy (%)\") plt.ylabel(\"model\"); In\u00a0[46]: Copied! <pre>def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n    pred_probs = []\n    model.eval()\n    with torch.inference_mode():\n        for sample in data:\n            # Prepare sample\n            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n\n            # Forward pass (model outputs raw logit)\n            pred_logit = model(sample)\n\n            # Get prediction probability (logit -&gt; prediction probability)\n            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)\n\n            # Get pred_prob off GPU for further calculations\n            pred_probs.append(pred_prob.cpu())\n            \n    # Stack the pred_probs to turn list into a tensor\n    return torch.stack(pred_probs)\n</pre> def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):     pred_probs = []     model.eval()     with torch.inference_mode():         for sample in data:             # Prepare sample             sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device              # Forward pass (model outputs raw logit)             pred_logit = model(sample)              # Get prediction probability (logit -&gt; prediction probability)             pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)              # Get pred_prob off GPU for further calculations             pred_probs.append(pred_prob.cpu())                  # Stack the pred_probs to turn list into a tensor     return torch.stack(pred_probs) In\u00a0[47]: Copied! <pre>import random\nrandom.seed(42)\ntest_samples = []\ntest_labels = []\nfor sample, label in random.sample(list(test_data), k=9):\n    test_samples.append(sample)\n    test_labels.append(label)\n\n# View the first test sample shape and label\nprint(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")\n</pre> import random random.seed(42) test_samples = [] test_labels = [] for sample, label in random.sample(list(test_data), k=9):     test_samples.append(sample)     test_labels.append(label)  # View the first test sample shape and label print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\") <pre>Test sample image shape: torch.Size([1, 28, 28])\nTest sample label: 5 (Sandal)\n</pre> In\u00a0[48]: Copied! <pre># Make predictions on test samples with model 2\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# View first two prediction probabilities list\npred_probs[:2]\n</pre> # Make predictions on test samples with model 2 pred_probs= make_predictions(model=model_2,                               data=test_samples)  # View first two prediction probabilities list pred_probs[:2] Out[48]: <pre>tensor([[2.1181e-08, 6.1428e-09, 1.3147e-08, 4.4508e-08, 4.4754e-09, 9.9998e-01,\n         3.1568e-08, 8.8105e-07, 4.2129e-06, 1.6747e-05],\n        [1.9718e-01, 2.6818e-01, 1.2401e-03, 1.3616e-01, 9.0187e-02, 6.7458e-04,\n         3.0547e-01, 4.5653e-04, 8.2568e-05, 3.7707e-04]])</pre> <p>And now we can use our <code>make_predictions()</code> function to predict on <code>test_samples</code>.</p> In\u00a0[49]: Copied! <pre># Make predictions on test samples with model 2\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# View first two prediction probabilities list\npred_probs[:2]\n</pre> # Make predictions on test samples with model 2 pred_probs= make_predictions(model=model_2,                               data=test_samples)  # View first two prediction probabilities list pred_probs[:2] Out[49]: <pre>tensor([[2.1181e-08, 6.1428e-09, 1.3147e-08, 4.4508e-08, 4.4754e-09, 9.9998e-01,\n         3.1568e-08, 8.8105e-07, 4.2129e-06, 1.6747e-05],\n        [1.9718e-01, 2.6818e-01, 1.2401e-03, 1.3616e-01, 9.0187e-02, 6.7458e-04,\n         3.0547e-01, 4.5653e-04, 8.2568e-05, 3.7707e-04]])</pre> <p>Excellent!</p> <p>And now we can go from prediction probabilities to prediction labels by taking the <code>torch.argmax()</code> of the output of the <code>torch.softmax()</code> activation function.</p> In\u00a0[50]: Copied! <pre># Turn the prediction probabilities into prediction labels by taking the argmax()\npred_classes = pred_probs.argmax(dim=1)\npred_classes\n</pre> # Turn the prediction probabilities into prediction labels by taking the argmax() pred_classes = pred_probs.argmax(dim=1) pred_classes Out[50]: <pre>tensor([5, 6, 7, 4, 3, 0, 4, 7, 1])</pre> In\u00a0[51]: Copied! <pre># Are our predictions in the same form as our test labels? \ntest_labels, pred_classes\n</pre> # Are our predictions in the same form as our test labels?  test_labels, pred_classes Out[51]: <pre>([5, 1, 7, 4, 3, 0, 4, 7, 1], tensor([5, 6, 7, 4, 3, 0, 4, 7, 1]))</pre> <p>Now our predicted classes are in the same format as our test labels, we can compare.</p> <p>Since we're dealing with image data, let's stay true to the data explorer's motto.</p> <p>\"Visualize, visualize, visualize!\"</p> In\u00a0[52]: Copied! <pre># Plot predictions\nplt.figure(figsize=(9, 9))\nnrows = 3\nncols = 3\nfor i, sample in enumerate(test_samples):\n  # Create a subplot\n  plt.subplot(nrows, ncols, i+1)\n\n  # Plot the target image\n  plt.imshow(sample.squeeze(), cmap=\"gray\")\n\n  # Find the prediction label (in text form, e.g. \"Sandal\")\n  pred_label = class_names[pred_classes[i]]\n\n  # Get the truth label (in text form, e.g. \"T-shirt\")\n  truth_label = class_names[test_labels[i]] \n\n  # Create the title text of the plot\n  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n  \n  # Check for equality and change title colour accordingly\n  if pred_label == truth_label:\n      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n  else:\n      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n  plt.axis(False);\n</pre> # Plot predictions plt.figure(figsize=(9, 9)) nrows = 3 ncols = 3 for i, sample in enumerate(test_samples):   # Create a subplot   plt.subplot(nrows, ncols, i+1)    # Plot the target image   plt.imshow(sample.squeeze(), cmap=\"gray\")    # Find the prediction label (in text form, e.g. \"Sandal\")   pred_label = class_names[pred_classes[i]]    # Get the truth label (in text form, e.g. \"T-shirt\")   truth_label = class_names[test_labels[i]]     # Create the title text of the plot   title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"      # Check for equality and change title colour accordingly   if pred_label == truth_label:       plt.title(title_text, fontsize=10, c=\"g\") # green text if correct   else:       plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong   plt.axis(False); <p>Well, well, well, doesn't that look good!</p> <p>Not bad for a couple dozen lines of PyTorch code!</p> In\u00a0[53]: Copied! <pre># Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# 1. Make predictions with trained model\ny_preds = []\nmodel_2.eval()\nwith torch.inference_mode():\n  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n    # Send data and targets to target device\n    X, y = X.to(device), y.to(device)\n    # Do the forward pass\n    y_logit = model_2(X)\n    # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels\n    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n    # Put predictions on CPU for evaluation\n    y_preds.append(y_pred.cpu())\n# Concatenate list of predictions into a tensor\ny_pred_tensor = torch.cat(y_preds)\n</pre> # Import tqdm for progress bar from tqdm.auto import tqdm  # 1. Make predictions with trained model y_preds = [] model_2.eval() with torch.inference_mode():   for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):     # Send data and targets to target device     X, y = X.to(device), y.to(device)     # Do the forward pass     y_logit = model_2(X)     # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels     y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)     # Put predictions on CPU for evaluation     y_preds.append(y_pred.cpu()) # Concatenate list of predictions into a tensor y_pred_tensor = torch.cat(y_preds) <pre>Making predictions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 313/313 [00:00&lt;00:00, 1372.88it/s]\n</pre> <p>Wonderful!</p> <p>Now we've got predictions, let's go through steps 2 &amp; 3: 2. Make a confusion matrix using <code>torchmetrics.ConfusionMatrix</code>. 3. Plot the confusion matrix using <code>mlxtend.plotting.plot_confusion_matrix()</code>.</p> <p>First we'll need to make sure we've got <code>torchmetrics</code> and <code>mlxtend</code> installed (these two libraries will help us make and visual a confusion matrix).</p> <p>Note: If you're using Google Colab, the default version of <code>mlxtend</code> installed is 0.14.0 (as of March 2022), however, for the parameters of the <code>plot_confusion_matrix()</code> function we'd like use, we need 0.19.0 or higher.</p> In\u00a0[54]: Copied! <pre># See if torchmetrics exists, if not, install it\ntry:\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n    assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\"\nexcept:\n    !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you're using Google Colab, this may require restarting the runtime\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n</pre> # See if torchmetrics exists, if not, install it try:     import torchmetrics, mlxtend     print(f\"mlxtend version: {mlxtend.__version__}\")     assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\" except:     !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you're using Google Colab, this may require restarting the runtime     import torchmetrics, mlxtend     print(f\"mlxtend version: {mlxtend.__version__}\") <pre>mlxtend version: 0.23.1\n</pre> <p>To plot the confusion matrix, we need to make sure we've got and <code>mlxtend</code> version of 0.19.0 or higher.</p> In\u00a0[55]: Copied! <pre># Import mlxtend upgraded version\nimport mlxtend \nprint(mlxtend.__version__)\nassert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher\n</pre> # Import mlxtend upgraded version import mlxtend  print(mlxtend.__version__) assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher <pre>0.23.1\n</pre> <p><code>torchmetrics</code> and <code>mlxtend</code> installed, let's make a confusion matrix!</p> <p>First we'll create a <code>torchmetrics.ConfusionMatrix</code> instance telling it how many classes we're dealing with by setting <code>num_classes=len(class_names)</code>.</p> <p>Then we'll create a confusion matrix (in tensor format) by passing our instance our model's predictions (<code>preds=y_pred_tensor</code>) and targets (<code>target=test_data.targets</code>).</p> <p>Finally we can plot our confision matrix using the <code>plot_confusion_matrix()</code> function from <code>mlxtend.plotting</code>.</p> In\u00a0[56]: Copied! <pre>from torchmetrics import ConfusionMatrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\n# 2. Setup confusion matrix instance and compare predictions to targets\nconfmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\nconfmat_tensor = confmat(preds=y_pred_tensor,\n                         target=test_data.targets)\n\n# 3. Plot the confusion matrix\nfig, ax = plot_confusion_matrix(\n    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n    class_names=class_names, # turn the row and column labels into class names\n    figsize=(10, 7)\n);\n</pre> from torchmetrics import ConfusionMatrix from mlxtend.plotting import plot_confusion_matrix  # 2. Setup confusion matrix instance and compare predictions to targets confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass') confmat_tensor = confmat(preds=y_pred_tensor,                          target=test_data.targets)  # 3. Plot the confusion matrix fig, ax = plot_confusion_matrix(     conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy      class_names=class_names, # turn the row and column labels into class names     figsize=(10, 7) ); <p>Woah! Doesn't that look good?</p> <p>We can see our model does fairly well since most of the dark squares are down the diagonal from top left to bottom right (and ideal model will have only values in these squares and 0 everywhere else).</p> <p>The model gets most \"confused\" on classes that are similar, for example predicting \"Pullover\" for images that are actually labelled \"Shirt\".</p> <p>And the same for predicting \"Shirt\" for classes that are actually labelled \"T-shirt/top\".</p> <p>This kind of information is often more helpful than a single accuracy metric because it tells use where a model is getting things wrong.</p> <p>It also hints at why the model may be getting certain things wrong.</p> <p>It's understandable the model sometimes predicts \"Shirt\" for images labelled \"T-shirt/top\".</p> <p>We can use this kind of information to further inspect our models and data to see how it could be improved.</p> <p>Exercise: Use the trained <code>model_2</code> to make predictions on the test FashionMNIST dataset. Then plot some predictions where the model was wrong alongside what the label of the image should've been. After visualing these predictions do you think it's more of a modelling error or a data error? As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?</p> In\u00a0[57]: Copied! <pre>from pathlib import Path\n\n# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, # create parent directories if needed\n                 exist_ok=True # if models directory already exists, don't error\n)\n\n# Create model save path\nMODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# Save the model state dict\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, # create parent directories if needed                  exist_ok=True # if models directory already exists, don't error )  # Create model save path MODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # Save the model state dict print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters            f=MODEL_SAVE_PATH) <pre>Saving model to: models/03_pytorch_computer_vision_model_2.pth\n</pre> <p>Now we've got a saved model <code>state_dict()</code> we can load it back in using a combination of <code>load_state_dict()</code> and <code>torch.load()</code>.</p> <p>Since we're using <code>load_state_dict()</code>, we'll need to create a new instance of <code>FashionMNISTModelV2()</code> with the same input parameters as our saved model <code>state_dict()</code>.</p> In\u00a0[58]: Copied! <pre># Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())\n# Note: loading model will error if the shapes here aren't the same as the saved version\nloaded_model_2 = FashionMNISTModelV2(input_shape=1, \n                                    hidden_units=10, # try changing this to 128 and seeing what happens \n                                    output_shape=10) \n\n# Load in the saved state_dict()\nloaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n# Send model to GPU\nloaded_model_2 = loaded_model_2.to(device)\n</pre> # Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict()) # Note: loading model will error if the shapes here aren't the same as the saved version loaded_model_2 = FashionMNISTModelV2(input_shape=1,                                      hidden_units=10, # try changing this to 128 and seeing what happens                                      output_shape=10)   # Load in the saved state_dict() loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))  # Send model to GPU loaded_model_2 = loaded_model_2.to(device) <p>And now we've got a loaded model we can evaluate it with <code>eval_model()</code> to make sure its parameters work similarly to <code>model_2</code> prior to saving.</p> In\u00a0[59]: Copied! <pre># Evaluate loaded model\ntorch.manual_seed(42)\n\nloaded_model_2_results = eval_model(\n    model=loaded_model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn\n)\n\nloaded_model_2_results\n</pre> # Evaluate loaded model torch.manual_seed(42)  loaded_model_2_results = eval_model(     model=loaded_model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,      accuracy_fn=accuracy_fn )  loaded_model_2_results Out[59]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.328047513961792,\n 'model_acc': 88.02915335463258}</pre> <p>Do these results look the same as <code>model_2_results</code>?</p> In\u00a0[60]: Copied! <pre>model_2_results\n</pre> model_2_results Out[60]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.328047513961792,\n 'model_acc': 88.02915335463258}</pre> <p>We can find out if two tensors are close to each other using <code>torch.isclose()</code> and passing in a tolerance level of closeness via the parameters <code>atol</code> (absolute tolerance) and <code>rtol</code> (relative tolerance).</p> <p>If our model's results are close, the output of <code>torch.isclose()</code> should be true.</p> In\u00a0[61]: Copied! <pre># Check to see if results are close to each other (if they are very far away, there may be an error)\ntorch.isclose(torch.tensor(model_2_results[\"model_loss\"]), \n              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n              atol=1e-08, # absolute tolerance\n              rtol=0.0001) # relative tolerance\n</pre> # Check to see if results are close to each other (if they are very far away, there may be an error) torch.isclose(torch.tensor(model_2_results[\"model_loss\"]),                torch.tensor(loaded_model_2_results[\"model_loss\"]),               atol=1e-08, # absolute tolerance               rtol=0.0001) # relative tolerance Out[61]: <pre>tensor(True)</pre>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#03-pytorch-computer-vision","title":"03. PyTorch Computer Vision\u00b6","text":"<p>Computer vision is the art of teaching a computer to see.</p> <p>For example, it could involve building a model to classify whether a photo is of a cat or a dog (binary classification).</p> <p>Or whether a photo is of a cat, dog or chicken (multi-class classification).</p> <p>Or identifying where a car appears in a video frame (object detection).</p> <p>Or figuring out where different objects in an image can be separated (panoptic segmentation).</p> <p> Example computer vision problems for binary classification, multiclass classification, object detection and segmentation.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#where-does-computer-vision-get-used","title":"Where does computer vision get used?\u00b6","text":"<p>If you use a smartphone, you've already used computer vision.</p> <p>Camera and photo apps use computer vision to enhance and sort images.</p> <p>Modern cars use computer vision to avoid other cars and stay within lane lines.</p> <p>Manufacturers use computer vision to identify defects in various products.</p> <p>Security cameras use computer vision to detect potential intruders.</p> <p>In essence, anything that can described in a visual sense can be a potential computer vision problem.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to apply the PyTorch Workflow we've been learning in the past couple of sections to computer vision.</p> <p></p> <p>Specifically, we're going to cover:</p> Topic Contents 0. Computer vision libraries in PyTorch PyTorch has a bunch of built-in helpful computer vision libraries, let's check them out. 1. Load data To practice computer vision, we'll start with some images of different pieces of clothing from FashionMNIST. 2. Prepare data We've got some images, let's load them in with a PyTorch <code>DataLoader</code> so we can use them with our training loop. 3. Model 0: Building a baseline model Here we'll create a multi-class classification model to learn patterns in the data, we'll also choose a loss function, optimizer and build a training loop. 4. Making predictions and evaluting model 0 Let's make some predictions with our baseline model and evaluate them. 5. Setup device agnostic code for future models It's best practice to write device-agnostic code, so let's set it up. 6. Model 1: Adding non-linearity Experimenting is a large part of machine learning, let's try and improve upon our baseline model by adding non-linear layers. 7. Model 2: Convolutional Neural Network (CNN) Time to get computer vision specific and introduce the powerful convolutional neural network architecture. 8. Comparing our models We've built three different models, let's compare them. 9. Evaluating our best model Let's make some predictons on random images and evaluate our best model. 10. Making a confusion matrix A confusion matrix is a great way to evaluate a classification model, let's see how we can make one. 11. Saving and loading the best performing model Since we might want to use our model for later, let's save it and make sure it loads back in correctly."},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#where-can-can-you-get-help","title":"Where can can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page there too.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#0-computer-vision-libraries-in-pytorch","title":"0. Computer vision libraries in PyTorch\u00b6","text":"<p>Before we get started writing code, let's talk about some PyTorch computer vision libraries you should be aware of.</p> PyTorch module What does it do? <code>torchvision</code> Contains datasets, model architectures and image transformations often used for computer vision problems. <code>torchvision.datasets</code> Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains a series of base classes for making custom datasets. <code>torchvision.models</code> This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems. <code>torchvision.transforms</code> Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here. <code>torch.utils.data.Dataset</code> Base dataset class for PyTorch. <code>torch.utils.data.DataLoader</code> Creates a Python iterable over a dataset (created with <code>torch.utils.data.Dataset</code>). <p>Note: The <code>torch.utils.data.Dataset</code> and <code>torch.utils.data.DataLoader</code> classes aren't only for computer vision in PyTorch, they are capable of dealing with many different types of data.</p> <p>Now we've covered some of the most important PyTorch computer vision libraries, let's import the relevant dependencies.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#1-getting-a-dataset","title":"1. Getting a dataset\u00b6","text":"<p>To begin working on a computer vision problem, let's get a computer vision dataset.</p> <p>We're going to start with FashionMNIST.</p> <p>MNIST stands for Modified National Institute of Standards and Technology.</p> <p>The original MNIST dataset contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.</p> <p>FashionMNIST, made by Zalando Research, is a similar setup.</p> <p>Except it contains grayscale images of 10 different kinds of clothing.</p> <p> <code>torchvision.datasets</code> contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.</p> <p>Later, we'll be building a computer vision neural network to identify the different styles of clothing in these images.</p> <p>PyTorch has a bunch of common computer vision datasets stored in <code>torchvision.datasets</code>.</p> <p>Including FashionMNIST in <code>torchvision.datasets.FashionMNIST()</code>.</p> <p>To download it, we provide the following parameters:</p> <ul> <li><code>root: str</code> - which folder do you want to download the data to?</li> <li><code>train: Bool</code> - do you want the training or test split?</li> <li><code>download: Bool</code> - should the data be downloaded?</li> <li><code>transform: torchvision.transforms</code> - what transformations would you like to do on the data?</li> <li><code>target_transform</code> - you can transform the targets (labels) if you like too.</li> </ul> <p>Many other datasets in <code>torchvision</code> have these parameter options.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#11-input-and-output-shapes-of-a-computer-vision-model","title":"1.1 Input and output shapes of a computer vision model\u00b6","text":"<p>We've got a big tensor of values (the image) leading to a single value for the target (the label).</p> <p>Let's see the image shape.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#12-visualizing-our-data","title":"1.2 Visualizing our data\u00b6","text":""},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#2-prepare-dataloader","title":"2. Prepare DataLoader\u00b6","text":"<p>Now we've got a dataset ready to go.</p> <p>The next step is to prepare it with a <code>torch.utils.data.DataLoader</code> or <code>DataLoader</code> for short.</p> <p>The <code>DataLoader</code> does what you think it might do.</p> <p>It helps load data into a model.</p> <p>For training and for inference.</p> <p>It turns a large <code>Dataset</code> into a Python iterable of smaller chunks.</p> <p>These smaller chunks are called batches or mini-batches and can be set by the <code>batch_size</code> parameter.</p> <p>Why do this?</p> <p>Because it's more computationally efficient.</p> <p>In an ideal world you could do the forward pass and backward pass across all of your data at once.</p> <p>But once you start using really large datasets, unless you've got infinite computing power, it's easier to break them up into batches.</p> <p>It also gives your model more opportunities to improve.</p> <p>With mini-batches (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).</p> <p>What's a good batch size?</p> <p>32 is a good place to start for a fair amount of problems.</p> <p>But since this is a value you can set (a hyperparameter) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512).</p> <p> Batching FashionMNIST with a batch size of 32 and shuffle turned on. A similar batching process will occur for other datasets but will differ depending on the batch size.</p> <p>Let's create <code>DataLoader</code>'s for our training and test sets.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#3-model-0-build-a-baseline-model","title":"3. Model 0: Build a baseline model\u00b6","text":"<p>Data loaded and prepared!</p> <p>Time to build a baseline model by subclassing <code>nn.Module</code>.</p> <p>A baseline model is one of the simplest models you can imagine.</p> <p>You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.</p> <p>Our baseline will consist of two <code>nn.Linear()</code> layers.</p> <p>We've done this in a previous section but there's going to one slight difference.</p> <p>Because we're working with image data, we're going to use a different layer to start things off.</p> <p>And that's the <code>nn.Flatten()</code> layer.</p> <p><code>nn.Flatten()</code> compresses the dimensions of a tensor into a single vector.</p> <p>This is easier to understand when you see it.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#31-setup-loss-optimizer-and-evaluation-metrics","title":"3.1 Setup loss, optimizer and evaluation metrics\u00b6","text":"<p>Since we're working on a classification problem, let's bring in our <code>helper_functions.py</code> script and subsequently the <code>accuracy_fn()</code> we defined in notebook 02.</p> <p>Note: Rather than importing and using our own accuracy function or evaluation metric(s), you could import various evaluation metrics from the TorchMetrics package.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#32-creating-a-function-to-time-our-experiments","title":"3.2 Creating a function to time our experiments\u00b6","text":"<p>Loss function and optimizer ready!</p> <p>It's time to start training a model.</p> <p>But how about we do a little experiment while we train.</p> <p>I mean, let's make a timing function to measure the time it takes our model to train on CPU versus using a GPU.</p> <p>We'll train this model on the CPU but the next one on the GPU and see what happens.</p> <p>Our timing function will import the <code>timeit.default_timer()</code> function from the Python <code>timeit</code> module.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#33-creating-a-training-loop-and-training-a-model-on-batches-of-data","title":"3.3 Creating a training loop and training a model on batches of data\u00b6","text":"<p>Beautiful!</p> <p>Looks like we've got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.</p> <p>Let's now create a training loop and a testing loop to train and evaluate our model.</p> <p>We'll be using the same steps as the previous notebook(s), though since our data is now in batch form, we'll add another loop to loop through our data batches.</p> <p>Our data batches are contained within our <code>DataLoader</code>s, <code>train_dataloader</code> and <code>test_dataloader</code> for the training and test data splits respectively.</p> <p>A batch is <code>BATCH_SIZE</code> samples of <code>X</code> (features) and <code>y</code> (labels), since we're using <code>BATCH_SIZE=32</code>, our batches have 32 samples of images and targets.</p> <p>And since we're computing on batches of data, our loss and evaluation metrics will be calculated per batch rather than across the whole dataset.</p> <p>This means we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader.</p> <p>Let's step through it:</p> <ol> <li>Loop through epochs.</li> <li>Loop through training batches, perform training steps, calculate the train loss per batch.</li> <li>Loop through testing batches, perform testing steps, calculate the test loss per batch.</li> <li>Print out what's happening.</li> <li>Time it all (for fun).</li> </ol> <p>A fair few steps but...</p> <p>...if in doubt, code it out.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#4-make-predictions-and-get-model-0-results","title":"4. Make predictions and get Model 0 results\u00b6","text":"<p>Since we're going to be building a few models, it's a good idea to write some code to evaluate them all in similar ways.</p> <p>Namely, let's create a function that takes in a trained model, a <code>DataLoader</code>, a loss function and an accuracy function.</p> <p>The function will use the model to make predictions on the data in the <code>DataLoader</code> and then we can evaluate those predictions using the loss function and accuracy function.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#5-setup-device-agnostic-code-for-using-a-gpu-if-there-is-one","title":"5. Setup device agnostic-code (for using a GPU if there is one)\u00b6","text":"<p>We've seen how long it takes to train ma PyTorch model on 60,000 samples on CPU.</p> <p>Note: Model training time is dependent on hardware used. Generally, more processors means faster training and smaller models on smaller datasets will often train faster than large models and large datasets.</p> <p>Now let's setup some device-agnostic code for our models and data to run on GPU if it's available.</p> <p>If you're running this notebook on Google Colab, and you don't a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going <code>Runtime -&gt; Run before</code>.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#6-model-1-building-a-better-model-with-non-linearity","title":"6. Model 1: Building a better model with non-linearity\u00b6","text":"<p>We learned about the power of non-linearity in notebook 02.</p> <p>Seeing the data we've been working with, do you think it needs non-linear functions?</p> <p>And remember, linear means straight and non-linear means non-straight.</p> <p>Let's find out.</p> <p>We'll do so by recreating a similar model to before, except this time we'll put non-linear functions (<code>nn.ReLU()</code>) in between each linear layer.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#61-setup-loss-optimizer-and-evaluation-metrics","title":"6.1 Setup loss, optimizer and evaluation metrics\u00b6","text":"<p>As usual, we'll setup a loss function, an optimizer and an evaluation metric (we could do multiple evaluation metrics but we'll stick with accuracy for now).</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#62-functionizing-training-and-test-loops","title":"6.2 Functionizing training and test loops\u00b6","text":"<p>So far we've been writing train and test loops over and over.</p> <p>Let's write them again but this time we'll put them in functions so they can be called again and again.</p> <p>And because we're using device-agnostic code now, we'll be sure to call <code>.to(device)</code> on our feature (<code>X</code>) and target (<code>y</code>) tensors.</p> <p>For the training loop we'll create a function called <code>train_step()</code> which takes in a model, a <code>DataLoader</code> a loss function and an optimizer.</p> <p>The testing loop will be similar but it'll be called <code>test_step()</code> and it'll take in a model, a <code>DataLoader</code>, a loss function and an evaluation function.</p> <p>Note: Since these are functions, you can customize them in any way you like. What we're making here can be considered barebones training and testing functions for our specific classification use case.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#7-model-2-building-a-convolutional-neural-network-cnn","title":"7. Model 2: Building a Convolutional Neural Network (CNN)\u00b6","text":"<p>Alright, time to step things up a notch.</p> <p>It's time to create a Convolutional Neural Network (CNN or ConvNet).</p> <p>CNN's are known for their capabilities to find patterns in visual data.</p> <p>And since we're dealing with visual data, let's see if using a CNN model can improve upon our baseline.</p> <p>The CNN model we're going to be using is known as TinyVGG from the CNN Explainer website.</p> <p>It follows the typical structure of a convolutional neural network:</p> <p><code>Input layer -&gt; [Convolutional layer -&gt; activation layer -&gt; pooling layer] -&gt; Output layer</code></p> <p>Where the contents of <code>[Convolutional layer -&gt; activation layer -&gt; pooling layer]</code> can be upscaled and repeated multiple times, depending on requirements.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#what-model-should-i-use","title":"What model should I use?\u00b6","text":"<p>Question: Wait, you say CNN's are good for images, are there any other model types I should be aware of?</p> <p>Good question.</p> <p>This table is a good general guide for which model to use (though there are exceptions).</p> Problem type Model to use (generally) Code example Structured data (Excel spreadsheets, row and column data) Gradient boosted models, Random Forests, XGBoost <code>sklearn.ensemble</code>, XGBoost library Unstructured data (images, audio, language) Convolutional Neural Networks, Transformers <code>torchvision.models</code>, HuggingFace Transformers <p>Note: The table above is only for reference, the model you end up using will be highly dependant on the problem you're working on and the constraints you have (amount of data, latency requirements).</p> <p>Enough talking about models, let's now build a CNN that replicates the model on the CNN Explainer website.</p> <p></p> <p>To do so, we'll leverage the <code>nn.Conv2d()</code> and <code>nn.MaxPool2d()</code> layers from <code>torch.nn</code>.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#71-stepping-through-nnconv2d","title":"7.1 Stepping through <code>nn.Conv2d()</code>\u00b6","text":"<p>We could start using our model above and see what happens but let's first step through the two new layers we've added:</p> <ul> <li><code>nn.Conv2d()</code>, also known as a convolutional layer.</li> <li><code>nn.MaxPool2d()</code>, also known as a max pooling layer.</li> </ul> <p>Question: What does the \"2d\" in <code>nn.Conv2d()</code> stand for?</p> <p>The 2d is for 2-dimensional data. As in, our images have two dimensions: height and width. Yes, there's color channel dimension but each of the color channel dimensions have two dimensions too: height and width.</p> <p>For other dimensional data (such as 1D for text or 3D for 3D objects) there's also <code>nn.Conv1d()</code> and <code>nn.Conv3d()</code>.</p> <p>To test the layers out, let's create some toy data just like the data used on CNN Explainer.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#72-stepping-through-nnmaxpool2d","title":"7.2 Stepping through <code>nn.MaxPool2d()</code>\u00b6","text":"<p>Now let's check out what happens when we move data through <code>nn.MaxPool2d()</code>.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#73-setup-a-loss-function-and-optimizer-for-model_2","title":"7.3 Setup a loss function and optimizer for <code>model_2</code>\u00b6","text":"<p>We've stepped through the layers in our first CNN enough.</p> <p>But remember, if something still isn't clear, try starting small.</p> <p>Pick a single layer of a model, pass some data through it and see what happens.</p> <p>Now it's time to move forward and get to training!</p> <p>Let's setup a loss function and an optimizer.</p> <p>We'll use the functions as before, <code>nn.CrossEntropyLoss()</code> as the loss function (since we're working with multi-class classification data).</p> <p>And <code>torch.optim.SGD()</code> as the optimizer to optimize <code>model_2.parameters()</code> with a learning rate of <code>0.1</code>.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#74-training-and-testing-model_2-using-our-training-and-test-functions","title":"7.4 Training and testing <code>model_2</code> using our training and test functions\u00b6","text":"<p>Loss and optimizer ready!</p> <p>Time to train and test.</p> <p>We'll use our <code>train_step()</code> and <code>test_step()</code> functions we created before.</p> <p>We'll also measure the time to compare it to our other models.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#8-compare-model-results-and-training-time","title":"8. Compare model results and training time\u00b6","text":"<p>We've trained three different models.</p> <ol> <li><code>model_0</code> - our baseline model with two <code>nn.Linear()</code> layers.</li> <li><code>model_1</code> - the same setup as our baseline model except with <code>nn.ReLU()</code> layers in between the <code>nn.Linear()</code> layers.</li> <li><code>model_2</code> - our first CNN model that mimics the TinyVGG architecture on the CNN Explainer website.</li> </ol> <p>This is a regular practice in machine learning.</p> <p>Building multiple models and performing multiple training experiments to see which performs best.</p> <p>Let's combine our model results dictionaries into a DataFrame and find out.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#performance-speed-tradeoff","title":"Performance-speed tradeoff\u00b6","text":"<p>Something to be aware of in machine learning is the performance-speed tradeoff.</p> <p>Generally, you get better performance out of a larger, more complex model (like we did with <code>model_2</code>).</p> <p>However, this performance increase often comes at a sacrifice of training speed and inference speed.</p> <p>Note: The training times you get will be very dependant on the hardware you use.</p> <p>Generally, the more CPU cores you have, the faster your models will train on CPU. And similar for GPUs.</p> <p>Newer hardware (in terms of age) will also often train models faster due to incorporating technology advances.</p> <p>How about we get visual?</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#9-make-and-evaluate-random-predictions-with-best-model","title":"9. Make and evaluate random predictions with best model\u00b6","text":"<p>Alright, we've compared our models to each other, let's further evaluate our best performing model, <code>model_2</code>.</p> <p>To do so, let's create a function <code>make_predictions()</code> where we can pass the model and some data for it to predict on.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#10-making-a-confusion-matrix-for-further-prediction-evaluation","title":"10. Making a confusion matrix for further prediction evaluation\u00b6","text":"<p>There are many different evaluation metrics we can use for classification problems.</p> <p>One of the most visual is a confusion matrix.</p> <p>A confusion matrix shows you where your classification model got confused between predicitons and true labels.</p> <p>To make a confusion matrix, we'll go through three steps:</p> <ol> <li>Make predictions with our trained model, <code>model_2</code> (a confusion matrix compares predictions to true labels).</li> <li>Make a confusion matrix using <code>torchmetrics.ConfusionMatrix</code>.</li> <li>Plot the confusion matrix using <code>mlxtend.plotting.plot_confusion_matrix()</code>.</li> </ol> <p>Let's start by making predictions with our trained model.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#11-save-and-load-best-performing-model","title":"11. Save and load best performing model\u00b6","text":"<p>Let's finish this section off by saving and loading in our best performing model.</p> <p>Recall from notebook 01 we can save and load a PyTorch model using a combination of:</p> <ul> <li><code>torch.save</code> - a function to save a whole PyTorch model or a model's <code>state_dict()</code>.</li> <li><code>torch.load</code> - a function to load in a saved PyTorch object.</li> <li><code>torch.nn.Module.load_state_dict()</code> - a function to load a saved <code>state_dict()</code> into an existing model instance.</li> </ul> <p>You can see more of these three in the PyTorch saving and loading models documentation.</p> <p>For now, let's save our <code>model_2</code>'s <code>state_dict()</code> then load it back in and evaluate it to make sure the save and load went correctly.</p>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code in the sections above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 03</li> <li>Example solutions notebook for 03 (try the exercises before looking at this)</li> </ul> <ol> <li>What are 3 areas in industry where computer vision is currently being used?</li> <li>Search \"what is overfitting in machine learning\" and write down a sentence about what you find.</li> <li>Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each. Note: there are lots of these, so don't worry too much about all of them, just pick 3 and start with those.</li> <li>Spend 20-minutes reading and clicking through the CNN Explainer website.<ul> <li>Upload your own example image using the \"upload\" button and see what happens in each layer of a CNN as your image passes through it.</li> </ul> </li> <li>Load the <code>torchvision.datasets.MNIST()</code> train and test datasets.</li> <li>Visualize at least 5 different samples of the MNIST training dataset.</li> <li>Turn the MNIST train and test datasets into dataloaders using <code>torch.utils.data.DataLoader</code>, set the <code>batch_size=32</code>.</li> <li>Recreate <code>model_2</code> used in this notebook (the same model from the CNN Explainer website, also known as TinyVGG) capable of fitting on the MNIST dataset.</li> <li>Train the model you built in exercise 8. on CPU and GPU and see how long it takes on each.</li> <li>Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label.</li> <li>Plot a confusion matrix comparing your model's predictions to the truth labels.</li> <li>Create a random tensor of shape <code>[1, 3, 64, 64]</code> and pass it through a <code>nn.Conv2d()</code> layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the <code>kernel_size</code> parameter goes up and down?</li> <li>Use a model similar to the trained <code>model_2</code> from this notebook to make predictions on the test <code>torchvision.datasets.FashionMNIST</code> dataset.<ul> <li>Then plot some predictions where the model was wrong alongside what the label of the image should've been.</li> <li>After visualing these predictions do you think it's more of a modelling error or a data error?</li> <li>As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?</li> </ul> </li> </ol>"},{"location":"Learning/Pytorch/03_pytorch_computer_vision/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Watch: MIT's Introduction to Deep Computer Vision lecture. This will give you a great intuition behind convolutional neural networks.</li> <li>Spend 10-minutes clicking thorugh the different options of the PyTorch vision library, what different modules are available?</li> <li>Lookup \"most common convolutional neural networks\", what architectures do you find? Are any of them contained within the <code>torchvision.models</code> library? What do you think you could do with these?</li> <li>For a large number of pretrained PyTorch computer vision models as well as many different extensions to PyTorch's computer vision functionalities check out the PyTorch Image Models library <code>timm</code> (Torch Image Models) by Ross Wightman.</li> </ul>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/","title":"04. PyTorch Custom Datasets","text":"<p>View Source Code | View Slides | Watch Video Walkthrough</p> In\u00a0[1]: Copied! <pre>import torch\nfrom torch import nn\n\n# Note: this notebook requires torch &gt;= 1.10.0\ntorch.__version__\n</pre> import torch from torch import nn  # Note: this notebook requires torch &gt;= 1.10.0 torch.__version__ Out[1]: <pre>'2.2.2'</pre> <p>And now let's follow best practice and setup device-agnostic code.</p> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going <code>Runtime -&gt; Run before</code>.</p> In\u00a0[2]: Copied! <pre># Setup device-agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device-agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[2]: <pre>'cuda'</pre> In\u00a0[3]: Copied! <pre>import requests\nimport zipfile\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n</pre> import requests import zipfile from pathlib import Path  # Setup path to data folder data_path = Path(\"data/\") image_path = data_path / \"pizza_steak_sushi\"  # If the image folder doesn't exist, download it and prepare it...  if image_path.is_dir():     print(f\"{image_path} directory exists.\") else:     print(f\"Did not find {image_path} directory, creating one...\")     image_path.mkdir(parents=True, exist_ok=True)          # Download pizza, steak, sushi data     with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")         print(\"Downloading pizza, steak, sushi data...\")         f.write(request.content)      # Unzip pizza, steak, sushi data     with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:         print(\"Unzipping pizza, steak, sushi data...\")          zip_ref.extractall(image_path) <pre>Did not find data/pizza_steak_sushi directory, creating one...\nDownloading pizza, steak, sushi data...\nUnzipping pizza, steak, sushi data...\n</pre> In\u00a0[4]: Copied! <pre>import os\ndef walk_through_dir(dir_path):\n  \"\"\"\n  Walks through dir_path returning its contents.\n  Args:\n    dir_path (str or pathlib.Path): target directory\n  \n  Returns:\n    A print out of:\n      number of subdiretories in dir_path\n      number of images (files) in each subdirectory\n      name of each subdirectory\n  \"\"\"\n  for dirpath, dirnames, filenames in os.walk(dir_path):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n</pre> import os def walk_through_dir(dir_path):   \"\"\"   Walks through dir_path returning its contents.   Args:     dir_path (str or pathlib.Path): target directory      Returns:     A print out of:       number of subdiretories in dir_path       number of images (files) in each subdirectory       name of each subdirectory   \"\"\"   for dirpath, dirnames, filenames in os.walk(dir_path):     print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\") In\u00a0[5]: Copied! <pre>walk_through_dir(image_path)\n</pre> walk_through_dir(image_path) <pre>There are 2 directories and 0 images in 'data/pizza_steak_sushi'.\nThere are 3 directories and 0 images in 'data/pizza_steak_sushi/test'.\nThere are 0 directories and 19 images in 'data/pizza_steak_sushi/test/steak'.\nThere are 0 directories and 25 images in 'data/pizza_steak_sushi/test/pizza'.\nThere are 0 directories and 31 images in 'data/pizza_steak_sushi/test/sushi'.\nThere are 3 directories and 0 images in 'data/pizza_steak_sushi/train'.\nThere are 0 directories and 75 images in 'data/pizza_steak_sushi/train/steak'.\nThere are 0 directories and 78 images in 'data/pizza_steak_sushi/train/pizza'.\nThere are 0 directories and 72 images in 'data/pizza_steak_sushi/train/sushi'.\n</pre> <p>Excellent!</p> <p>It looks like we've got about 75 images per training class and 25 images per testing class.</p> <p>That should be enough to get started.</p> <p>Remember, these images are subsets of the original Food101 dataset.</p> <p>You can see how they were created in the data creation notebook.</p> <p>While we're at it, let's setup our training and testing paths.</p> In\u00a0[6]: Copied! <pre># Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir\n</pre> # Setup train and testing paths train_dir = image_path / \"train\" test_dir = image_path / \"test\"  train_dir, test_dir Out[6]: <pre>(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))</pre> In\u00a0[7]: Copied! <pre>import random\nfrom PIL import Image\n\n# Set seed\nrandom.seed(42) # &lt;- try changing this and see what happens\n\n# 1. Get all image paths (* means \"any combination\")\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# 2. Get random image path\nrandom_image_path = random.choice(image_path_list)\n\n# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\nimage_class = random_image_path.parent.stem\n\n# 4. Open image\nimg = Image.open(random_image_path)\n\n# 5. Print metadata\nprint(f\"Random image path: {random_image_path}\")\nprint(f\"Image class: {image_class}\")\nprint(f\"Image height: {img.height}\") \nprint(f\"Image width: {img.width}\")\nimg\n</pre> import random from PIL import Image  # Set seed random.seed(42) # &lt;- try changing this and see what happens  # 1. Get all image paths (* means \"any combination\") image_path_list = list(image_path.glob(\"*/*/*.jpg\"))  # 2. Get random image path random_image_path = random.choice(image_path_list)  # 3. Get image class from path name (the image class is the name of the directory where the image is stored) image_class = random_image_path.parent.stem  # 4. Open image img = Image.open(random_image_path)  # 5. Print metadata print(f\"Random image path: {random_image_path}\") print(f\"Image class: {image_class}\") print(f\"Image height: {img.height}\")  print(f\"Image width: {img.width}\") img <pre>Random image path: data/pizza_steak_sushi/test/sushi/2385731.jpg\nImage class: sushi\nImage height: 512\nImage width: 512\n</pre> Out[7]: <p>We can do the same with <code>matplotlib.pyplot.imshow()</code>, except we have to convert the image to a NumPy array first.</p> In\u00a0[8]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Turn the image into an array\nimg_as_array = np.asarray(img)\n\n# Plot the image with matplotlib\nplt.figure(figsize=(10, 7))\nplt.imshow(img_as_array)\nplt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\")\nplt.axis(False);\n</pre> import numpy as np import matplotlib.pyplot as plt  # Turn the image into an array img_as_array = np.asarray(img)  # Plot the image with matplotlib plt.figure(figsize=(10, 7)) plt.imshow(img_as_array) plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\") plt.axis(False); In\u00a0[9]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n</pre> import torch from torch.utils.data import DataLoader from torchvision import datasets, transforms <pre>/home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[10]: Copied! <pre># Write transform for image\ndata_transform = transforms.Compose([\n    # Resize the images to 64x64\n    transforms.Resize(size=(64, 64)),\n    # Flip the images randomly on the horizontal\n    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n    # Turn the image into a torch.Tensor\n    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n])\n</pre> # Write transform for image data_transform = transforms.Compose([     # Resize the images to 64x64     transforms.Resize(size=(64, 64)),     # Flip the images randomly on the horizontal     transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance     # Turn the image into a torch.Tensor     transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0  ]) <p>Now we've got a composition of transforms, let's write a function to try them out on various images.</p> In\u00a0[11]: Copied! <pre>def plot_transformed_images(image_paths, transform, n=3, seed=42):\n    \"\"\"Plots a series of random images from image_paths.\n\n    Will open n image paths from image_paths, transform them\n    with transform and plot them side by side.\n\n    Args:\n        image_paths (list): List of target image paths. \n        transform (PyTorch Transforms): Transforms to apply to images.\n        n (int, optional): Number of images to plot. Defaults to 3.\n        seed (int, optional): Random seed for the random generator. Defaults to 42.\n    \"\"\"\n    random.seed(seed)\n    random_image_paths = random.sample(image_paths, k=n)\n    for image_path in random_image_paths:\n        with Image.open(image_path) as f:\n            fig, ax = plt.subplots(1, 2)\n            ax[0].imshow(f) \n            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n            ax[0].axis(\"off\")\n\n            # Transform and plot image\n            # Note: permute() will change shape of image to suit matplotlib \n            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n            transformed_image = transform(f).permute(1, 2, 0) \n            ax[1].imshow(transformed_image) \n            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n            ax[1].axis(\"off\")\n\n            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n\nplot_transformed_images(image_path_list, \n                        transform=data_transform, \n                        n=3)\n</pre> def plot_transformed_images(image_paths, transform, n=3, seed=42):     \"\"\"Plots a series of random images from image_paths.      Will open n image paths from image_paths, transform them     with transform and plot them side by side.      Args:         image_paths (list): List of target image paths.          transform (PyTorch Transforms): Transforms to apply to images.         n (int, optional): Number of images to plot. Defaults to 3.         seed (int, optional): Random seed for the random generator. Defaults to 42.     \"\"\"     random.seed(seed)     random_image_paths = random.sample(image_paths, k=n)     for image_path in random_image_paths:         with Image.open(image_path) as f:             fig, ax = plt.subplots(1, 2)             ax[0].imshow(f)              ax[0].set_title(f\"Original \\nSize: {f.size}\")             ax[0].axis(\"off\")              # Transform and plot image             # Note: permute() will change shape of image to suit matplotlib              # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])             transformed_image = transform(f).permute(1, 2, 0)              ax[1].imshow(transformed_image)              ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")             ax[1].axis(\"off\")              fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)  plot_transformed_images(image_path_list,                          transform=data_transform,                          n=3) <p>Nice!</p> <p>We've now got a way to convert our images to tensors using <code>torchvision.transforms</code>.</p> <p>We also manipulate their size and orientation if needed (some models prefer images of different sizes and shapes).</p> <p>Generally, the larger the shape of the image, the more information a model can recover.</p> <p>For example, an image of size <code>[256, 256, 3]</code> will have 16x more pixels than an image of size <code>[64, 64, 3]</code> (<code>(256*256*3)/(64*64*3)=16</code>).</p> <p>However, the tradeoff is that more pixels requires more computations.</p> <p>Exercise: Try commenting out one of the transforms in <code>data_transform</code> and running the plotting function <code>plot_transformed_images()</code> again, what happens?</p> In\u00a0[12]: Copied! <pre># Use ImageFolder to create dataset(s)\nfrom torchvision import datasets\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\n\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n</pre> # Use ImageFolder to create dataset(s) from torchvision import datasets train_data = datasets.ImageFolder(root=train_dir, # target folder of images                                   transform=data_transform, # transforms to perform on data (images)                                   target_transform=None) # transforms to perform on labels (if necessary)  test_data = datasets.ImageFolder(root=test_dir,                                   transform=data_transform)  print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\") <pre>Train data:\nDataset ImageFolder\n    Number of datapoints: 225\n    Root location: data/pizza_steak_sushi/train\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\nTest data:\nDataset ImageFolder\n    Number of datapoints: 75\n    Root location: data/pizza_steak_sushi/test\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\n</pre> <p>Beautiful!</p> <p>It looks like PyTorch has registered our <code>Dataset</code>'s.</p> <p>Let's inspect them by checking out the <code>classes</code> and <code>class_to_idx</code> attributes as well as the lengths of our training and test sets.</p> In\u00a0[13]: Copied! <pre># Get class names as a list\nclass_names = train_data.classes\nclass_names\n</pre> # Get class names as a list class_names = train_data.classes class_names Out[13]: <pre>['pizza', 'steak', 'sushi']</pre> In\u00a0[14]: Copied! <pre># Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict\n</pre> # Can also get class names as a dict class_dict = train_data.class_to_idx class_dict Out[14]: <pre>{'pizza': 0, 'steak': 1, 'sushi': 2}</pre> In\u00a0[15]: Copied! <pre># Check the lengths\nlen(train_data), len(test_data)\n</pre> # Check the lengths len(train_data), len(test_data) Out[15]: <pre>(225, 75)</pre> <p>Nice! Looks like we'll be able to use these to reference for later.</p> <p>How about our images and labels?</p> <p>How do they look?</p> <p>We can index on our <code>train_data</code> and <code>test_data</code> <code>Dataset</code>'s to find samples and their target labels.</p> In\u00a0[16]: Copied! <pre>img, label = train_data[0][0], train_data[0][1]\nprint(f\"Image tensor:\\n{img}\")\nprint(f\"Image shape: {img.shape}\")\nprint(f\"Image datatype: {img.dtype}\")\nprint(f\"Image label: {label}\")\nprint(f\"Label datatype: {type(label)}\")\n</pre> img, label = train_data[0][0], train_data[0][1] print(f\"Image tensor:\\n{img}\") print(f\"Image shape: {img.shape}\") print(f\"Image datatype: {img.dtype}\") print(f\"Image label: {label}\") print(f\"Label datatype: {type(label)}\") <pre>Image tensor:\ntensor([[[0.1137, 0.1020, 0.0980,  ..., 0.1255, 0.1216, 0.1176],\n         [0.1059, 0.0980, 0.0980,  ..., 0.1294, 0.1294, 0.1294],\n         [0.1020, 0.0980, 0.0941,  ..., 0.1333, 0.1333, 0.1333],\n         ...,\n         [0.1098, 0.1098, 0.1255,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0902, 0.0941, 0.1098,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0863, 0.0980,  ..., 0.1686, 0.1647, 0.1647]],\n\n        [[0.0745, 0.0706, 0.0745,  ..., 0.0588, 0.0588, 0.0588],\n         [0.0745, 0.0706, 0.0745,  ..., 0.0627, 0.0627, 0.0627],\n         [0.0706, 0.0745, 0.0745,  ..., 0.0706, 0.0706, 0.0706],\n         ...,\n         [0.1255, 0.1333, 0.1373,  ..., 0.2510, 0.2392, 0.2392],\n         [0.1098, 0.1176, 0.1255,  ..., 0.2510, 0.2392, 0.2314],\n         [0.1020, 0.1059, 0.1137,  ..., 0.2431, 0.2353, 0.2275]],\n\n        [[0.0941, 0.0902, 0.0902,  ..., 0.0157, 0.0196, 0.0196],\n         [0.0902, 0.0863, 0.0902,  ..., 0.0196, 0.0157, 0.0196],\n         [0.0902, 0.0902, 0.0902,  ..., 0.0157, 0.0157, 0.0196],\n         ...,\n         [0.1294, 0.1333, 0.1490,  ..., 0.1961, 0.1882, 0.1843],\n         [0.1098, 0.1137, 0.1255,  ..., 0.1922, 0.1843, 0.1804],\n         [0.1059, 0.0980, 0.1059,  ..., 0.1882, 0.1804, 0.1765]]])\nImage shape: torch.Size([3, 64, 64])\nImage datatype: torch.float32\nImage label: 0\nLabel datatype: &lt;class 'int'&gt;\n</pre> <p>Our images are now in the form of a tensor (with shape <code>[3, 64, 64]</code>) and the labels are in the form of an integer relating to a specific class (as referenced by the <code>class_to_idx</code> attribute).</p> <p>How about we plot a single image tensor using <code>matplotlib</code>?</p> <p>We'll first have to to permute (rearrange the order of its dimensions) so it's compatible.</p> <p>Right now our image dimensions are in the format <code>CHW</code> (color channels, height, width) but <code>matplotlib</code> prefers <code>HWC</code> (height, width, color channels).</p> In\u00a0[17]: Copied! <pre># Rearrange the order of dimensions\nimg_permute = img.permute(1, 2, 0)\n\n# Print out different shapes (before and after permute)\nprint(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")\n\n# Plot the image\nplt.figure(figsize=(10, 7))\nplt.imshow(img.permute(1, 2, 0))\nplt.axis(\"off\")\nplt.title(class_names[label], fontsize=14);\n</pre> # Rearrange the order of dimensions img_permute = img.permute(1, 2, 0)  # Print out different shapes (before and after permute) print(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\") print(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")  # Plot the image plt.figure(figsize=(10, 7)) plt.imshow(img.permute(1, 2, 0)) plt.axis(\"off\") plt.title(class_names[label], fontsize=14); <pre>Original shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nImage permute shape: torch.Size([64, 64, 3]) -&gt; [height, width, color_channels]\n</pre> <p>Notice the image is now more pixelated (less quality).</p> <p>This is due to it being resized from <code>512x512</code> to <code>64x64</code> pixels.</p> <p>The intuition here is that if you think the image is harder to recognize what's going on, chances are a model will find it harder to understand too.</p> In\u00a0[18]: Copied! <pre># Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader\n</pre> # Turn train and test Datasets into DataLoaders from torch.utils.data import DataLoader train_dataloader = DataLoader(dataset=train_data,                                batch_size=1, # how many samples per batch?                               num_workers=1, # how many subprocesses to use for data loading? (higher = more)                               shuffle=True) # shuffle the data?  test_dataloader = DataLoader(dataset=test_data,                               batch_size=1,                               num_workers=1,                               shuffle=False) # don't usually need to shuffle testing data  train_dataloader, test_dataloader Out[18]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fae875e25b0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fae875e2e50&gt;)</pre> <p>Wonderful!</p> <p>Now our data is iterable.</p> <p>Let's try it out and check the shapes.</p> In\u00a0[19]: Copied! <pre>img, label = next(iter(train_dataloader))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")\n</pre> img, label = next(iter(train_dataloader))  # Batch size will now be 1, try changing the batch_size parameter above and see what happens print(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Label shape: {label.shape}\") <pre>Image shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n</pre> <p>We could now use these <code>DataLoader</code>'s with a training and testing loop to train a model.</p> <p>But before we do, let's look at another option to load images (or almost any other kind of data).</p> In\u00a0[20]: Copied! <pre>import os\nimport pathlib\nimport torch\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom typing import Tuple, Dict, List\n</pre> import os import pathlib import torch  from PIL import Image from torch.utils.data import Dataset from torchvision import transforms from typing import Tuple, Dict, List <p>Remember how our instances of <code>torchvision.datasets.ImageFolder()</code> allowed us to use the <code>classes</code> and <code>class_to_idx</code> attributes?</p> In\u00a0[21]: Copied! <pre># Instance of torchvision.datasets.ImageFolder()\ntrain_data.classes, train_data.class_to_idx\n</pre> # Instance of torchvision.datasets.ImageFolder() train_data.classes, train_data.class_to_idx Out[21]: <pre>(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})</pre> In\u00a0[22]: Copied! <pre># Setup path for target directory\ntarget_directory = train_dir\nprint(f\"Target directory: {target_directory}\")\n\n# Get the class names from the target directory\nclass_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\nprint(f\"Class names found: {class_names_found}\")\n</pre> # Setup path for target directory target_directory = train_dir print(f\"Target directory: {target_directory}\")  # Get the class names from the target directory class_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))]) print(f\"Class names found: {class_names_found}\") <pre>Target directory: data/pizza_steak_sushi/train\nClass names found: ['pizza', 'steak', 'sushi']\n</pre> <p>Excellent!</p> <p>How about we turn it into a full function?</p> In\u00a0[23]: Copied! <pre># Make function to find classes in target directory\ndef find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:\n    \"\"\"Finds the class folder names in a target directory.\n    \n    Assumes target directory is in standard image classification format.\n\n    Args:\n        directory (str): target directory to load classnames from.\n\n    Returns:\n        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n    \n    Example:\n        find_classes(\"food_images/train\")\n        &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n    \"\"\"\n    # 1. Get the class names by scanning the target directory\n    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n    \n    # 2. Raise an error if class names not found\n    if not classes:\n        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n        \n    # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return classes, class_to_idx\n</pre> # Make function to find classes in target directory def find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:     \"\"\"Finds the class folder names in a target directory.          Assumes target directory is in standard image classification format.      Args:         directory (str): target directory to load classnames from.      Returns:         Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))          Example:         find_classes(\"food_images/train\")         &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})     \"\"\"     # 1. Get the class names by scanning the target directory     classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())          # 2. Raise an error if class names not found     if not classes:         raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")              # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)     class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}     return classes, class_to_idx <p>Looking good!</p> <p>Now let's test out our <code>find_classes()</code> function.</p> In\u00a0[24]: Copied! <pre>find_classes(train_dir)\n</pre> find_classes(train_dir) Out[24]: <pre>(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})</pre> <p>Woohoo! Looking good!</p> In\u00a0[25]: Copied! <pre># Write a custom dataset class (inherits from torch.utils.data.Dataset)\nfrom torch.utils.data import Dataset\n\n# 1. Subclass torch.utils.data.Dataset\nclass ImageFolderCustom(Dataset):\n    \n    # 2. Initialize with a targ_dir and transform (optional) parameter\n    def __init__(self, targ_dir: str, transform=None) -&gt; None:\n        \n        # 3. Create class attributes\n        # Get all image paths\n        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n        # Setup transforms\n        self.transform = transform\n        # Create classes and class_to_idx attributes\n        self.classes, self.class_to_idx = find_classes(targ_dir)\n\n    # 4. Make function to load images\n    def load_image(self, index: int) -&gt; Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.paths[index]\n        return Image.open(image_path) \n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -&gt; int:\n        \"Returns the total number of samples.\"\n        return len(self.paths)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:\n        \"Returns one sample of data, data and label (X, y).\"\n        img = self.load_image(index)\n        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n        class_idx = self.class_to_idx[class_name]\n\n        # Transform if necessary\n        if self.transform:\n            return self.transform(img), class_idx # return data, label (X, y)\n        else:\n            return img, class_idx # return data, label (X, y)\n</pre> # Write a custom dataset class (inherits from torch.utils.data.Dataset) from torch.utils.data import Dataset  # 1. Subclass torch.utils.data.Dataset class ImageFolderCustom(Dataset):          # 2. Initialize with a targ_dir and transform (optional) parameter     def __init__(self, targ_dir: str, transform=None) -&gt; None:                  # 3. Create class attributes         # Get all image paths         self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's         # Setup transforms         self.transform = transform         # Create classes and class_to_idx attributes         self.classes, self.class_to_idx = find_classes(targ_dir)      # 4. Make function to load images     def load_image(self, index: int) -&gt; Image.Image:         \"Opens an image via a path and returns it.\"         image_path = self.paths[index]         return Image.open(image_path)           # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)     def __len__(self) -&gt; int:         \"Returns the total number of samples.\"         return len(self.paths)          # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)     def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:         \"Returns one sample of data, data and label (X, y).\"         img = self.load_image(index)         class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg         class_idx = self.class_to_idx[class_name]          # Transform if necessary         if self.transform:             return self.transform(img), class_idx # return data, label (X, y)         else:             return img, class_idx # return data, label (X, y) <p>Woah! A whole bunch of code to load in our images.</p> <p>This is one of the downsides of creating your own custom <code>Dataset</code>'s.</p> <p>However, now we've written it once, we could move it into a <code>.py</code> file such as <code>data_loader.py</code> along with some other helpful data functions and reuse it later on.</p> <p>Before we test out our new <code>ImageFolderCustom</code> class, let's create some transforms to prepare our images.</p> In\u00a0[26]: Copied! <pre># Augment train data\ntrain_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n# Don't augment test data, only reshape\ntest_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n</pre> # Augment train data train_transforms = transforms.Compose([     transforms.Resize((64, 64)),     transforms.RandomHorizontalFlip(p=0.5),     transforms.ToTensor() ])  # Don't augment test data, only reshape test_transforms = transforms.Compose([     transforms.Resize((64, 64)),     transforms.ToTensor() ]) <p>Now comes the moment of truth!</p> <p>Let's turn our training images (contained in <code>train_dir</code>) and our testing images (contained in <code>test_dir</code>) into <code>Dataset</code>'s using our own <code>ImageFolderCustom</code> class.</p> In\u00a0[27]: Copied! <pre>train_data_custom = ImageFolderCustom(targ_dir=train_dir, \n                                      transform=train_transforms)\ntest_data_custom = ImageFolderCustom(targ_dir=test_dir, \n                                     transform=test_transforms)\ntrain_data_custom, test_data_custom\n</pre> train_data_custom = ImageFolderCustom(targ_dir=train_dir,                                        transform=train_transforms) test_data_custom = ImageFolderCustom(targ_dir=test_dir,                                       transform=test_transforms) train_data_custom, test_data_custom Out[27]: <pre>(&lt;__main__.ImageFolderCustom at 0x7fae87600a30&gt;,\n &lt;__main__.ImageFolderCustom at 0x7fae876007f0&gt;)</pre> <p>Hmm... no errors, did it work?</p> <p>Let's try calling <code>len()</code> on our new <code>Dataset</code>'s and find the <code>classes</code> and <code>class_to_idx</code> attributes.</p> In\u00a0[28]: Copied! <pre>len(train_data_custom), len(test_data_custom)\n</pre> len(train_data_custom), len(test_data_custom) Out[28]: <pre>(225, 75)</pre> In\u00a0[29]: Copied! <pre>train_data_custom.classes\n</pre> train_data_custom.classes Out[29]: <pre>['pizza', 'steak', 'sushi']</pre> In\u00a0[30]: Copied! <pre>train_data_custom.class_to_idx\n</pre> train_data_custom.class_to_idx Out[30]: <pre>{'pizza': 0, 'steak': 1, 'sushi': 2}</pre> <p><code>len(test_data_custom) == len(test_data)</code> and <code>len(test_data_custom) == len(test_data)</code> Yes!!!</p> <p>It looks like it worked.</p> <p>We could check for equality with the <code>Dataset</code>'s made by the <code>torchvision.datasets.ImageFolder()</code> class too.</p> In\u00a0[31]: Copied! <pre># Check for equality amongst our custom Dataset and ImageFolder Dataset\nprint((len(train_data_custom) == len(train_data)) &amp; (len(test_data_custom) == len(test_data)))\nprint(train_data_custom.classes == train_data.classes)\nprint(train_data_custom.class_to_idx == train_data.class_to_idx)\n</pre> # Check for equality amongst our custom Dataset and ImageFolder Dataset print((len(train_data_custom) == len(train_data)) &amp; (len(test_data_custom) == len(test_data))) print(train_data_custom.classes == train_data.classes) print(train_data_custom.class_to_idx == train_data.class_to_idx) <pre>True\nTrue\nTrue\n</pre> <p>Ho ho!</p> <p>Look at us go!</p> <p>Three <code>True</code>'s!</p> <p>You can't get much better than that.</p> <p>How about we take it up a notch and plot some random images to test our <code>__getitem__</code> override?</p> In\u00a0[32]: Copied! <pre># 1. Take in a Dataset as well as a list of class names\ndef display_random_images(dataset: torch.utils.data.dataset.Dataset,\n                          classes: List[str] = None,\n                          n: int = 10,\n                          display_shape: bool = True,\n                          seed: int = None):\n    \n    # 2. Adjust display if n too high\n    if n &gt; 10:\n        n = 10\n        display_shape = False\n        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n    \n    # 3. Set random seed\n    if seed:\n        random.seed(seed)\n\n    # 4. Get random sample indexes\n    random_samples_idx = random.sample(range(len(dataset)), k=n)\n\n    # 5. Setup plot\n    plt.figure(figsize=(16, 8))\n\n    # 6. Loop through samples and display random samples \n    for i, targ_sample in enumerate(random_samples_idx):\n        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n\n        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]\n        targ_image_adjust = targ_image.permute(1, 2, 0)\n\n        # Plot adjusted samples\n        plt.subplot(1, n, i+1)\n        plt.imshow(targ_image_adjust)\n        plt.axis(\"off\")\n        if classes:\n            title = f\"class: {classes[targ_label]}\"\n            if display_shape:\n                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n        plt.title(title)\n</pre> # 1. Take in a Dataset as well as a list of class names def display_random_images(dataset: torch.utils.data.dataset.Dataset,                           classes: List[str] = None,                           n: int = 10,                           display_shape: bool = True,                           seed: int = None):          # 2. Adjust display if n too high     if n &gt; 10:         n = 10         display_shape = False         print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")          # 3. Set random seed     if seed:         random.seed(seed)      # 4. Get random sample indexes     random_samples_idx = random.sample(range(len(dataset)), k=n)      # 5. Setup plot     plt.figure(figsize=(16, 8))      # 6. Loop through samples and display random samples      for i, targ_sample in enumerate(random_samples_idx):         targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]          # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]         targ_image_adjust = targ_image.permute(1, 2, 0)          # Plot adjusted samples         plt.subplot(1, n, i+1)         plt.imshow(targ_image_adjust)         plt.axis(\"off\")         if classes:             title = f\"class: {classes[targ_label]}\"             if display_shape:                 title = title + f\"\\nshape: {targ_image_adjust.shape}\"         plt.title(title) <p>What a good looking function!</p> <p>Let's test it out first with the <code>Dataset</code> we created with <code>torchvision.datasets.ImageFolder()</code>.</p> In\u00a0[33]: Copied! <pre># Display random images from ImageFolder created Dataset\ndisplay_random_images(train_data, \n                      n=5, \n                      classes=class_names,\n                      seed=None)\n</pre> # Display random images from ImageFolder created Dataset display_random_images(train_data,                        n=5,                        classes=class_names,                       seed=None) <p>And now with the <code>Dataset</code> we created with our own <code>ImageFolderCustom</code>.</p> In\u00a0[34]: Copied! <pre># Display random images from ImageFolderCustom Dataset\ndisplay_random_images(train_data_custom, \n                      n=12, \n                      classes=class_names,\n                      seed=None) # Try setting the seed for reproducible images\n</pre> # Display random images from ImageFolderCustom Dataset display_random_images(train_data_custom,                        n=12,                        classes=class_names,                       seed=None) # Try setting the seed for reproducible images <pre>For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\n</pre> <p>Nice!!!</p> <p>Looks like our <code>ImageFolderCustom</code> is working just as we'd like it to.</p> In\u00a0[35]: Copied! <pre># Turn train and test custom Dataset's into DataLoader's\nfrom torch.utils.data import DataLoader\ntrain_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n                                     batch_size=1, # how many samples per batch?\n                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n                                     shuffle=True) # shuffle the data?\n\ntest_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader_custom, test_dataloader_custom\n</pre> # Turn train and test custom Dataset's into DataLoader's from torch.utils.data import DataLoader train_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset                                      batch_size=1, # how many samples per batch?                                      num_workers=0, # how many subprocesses to use for data loading? (higher = more)                                      shuffle=True) # shuffle the data?  test_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset                                     batch_size=1,                                      num_workers=0,                                      shuffle=False) # don't usually need to shuffle testing data  train_dataloader_custom, test_dataloader_custom Out[35]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fae86b5b940&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fae86b5bfa0&gt;)</pre> <p>Do the shapes of the samples look the same?</p> In\u00a0[36]: Copied! <pre># Get image and label from custom DataLoader\nimg_custom, label_custom = next(iter(train_dataloader_custom))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label_custom.shape}\")\n</pre> # Get image and label from custom DataLoader img_custom, label_custom = next(iter(train_dataloader_custom))  # Batch size will now be 1, try changing the batch_size parameter above and see what happens print(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Label shape: {label_custom.shape}\") <pre>Image shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n</pre> <p>They sure do!</p> <p>Let's now take a lot at some other forms of data transforms.</p> In\u00a0[37]: Copied! <pre>from torchvision import transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense \n    transforms.ToTensor() # use ToTensor() last to get everything between 0 &amp; 1\n])\n\n# Don't need to perform augmentation on the test data\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.ToTensor()\n])\n</pre> from torchvision import transforms  train_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense      transforms.ToTensor() # use ToTensor() last to get everything between 0 &amp; 1 ])  # Don't need to perform augmentation on the test data test_transforms = transforms.Compose([     transforms.Resize((224, 224)),      transforms.ToTensor() ]) <p>Note: You usually don't perform data augmentation on the test set. The idea of data augmentation is to to artificially increase the diversity of the training set to better predict on the testing set.</p> <p>However, you do need to make sure your test set images are transformed to tensors. We size the test images to the same size as our training images too, however, inference can be done on different size images if necessary (though this may alter performance).</p> <p>Beautiful, now we've got a training transform (with data augmentation) and test transform (without data augmentation).</p> <p>Let's test our data augmentation out!</p> In\u00a0[38]: Copied! <pre># Get all image paths\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# Plot random images\nplot_transformed_images(\n    image_paths=image_path_list,\n    transform=train_transforms,\n    n=3,\n    seed=None\n)\n</pre> # Get all image paths image_path_list = list(image_path.glob(\"*/*/*.jpg\"))  # Plot random images plot_transformed_images(     image_paths=image_path_list,     transform=train_transforms,     n=3,     seed=None ) <p>Try running the cell above a few times and seeing how the original image changes as it goes through the transform.</p> In\u00a0[39]: Copied! <pre># Create simple transform\nsimple_transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\n</pre> # Create simple transform simple_transform = transforms.Compose([      transforms.Resize((64, 64)),     transforms.ToTensor(), ]) <p>Excellent, now we've got a simple transform, let's:</p> <ol> <li>Load the data, turning each of our training and test folders first into a <code>Dataset</code> with <code>torchvision.datasets.ImageFolder()</code></li> <li>Then into a <code>DataLoader</code> using <code>torch.utils.data.DataLoader()</code>.<ul> <li>We'll set the <code>batch_size=32</code> and <code>num_workers</code> to as many CPUs on our machine (this will depend on what machine you're using).</li> </ul> </li> </ol> In\u00a0[40]: Copied! <pre># 1. Load and transform data\nfrom torchvision import datasets\ntrain_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\ntest_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n\n# 2. Turn data into DataLoaders\nimport os\nfrom torch.utils.data import DataLoader\n\n# Setup batch size and number of workers \nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\nprint(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n\n# Create DataLoader's\ntrain_dataloader_simple = DataLoader(train_data_simple, \n                                     batch_size=BATCH_SIZE, \n                                     shuffle=True, \n                                     num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_simple, test_dataloader_simple\n</pre> # 1. Load and transform data from torchvision import datasets train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform) test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)  # 2. Turn data into DataLoaders import os from torch.utils.data import DataLoader  # Setup batch size and number of workers  BATCH_SIZE = 32 NUM_WORKERS = os.cpu_count() print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")  # Create DataLoader's train_dataloader_simple = DataLoader(train_data_simple,                                       batch_size=BATCH_SIZE,                                       shuffle=True,                                       num_workers=NUM_WORKERS)  test_dataloader_simple = DataLoader(test_data_simple,                                      batch_size=BATCH_SIZE,                                      shuffle=False,                                      num_workers=NUM_WORKERS)  train_dataloader_simple, test_dataloader_simple <pre>Creating DataLoader's with batch size 32 and 32 workers.\n</pre> Out[40]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fae877121f0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fae875dbb80&gt;)</pre> <p><code>DataLoader</code>'s created!</p> <p>Let's build a model.</p> In\u00a0[41]: Copied! <pre>class TinyVGG(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*16*16,\n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block_1(x)\n        # print(x.shape)\n        x = self.conv_block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion\n\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0\n</pre> class TinyVGG(nn.Module):     \"\"\"     Model architecture copying TinyVGG from:      https://poloclub.github.io/cnn-explainer/     \"\"\"     def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:         super().__init__()         self.conv_block_1 = nn.Sequential(             nn.Conv2d(in_channels=input_shape,                        out_channels=hidden_units,                        kernel_size=3, # how big is the square that's going over the image?                       stride=1, # default                       padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number              nn.ReLU(),             nn.Conv2d(in_channels=hidden_units,                        out_channels=hidden_units,                       kernel_size=3,                       stride=1,                       padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2,                          stride=2) # default stride value is same as kernel_size         )         self.conv_block_2 = nn.Sequential(             nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),             nn.ReLU(),             nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),             nn.ReLU(),             nn.MaxPool2d(2)         )         self.classifier = nn.Sequential(             nn.Flatten(),             # Where did this in_features shape come from?              # It's because each layer of our network compresses and changes the shape of our inputs data.             nn.Linear(in_features=hidden_units*16*16,                       out_features=output_shape)         )          def forward(self, x: torch.Tensor):         x = self.conv_block_1(x)         # print(x.shape)         x = self.conv_block_2(x)         # print(x.shape)         x = self.classifier(x)         # print(x.shape)         return x         # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion  torch.manual_seed(42) model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)                    hidden_units=10,                    output_shape=len(train_data.classes)).to(device) model_0 Out[41]: <pre>TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)</pre> <p>Note: One of the ways to speed up deep learning models computing on a GPU is to leverage operator fusion.</p> <p>This means in the <code>forward()</code> method in our model above, instead of calling a layer block and reassigning <code>x</code> every time, we call each block in succession (see the final line of the <code>forward()</code> method in the model above for an example).</p> <p>This saves the time spent reassigning <code>x</code> (memory heavy) and focuses on only computing on <code>x</code>.</p> <p>See Making Deep Learning Go Brrrr From First Principles by Horace He for more ways on how to speed up machine learning models.</p> <p>Now that's a nice looking model!</p> <p>How about we test it out with a forward pass on a single image?</p> In\u00a0[42]: Copied! <pre># 1. Get a batch of images and labels from the DataLoader\nimg_batch, label_batch = next(iter(train_dataloader_simple))\n\n# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Perform a forward pass on a single image\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n    \n# 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")\n</pre> # 1. Get a batch of images and labels from the DataLoader img_batch, label_batch = next(iter(train_dataloader_simple))  # 2. Get a single image from the batch and unsqueeze the image so its shape fits the model img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0] print(f\"Single image shape: {img_single.shape}\\n\")  # 3. Perform a forward pass on a single image model_0.eval() with torch.inference_mode():     pred = model_0(img_single.to(device))      # 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label print(f\"Output logits:\\n{pred}\\n\") print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\") print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\") print(f\"Actual label:\\n{label_single}\") <pre>Single image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[0.0578, 0.0634, 0.0351]], device='cuda:0')\n\nOutput prediction probabilities:\ntensor([[0.3352, 0.3371, 0.3277]], device='cuda:0')\n\nOutput prediction label:\ntensor([1], device='cuda:0')\n\nActual label:\n2\n</pre> <p>Wonderful, it looks like our model is outputting what we'd expect it to output.</p> <p>You can run the cell above a few times and each time have a different image be predicted on.</p> <p>And you'll probably notice the predictions are often wrong.</p> <p>This is to be expected because the model hasn't been trained yet and it's essentially guessing using random weights.</p> In\u00a0[43]: Copied! <pre># Install torchinfo if it's not available, import it if it is\ntry: \n    import torchinfo\nexcept:\n    !pip install torchinfo\n    import torchinfo\n    \nfrom torchinfo import summary\nsummary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size \n</pre> # Install torchinfo if it's not available, import it if it is try:      import torchinfo except:     !pip install torchinfo     import torchinfo      from torchinfo import summary summary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size  <pre>Collecting torchinfo\n  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\nDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-1.8.0\n</pre> Out[43]: <pre>==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTinyVGG                                  [1, 3]                    --\n\u251c\u2500Sequential: 1-1                        [1, 10, 32, 32]           --\n\u2502    \u2514\u2500Conv2d: 2-1                       [1, 10, 64, 64]           280\n\u2502    \u2514\u2500ReLU: 2-2                         [1, 10, 64, 64]           --\n\u2502    \u2514\u2500Conv2d: 2-3                       [1, 10, 64, 64]           910\n\u2502    \u2514\u2500ReLU: 2-4                         [1, 10, 64, 64]           --\n\u2502    \u2514\u2500MaxPool2d: 2-5                    [1, 10, 32, 32]           --\n\u251c\u2500Sequential: 1-2                        [1, 10, 16, 16]           --\n\u2502    \u2514\u2500Conv2d: 2-6                       [1, 10, 32, 32]           910\n\u2502    \u2514\u2500ReLU: 2-7                         [1, 10, 32, 32]           --\n\u2502    \u2514\u2500Conv2d: 2-8                       [1, 10, 32, 32]           910\n\u2502    \u2514\u2500ReLU: 2-9                         [1, 10, 32, 32]           --\n\u2502    \u2514\u2500MaxPool2d: 2-10                   [1, 10, 16, 16]           --\n\u251c\u2500Sequential: 1-3                        [1, 3]                    --\n\u2502    \u2514\u2500Flatten: 2-11                     [1, 2560]                 --\n\u2502    \u2514\u2500Linear: 2-12                      [1, 3]                    7,683\n==========================================================================================\nTotal params: 10,693\nTrainable params: 10,693\nNon-trainable params: 0\nTotal mult-adds (M): 6.75\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.82\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.91\n==========================================================================================</pre> <p>Nice!</p> <p>The output of <code>torchinfo.summary()</code> gives us a whole bunch of information about our model.</p> <p>Such as <code>Total params</code>, the total number of parameters in our model, the <code>Estimated Total Size (MB)</code> which is the size of our model.</p> <p>You can also see the change in input and output shapes as data of a certain <code>input_size</code> moves through our model.</p> <p>Right now, our parameter numbers and total model size is low.</p> <p>This because we're starting with a small model.</p> <p>And if we need to increase its size later, we can.</p> In\u00a0[44]: Copied! <pre>def train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer):\n    # Put model in train mode\n    model.train()\n    \n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n    \n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n</pre> def train_step(model: torch.nn.Module,                 dataloader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 optimizer: torch.optim.Optimizer):     # Put model in train mode     model.train()          # Setup train loss and train accuracy values     train_loss, train_acc = 0, 0          # Loop through data loader data batches     for batch, (X, y) in enumerate(dataloader):         # Send data to target device         X, y = X.to(device), y.to(device)          # 1. Forward pass         y_pred = model(X)          # 2. Calculate  and accumulate loss         loss = loss_fn(y_pred, y)         train_loss += loss.item()           # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()          # Calculate and accumulate accuracy metric across all batches         y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)         train_acc += (y_pred_class == y).sum().item()/len(y_pred)      # Adjust metrics to get average loss and accuracy per batch      train_loss = train_loss / len(dataloader)     train_acc = train_acc / len(dataloader)     return train_loss, train_acc <p>Woohoo! <code>train_step()</code> function done.</p> <p>Now let's do the same for the <code>test_step()</code> function.</p> <p>The main difference here will be the <code>test_step()</code> won't take in an optimizer and therefore won't perform gradient descent.</p> <p>But since we'll be doing inference, we'll make sure to turn on the <code>torch.inference_mode()</code> context manager for making predictions.</p> In\u00a0[45]: Copied! <pre>def test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module):\n    # Put model in eval mode\n    model.eval() \n    \n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n    \n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n    \n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n            \n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n            \n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc\n</pre> def test_step(model: torch.nn.Module,                dataloader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module):     # Put model in eval mode     model.eval()           # Setup test loss and test accuracy values     test_loss, test_acc = 0, 0          # Turn on inference context manager     with torch.inference_mode():         # Loop through DataLoader batches         for batch, (X, y) in enumerate(dataloader):             # Send data to target device             X, y = X.to(device), y.to(device)                  # 1. Forward pass             test_pred_logits = model(X)              # 2. Calculate and accumulate loss             loss = loss_fn(test_pred_logits, y)             test_loss += loss.item()                          # Calculate and accumulate accuracy             test_pred_labels = test_pred_logits.argmax(dim=1)             test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))                  # Adjust metrics to get average loss and accuracy per batch      test_loss = test_loss / len(dataloader)     test_acc = test_acc / len(dataloader)     return test_loss, test_acc <p>Excellent!</p> In\u00a0[46]: Copied! <pre>from tqdm.auto import tqdm\n\n# 1. Take in various parameters required for training and test steps\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n          epochs: int = 5):\n    \n    # 2. Create empty results dictionary\n    results = {\"train_loss\": [],\n        \"train_acc\": [],\n        \"test_loss\": [],\n        \"test_acc\": []\n    }\n    \n    # 3. Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer)\n        test_loss, test_acc = test_step(model=model,\n            dataloader=test_dataloader,\n            loss_fn=loss_fn)\n        \n        # 4. Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # 5. Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # 6. Return the filled results at the end of the epochs\n    return results\n</pre> from tqdm.auto import tqdm  # 1. Take in various parameters required for training and test steps def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),           epochs: int = 5):          # 2. Create empty results dictionary     results = {\"train_loss\": [],         \"train_acc\": [],         \"test_loss\": [],         \"test_acc\": []     }          # 3. Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                            dataloader=train_dataloader,                                            loss_fn=loss_fn,                                            optimizer=optimizer)         test_loss, test_acc = test_step(model=model,             dataloader=test_dataloader,             loss_fn=loss_fn)                  # 4. Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # 5. Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)      # 6. Return the filled results at the end of the epochs     return results In\u00a0[47]: Copied! <pre># Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Recreate an instance of TinyVGG\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_0 \nmodel_0_results = train(model=model_0, \n                        train_dataloader=train_dataloader_simple,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set random seeds torch.manual_seed(42)  torch.cuda.manual_seed(42)  # Set number of epochs NUM_EPOCHS = 5  # Recreate an instance of TinyVGG model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)                    hidden_units=10,                    output_shape=len(train_data.classes)).to(device)  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Train model_0  model_0_results = train(model=model_0,                          train_dataloader=train_dataloader_simple,                         test_dataloader=test_dataloader_simple,                         optimizer=optimizer,                         loss_fn=loss_fn,                          epochs=NUM_EPOCHS)  # End the timer and print out how long it took end_time = timer() print(f\"Total training time: {end_time-start_time:.3f} seconds\") <pre> 20%|\u2588\u2588        | 1/5 [00:01&lt;00:06,  1.59s/it]</pre> <pre>Epoch: 1 | train_loss: 1.1078 | train_acc: 0.2578 | test_loss: 1.1362 | test_acc: 0.2604\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:03&lt;00:04,  1.56s/it]</pre> <pre>Epoch: 2 | train_loss: 1.0846 | train_acc: 0.4258 | test_loss: 1.1622 | test_acc: 0.1979\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:04&lt;00:03,  1.57s/it]</pre> <pre>Epoch: 3 | train_loss: 1.1153 | train_acc: 0.2930 | test_loss: 1.1695 | test_acc: 0.1979\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:06&lt;00:01,  1.53s/it]</pre> <pre>Epoch: 4 | train_loss: 1.0990 | train_acc: 0.2891 | test_loss: 1.1341 | test_acc: 0.1979\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:07&lt;00:00,  1.53s/it]</pre> <pre>Epoch: 5 | train_loss: 1.0989 | train_acc: 0.2930 | test_loss: 1.1434 | test_acc: 0.1979\nTotal training time: 7.671 seconds\n</pre> <pre>\n</pre> <p>Hmm...</p> <p>It looks like our model performed pretty poorly.</p> <p>But that's okay for now, we'll keep persevering.</p> <p>What are some ways you could potentially improve it?</p> <p>Note: Check out the Improving a model (from a model perspective) section in notebook 02 for ideas on improving our TinyVGG model.</p> In\u00a0[48]: Copied! <pre># Check the model_0_results keys\nmodel_0_results.keys()\n</pre> # Check the model_0_results keys model_0_results.keys() Out[48]: <pre>dict_keys(['train_loss', 'train_acc', 'test_loss', 'test_acc'])</pre> <p>We'll need to extract each of these keys and turn them into a plot.</p> In\u00a0[49]: Copied! <pre>def plot_loss_curves(results: Dict[str, List[float]]):\n    \"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    \n    # Get the loss values of the results dictionary (training and test)\n    loss = results['train_loss']\n    test_loss = results['test_loss']\n\n    # Get the accuracy values of the results dictionary (training and test)\n    accuracy = results['train_acc']\n    test_accuracy = results['test_acc']\n\n    # Figure out how many epochs there were\n    epochs = range(len(results['train_loss']))\n\n    # Setup a plot \n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='train_loss')\n    plt.plot(epochs, test_loss, label='test_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label='train_accuracy')\n    plt.plot(epochs, test_accuracy, label='test_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();\n</pre> def plot_loss_curves(results: Dict[str, List[float]]):     \"\"\"Plots training curves of a results dictionary.      Args:         results (dict): dictionary containing list of values, e.g.             {\"train_loss\": [...],              \"train_acc\": [...],              \"test_loss\": [...],              \"test_acc\": [...]}     \"\"\"          # Get the loss values of the results dictionary (training and test)     loss = results['train_loss']     test_loss = results['test_loss']      # Get the accuracy values of the results dictionary (training and test)     accuracy = results['train_acc']     test_accuracy = results['test_acc']      # Figure out how many epochs there were     epochs = range(len(results['train_loss']))      # Setup a plot      plt.figure(figsize=(15, 7))      # Plot loss     plt.subplot(1, 2, 1)     plt.plot(epochs, loss, label='train_loss')     plt.plot(epochs, test_loss, label='test_loss')     plt.title('Loss')     plt.xlabel('Epochs')     plt.legend()      # Plot accuracy     plt.subplot(1, 2, 2)     plt.plot(epochs, accuracy, label='train_accuracy')     plt.plot(epochs, test_accuracy, label='test_accuracy')     plt.title('Accuracy')     plt.xlabel('Epochs')     plt.legend(); <p>Okay, let's test our <code>plot_loss_curves()</code> function out.</p> In\u00a0[50]: Copied! <pre>plot_loss_curves(model_0_results)\n</pre> plot_loss_curves(model_0_results) <p>Woah.</p> <p>Looks like things are all over the place...</p> <p>But we kind of knew that because our model's print out results during training didn't show much promise.</p> <p>You could try training the model for longer and see what happens when you plot a loss curve over a longer time horizon.</p> In\u00a0[51]: Copied! <pre># Create training transform with TrivialAugment\ntrain_transform_trivial_augment = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n    transforms.ToTensor() \n])\n\n# Create testing transform (no data augmentation)\ntest_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n</pre> # Create training transform with TrivialAugment train_transform_trivial_augment = transforms.Compose([     transforms.Resize((64, 64)),     transforms.TrivialAugmentWide(num_magnitude_bins=31),     transforms.ToTensor()  ])  # Create testing transform (no data augmentation) test_transform = transforms.Compose([     transforms.Resize((64, 64)),     transforms.ToTensor() ]) <p>Wonderful!</p> <p>Now let's turn our images into <code>Dataset</code>'s using <code>torchvision.datasets.ImageFolder()</code> and then into <code>DataLoader</code>'s with <code>torch.utils.data.DataLoader()</code>.</p> In\u00a0[52]: Copied! <pre># Turn image folders into Datasets\ntrain_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment)\ntest_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)\n\ntrain_data_augmented, test_data_simple\n</pre> # Turn image folders into Datasets train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment) test_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)  train_data_augmented, test_data_simple Out[52]: <pre>(Dataset ImageFolder\n     Number of datapoints: 225\n     Root location: data/pizza_steak_sushi/train\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n                TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n                ToTensor()\n            ),\n Dataset ImageFolder\n     Number of datapoints: 75\n     Root location: data/pizza_steak_sushi/test\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n                ToTensor()\n            ))</pre> <p>And we'll make <code>DataLoader</code>'s with a <code>batch_size=32</code> and with <code>num_workers</code> set to the number of CPUs available on our machine (we can get this using Python's <code>os.cpu_count()</code>).</p> In\u00a0[53]: Copied! <pre># Turn Datasets into DataLoader's\nimport os\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\n\ntorch.manual_seed(42)\ntrain_dataloader_augmented = DataLoader(train_data_augmented, \n                                        batch_size=BATCH_SIZE, \n                                        shuffle=True,\n                                        num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_augmented, test_dataloader\n</pre> # Turn Datasets into DataLoader's import os BATCH_SIZE = 32 NUM_WORKERS = os.cpu_count()  torch.manual_seed(42) train_dataloader_augmented = DataLoader(train_data_augmented,                                          batch_size=BATCH_SIZE,                                          shuffle=True,                                         num_workers=NUM_WORKERS)  test_dataloader_simple = DataLoader(test_data_simple,                                      batch_size=BATCH_SIZE,                                      shuffle=False,                                      num_workers=NUM_WORKERS)  train_dataloader_augmented, test_dataloader Out[53]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fae84165fa0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fae875e2e50&gt;)</pre> In\u00a0[54]: Copied! <pre># Create model_1 and send it to the target device\ntorch.manual_seed(42)\nmodel_1 = TinyVGG(\n    input_shape=3,\n    hidden_units=10,\n    output_shape=len(train_data_augmented.classes)).to(device)\nmodel_1\n</pre> # Create model_1 and send it to the target device torch.manual_seed(42) model_1 = TinyVGG(     input_shape=3,     hidden_units=10,     output_shape=len(train_data_augmented.classes)).to(device) model_1 Out[54]: <pre>TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)</pre> <p>Model ready!</p> <p>Time to train!</p> <p>Since we've already got functions for the training loop (<code>train_step()</code>) and testing loop (<code>test_step()</code>) and a function to put them together in <code>train()</code>, let's reuse those.</p> <p>We'll use the same setup as <code>model_0</code> with only the <code>train_dataloader</code> parameter varying:</p> <ul> <li>Train for 5 epochs.</li> <li>Use <code>train_dataloader=train_dataloader_augmented</code> as the training data in <code>train()</code>.</li> <li>Use <code>torch.nn.CrossEntropyLoss()</code> as the loss function (since we're working with multi-class classification).</li> <li>Use <code>torch.optim.Adam()</code> with <code>lr=0.001</code> as the learning rate as the optimizer.</li> </ul> In\u00a0[55]: Copied! <pre># Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_1\nmodel_1_results = train(model=model_1, \n                        train_dataloader=train_dataloader_augmented,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set random seeds torch.manual_seed(42)  torch.cuda.manual_seed(42)  # Set number of epochs NUM_EPOCHS = 5  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Train model_1 model_1_results = train(model=model_1,                          train_dataloader=train_dataloader_augmented,                         test_dataloader=test_dataloader_simple,                         optimizer=optimizer,                         loss_fn=loss_fn,                          epochs=NUM_EPOCHS)  # End the timer and print out how long it took end_time = timer() print(f\"Total training time: {end_time-start_time:.3f} seconds\") <pre> 20%|\u2588\u2588        | 1/5 [00:01&lt;00:05,  1.46s/it]</pre> <pre>Epoch: 1 | train_loss: 1.1074 | train_acc: 0.2500 | test_loss: 1.1059 | test_acc: 0.2604\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:03&lt;00:04,  1.52s/it]</pre> <pre>Epoch: 2 | train_loss: 1.0790 | train_acc: 0.4258 | test_loss: 1.1384 | test_acc: 0.2604\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:04&lt;00:03,  1.51s/it]</pre> <pre>Epoch: 3 | train_loss: 1.0802 | train_acc: 0.4258 | test_loss: 1.1694 | test_acc: 0.2604\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:05&lt;00:01,  1.47s/it]</pre> <pre>Epoch: 4 | train_loss: 1.1286 | train_acc: 0.3047 | test_loss: 1.1621 | test_acc: 0.2604\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:07&lt;00:00,  1.51s/it]</pre> <pre>Epoch: 5 | train_loss: 1.0884 | train_acc: 0.4258 | test_loss: 1.1477 | test_acc: 0.2604\nTotal training time: 7.551 seconds\n</pre> <pre>\n</pre> <p>Hmm...</p> <p>It doesn't look like our model performed very well again.</p> <p>Let's check out its loss curves.</p> In\u00a0[56]: Copied! <pre>plot_loss_curves(model_1_results)\n</pre> plot_loss_curves(model_1_results) <p>Wow...</p> <p>These don't look very good either...</p> <p>Is our model underfitting or overfitting?</p> <p>Or both?</p> <p>Ideally we'd like it have higher accuracy and lower loss right?</p> <p>What are some methods you could try to use to achieve these?</p> In\u00a0[57]: Copied! <pre>import pandas as pd\nmodel_0_df = pd.DataFrame(model_0_results)\nmodel_1_df = pd.DataFrame(model_1_results)\nmodel_0_df\n</pre> import pandas as pd model_0_df = pd.DataFrame(model_0_results) model_1_df = pd.DataFrame(model_1_results) model_0_df Out[57]: train_loss train_acc test_loss test_acc 0 1.107836 0.257812 1.136208 0.260417 1 1.084645 0.425781 1.162170 0.197917 2 1.115274 0.292969 1.169458 0.197917 3 1.099046 0.289062 1.134122 0.197917 4 1.098948 0.292969 1.143407 0.197917 <p>And now we can write some plotting code using <code>matplotlib</code> to visualize the results of <code>model_0</code> and <code>model_1</code> together.</p> In\u00a0[58]: Copied! <pre># Setup a plot \nplt.figure(figsize=(15, 10))\n\n# Get number of epochs\nepochs = range(len(model_0_df))\n\n# Plot train loss\nplt.subplot(2, 2, 1)\nplt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test loss\nplt.subplot(2, 2, 2)\nplt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot train accuracy\nplt.subplot(2, 2, 3)\nplt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\nplt.title(\"Train Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test accuracy\nplt.subplot(2, 2, 4)\nplt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\nplt.title(\"Test Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</pre> # Setup a plot  plt.figure(figsize=(15, 10))  # Get number of epochs epochs = range(len(model_0_df))  # Plot train loss plt.subplot(2, 2, 1) plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\") plt.title(\"Train Loss\") plt.xlabel(\"Epochs\") plt.legend()  # Plot test loss plt.subplot(2, 2, 2) plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\") plt.title(\"Test Loss\") plt.xlabel(\"Epochs\") plt.legend()  # Plot train accuracy plt.subplot(2, 2, 3) plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\") plt.title(\"Train Accuracy\") plt.xlabel(\"Epochs\") plt.legend()  # Plot test accuracy plt.subplot(2, 2, 4) plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\") plt.title(\"Test Accuracy\") plt.xlabel(\"Epochs\") plt.legend(); <p>It looks like our models both performed equally poorly and were kind of sporadic (the metrics go up and down sharply).</p> <p>If you built <code>model_2</code>, what would you do differently to try and improve performance?</p> In\u00a0[59]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = data_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\") <pre>Downloading data/04-pizza-dad.jpeg...\n</pre> In\u00a0[60]: Copied! <pre>import torchvision\n\n# Read in custom image\ncustom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\nprint(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n</pre> import torchvision  # Read in custom image custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))  # Print out image data print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\") print(f\"Custom image shape: {custom_image_uint8.shape}\\n\") print(f\"Custom image dtype: {custom_image_uint8.dtype}\") <pre>Custom image tensor:\ntensor([[[154, 173, 181,  ...,  21,  18,  14],\n         [146, 165, 181,  ...,  21,  18,  15],\n         [124, 146, 172,  ...,  18,  17,  15],\n         ...,\n         [ 72,  59,  45,  ..., 152, 150, 148],\n         [ 64,  55,  41,  ..., 150, 147, 144],\n         [ 64,  60,  46,  ..., 149, 146, 143]],\n\n        [[171, 190, 193,  ...,  22,  19,  15],\n         [163, 182, 193,  ...,  22,  19,  16],\n         [141, 163, 184,  ...,  19,  18,  16],\n         ...,\n         [ 55,  42,  28,  ..., 107, 104, 103],\n         [ 47,  38,  24,  ..., 108, 104, 102],\n         [ 47,  43,  29,  ..., 107, 104, 101]],\n\n        [[119, 138, 147,  ...,  17,  14,  10],\n         [111, 130, 145,  ...,  17,  14,  11],\n         [ 87, 111, 136,  ...,  14,  13,  11],\n         ...,\n         [ 35,  22,   8,  ...,  52,  52,  48],\n         [ 27,  18,   4,  ...,  50,  49,  44],\n         [ 27,  23,   9,  ...,  49,  46,  43]]], dtype=torch.uint8)\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.uint8\n</pre> <p>Nice! Looks like our image is in tensor format, however, is this image format compatible with our model?</p> <p>Our <code>custom_image</code> tensor is of datatype <code>torch.uint8</code> and its values are between <code>[0, 255]</code>.</p> <p>But our model takes image tensors of datatype <code>torch.float32</code> and with values between <code>[0, 1]</code>.</p> <p>So before we use our custom image with our model, we'll need to convert it to the same format as the data our model is trained on.</p> <p>If we don't do this, our model will error.</p> In\u00a0[61]: Copied! <pre># Try to make a prediction on image in uint8 format (this will error)\nmodel_1.eval()\nwith torch.inference_mode():\n    model_1(custom_image_uint8.to(device))\n</pre> # Try to make a prediction on image in uint8 format (this will error) model_1.eval() with torch.inference_mode():     model_1(custom_image_uint8.to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[61], line 4\n      2 model_1.eval()\n      3 with torch.inference_mode():\n----&gt; 4     model_1(custom_image_uint8.to(device))\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nCell In[41], line 40, in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/container.py:217, in Sequential.forward(self, input)\n    215 def forward(self, input):\n    216     for module in self:\n--&gt; 217         input = module(input)\n    218     return input\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/conv.py:460, in Conv2d.forward(self, input)\n    459 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 460     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/conv.py:456, in Conv2d._conv_forward(self, input, weight, bias)\n    452 if self.padding_mode != 'zeros':\n    453     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    454                     weight, bias, self.stride,\n    455                     _pair(0), self.dilation, self.groups)\n--&gt; 456 return F.conv2d(input, weight, bias, self.stride,\n    457                 self.padding, self.dilation, self.groups)\n\nRuntimeError: Input type (unsigned char) and bias type (float) should be the same</pre> <p>If we try to make a prediction on an image in a different datatype to what our model was trained on, we get an error like the following:</p> <p><code>RuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same</code></p> <p>Let's fix this by converting our custom image to the same datatype as what our model was trained on (<code>torch.float32</code>).</p> In\u00a0[62]: Copied! <pre># Load in custom image and convert the tensor values to float32\ncustom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n\n# Divide the image pixel values by 255 to get them between [0, 1]\ncustom_image = custom_image / 255. \n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image}\\n\")\nprint(f\"Custom image shape: {custom_image.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image.dtype}\")\n</pre> # Load in custom image and convert the tensor values to float32 custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)  # Divide the image pixel values by 255 to get them between [0, 1] custom_image = custom_image / 255.   # Print out image data print(f\"Custom image tensor:\\n{custom_image}\\n\") print(f\"Custom image shape: {custom_image.shape}\\n\") print(f\"Custom image dtype: {custom_image.dtype}\") <pre>Custom image tensor:\ntensor([[[0.6039, 0.6784, 0.7098,  ..., 0.0824, 0.0706, 0.0549],\n         [0.5725, 0.6471, 0.7098,  ..., 0.0824, 0.0706, 0.0588],\n         [0.4863, 0.5725, 0.6745,  ..., 0.0706, 0.0667, 0.0588],\n         ...,\n         [0.2824, 0.2314, 0.1765,  ..., 0.5961, 0.5882, 0.5804],\n         [0.2510, 0.2157, 0.1608,  ..., 0.5882, 0.5765, 0.5647],\n         [0.2510, 0.2353, 0.1804,  ..., 0.5843, 0.5725, 0.5608]],\n\n        [[0.6706, 0.7451, 0.7569,  ..., 0.0863, 0.0745, 0.0588],\n         [0.6392, 0.7137, 0.7569,  ..., 0.0863, 0.0745, 0.0627],\n         [0.5529, 0.6392, 0.7216,  ..., 0.0745, 0.0706, 0.0627],\n         ...,\n         [0.2157, 0.1647, 0.1098,  ..., 0.4196, 0.4078, 0.4039],\n         [0.1843, 0.1490, 0.0941,  ..., 0.4235, 0.4078, 0.4000],\n         [0.1843, 0.1686, 0.1137,  ..., 0.4196, 0.4078, 0.3961]],\n\n        [[0.4667, 0.5412, 0.5765,  ..., 0.0667, 0.0549, 0.0392],\n         [0.4353, 0.5098, 0.5686,  ..., 0.0667, 0.0549, 0.0431],\n         [0.3412, 0.4353, 0.5333,  ..., 0.0549, 0.0510, 0.0431],\n         ...,\n         [0.1373, 0.0863, 0.0314,  ..., 0.2039, 0.2039, 0.1882],\n         [0.1059, 0.0706, 0.0157,  ..., 0.1961, 0.1922, 0.1725],\n         [0.1059, 0.0902, 0.0353,  ..., 0.1922, 0.1804, 0.1686]]])\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.float32\n</pre> In\u00a0[63]: Copied! <pre># Plot custom image\nplt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error\nplt.title(f\"Image shape: {custom_image.shape}\")\nplt.axis(False);\n</pre> # Plot custom image plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error plt.title(f\"Image shape: {custom_image.shape}\") plt.axis(False); <p>Two thumbs up!</p> <p>Now how could we get our image to be the same size as the images our model was trained on?</p> <p>One way to do so is with <code>torchvision.transforms.Resize()</code>.</p> <p>Let's compose a transform pipeline to do so.</p> In\u00a0[64]: Copied! <pre># Create transform pipleine to resize image\ncustom_image_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n])\n\n# Transform target image\ncustom_image_transformed = custom_image_transform(custom_image)\n\n# Print out original shape and new shape\nprint(f\"Original shape: {custom_image.shape}\")\nprint(f\"New shape: {custom_image_transformed.shape}\")\n</pre> # Create transform pipleine to resize image custom_image_transform = transforms.Compose([     transforms.Resize((64, 64)), ])  # Transform target image custom_image_transformed = custom_image_transform(custom_image)  # Print out original shape and new shape print(f\"Original shape: {custom_image.shape}\") print(f\"New shape: {custom_image_transformed.shape}\") <pre>Original shape: torch.Size([3, 4032, 3024])\nNew shape: torch.Size([3, 64, 64])\n</pre> <p>Woohoo!</p> <p>Let's finally make a prediction on our own custom image.</p> In\u00a0[65]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed)\n</pre> model_1.eval() with torch.inference_mode():     custom_image_pred = model_1(custom_image_transformed) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[65], line 3\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nCell In[41], line 40, in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/container.py:217, in Sequential.forward(self, input)\n    215 def forward(self, input):\n    216     for module in self:\n--&gt; 217         input = module(input)\n    218     return input\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/conv.py:460, in Conv2d.forward(self, input)\n    459 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 460     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/conv.py:456, in Conv2d._conv_forward(self, input, weight, bias)\n    452 if self.padding_mode != 'zeros':\n    453     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    454                     weight, bias, self.stride,\n    455                     _pair(0), self.dilation, self.groups)\n--&gt; 456 return F.conv2d(input, weight, bias, self.stride,\n    457                 self.padding, self.dilation, self.groups)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA___slow_conv2d_forward)</pre> <p>Oh my goodness...</p> <p>Despite our preparations our custom image and model are on different devices.</p> <p>And we get the error:</p> <p><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)</code></p> <p>Let's fix that by putting our <code>custom_image_transformed</code> on the target device.</p> In\u00a0[66]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed.to(device))\n</pre> model_1.eval() with torch.inference_mode():     custom_image_pred = model_1(custom_image_transformed.to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[66], line 3\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed.to(device))\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nCell In[41], line 44, in TinyVGG.forward(self, x)\n     42 x = self.conv_block_2(x)\n     43 # print(x.shape)\n---&gt; 44 x = self.classifier(x)\n     45 # print(x.shape)\n     46 return x\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/container.py:217, in Sequential.forward(self, input)\n    215 def forward(self, input):\n    216     for module in self:\n--&gt; 217         input = module(input)\n    218     return input\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/torch/nn/modules/linear.py:116, in Linear.forward(self, input)\n    115 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 116     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (10x256 and 2560x3)</pre> <p>What now?</p> <p>It looks like we're getting a shape error.</p> <p>Why might this be?</p> <p>We converted our custom image to be the same size as the images our model was trained on...</p> <p>Oh wait...</p> <p>There's one dimension we forgot about.</p> <p>The batch size.</p> <p>Our model expects image tensors with a batch size dimension at the start (<code>NCHW</code> where <code>N</code> is the batch size).</p> <p>Except our custom image is currently only <code>CHW</code>.</p> <p>We can add a batch size dimension using <code>torch.unsqueeze(dim=0)</code> to add an extra dimension our image and finally make a prediction.</p> <p>Essentially we'll be telling our model to predict on a single image (an image with a <code>batch_size</code> of 1).</p> In\u00a0[67]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    # Add an extra dimension to image\n    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n    \n    # Print out different shapes\n    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n    \n    # Make a prediction on image with an extra dimension\n    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device))\n</pre> model_1.eval() with torch.inference_mode():     # Add an extra dimension to image     custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)          # Print out different shapes     print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")     print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")          # Make a prediction on image with an extra dimension     custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device)) <pre>Custom image transformed shape: torch.Size([3, 64, 64])\nUnsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n</pre> <p>Yes!!!</p> <p>It looks like it worked!</p> <p>Note: What we've just gone through are three of the classical and most common deep learning and PyTorch issues:</p> <ol> <li>Wrong datatypes - our model expects <code>torch.float32</code> where our original custom image was <code>uint8</code>.</li> <li>Wrong device - our model was on the target <code>device</code> (in our case, the GPU) whereas our target data hadn't been moved to the target <code>device</code> yet.</li> <li>Wrong shapes - our model expected an input image of shape <code>[N, C, H, W]</code> or <code>[batch_size, color_channels, height, width]</code> whereas our custom image tensor was of shape <code>[color_channels, height, width]</code>.</li> </ol> <p>Keep in mind, these errors aren't just for predicting on custom images.</p> <p>They will be present with almost every kind of data type (text, audio, structured data) and problem you work with.</p> <p>Now let's take a look at our model's predictions.</p> In\u00a0[68]: Copied! <pre>custom_image_pred\n</pre> custom_image_pred Out[68]: <pre>tensor([[ 0.1185,  0.0272, -0.1456]], device='cuda:0')</pre> <p>Alright, these are still in logit form (the raw outputs of a model are called logits).</p> <p>Let's convert them from logits -&gt; prediction probabilities -&gt; prediction labels.</p> In\u00a0[69]: Copied! <pre># Print out prediction logits\nprint(f\"Prediction logits: {custom_image_pred}\")\n\n# Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\ncustom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\nprint(f\"Prediction probabilities: {custom_image_pred_probs}\")\n\n# Convert prediction probabilities -&gt; prediction labels\ncustom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\nprint(f\"Prediction label: {custom_image_pred_label}\")\n</pre> # Print out prediction logits print(f\"Prediction logits: {custom_image_pred}\")  # Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification) custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1) print(f\"Prediction probabilities: {custom_image_pred_probs}\")  # Convert prediction probabilities -&gt; prediction labels custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1) print(f\"Prediction label: {custom_image_pred_label}\") <pre>Prediction logits: tensor([[ 0.1185,  0.0272, -0.1456]], device='cuda:0')\nPrediction probabilities: tensor([[0.3731, 0.3405, 0.2865]], device='cuda:0')\nPrediction label: tensor([0], device='cuda:0')\n</pre> <p>Alright!</p> <p>Looking good.</p> <p>But of course our prediction label is still in index/tensor form.</p> <p>We can convert it to a string class name prediction by indexing on the <code>class_names</code> list.</p> In\u00a0[70]: Copied! <pre># Find the predicted label\ncustom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\ncustom_image_pred_class\n</pre> # Find the predicted label custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error custom_image_pred_class Out[70]: <pre>'pizza'</pre> <p>Wow.</p> <p>It looks like the model gets the prediction right, even though it was performing poorly based on our evaluation metrics.</p> <p>Note: The model in its current form will predict \"pizza\", \"steak\" or \"sushi\" no matter what image it's given. If you wanted your model to predict on a different class, you'd have to train it to do so.</p> <p>But if we check the <code>custom_image_pred_probs</code>, we'll notice that the model gives almost equal weight (the values are similar) to every class.</p> In\u00a0[71]: Copied! <pre># The values of the prediction probabilities are quite similar\ncustom_image_pred_probs\n</pre> # The values of the prediction probabilities are quite similar custom_image_pred_probs Out[71]: <pre>tensor([[0.3731, 0.3405, 0.2865]], device='cuda:0')</pre> <p>Having prediction probabilities this similar could mean a couple of things:</p> <ol> <li>The model is trying to predict all three classes at the same time (there may be an image containing pizza, steak and sushi).</li> <li>The model doesn't really know what it wants to predict and is in turn just assigning similar values to each of the classes.</li> </ol> <p>Our case is number 2, since our model is poorly trained, it is basically guessing the prediction.</p> In\u00a0[72]: Copied! <pre>def pred_and_plot_image(model: torch.nn.Module, \n                        image_path: str, \n                        class_names: List[str] = None, \n                        transform=None,\n                        device: torch.device = device):\n    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n    \n    # 1. Load in image and convert the tensor values to float32\n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n    \n    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n    target_image = target_image / 255. \n    \n    # 3. Transform if necessary\n    if transform:\n        target_image = transform(target_image)\n    \n    # 4. Make sure the model is on the target device\n    model.to(device)\n    \n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # Add an extra dimension to the image\n        target_image = target_image.unsqueeze(dim=0)\n    \n        # Make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(target_image.to(device))\n        \n    # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 7. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n    \n    # 8. Plot the image alongside the prediction and prediction probability\n    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n    if class_names:\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    else: \n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    plt.title(title)\n    plt.axis(False);\n</pre> def pred_and_plot_image(model: torch.nn.Module,                          image_path: str,                          class_names: List[str] = None,                          transform=None,                         device: torch.device = device):     \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"          # 1. Load in image and convert the tensor values to float32     target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)          # 2. Divide the image pixel values by 255 to get them between [0, 1]     target_image = target_image / 255.           # 3. Transform if necessary     if transform:         target_image = transform(target_image)          # 4. Make sure the model is on the target device     model.to(device)          # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():         # Add an extra dimension to the image         target_image = target_image.unsqueeze(dim=0)              # Make a prediction on image with an extra dimension and send it to the target device         target_image_pred = model(target_image.to(device))              # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 7. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)          # 8. Plot the image alongside the prediction and prediction probability     plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib     if class_names:         title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     else:          title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     plt.title(title)     plt.axis(False); <p>What a nice looking function, let's test it out.</p> In\u00a0[73]: Copied! <pre># Pred on our custom image\npred_and_plot_image(model=model_1,\n                    image_path=custom_image_path,\n                    class_names=class_names,\n                    transform=custom_image_transform,\n                    device=device)\n</pre> # Pred on our custom image pred_and_plot_image(model=model_1,                     image_path=custom_image_path,                     class_names=class_names,                     transform=custom_image_transform,                     device=device) <p>Two thumbs up again!</p> <p>Looks like our model got the prediction right just by guessing.</p> <p>This won't always be the case with other images though...</p> <p>The image is pixelated too because we resized it to <code>[64, 64]</code> using <code>custom_image_transform</code>.</p> <p>Exercise: Try making a prediction with one of your own images of pizza, steak or sushi and see what happens.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#04-pytorch-custom-datasets","title":"04. PyTorch Custom Datasets\u00b6","text":"<p>In the last notebook, notebook 03, we looked at how to build computer vision models on an in-built dataset in PyTorch (FashionMNIST).</p> <p>The steps we took are similar across many different problems in machine learning.</p> <p>Find a dataset, turn the dataset into numbers, build a model (or find an existing model) to find patterns in those numbers that can be used for prediction.</p> <p>PyTorch has many built-in datasets used for a wide number of machine learning benchmarks, however, you'll often want to use your own custom dataset.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#what-is-a-custom-dataset","title":"What is a custom dataset?\u00b6","text":"<p>A custom dataset is a collection of data relating to a specific problem you're working on.</p> <p>In essence, a custom dataset can be comprised of almost anything.</p> <p>For example, if we were building a food image classification app like Nutrify, our custom dataset might be images of food.</p> <p>Or if we were trying to build a model to classify whether or not a text-based review on a website was positive or negative, our custom dataset might be examples of existing customer reviews and their ratings.</p> <p>Or if we were trying to build a sound classification app, our custom dataset might be sound samples alongside their sample labels.</p> <p>Or if we were trying to build a recommendation system for customers purchasing things on our website, our custom dataset might be examples of products other people have bought.</p> <p>PyTorch includes many existing functions to load in various custom datasets in the <code>TorchVision</code>, <code>TorchText</code>, <code>TorchAudio</code> and <code>TorchRec</code> domain libraries.</p> <p>But sometimes these existing functions may not be enough.</p> <p>In that case, we can always subclass <code>torch.utils.data.Dataset</code> and customize it to our liking.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to be applying the PyTorch Workflow we covered in notebook 01 and notebook 02 to a computer vision problem.</p> <p>But instead of using an in-built PyTorch dataset, we're going to be using our own dataset of pizza, steak and sushi images.</p> <p>The goal will be to load these images and then build a model to train and predict on them.</p> <p></p> <p>What we're going to build. We'll use <code>torchvision.datasets</code> as well as our own custom <code>Dataset</code> class to load in images of food and then we'll build a PyTorch computer vision model to hopefully be able to classify them.</p> <p>Specifically, we're going to cover:</p> Topic Contents 0. Importing PyTorch and setting up device-agnostic code Let's get PyTorch loaded and then follow best practice to setup our code to be device-agnostic. 1. Get data We're going to be using our own custom dataset of pizza, steak and sushi images. 2. Become one with the data (data preparation) At the beginning of any new machine learning problem, it's paramount to understand the data you're working with. Here we'll take some steps to figure out what data we have. 3. Transforming data Often, the data you get won't be 100% ready to use with a machine learning model, here we'll look at some steps we can take to transform our images so they're ready to be used with a model. 4. Loading data with <code>ImageFolder</code> (option 1) PyTorch has many in-built data loading functions for common types of data. <code>ImageFolder</code> is helpful if our images are in standard image classification format. 5. Loading image data with a custom <code>Dataset</code> What if PyTorch didn't have an in-built function to load data with? This is where we can build our own custom subclass of <code>torch.utils.data.Dataset</code>. 6. Other forms of transforms (data augmentation) Data augmentation is a common technique for expanding the diversity of your training data. Here we'll explore some of <code>torchvision</code>'s in-built data augmentation functions. 7. Model 0: TinyVGG without data augmentation By this stage, we'll have our data ready, let's build a model capable of fitting it. We'll also create some training and testing functions for training and evaluating our model. 8. Exploring loss curves Loss curves are a great way to see how your model is training/improving over time. They're also a good way to see if your model is underfitting or overfitting. 9. Model 1: TinyVGG with data augmentation By now, we've tried a model without, how about we try one with data augmentation? 10. Compare model results Let's compare our different models' loss curves and see which performed better and discuss some options for improving performance. 11. Making a prediction on a custom image Our model is trained to on a dataset of pizza, steak and sushi images. In this section we'll cover how to use our trained model to predict on an image outside of our existing dataset."},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#where-can-can-you-get-help","title":"Where can can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page there too.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#0-importing-pytorch-and-setting-up-device-agnostic-code","title":"0. Importing PyTorch and setting up device-agnostic code\u00b6","text":""},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#1-get-data","title":"1. Get data\u00b6","text":"<p>First thing's first we need some data.</p> <p>And like any good cooking show, some data has already been prepared for us.</p> <p>We're going to start small.</p> <p>Because we're not looking to train the biggest model or use the biggest dataset yet.</p> <p>Machine learning is an iterative process, start small, get something working and increase when necessary.</p> <p>The data we're going to be using is a subset of the Food101 dataset.</p> <p>Food101 is popular computer vision benchmark as it contains 1000 images of 101 different kinds of foods, totaling 101,000 images (75,750 train and 25,250 test).</p> <p>Can you think of 101 different foods?</p> <p>Can you think of a computer program to classify 101 foods?</p> <p>I can.</p> <p>A machine learning model!</p> <p>Specifically, a PyTorch computer vision model like we covered in notebook 03.</p> <p>Instead of 101 food classes though, we're going to start with 3: pizza, steak and sushi.</p> <p>And instead of 1,000 images per class, we're going to start with a random 10% (start small, increase when necessary).</p> <p>If you'd like to see where the data came from you see the following resources:</p> <ul> <li>Original Food101 dataset and paper website.</li> <li><code>torchvision.datasets.Food101</code> - the version of the data I downloaded for this notebook.</li> <li><code>extras/04_custom_data_creation.ipynb</code> - a notebook I used to format the Food101 dataset to use for this notebook.</li> <li><code>data/pizza_steak_sushi.zip</code> - the zip archive of pizza, steak and sushi images from Food101, created with the notebook linked above.</li> </ul> <p>Let's write some code to download the formatted data from GitHub.</p> <p>Note: The dataset we're about to use has been pre-formatted for what we'd like to use it for. However, you'll often have to format your own datasets for whatever problem you're working on. This is a regular practice in the machine learning world.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#2-become-one-with-the-data-data-preparation","title":"2. Become one with the data (data preparation)\u00b6","text":"<p>Dataset downloaded!</p> <p>Time to become one with it.</p> <p>This is another important step before building a model.</p> <p>As Abraham Lossfunction said...</p> <p>Data preparation is paramount. Before building a model, become one with the data. Ask: What am I trying to do here? Source: @mrdbourke Twitter.</p> <p>What's inspecting the data and becoming one with it?</p> <p>Before starting a project or building any kind of model, it's important to know what data you're working with.</p> <p>In our case, we have images of pizza, steak and sushi in standard image classification format.</p> <p>Image classification format contains separate classes of images in seperate directories titled with a particular class name.</p> <p>For example, all images of <code>pizza</code> are contained in the <code>pizza/</code> directory.</p> <p>This format is popular across many different image classification benchmarks, including ImageNet (of the most popular computer vision benchmark datasets).</p> <p>You can see an example of the storage format below, the images numbers are arbitrary.</p> <pre><code>pizza_steak_sushi/ &lt;- overall dataset folder\n    train/ &lt;- training images\n        pizza/ &lt;- class name as folder name\n            image01.jpeg\n            image02.jpeg\n            ...\n        steak/\n            image24.jpeg\n            image25.jpeg\n            ...\n        sushi/\n            image37.jpeg\n            ...\n    test/ &lt;- testing images\n        pizza/\n            image101.jpeg\n            image102.jpeg\n            ...\n        steak/\n            image154.jpeg\n            image155.jpeg\n            ...\n        sushi/\n            image167.jpeg\n            ...\n</code></pre> <p>The goal will be to take this data storage structure and turn it into a dataset usable with PyTorch.</p> <p>Note: The structure of the data you work with will vary depending on the problem you're working on. But the premise still remains: become one with the data, then find a way to best turn it into a dataset compatible with PyTorch.</p> <p>We can inspect what's in our data directory by writing a small helper function to walk through each of the subdirectories and count the files present.</p> <p>To do so, we'll use Python's in-built <code>os.walk()</code>.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#21-visualize-an-image","title":"2.1 Visualize an image\u00b6","text":"<p>Okay, we've seen how our directory structure is formatted.</p> <p>Now in the spirit of the data explorer, it's time to visualize, visualize, visualize!</p> <p>Let's write some code to:</p> <ol> <li>Get all of the image paths using <code>pathlib.Path.glob()</code> to find all of the files ending in <code>.jpg</code>.</li> <li>Pick a random image path using Python's <code>random.choice()</code>.</li> <li>Get the image class name using <code>pathlib.Path.parent.stem</code>.</li> <li>And since we're working with images, we'll open the random image path using <code>PIL.Image.open()</code> (PIL stands for Python Image Library).</li> <li>We'll then show the image and print some metadata.</li> </ol>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#3-transforming-data","title":"3. Transforming data\u00b6","text":"<p>Now what if we wanted to load our image data into PyTorch?</p> <p>Before we can use our image data with PyTorch we need to:</p> <ol> <li>Turn it into tensors (numerical representations of our images).</li> <li>Turn it into a <code>torch.utils.data.Dataset</code> and subsequently a <code>torch.utils.data.DataLoader</code>, we'll call these <code>Dataset</code> and <code>DataLoader</code> for short.</li> </ol> <p>There are several different kinds of pre-built datasets and dataset loaders for PyTorch, depending on the problem you're working on.</p> Problem space Pre-built Datasets and Functions Vision <code>torchvision.datasets</code> Audio <code>torchaudio.datasets</code> Text <code>torchtext.datasets</code> Recommendation system <code>torchrec.datasets</code> <p>Since we're working with a vision problem, we'll be looking at <code>torchvision.datasets</code> for our data loading functions as well as <code>torchvision.transforms</code> for preparing our data.</p> <p>Let's import some base libraries.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#31-transforming-data-with-torchvisiontransforms","title":"3.1 Transforming data with <code>torchvision.transforms</code>\u00b6","text":"<p>We've got folders of images but before we can use them with PyTorch, we need to convert them into tensors.</p> <p>One of the ways we can do this is by using the <code>torchvision.transforms</code> module.</p> <p><code>torchvision.transforms</code> contains many pre-built methods for formatting images, turning them into tensors and even manipulating them for data augmentation (the practice of altering data to make it harder for a model to learn, we'll see this later on) purposes .</p> <p>To get experience with <code>torchvision.transforms</code>, let's write a series of transform steps that:</p> <ol> <li>Resize the images using <code>transforms.Resize()</code> (from about 512x512 to 64x64, the same shape as the images on the CNN Explainer website).</li> <li>Flip our images randomly on the horizontal using <code>transforms.RandomHorizontalFlip()</code> (this could be considered a form of data augmentation because it will artificially change our image data).</li> <li>Turn our images from a PIL image to a PyTorch tensor using <code>transforms.ToTensor()</code>.</li> </ol> <p>We can compile all of these steps using <code>torchvision.transforms.Compose()</code>.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#4-option-1-loading-image-data-using-imagefolder","title":"4. Option 1: Loading Image Data Using <code>ImageFolder</code>\u00b6","text":"<p>Alright, time to turn our image data into a <code>Dataset</code> capable of being used with PyTorch.</p> <p>Since our data is in standard image classification format, we can use the class <code>torchvision.datasets.ImageFolder</code>.</p> <p>Where we can pass it the file path of a target image directory as well as a series of transforms we'd like to perform on our images.</p> <p>Let's test it out on our data folders <code>train_dir</code> and <code>test_dir</code> passing in <code>transform=data_transform</code> to turn our images into tensors.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#41-turn-loaded-images-into-dataloaders","title":"4.1 Turn loaded images into <code>DataLoader</code>'s\u00b6","text":"<p>We've got our images as PyTorch <code>Dataset</code>'s but now let's turn them into <code>DataLoader</code>'s.</p> <p>We'll do so using <code>torch.utils.data.DataLoader</code>.</p> <p>Turning our <code>Dataset</code>'s into <code>DataLoader</code>'s makes them iterable so a model can go through learn the relationships between samples and targets (features and labels).</p> <p>To keep things simple, we'll use a <code>batch_size=1</code> and <code>num_workers=1</code>.</p> <p>What's <code>num_workers</code>?</p> <p>Good question.</p> <p>It defines how many subprocesses will be created to load your data.</p> <p>Think of it like this, the higher value <code>num_workers</code> is set to, the more compute power PyTorch will use to load your data.</p> <p>Personally, I usually set it to the total number of CPUs on my machine via Python's <code>os.cpu_count()</code>.</p> <p>This ensures the <code>DataLoader</code> recruits as many cores as possible to load data.</p> <p>Note: There are more parameters you can get familiar with using <code>torch.utils.data.DataLoader</code> in the PyTorch documentation.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#5-option-2-loading-image-data-with-a-custom-dataset","title":"5. Option 2: Loading Image Data with a Custom <code>Dataset</code>\u00b6","text":"<p>What if a pre-built <code>Dataset</code> creator like <code>torchvision.datasets.ImageFolder()</code> didn't exist?</p> <p>Or one for your specific problem didn't exist?</p> <p>Well, you could build your own.</p> <p>But wait, what are the pros and cons of creating your own custom way to load <code>Dataset</code>'s?</p> Pros of creating a custom <code>Dataset</code> Cons of creating a custom <code>Dataset</code> Can create a <code>Dataset</code> out of almost anything. Even though you could create a <code>Dataset</code> out of almost anything, it doesn't mean it will work. Not limited to PyTorch pre-built <code>Dataset</code> functions. Using a custom <code>Dataset</code> often results in writing more code, which could be prone to errors or performance issues. <p>To see this in action, let's work towards replicating <code>torchvision.datasets.ImageFolder()</code> by subclassing <code>torch.utils.data.Dataset</code> (the base class for all <code>Dataset</code>'s in PyTorch).</p> <p>We'll start by importing the modules we need:</p> <ul> <li>Python's <code>os</code> for dealing with directories (our data is stored in directories).</li> <li>Python's <code>pathlib</code> for dealing with filepaths (each of our images has a unique filepath).</li> <li><code>torch</code> for all things PyTorch.</li> <li>PIL's <code>Image</code> class for loading images.</li> <li><code>torch.utils.data.Dataset</code> to subclass and create our own custom <code>Dataset</code>.</li> <li><code>torchvision.transforms</code> to turn our images into tensors.</li> <li>Various types from Python's <code>typing</code> module to add type hints to our code.</li> </ul> <p>Note: You can customize the following steps for your own dataset. The premise remains: write code to load your data in the format you'd like it.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#51-creating-a-helper-function-to-get-class-names","title":"5.1 Creating a helper function to get class names\u00b6","text":"<p>Let's write a helper function capable of creating a list of class names and a dictionary of class names and their indexes given a directory path.</p> <p>To do so, we'll:</p> <ol> <li>Get the class names using <code>os.scandir()</code> to traverse a target directory (ideally the directory is in standard image classification format).</li> <li>Raise an error if the class names aren't found (if this happens, there might be something wrong with the directory structure).</li> <li>Turn the class names into a dictionary of numerical labels, one for each class.</li> </ol> <p>Let's see a small example of step 1 before we write the full function.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#52-create-a-custom-dataset-to-replicate-imagefolder","title":"5.2 Create a custom <code>Dataset</code> to replicate <code>ImageFolder</code>\u00b6","text":"<p>Now we're ready to build our own custom <code>Dataset</code>.</p> <p>We'll build one to replicate the functionality of <code>torchvision.datasets.ImageFolder()</code>.</p> <p>This will be good practice, plus, it'll reveal a few of the required steps to make your own custom <code>Dataset</code>.</p> <p>It'll be a fair bit of a code... but nothing we can't handle!</p> <p>Let's break it down:</p> <ol> <li>Subclass <code>torch.utils.data.Dataset</code>.</li> <li>Initialize our subclass with a <code>targ_dir</code> parameter (the target data directory) and <code>transform</code> parameter (so we have the option to transform our data if needed).</li> <li>Create several attributes for <code>paths</code> (the paths of our target images), <code>transform</code> (the transforms we might like to use, this can be <code>None</code>), <code>classes</code> and <code>class_to_idx</code> (from our <code>find_classes()</code> function).</li> <li>Create a function to load images from file and return them, this could be using <code>PIL</code> or <code>torchvision.io</code> (for input/output of vision data).</li> <li>Overwrite the <code>__len__</code> method of <code>torch.utils.data.Dataset</code> to return the number of samples in the <code>Dataset</code>, this is recommended but not required. This is so you can call <code>len(Dataset)</code>.</li> <li>Overwrite the <code>__getitem__</code> method of <code>torch.utils.data.Dataset</code> to return a single sample from the <code>Dataset</code>, this is required.</li> </ol> <p>Let's do it!</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#53-create-a-function-to-display-random-images","title":"5.3 Create a function to display random images\u00b6","text":"<p>You know what time it is!</p> <p>Time to put on our data explorer's hat and visualize, visualize, visualize!</p> <p>Let's create a helper function called <code>display_random_images()</code> that helps us visualize images in our <code>Dataset'</code>s.</p> <p>Specifically, it'll:</p> <ol> <li>Take in a <code>Dataset</code> and a number of other parameters such as <code>classes</code> (the names of our target classes), the number of images to display (<code>n</code>) and a random seed.</li> <li>To prevent the display getting out of hand, we'll cap <code>n</code> at 10 images.</li> <li>Set the random seed for reproducible plots (if <code>seed</code> is set).</li> <li>Get a list of random sample indexes (we can use Python's <code>random.sample()</code> for this) to plot.</li> <li>Setup a <code>matplotlib</code> plot.</li> <li>Loop through the random sample indexes found in step 4 and plot them with <code>matplotlib</code>.</li> <li>Make sure the sample images are of shape <code>HWC</code> (height, width, color channels) so we can plot them.</li> </ol>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#54-turn-custom-loaded-images-into-dataloaders","title":"5.4 Turn custom loaded images into <code>DataLoader</code>'s\u00b6","text":"<p>We've got a way to turn our raw images into <code>Dataset</code>'s (features mapped to labels or <code>X</code>'s mapped to <code>y</code>'s) through our <code>ImageFolderCustom</code> class.</p> <p>Now how could we turn our custom <code>Dataset</code>'s into <code>DataLoader</code>'s?</p> <p>If you guessed by using <code>torch.utils.data.DataLoader()</code>, you'd be right!</p> <p>Because our custom <code>Dataset</code>'s subclass <code>torch.utils.data.Dataset</code>, we can use them directly with <code>torch.utils.data.DataLoader()</code>.</p> <p>And we can do using very similar steps to before except this time we'll be using our custom created <code>Dataset</code>'s.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation","title":"6. Other forms of transforms (data augmentation)\u00b6","text":"<p>We've seen a couple of transforms on our data already but there's plenty more.</p> <p>You can see them all in the <code>torchvision.transforms</code> documentation.</p> <p>The purpose of tranforms is to alter your images in some way.</p> <p>That may be turning your images into a tensor (as we've seen before).</p> <p>Or cropping it or randomly erasing a portion or randomly rotating them.</p> <p>Doing this kinds of transforms is often referred to as data augmentation.</p> <p>Data augmentation is the process of altering your data in such a way that you artificially increase the diversity of your training set.</p> <p>Training a model on this artificially altered dataset hopefully results in a model that is capable of better generalization (the patterns it learns are more robust to future unseen examples).</p> <p>You can see many different examples of data augmentation performed on images using <code>torchvision.transforms</code> in PyTorch's Illustration of Transforms example.</p> <p>But let's try one out ourselves.</p> <p>Machine learning is all about harnessing the power of randomness and research shows that random transforms (like <code>transforms.RandAugment()</code> and <code>transforms.TrivialAugmentWide()</code>) generally perform better than hand-picked transforms.</p> <p>The idea behind TrivialAugment is... well, trivial.</p> <p>You have a set of transforms and you randomly pick a number of them to perform on an image and at a random magnitude between a given range (a higher magnitude means more instense).</p> <p>The PyTorch team even used TrivialAugment it to train their latest state-of-the-art vision models.</p> <p></p> <p>TrivialAugment was one of the ingredients used in a recent state of the art training upgrade to various PyTorch vision models.</p> <p>How about we test it out on some of our own images?</p> <p>The main parameter to pay attention to in <code>transforms.TrivialAugmentWide()</code> is <code>num_magnitude_bins=31</code>.</p> <p>It defines how much of a range an intensity value will be picked to apply a certain transform, <code>0</code> being no range and <code>31</code> being maximum range (highest chance for highest intensity).</p> <p>We can incorporate <code>transforms.TrivialAugmentWide()</code> into <code>transforms.Compose()</code>.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#7-model-0-tinyvgg-without-data-augmentation","title":"7. Model 0: TinyVGG without data augmentation\u00b6","text":"<p>Alright, we've seen how to turn our data from images in folders to transformed tensors.</p> <p>Now let's construct a computer vision model to see if we can classify if an image is of pizza, steak or sushi.</p> <p>To begin, we'll start with a simple transform, only resizing the images to <code>(64, 64)</code> and turning them into tensors.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#71-creating-transforms-and-loading-data-for-model-0","title":"7.1 Creating transforms and loading data for Model 0\u00b6","text":""},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#72-create-tinyvgg-model-class","title":"7.2 Create TinyVGG model class\u00b6","text":"<p>In notebook 03, we used the TinyVGG model from the CNN Explainer website.</p> <p>Let's recreate the same model, except this time we'll be using color images instead of grayscale (<code>in_channels=3</code> instead of <code>in_channels=1</code> for RGB pixels).</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#73-try-a-forward-pass-on-a-single-image-to-test-the-model","title":"7.3 Try a forward pass on a single image (to test the model)\u00b6","text":"<p>A good way to test a model is to do a forward pass on a single piece of data.</p> <p>It's also handy way to test the input and output shapes of our different layers.</p> <p>To do a forward pass on a single image, let's:</p> <ol> <li>Get a batch of images and labels from the <code>DataLoader</code>.</li> <li>Get a single image from the batch and <code>unsqueeze()</code> the image so it has a batch size of <code>1</code> (so its shape fits the model).</li> <li>Perform inference on a single image (making sure to send the image to the target <code>device</code>).</li> <li>Print out what's happening and convert the model's raw output logits to prediction probabilities with <code>torch.softmax()</code> (since we're working with multi-class data) and convert the prediction probabilities to prediction labels with <code>torch.argmax()</code>.</li> </ol>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#74-use-torchinfo-to-get-an-idea-of-the-shapes-going-through-our-model","title":"7.4 Use <code>torchinfo</code> to get an idea of the shapes going through our model\u00b6","text":"<p>Printing out our model with <code>print(model)</code> gives us an idea of what's going on with our model.</p> <p>And we can print out the shapes of our data throughout the <code>forward()</code> method.</p> <p>However, a helpful way to get information from our model is to use <code>torchinfo</code>.</p> <p><code>torchinfo</code> comes with a <code>summary()</code> method that takes a PyTorch model as well as an <code>input_shape</code> and returns what happens as a tensor moves through your model.</p> <p>Note: If you're using Google Colab, you'll need to install <code>torchinfo</code>.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#75-create-train-test-loop-functions","title":"7.5 Create train &amp; test loop functions\u00b6","text":"<p>We've got data and we've got a model.</p> <p>Now let's make some training and test loop functions to train our model on the training data and evaluate our model on the testing data.</p> <p>And to make sure we can use these the training and testing loops again, we'll functionize them.</p> <p>Specifically, we're going to make three functions:</p> <ol> <li><code>train_step()</code> - takes in a model, a <code>DataLoader</code>, a loss function and an optimizer and trains the model on the <code>DataLoader</code>.</li> <li><code>test_step()</code> - takes in a model, a <code>DataLoader</code> and a loss function and evaluates the model on the <code>DataLoader</code>.</li> <li><code>train()</code> - performs 1. and 2. together for a given number of epochs and returns a results dictionary.</li> </ol> <p>Note: We covered the steps in a PyTorch opimization loop in notebook 01, as well as the Unofficial PyTorch Optimization Loop Song and we've built similar functions in notebook 03.</p> <p>Let's start by building <code>train_step()</code>.</p> <p>Because we're dealing with batches in the <code>DataLoader</code>'s, we'll accumulate the model loss and accuracy values during training (by adding them up for each batch) and then adjust them at the end before we return them.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#76-creating-a-train-function-to-combine-train_step-and-test_step","title":"7.6 Creating a <code>train()</code> function to combine <code>train_step()</code> and <code>test_step()</code>\u00b6","text":"<p>Now we need a way to put our <code>train_step()</code> and <code>test_step()</code> functions together.</p> <p>To do so, we'll package them up in a <code>train()</code> function.</p> <p>This function will train the model as well as evaluate it.</p> <p>Specificially, it'll:</p> <ol> <li>Take in a model, a <code>DataLoader</code> for training and test sets, an optimizer, a loss function and how many epochs to perform each train and test step for.</li> <li>Create an empty results dictionary for <code>train_loss</code>, <code>train_acc</code>, <code>test_loss</code> and <code>test_acc</code> values (we can fill this up as training goes on).</li> <li>Loop through the training and test step functions for a number of epochs.</li> <li>Print out what's happening at the end of each epoch.</li> <li>Update the empty results dictionary with the updated metrics each epoch.</li> <li>Return the filled</li> </ol> <p>To keep track of the number of epochs we've been through, let's import <code>tqdm</code> from <code>tqdm.auto</code> (<code>tqdm</code> is one of the most popular progress bar libraries for Python and <code>tqdm.auto</code> automatically decides what kind of progress bar is best for your computing environment, e.g. Jupyter Notebook vs. Python script).</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#77-train-and-evaluate-model-0","title":"7.7 Train and Evaluate Model 0\u00b6","text":"<p>Alright, alright, alright we've got all of the ingredients we need to train and evaluate our model.</p> <p>Time to put our <code>TinyVGG</code> model, <code>DataLoader</code>'s and <code>train()</code> function together to see if we can build a model capable of discerning between pizza, steak and sushi!</p> <p>Let's recreate <code>model_0</code> (we don't need to but we will for completeness) then call our <code>train()</code> function passing in the necessary parameters.</p> <p>To keep our experiments quick, we'll train our model for 5 epochs (though you could increase this if you want).</p> <p>As for an optimizer and loss function, we'll use <code>torch.nn.CrossEntropyLoss()</code> (since we're working with multi-class classification data) and <code>torch.optim.Adam()</code> with a learning rate of <code>1e-3</code> respecitvely.</p> <p>To see how long things take, we'll import Python's <code>timeit.default_timer()</code> method to calculate the training time.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0","title":"7.8 Plot the loss curves of Model 0\u00b6","text":"<p>From the print outs of our <code>model_0</code> training, it didn't look like it did too well.</p> <p>But we can further evaluate it by plotting the model's loss curves.</p> <p>Loss curves show the model's results over time.</p> <p>And they're a great way to see how your model performs on different datasets (e.g. training and test).</p> <p>Let's create a function to plot the values in our <code>model_0_results</code> dictionary.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like","title":"8. What should an ideal loss curve look like?\u00b6","text":"<p>Looking at training and test loss curves is a great way to see if your model is overfitting.</p> <p>An overfitting model is one that performs better (often by a considerable margin) on the training set than the validation/test set.</p> <p>If your training loss is far lower than your test loss, your model is overfitting.</p> <p>As in, it's learning the patterns in the training too well and those patterns aren't generalizing to the test data.</p> <p>The other side is when your training and test loss are not as low as you'd like, this is considered underfitting.</p> <p>The ideal position for a training and test loss curve is for them to line up closely with each other.</p> <p></p> <p>Left: If your training and test loss curves aren't as low as you'd like, this is considered underfitting. *Middle: When your test/validation loss is higher than your training loss this is considered overfitting. Right: The ideal scenario is when your training and test loss curves line up over time. This means your model is generalizing well. There are more combinations and different things loss curves can do, for more on these, see Google's Interpreting Loss Curves guide.*</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#81-how-to-deal-with-overfitting","title":"8.1 How to deal with overfitting\u00b6","text":"<p>Since the main problem with overfitting is that you're model is fitting the training data too well, you'll want to use techniques to \"reign it in\".</p> <p>A common technique of preventing overfitting is known as regularization.</p> <p>I like to think of this as \"making our models more regular\", as in, capable of fitting more kinds of data.</p> <p>Let's discuss a few methods to prevent overfitting.</p> Method to prevent overfitting What is it? Get more data Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples. Simplify your model If the current model is already overfitting the training data, it may be too complicated of a model. This means it's learning the patterns of the data too well and isn't able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer. Use data augmentation Data augmentation manipulates the training data in a way so that's harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data. Use transfer learning Transfer learning involves leveraging the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images. Use dropout layers Dropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model but also making the remaining connections better. See <code>torch.nn.Dropout()</code> for more. Use learning rate decay The idea here is to slowly decrease the learning rate as a model trains. This is akin to reaching for a coin at the back of a couch. The closer you get, the smaller your steps. The same with the learning rate, the closer you get to convergence, the smaller you'll want your weight updates to be. Use early stopping Early stopping stops model training before it begins to overfit. As in, say the model's loss has stopped decreasing for the past 10 epochs (this number is arbitrary), you may want to stop the model training here and go with the model weights that had the lowest loss (10 epochs prior). <p>There are more methods for dealing with overfitting but these are some of the main ones.</p> <p>As you start to build more and more deep models, you'll find because deep learnings are so good at learning patterns in data, dealing with overfitting is one of the primary problems of deep learning.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#82-how-to-deal-with-underfitting","title":"8.2 How to deal with underfitting\u00b6","text":"<p>When a model is underfitting it is considered to have poor predictive power on the training and test sets.</p> <p>In essence, an underfitting model will fail to reduce the loss values to a desired level.</p> <p>Right now, looking at our current loss curves, I'd considered our <code>TinyVGG</code> model, <code>model_0</code>, to be underfitting the data.</p> <p>The main idea behind dealing with underfitting is to increase your model's predictive power.</p> <p>There are several ways to do this.</p> Method to prevent underfitting What is it? Add more layers/units to your model If your model is underfitting, it may not have enough capability to learn the required patterns/weights/representations of the data to be predictive. One way to add more predictive power to your model is to increase the number of hidden layers/units within those layers. Tweak the learning rate Perhaps your model's learning rate is too high to begin with. And it's trying to update its weights each epoch too much, in turn not learning anything. In this case, you might lower the learning rate and see what happens. Use transfer learning Transfer learning is capable of preventing overfitting and underfitting. It involves using the patterns from a previously working model and adjusting them to your own problem. Train for longer Sometimes a model just needs more time to learn representations of data. If you find in your smaller experiments your model isn't learning anything, perhaps leaving it train for a more epochs may result in better performance. Use less regularization Perhaps your model is underfitting because you're trying to prevent overfitting too much. Holding back on regularization techniques can help your model fit the data better."},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#83-the-balance-between-overfitting-and-underfitting","title":"8.3 The balance between overfitting and underfitting\u00b6","text":"<p>None of the methods discussed above are silver bullets, meaning, they don't always work.</p> <p>And preventing overfitting and underfitting is possibly the most active area of machine learning research.</p> <p>Since everone wants their models to fit better (less underfitting) but not so good they don't generalize well and perform in the real world (less overfitting).</p> <p>There's a fine line between overfitting and underfitting.</p> <p>Because too much of each can cause the other.</p> <p>Transfer learning is perhaps one of the most powerful techniques when it comes to dealing with both overfitting and underfitting on your own problems.</p> <p>Rather than handcraft different overfitting and underfitting techniques, transfer learning enables you to take an already working model in a similar problem space to yours (say one from paperswithcode.com/sota or Hugging Face models) and apply it to your own dataset.</p> <p>We'll see the power of transfer learning in a later notebook.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#9-model-1-tinyvgg-with-data-augmentation","title":"9. Model 1: TinyVGG with Data Augmentation\u00b6","text":"<p>Time to try out another model!</p> <p>This time, let's load in the data and use data augmentation to see if it improves our results in anyway.</p> <p>First, we'll compose a training transform to include <code>transforms.TrivialAugmentWide()</code> as well as resize and turn our images into tensors.</p> <p>We'll do the same for a testing transform except without the data augmentation.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#91-create-transform-with-data-augmentation","title":"9.1 Create transform with data augmentation\u00b6","text":""},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#92-create-train-and-test-datasets-and-dataloaders","title":"9.2 Create train and test <code>Dataset</code>'s and <code>DataLoader</code>'s\u00b6","text":"<p>We'll make sure the train <code>Dataset</code> uses the <code>train_transform_trivial_augment</code> and the test <code>Dataset</code> uses the <code>test_transform</code>.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#93-construct-and-train-model-1","title":"9.3 Construct and train Model 1\u00b6","text":"<p>Data loaded!</p> <p>Now to build our next model, <code>model_1</code>, we can reuse our <code>TinyVGG</code> class from before.</p> <p>We'll make sure to send it to the target device.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#94-plot-the-loss-curves-of-model-1","title":"9.4 Plot the loss curves of Model 1\u00b6","text":"<p>Since we've got the results of <code>model_1</code> saved in a results dictionary, <code>model_1_results</code>, we can plot them using <code>plot_loss_curves()</code>.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#10-compare-model-results","title":"10. Compare model results\u00b6","text":"<p>Even though our models our performing quite poorly, we can still write code to compare them.</p> <p>Let's first turn our model results in pandas DataFrames.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#11-make-a-prediction-on-a-custom-image","title":"11. Make a prediction on a custom image\u00b6","text":"<p>If you've trained a model on a certain dataset, chances are you'd like to make a prediction on on your own custom data.</p> <p>In our case, since we've trained a model on pizza, steak and sushi images, how could we use our model to make a prediction on one of our own images?</p> <p>To do so, we can load an image and then preprocess it in a way that matches the type of data our model was trained on.</p> <p>In other words, we'll have to convert our own custom image to a tensor and make sure it's in the right datatype before passing it to our model.</p> <p>Let's start by downloading a custom image.</p> <p>Since our model predicts whether an image contains pizza, steak or sushi, let's download a photo of my Dad giving two thumbs up to a big pizza from the Learn PyTorch for Deep Learning GitHub.</p> <p>We download the image using Python's <code>requests</code> module.</p> <p>Note: If you're using Google Colab, you can also upload an image to the current session by going to the left hand side menu -&gt; Files -&gt; Upload to session storage. Beware though, this image will delete when your Google Colab session ends.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#111-loading-in-a-custom-image-with-pytorch","title":"11.1 Loading in a custom image with PyTorch\u00b6","text":"<p>Excellent!</p> <p>Looks like we've got a custom image downloaded and ready to go at <code>data/04-pizza-dad.jpeg</code>.</p> <p>Time to load it in.</p> <p>PyTorch's <code>torchvision</code> has several input and output (\"IO\" or \"io\" for short) methods for reading and writing images and video in <code>torchvision.io</code>.</p> <p>Since we want to load in an image, we'll use <code>torchvision.io.read_image()</code>.</p> <p>This method will read a JPEG or PNG image and turn it into a 3 dimensional RGB or grayscale <code>torch.Tensor</code> with values of datatype <code>uint8</code> in range <code>[0, 255]</code>.</p> <p>Let's try it out.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#112-predicting-on-custom-images-with-a-trained-pytorch-model","title":"11.2 Predicting on custom images with a trained PyTorch model\u00b6","text":"<p>Beautiful, it looks like our image data is now in the same format our model was trained on.</p> <p>Except for one thing...</p> <p>It's <code>shape</code>.</p> <p>Our model was trained on images with shape <code>[3, 64, 64]</code>, whereas our custom image is currently <code>[3, 4032, 3024]</code>.</p> <p>How could we make sure our custom image is the same shape as the images our model was trained on?</p> <p>Are there any <code>torchvision.transforms</code> that could help?</p> <p>Before we answer that question, let's plot the image with <code>matplotlib</code> to make sure it looks okay, remember we'll have to permute the dimensions from <code>CHW</code> to <code>HWC</code> to suit <code>matplotlib</code>'s requirements.</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function","title":"11.3 Putting custom image prediction together: building a function\u00b6","text":"<p>Doing all of the above steps every time you'd like to make a prediction on a custom image would quickly become tedious.</p> <p>So let's put them all together in a function we can easily use over and over again.</p> <p>Specifically, let's make a function that:</p> <ol> <li>Takes in a target image path and converts to the right datatype for our model (<code>torch.float32</code>).</li> <li>Makes sure the target image pixel values are in the range <code>[0, 1]</code>.</li> <li>Transforms the target image if necessary.</li> <li>Makes sure the model is on the target device.</li> <li>Makes a prediction on the target image with a trained model (ensuring the image is the right size and on the same device as the model).</li> <li>Converts the model's output logits to prediction probabilities.</li> <li>Converts the prediction probabilities to prediction labels.</li> <li>Plots the target image alongside the model prediction and prediction probability.</li> </ol> <p>A fair few steps but we've got this!</p>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#main-takeaways","title":"Main takeaways\u00b6","text":"<p>We've covered a fair bit in this module.</p> <p>Let's summarise it with a few dot points.</p> <ul> <li>PyTorch has many in-built functions to deal with all kinds of data, from vision to text to audio to recommendation systems.</li> <li>If PyTorch's built-in data loading functions don't suit your requirements, you can write code to create your own custom datasets by subclassing <code>torch.utils.data.Dataset</code>.</li> <li><code>torch.utils.data.DataLoader</code>'s in PyTorch help turn your <code>Dataset</code>'s into iterables that can be used when training and testing a model.</li> <li>A lot of machine learning is dealing with the balance between overfitting and underfitting (we discussed different methods for each above, so a good exercise would be to research more and writing code to try out the different techniques).</li> <li>Predicting on your own custom data with a trained model is possible, as long as you format the data into a similar format to what the model was trained on. Make sure you take care of the three big PyTorch and deep learning errors:<ol> <li>Wrong datatypes - Your model expected <code>torch.float32</code> when your data is <code>torch.uint8</code>.</li> <li>Wrong data shapes - Your model expected <code>[batch_size, color_channels, height, width]</code> when your data is <code>[color_channels, height, width]</code>.</li> <li>Wrong devices - Your model is on the GPU but your data is on the CPU.</li> </ol> </li> </ul>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code in the sections above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 04</li> <li>Example solutions notebook for 04 (try the exercises before looking at this)</li> </ul> <ol> <li>Our models are underperforming (not fitting the data well). What are 3 methods for preventing underfitting? Write them down and explain each with a sentence.</li> <li>Recreate the data loading functions we built in sections 1, 2, 3 and 4. You should have train and test <code>DataLoader</code>'s ready to use.</li> <li>Recreate <code>model_0</code> we built in section 7.</li> <li>Create training and testing functions for <code>model_0</code>.</li> <li>Try training the model you made in exercise 3 for 5, 20 and 50 epochs, what happens to the results?<ul> <li>Use <code>torch.optim.Adam()</code> with a learning rate of 0.001 as the optimizer.</li> </ul> </li> <li>Double the number of hidden units in your model and train it for 20 epochs, what happens to the results?</li> <li>Double the data you're using with your model and train it for 20 epochs, what happens to the results?<ul> <li>Note: You can use the custom data creation notebook to scale up your Food101 dataset.</li> <li>You can also find the already formatted double data (20% instead of 10% subset) dataset on GitHub, you will need to write download code like in exercise 2 to get it into this notebook.</li> </ul> </li> <li>Make a prediction on your own custom image of pizza/steak/sushi (you could even download one from the internet) and share your prediction.<ul> <li>Does the model you trained in exercise 7 get it right?</li> <li>If not, what do you think you could do to improve it?</li> </ul> </li> </ol>"},{"location":"Learning/Pytorch/04_pytorch_custom_datasets/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>To practice your knowledge of PyTorch <code>Dataset</code>'s and <code>DataLoader</code>'s through PyTorch datasets and dataloaders tutorial notebook.</li> <li>Spend 10-minutes reading the PyTorch <code>torchvision.transforms</code> documentation.<ul> <li>You can see demos of transforms in action in the illustrations of transforms tutorial.</li> </ul> </li> <li>Spend 10-minutes reading the PyTorch <code>torchvision.datasets</code> documentation.<ul> <li>What are some datasets that stand out to you?</li> <li>How could you try building a model on these?</li> </ul> </li> <li>TorchData is currently in beta (as of April 2022), it'll be a future way of loading data in PyTorch, but you can start to check it out now.</li> <li>To speed up deep learning models, you can do a few tricks to improve compute, memory and overhead computations, for more read the post Making Deep Learning Go Brrrr From First Principles by Horace He.</li> </ul>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/","title":"05 pytorch going modular","text":"<p>View Source Code | View Slides </p>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#05-pytorch-going-modular","title":"05. PyTorch Going Modular","text":"<p>This section answers the question, \"how do I turn my notebook code into Python scripts?\"</p> <p>To do so, we're going to turn the most useful code cells in notebook 04. PyTorch Custom Datasets into a series of Python scripts saved to a directory called <code>going_modular</code>.</p>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#what-is-going-modular","title":"What is going modular?","text":"<p>Going modular involves turning notebook code (from a Jupyter Notebook or Google Colab notebook) into a series of different Python scripts that offer similar functionality.</p> <p>For example, we could turn our notebook code from a series of cells into the following Python files:</p> <ul> <li><code>data_setup.py</code> - a file to prepare and download data if needed.</li> <li><code>engine.py</code> - a file containing various training functions.</li> <li><code>model_builder.py</code> or <code>model.py</code> - a file to create a PyTorch model.</li> <li><code>train.py</code> - a file to leverage all other files and train a target PyTorch model.</li> <li><code>utils.py</code> - a file dedicated to helpful utility functions.</li> </ul> <p>Note: The naming and layout of the above files will depend on your use case and code requirements. Python scripts are as general as individual notebook cells, meaning, you could create one for almost any kind of functionality.</p>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#why-would-you-want-to-go-modular","title":"Why would you want to go modular?","text":"<p>Notebooks are fantastic for iteratively exploring and running experiments quickly.</p> <p>However, for larger scale projects you may find Python scripts more reproducible and easier to run.</p> <p>Though this is a debated topic, as companies like Netflix have shown how they use notebooks for production code.</p> <p>Production code is code that runs to offer a service to someone or something.</p> <p>For example, if you have an app running online that other people can access and use, the code running that app is considered production code.</p> <p>And libraries like fast.ai's <code>nb-dev</code> (short for notebook development) enable you to write whole Python libraries (including documentation) with Jupyter Notebooks.</p>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#pros-and-cons-of-notebooks-vs-python-scripts","title":"Pros and cons of notebooks vs Python scripts","text":"<p>There's arguments for both sides.</p> <p>But this list sums up a few of the main topics.</p> Pros Cons Notebooks Easy to experiment/get started Versioning can be hard Easy to share (e.g. a link to a Google Colab notebook) Hard to use only specific parts Very visual Text and graphics can get in the way of code Pros Cons Python scripts Can package code together (saves rewriting similar code across different notebooks) Experimenting isn't as visual (usually have to run the whole script rather than one cell) Can use git for versioning Many open source projects use scripts Larger projects can be run on cloud vendors (not as much support for notebooks)"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#my-workflow","title":"My workflow","text":"<p>I usually start machine learning projects in Jupyter/Google Colab notebooks for quick experimentation and visualization.</p> <p>Then when I've got something working, I move the most useful pieces of code to Python scripts.</p> <p></p> <p>There are many possible workflows for writing machine learning code. Some prefer to start with scripts, others (like me) prefer to start with notebooks and go to scripts later on.</p>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#pytorch-in-the-wild","title":"PyTorch in the wild","text":"<p>In your travels, you'll see many code repositories for PyTorch-based ML projects have instructions on how to run the PyTorch code in the form of Python scripts.</p> <p>For example, you might be instructed to run code like the following in a terminal/command line to train a model:</p> <pre><code>python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS\n</code></pre> <p> </p> <p>Running a PyTorch <code>train.py</code> script on the command line with various hyperparameter settings.</p> <p>In this case, <code>train.py</code> is the target Python script, it'll likely contain functions to train a PyTorch model.</p> <p>And <code>--model</code>, <code>--batch_size</code>, <code>--lr</code> and <code>--num_epochs</code> are known as argument flags.</p> <p>You can set these to whatever values you like and if they're compatible with <code>train.py</code>, they'll work, if not, they'll error.</p> <p>For example, let's say we wanted to train our TinyVGG model from notebook 04 for 10 epochs with a batch size of 32 and a learning rate of 0.001:</p> <pre><code>python train.py --model tinyvgg --batch_size 32 --lr 0.001 --num_epochs 10\n</code></pre> <p>You could setup any number of these argument flags in your <code>train.py</code> script to suit your needs.</p> <p>The PyTorch blog post for training state-of-the-art computer vision models uses this style.</p> <p></p> <p>PyTorch command line training script recipe for training state-of-the-art computer vision models with 8 GPUs. Source: PyTorch blog.</p>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#what-were-going-to-cover","title":"What we're going to cover","text":"<p>The main concept of this section is: turn useful notebook code cells into reusable Python files.</p> <p>Doing this will save us writing the same code over and over again.</p> <p>There are two notebooks for this section:</p> <ol> <li>05. Going Modular: Part 1 (cell mode) - this notebook is run as a traditional Jupyter Notebook/Google Colab notebook and is a condensed version of notebook 04.</li> <li>05. Going Modular: Part 2 (script mode) - this notebook is the same as number 1 but with added functionality to turn each of the major sections into Python scripts, such as, <code>data_setup.py</code> and <code>train.py</code>. </li> </ol> <p>The text in this document focuses on the code cells 05. Going Modular: Part 2 (script mode), the ones with <code>%%writefile ...</code> at the top.</p>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#why-two-parts","title":"Why two parts?","text":"<p>Because sometimes the best way to learn something is to see how it differs from something else.</p> <p>If you run each notebook side-by-side you'll see how they differ and that's where the key learnings are.</p> <p></p> <p>Running the two notebooks for section 05 side-by-side. You'll notice that the script mode notebook has extra code cells to turn code from the cell mode notebook into Python scripts.</p>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#what-were-working-towards","title":"What we're working towards","text":"<p>By the end of this section we want to have two things:</p> <ol> <li>The ability to train the model we built in notebook 04 (Food Vision Mini) with one line of code on the command line: <code>python train.py</code>.</li> <li>A directory structure of reusable Python scripts, such as: </li> </ol> <pre><code>going_modular/\n\u251c\u2500\u2500 going_modular/\n\u2502   \u251c\u2500\u2500 data_setup.py\n\u2502   \u251c\u2500\u2500 engine.py\n\u2502   \u251c\u2500\u2500 model_builder.py\n\u2502   \u251c\u2500\u2500 train.py\n\u2502   \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 05_going_modular_cell_mode_tinyvgg_model.pth\n\u2502   \u2514\u2500\u2500 05_going_modular_script_mode_tinyvgg_model.pth\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 pizza_steak_sushi/\n        \u251c\u2500\u2500 train/\n        \u2502   \u251c\u2500\u2500 pizza/\n        \u2502   \u2502   \u251c\u2500\u2500 image01.jpeg\n        \u2502   \u2502   \u2514\u2500\u2500 ...\n        \u2502   \u251c\u2500\u2500 steak/\n        \u2502   \u2514\u2500\u2500 sushi/\n        \u2514\u2500\u2500 test/\n            \u251c\u2500\u2500 pizza/\n            \u251c\u2500\u2500 steak/\n            \u2514\u2500\u2500 sushi/\n</code></pre>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#things-to-note","title":"Things to note","text":"<ul> <li>Docstrings - Writing reproducible and understandable code is important. And with this in mind, each of the functions/classes we'll be putting into scripts has been created with Google's Python docstring style in mind.</li> <li>Imports at the top of scripts - Since all of the Python scripts we're going to create could be considered a small program on their own, all of the scripts require their input modules be imported at the start of the script for example:</li> </ul> <pre><code># Import modules required for train.py\nimport os\nimport torch\nimport data_setup, engine, model_builder, utils\n\nfrom torchvision import transforms\n</code></pre>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#where-can-you-get-help","title":"Where can you get help?","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch. </p>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#0-cell-mode-vs-script-mode","title":"0. Cell mode vs. script mode","text":"<p>A cell mode notebook such as 05. Going Modular Part 1 (cell mode) is a notebook run normally, each cell in the notebook is either code or markdown.</p> <p>A script mode notebook such as 05. Going Modular Part 2 (script mode) is very similar to a cell mode notebook, however, many of the code cells may be turned into Python scripts.</p> <p>Note: You don't need to create Python scripts via a notebook, you can create them directly through an IDE (integrated developer environment) such as VS Code. Having the script mode notebook as part of this section is just to demonstrate one way of going from notebooks to Python scripts.</p>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#1-get-data","title":"1. Get data","text":"<p>Getting the data in each of the 05 notebooks happens the same as in notebook 04.</p> <p>A call is made to GitHub via Python's <code>requests</code> module to download a <code>.zip</code> file and unzip it.</p> <pre><code>import os\nimport requests\nimport zipfile\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n\n# Download pizza, steak, sushi data\nwith open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n    print(\"Downloading pizza, steak, sushi data...\")\n    f.write(request.content)\n\n# Unzip pizza, steak, sushi data\nwith zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n    print(\"Unzipping pizza, steak, sushi data...\") \n    zip_ref.extractall(image_path)\n\n# Remove zip file\nos.remove(data_path / \"pizza_steak_sushi.zip\")\n</code></pre> <p>This results in having a file called <code>data</code> that contains another directory called <code>pizza_steak_sushi</code> with images of pizza, steak and sushi in standard image classification format.</p> <pre><code>data/\n\u2514\u2500\u2500 pizza_steak_sushi/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 pizza/\n    \u2502   \u2502   \u251c\u2500\u2500 train_image01.jpeg\n    \u2502   \u2502   \u251c\u2500\u2500 test_image02.jpeg\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u251c\u2500\u2500 steak/\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 sushi/\n    \u2502       \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 test/\n        \u251c\u2500\u2500 pizza/\n        \u2502   \u251c\u2500\u2500 test_image01.jpeg\n        \u2502   \u2514\u2500\u2500 test_image02.jpeg\n        \u251c\u2500\u2500 steak/\n        \u2514\u2500\u2500 sushi/\n</code></pre>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy","title":"2. Create Datasets and DataLoaders (<code>data_setup.py</code>)","text":"<p>Once we've got data, we can then turn it into PyTorch <code>Dataset</code>'s and <code>DataLoader</code>'s (one for training data and one for testing data).</p> <p>We convert the useful <code>Dataset</code> and <code>DataLoader</code> creation code into a function called <code>create_dataloaders()</code>.</p> <p>And we write it to file using the line <code>%%writefile going_modular/data_setup.py</code>. </p> data_setup.py<pre><code>%%writefile going_modular/data_setup.py\n\"\"\"\nContains functionality for creating PyTorch DataLoaders for \nimage classification data.\n\"\"\"\nimport os\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nNUM_WORKERS = os.cpu_count()\n\ndef create_dataloaders(\n    train_dir: str, \n    test_dir: str, \n    transform: transforms.Compose, \n    batch_size: int, \n    num_workers: int=NUM_WORKERS\n):\n  \"\"\"Creates training and testing DataLoaders.\n\n  Takes in a training directory and testing directory path and turns\n  them into PyTorch Datasets and then into PyTorch DataLoaders.\n\n  Args:\n    train_dir: Path to training directory.\n    test_dir: Path to testing directory.\n    transform: torchvision transforms to perform on training and testing data.\n    batch_size: Number of samples per batch in each of the DataLoaders.\n    num_workers: An integer for number of workers per DataLoader.\n\n  Returns:\n    A tuple of (train_dataloader, test_dataloader, class_names).\n    Where class_names is a list of the target classes.\n    Example usage:\n      train_dataloader, test_dataloader, class_names = \\\n        = create_dataloaders(train_dir=path/to/train_dir,\n                             test_dir=path/to/test_dir,\n                             transform=some_transform,\n                             batch_size=32,\n                             num_workers=4)\n  \"\"\"\n  # Use ImageFolder to create dataset(s)\n  train_data = datasets.ImageFolder(train_dir, transform=transform)\n  test_data = datasets.ImageFolder(test_dir, transform=transform)\n\n  # Get class names\n  class_names = train_data.classes\n\n  # Turn images into data loaders\n  train_dataloader = DataLoader(\n      train_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n  test_dataloader = DataLoader(\n      test_data,\n      batch_size=batch_size,\n      shuffle=False, # don't need to shuffle test data\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n\n  return train_dataloader, test_dataloader, class_names\n</code></pre> <p>If we'd like to make <code>DataLoader</code>'s we can now use the function within <code>data_setup.py</code> like so:</p> <pre><code># Import data_setup.py\nfrom going_modular import data_setup\n\n# Create train/test dataloader and get class names as a list\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(...)\n</code></pre>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#3-making-a-model-model_builderpy","title":"3. Making a model (<code>model_builder.py</code>)","text":"<p>Over the past few notebooks (notebook 03 and notebook 04), we've built the TinyVGG model a few times.</p> <p>So it makes sense to put the model into its file so we can reuse it again and again.</p> <p>Let's put our <code>TinyVGG()</code> model class into a script with the line <code>%%writefile going_modular/model_builder.py</code>:</p> model_builder.py<pre><code>%%writefile going_modular/model_builder.py\n\"\"\"\nContains PyTorch model code to instantiate a TinyVGG model.\n\"\"\"\nimport torch\nfrom torch import nn \n\nclass TinyVGG(nn.Module):\n  \"\"\"Creates the TinyVGG architecture.\n\n  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n\n  Args:\n    input_shape: An integer indicating number of input channels.\n    hidden_units: An integer indicating number of hidden units between layers.\n    output_shape: An integer indicating number of output units.\n  \"\"\"\n  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n      super().__init__()\n      self.conv_block_1 = nn.Sequential(\n          nn.Conv2d(in_channels=input_shape, \n                    out_channels=hidden_units, \n                    kernel_size=3, \n                    stride=1, \n                    padding=0),  \n          nn.ReLU(),\n          nn.Conv2d(in_channels=hidden_units, \n                    out_channels=hidden_units,\n                    kernel_size=3,\n                    stride=1,\n                    padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2,\n                        stride=2)\n      )\n      self.conv_block_2 = nn.Sequential(\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(2)\n      )\n      self.classifier = nn.Sequential(\n          nn.Flatten(),\n          # Where did this in_features shape come from? \n          # It's because each layer of our network compresses and changes the shape of our inputs data.\n          nn.Linear(in_features=hidden_units*13*13,\n                    out_features=output_shape)\n      )\n\n  def forward(self, x: torch.Tensor):\n      x = self.conv_block_1(x)\n      x = self.conv_block_2(x)\n      x = self.classifier(x)\n      return x\n      # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion\n</code></pre> <p>Now instead of coding the TinyVGG model from scratch every time, we can import it using:</p> <pre><code>import torch\n# Import model_builder.py\nfrom going_modular import model_builder\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Instantiate an instance of the model from the \"model_builder.py\" script\ntorch.manual_seed(42)\nmodel = model_builder.TinyVGG(input_shape=3,\n                              hidden_units=10, \n                              output_shape=len(class_names)).to(device)\n</code></pre>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them","title":"4. Creating <code>train_step()</code> and <code>test_step()</code> functions and <code>train()</code> to combine them","text":"<p>We wrote several training functions in notebook 04:</p> <ol> <li><code>train_step()</code> - takes in a model, a <code>DataLoader</code>, a loss function and an optimizer and trains the model on the <code>DataLoader</code>.</li> <li><code>test_step()</code> - takes in a model, a <code>DataLoader</code> and a loss function and evaluates the model on the <code>DataLoader</code>.</li> <li><code>train()</code> - performs 1. and 2. together for a given number of epochs and returns a results dictionary.</li> </ol> <p>Since these will be the engine of our model training, we can put them all into a Python script called <code>engine.py</code> with the line <code>%%writefile going_modular/engine.py</code>:</p> engine.py<pre><code>%%writefile going_modular/engine.py\n\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -&gt; Tuple[float, float]:\n  \"\"\"Trains a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to training mode and then\n  runs through all of the required training steps (forward\n  pass, loss calculation, optimizer step).\n\n  Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  for batch, (X, y) in enumerate(dataloader):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -&gt; Tuple[float, float]:\n  \"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in enumerate(dataloader):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n  \"\"\"Trains and tests a PyTorch model.\n\n  Passes a target PyTorch models through train_step() and test_step()\n  functions for a number of epochs, training and testing the model\n  in the same epoch loop.\n\n  Calculates, prints and stores evaluation metrics throughout.\n\n  Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs)):\n      train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n      test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n\n  # Return the filled results at the end of the epochs\n  return results\n</code></pre> <p>Now we've got the <code>engine.py</code> script, we can import functions from it via:</p> <pre><code># Import engine.py\nfrom going_modular import engine\n\n# Use train() by calling it from engine.py\nengine.train(...)\n</code></pre>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy","title":"5. Creating a function to save the model (<code>utils.py</code>)","text":"<p>Often you'll want to save a model whilst it's training or after training.</p> <p>Since we've written the code to save a model a few times now in previous notebooks, it makes sense to turn it into a function and save it to file.</p> <p>It's common practice to store helper functions in a file called <code>utils.py</code> (short for utilities).</p> <p>Let's save our <code>save_model()</code> function to a file called <code>utils.py</code> with the line <code>%%writefile going_modular/utils.py</code>: </p> utils.py<pre><code>%%writefile going_modular/utils.py\n\"\"\"\nContains various utility functions for PyTorch model training and saving.\n\"\"\"\nimport torch\nfrom pathlib import Path\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n  \"\"\"Saves a PyTorch model to a target directory.\n\n  Args:\n    model: A target PyTorch model to save.\n    target_dir: A directory for saving the model to.\n    model_name: A filename for the saved model. Should include\n      either \".pth\" or \".pt\" as the file extension.\n\n  Example usage:\n    save_model(model=model_0,\n               target_dir=\"models\",\n               model_name=\"05_going_modular_tingvgg_model.pth\")\n  \"\"\"\n  # Create target directory\n  target_dir_path = Path(target_dir)\n  target_dir_path.mkdir(parents=True,\n                        exist_ok=True)\n\n  # Create model save path\n  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n  model_save_path = target_dir_path / model_name\n\n  # Save the model state_dict()\n  print(f\"[INFO] Saving model to: {model_save_path}\")\n  torch.save(obj=model.state_dict(),\n             f=model_save_path)\n</code></pre> <p>Now if we wanted to use our <code>save_model()</code> function, instead of writing it all over again, we can import it and use it via:</p> <pre><code># Import utils.py\nfrom going_modular import utils\n\n# Save a model to file\nsave_model(model=...\n           target_dir=...,\n           model_name=...)\n</code></pre>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#6-train-evaluate-and-save-the-model-trainpy","title":"6. Train, evaluate and save the model (<code>train.py</code>)","text":"<p>As previously discussed, you'll often come across PyTorch repositories that combine all of their functionality together in a <code>train.py</code> file.</p> <p>This file is essentially saying \"train the model using whatever data is available\".</p> <p>In our <code>train.py</code> file, we'll combine all of the functionality of the other Python scripts we've created and use it to train a model.</p> <p>This way we can train a PyTorch model using a single line of code on the command line:</p> <pre><code>python train.py\n</code></pre> <p>To create <code>train.py</code> we'll go through the following steps:</p> <ol> <li>Import the various dependencies, namely <code>torch</code>, <code>os</code>, <code>torchvision.transforms</code> and all of the scripts from the <code>going_modular</code> directory, <code>data_setup</code>, <code>engine</code>, <code>model_builder</code>, <code>utils</code>.</li> <li>Note: Since <code>train.py</code> will be inside the <code>going_modular</code> directory, we can import the other modules via <code>import ...</code> rather than <code>from going_modular import ...</code>.</li> <li>Setup various hyperparameters such as batch size, number of epochs, learning rate and number of hidden units (these could be set in the future via Python's <code>argparse</code>).</li> <li>Setup the training and test directories.</li> <li>Setup device-agnostic code.</li> <li>Create the necessary data transforms.</li> <li>Create the DataLoaders using <code>data_setup.py</code>.</li> <li>Create the model using <code>model_builder.py</code>.</li> <li>Setup the loss function and optimizer.</li> <li>Train the model using <code>engine.py</code>.</li> <li>Save the model using <code>utils.py</code>. </li> </ol> <p>And we can create the file from a notebook cell using the line <code>%%writefile going_modular/train.py</code>:</p> train.py<pre><code>%%writefile going_modular/train.py\n\"\"\"\nTrains a PyTorch image classification model using device-agnostic code.\n\"\"\"\n\nimport os\nimport torch\nimport data_setup, engine, model_builder, utils\n\nfrom torchvision import transforms\n\n# Setup hyperparameters\nNUM_EPOCHS = 5\nBATCH_SIZE = 32\nHIDDEN_UNITS = 10\nLEARNING_RATE = 0.001\n\n# Setup directories\ntrain_dir = \"data/pizza_steak_sushi/train\"\ntest_dir = \"data/pizza_steak_sushi/test\"\n\n# Setup target device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Create transforms\ndata_transform = transforms.Compose([\n  transforms.Resize((64, 64)),\n  transforms.ToTensor()\n])\n\n# Create DataLoaders with help from data_setup.py\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=data_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create model with help from model_builder.py\nmodel = model_builder.TinyVGG(\n    input_shape=3,\n    hidden_units=HIDDEN_UNITS,\n    output_shape=len(class_names)\n).to(device)\n\n# Set loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Start training with help from engine.py\nengine.train(model=model,\n             train_dataloader=train_dataloader,\n             test_dataloader=test_dataloader,\n             loss_fn=loss_fn,\n             optimizer=optimizer,\n             epochs=NUM_EPOCHS,\n             device=device)\n\n# Save the model with help from utils.py\nutils.save_model(model=model,\n                 target_dir=\"models\",\n                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")\n</code></pre> <p>Woohoo!</p> <p>Now we can train a PyTorch model by running the following line on the command line:</p> <pre><code>python train.py\n</code></pre> <p>Doing this will leverage all of the other code scripts we've created.</p> <p>And if we wanted to, we could adjust our <code>train.py</code> file to use argument flag inputs with Python's <code>argparse</code> module, this would allow us to provide different hyperparameter settings like previously discussed:</p> <pre><code>python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS\n</code></pre>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#exercises","title":"Exercises","text":"<p>Resources:</p> <ul> <li>Exercise template notebook for 05</li> <li>Example solutions notebook for 05<ul> <li>Live coding run through of solutions notebook for 05 on YouTube</li> </ul> </li> </ul> <p>Exercises:</p> <ol> <li>Turn the code to get the data (from section 1. Get Data above) into a Python script, such as <code>get_data.py</code>.<ul> <li>When you run the script using <code>python get_data.py</code> it should check if the data already exists and skip downloading if it does.</li> <li>If the data download is successful, you should be able to access the <code>pizza_steak_sushi</code> images from the <code>data</code> directory.</li> </ul> </li> <li>Use Python's <code>argparse</code> module to be able to send the <code>train.py</code> custom hyperparameter values for training procedures.<ul> <li>Add an argument for using a different:<ul> <li>Training/testing directory</li> <li>Learning rate</li> <li>Batch size</li> <li>Number of epochs to train for</li> <li>Number of hidden units in the TinyVGG model</li> </ul> </li> <li>Keep the default values for each of the above arguments as what they already are (as in notebook 05).</li> <li>For example, you should be able to run something similar to the following line to train a TinyVGG model with a learning rate of 0.003 and a batch size of 64 for 20 epochs: <code>python train.py --learning_rate 0.003 --batch_size 64 --num_epochs 20</code>.</li> <li>Note: Since <code>train.py</code> leverages the other scripts we created in section 05, such as, <code>model_builder.py</code>, <code>utils.py</code> and <code>engine.py</code>, you'll have to make sure they're available to use too. You can find these in the <code>going_modular</code> folder on the course GitHub. </li> </ul> </li> <li>Create a script to predict (such as <code>predict.py</code>) on a target image given a file path with a saved model.<ul> <li>For example, you should be able to run the command <code>python predict.py some_image.jpeg</code> and have a trained PyTorch model predict on the image and return its prediction.</li> <li>To see example prediction code, check out the predicting on a custom image section in notebook 04. </li> <li>You may also have to write code to load in a trained model.</li> </ul> </li> </ol>"},{"location":"Learning/Pytorch/05_pytorch_going_modular/#extra-curriculum","title":"Extra-curriculum","text":"<ul> <li>To learn more about structuring a Python project, check out Real Python's guide on Python Application Layouts. </li> <li>For ideas on styling your PyTorch code, check out the PyTorch style guide by Igor Susmelj (much of styling in this chapter is based off this guide + various similar PyTorch repositories).</li> <li>For an example <code>train.py</code> script and various other PyTorch scripts written by the PyTorch team to train state-of-the-art image classification models, check out their <code>classification</code> repository on GitHub. </li> </ul>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/","title":"06. PyTorch Transfer Learning","text":"<p>View Source Code | View Slides</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>[INFO] torch/torchvision versions not as required, installing nightly versions.\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\nRequirement already satisfied: torch in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (2.4.1)\nRequirement already satisfied: torchvision in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (0.19.1)\nRequirement already satisfied: torchaudio in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (2.4.1)\nRequirement already satisfied: filelock in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (1.13.2)\nRequirement already satisfied: networkx in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torch) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107-&gt;torch) (12.6.77)\nRequirement already satisfied: numpy in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torchvision) (1.24.4)\nRequirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from torchvision) (10.4.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from jinja2-&gt;torch) (2.1.5)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages (from sympy-&gt;torch) (1.3.0)\ntorch version: 2.4.1+cu121\ntorchvision version: 0.19.1+cu121\n</pre> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine <pre>/home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Now let's setup device agnostic code.</p> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p> In\u00a0[3]: Copied! <pre># Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre>import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n\n    # Remove .zip file\n    os.remove(data_path / \"pizza_steak_sushi.zip\")\n</pre> import os import zipfile  from pathlib import Path  import requests  # Setup path to data folder data_path = Path(\"data/\") image_path = data_path / \"pizza_steak_sushi\"  # If the image folder doesn't exist, download it and prepare it...  if image_path.is_dir():     print(f\"{image_path} directory exists.\") else:     print(f\"Did not find {image_path} directory, creating one...\")     image_path.mkdir(parents=True, exist_ok=True)          # Download pizza, steak, sushi data     with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")         print(\"Downloading pizza, steak, sushi data...\")         f.write(request.content)      # Unzip pizza, steak, sushi data     with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:         print(\"Unzipping pizza, steak, sushi data...\")          zip_ref.extractall(image_path)      # Remove .zip file     os.remove(data_path / \"pizza_steak_sushi.zip\") <pre>data/pizza_steak_sushi directory exists.\n</pre> <p>Excellent!</p> <p>Now we've got the same dataset we've been using previously, a series of images of pizza, steak and sushi in standard image classification format.</p> <p>Let's now create paths to our training and test directories.</p> In\u00a0[5]: Copied! <pre># Setup Dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n</pre> # Setup Dirs train_dir = image_path / \"train\" test_dir = image_path / \"test\" In\u00a0[6]: Copied! <pre># Create a transforms pipeline manually (required for torchvision &lt; 0.13)\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n    transforms.ToTensor(), # 2. Turn image values to between 0 &amp; 1 \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n])\n</pre> # Create a transforms pipeline manually (required for torchvision &lt; 0.13) manual_transforms = transforms.Compose([     transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)     transforms.ToTensor(), # 2. Turn image values to between 0 &amp; 1      transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)                          std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel), ]) <p>Wonderful!</p> <p>Now we've got a manually created series of transforms ready to prepare our images, let's create training and testing DataLoaders.</p> <p>We can create these using the <code>create_dataloaders</code> function from the <code>data_setup.py</code> script we created in 05. PyTorch Going Modular Part 2.</p> <p>We'll set <code>batch_size=32</code> so our model see's mini-batches of 32 samples at a time.</p> <p>And we can transform our images using the transform pipeline we created above by setting <code>transform=manual_transforms</code>.</p> <p>Note: I've included this manual creation of transforms in this notebook because you may come across resources that use this style. It's also important to note that because these transforms are manually created, they're also infinitely customizable. So if you wanted to included data augmentation techniques in your transforms pipeline, you could.</p> In\u00a0[7]: Copied! <pre># Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=manual_transforms, # resize, convert images to between 0 &amp; 1 and normalize them\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Create training and testing DataLoaders as well as get a list of class names train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                test_dir=test_dir,                                                                                transform=manual_transforms, # resize, convert images to between 0 &amp; 1 and normalize them                                                                                batch_size=32) # set mini-batch size to 32  train_dataloader, test_dataloader, class_names Out[7]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f6659bc0fa0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f672779ca90&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[8]: Copied! <pre># Get a set of pretrained model weights\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nweights\n</pre> # Get a set of pretrained model weights weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet weights Out[8]: <pre>EfficientNet_B0_Weights.IMAGENET1K_V1</pre> <p>And now to access the transforms assosciated with our <code>weights</code>, we can use the <code>transforms()</code> method.</p> <p>This is essentially saying \"get the data transforms that were used to train the <code>EfficientNet_B0_Weights</code> on ImageNet\".</p> In\u00a0[9]: Copied! <pre># Get the transforms used to create our pretrained weights\nauto_transforms = weights.transforms()\nauto_transforms\n</pre> # Get the transforms used to create our pretrained weights auto_transforms = weights.transforms() auto_transforms Out[9]: <pre>ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)</pre> <p>Notice how <code>auto_transforms</code> is very similar to <code>manual_transforms</code>, the only difference is that <code>auto_transforms</code> came with the model architecture we chose, where as we had to create <code>manual_transforms</code> by hand.</p> <p>The benefit of automatically creating a transform through <code>weights.transforms()</code> is that you ensure you're using the same data transformation as the pretrained model used when it was trained.</p> <p>However, the tradeoff of using automatically created transforms is a lack of customization.</p> <p>We can use <code>auto_transforms</code> to create DataLoaders with <code>create_dataloaders()</code> just as before.</p> In\u00a0[10]: Copied! <pre># Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Create training and testing DataLoaders as well as get a list of class names train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                test_dir=test_dir,                                                                                transform=auto_transforms, # perform same data transforms on our own data as the pretrained model                                                                                batch_size=32) # set mini-batch size to 32  train_dataloader, test_dataloader, class_names Out[10]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f66580e0f40&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f66580e0bb0&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[11]: Copied! <pre># OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n\n# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n#model # uncomment to output (it's very long)\n</pre> # OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13) # model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)  # NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+) weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights  model = torchvision.models.efficientnet_b0(weights=weights).to(device)  #model # uncomment to output (it's very long) <p>Note: In previous versions of <code>torchvision</code>, you'd create a pretrained model with code like:</p> <p><code>model = torchvision.models.efficientnet_b0(pretrained=True).to(device)</code></p> <p>However, running this using <code>torchvision</code> v0.13+ will result in errors such as the following:</p> <p><code>UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.</code></p> <p>And...</p> <p><code>UserWarning: Arguments other than a weight enum or None for weights are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing weights=EfficientNet_B0_Weights.IMAGENET1K_V1. You can also use weights=EfficientNet_B0_Weights.DEFAULT to get the most up-to-date weights.</code></p> <p>If we print the model, we get something similar to the following:</p> <p>Lots and lots and lots of layers.</p> <p>This is one of the benefits of transfer learning, taking an existing model, that's been crafted by some of the best engineers in the world and applying to your own problem.</p> <p>Our <code>efficientnet_b0</code> comes in three main parts:</p> <ol> <li><code>features</code> - A collection of convolutional layers and other various activation layers to learn a base representation of vision data (this base representation/collection of layers is often referred to as features or feature extractor, \"the base layers of the model learn the different features of images\").</li> <li><code>avgpool</code> - Takes the average of the output of the <code>features</code> layer(s) and turns it into a feature vector.</li> <li><code>classifier</code> - Turns the feature vector into a vector with the same dimensionality as the number of required output classes (since <code>efficientnet_b0</code> is pretrained on ImageNet and because ImageNet has 1000 classes, <code>out_features=1000</code> is the default).</li> </ol> In\u00a0[12]: Copied! <pre># Print a summary using torchinfo (uncomment for actual output)\nsummary(model=model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n) \n</pre> # Print a summary using torchinfo (uncomment for actual output) summary(model=model,          input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"         # col_names=[\"input_size\"], # uncomment for smaller output         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] )  Out[12]: <pre>============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\n\u251c\u2500Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\n\u2502    \u2514\u2500Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n\u2502    \u2514\u2500Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n\u2502    \u2514\u2500Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n\u2502    \u2514\u2500Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     15,350               True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     31,290               True\n\u2502    \u2514\u2500Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     37,130               True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n\u2502    \u2514\u2500Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    126,004              True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n\u2502    \u2514\u2500Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      262,492              True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2502    \u2514\u2500MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2514\u2500Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      717,232              True\n\u2502    \u2514\u2500Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   True\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     409,600              True\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     2,560                True\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n\u251c\u2500AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n\u251c\u2500Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\n\u2502    \u2514\u2500Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n\u2502    \u2514\u2500Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n============================================================================================================================================\nTotal params: 5,288,548\nTrainable params: 5,288,548\nNon-trainable params: 0\nTotal mult-adds (G): 12.35\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.35\nParams size (MB): 21.15\nEstimated Total Size (MB): 3492.77\n============================================================================================================================================</pre> <p>Woah!</p> <p>Now that's a big model!</p> <p>From the output of the summary, we can see all of the various input and output shape changes as our image data goes through the model.</p> <p>And there are a whole bunch more total parameters (pretrained weights) to recognize different patterns in our data.</p> <p>For reference, our model from previous sections, TinyVGG had 8,083 parameters vs. 5,288,548 parameters for <code>efficientnet_b0</code>, an increase of ~654x!</p> <p>What do you think, will this mean better performance?</p> In\u00a0[13]: Copied! <pre># Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False for param in model.features.parameters():     param.requires_grad = False <p>Feature extractor layers frozen!</p> <p>Let's now adjust the output layer or the <code>classifier</code> portion of our pretrained model to our needs.</p> <p>Right now our pretrained model has <code>out_features=1000</code> because there are 1000 classes in ImageNet.</p> <p>However, we don't have 1000 classes, we only have three, pizza, steak and sushi.</p> <p>We can change the <code>classifier</code> portion of our model by creating a new series of layers.</p> <p>The current <code>classifier</code> consists of:</p> <pre><code>(classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=True)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\n</code></pre> <p>We'll keep the <code>Dropout</code> layer the same using <code>torch.nn.Dropout(p=0.2, inplace=True)</code>.</p> <p>Note: Dropout layers randomly remove connections between two neural network layers with a probability of <code>p</code>. For example, if <code>p=0.2</code>, 20% of connections between neural network layers will be removed at random each pass. This practice is meant to help regularize (prevent overfitting) a model by making sure the connections that remain learn features to compensate for the removal of the other connections (hopefully these remaining features are more general).</p> <p>And we'll keep <code>in_features=1280</code> for our <code>Linear</code> output layer but we'll change the <code>out_features</code> value to the length of our <code>class_names</code> (<code>len(['pizza', 'steak', 'sushi']) = 3</code>).</p> <p>Our new <code>classifier</code> layer should be on the same device as our <code>model</code>.</p> In\u00a0[14]: Copied! <pre># Set the manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Get the length of class_names (one output unit for each class)\noutput_shape = len(class_names)\n\n# Recreate the classifier layer and seed it to the target device\nmodel.classifier = torch.nn.Sequential(\n    torch.nn.Dropout(p=0.2, inplace=True), \n    torch.nn.Linear(in_features=1280, \n                    out_features=output_shape, # same number of output units as our number of classes\n                    bias=True)).to(device)\n</pre> # Set the manual seeds torch.manual_seed(42) torch.cuda.manual_seed(42)  # Get the length of class_names (one output unit for each class) output_shape = len(class_names)  # Recreate the classifier layer and seed it to the target device model.classifier = torch.nn.Sequential(     torch.nn.Dropout(p=0.2, inplace=True),      torch.nn.Linear(in_features=1280,                      out_features=output_shape, # same number of output units as our number of classes                     bias=True)).to(device) <p>Nice!</p> <p>Output layer updated, let's get another summary of our model and see what's changed.</p> In\u00a0[15]: Copied! <pre># # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\nsummary(model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n        verbose=0,\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> # # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output) summary(model,          input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)         verbose=0,         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] ) Out[15]: <pre>============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n\u251c\u2500Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n\u2502    \u2514\u2500Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n\u2502    \u2514\u2500Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n\u2502    \u2514\u2500Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n\u2502    \u2514\u2500Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n\u2502    \u2514\u2500Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n\u2502    \u2514\u2500Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n\u2502    \u2514\u2500Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2502    \u2514\u2500MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2514\u2500Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n\u2502    \u2514\u2500Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n\u251c\u2500AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n\u251c\u2500Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n\u2502    \u2514\u2500Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n\u2502    \u2514\u2500Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n============================================================================================================================================\nTotal params: 4,011,391\nTrainable params: 3,843\nNon-trainable params: 4,007,548\nTotal mult-adds (G): 12.31\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.09\nParams size (MB): 16.05\nEstimated Total Size (MB): 3487.41\n============================================================================================================================================</pre> <p>Ho, ho! There's a fair few changes here!</p> <p>Let's go through them:</p> <ul> <li>Trainable column - You'll see that many of the base layers (the ones in the <code>features</code> portion) have their Trainable value as <code>False</code>. This is because we set their attribute <code>requires_grad=False</code>. Unless we change this, these layers won't be updated during furture training.</li> <li>Output shape of <code>classifier</code> - The <code>classifier</code> portion of the model now has an Output Shape value of <code>[32, 3]</code> instead of <code>[32, 1000]</code>. It's Trainable value is also <code>True</code>. This means its parameters will be updated during training. In essence, we're using the <code>features</code> portion to feed our <code>classifier</code> portion a base representation of an image and then our <code>classifier</code> layer is going to learn how to base representation aligns with our problem.</li> <li>Less trainable parameters - Previously there was 5,288,548 trainable parameters. But since we froze many of the layers of the model and only left the <code>classifier</code> as trainable, there's now only 3,843 trainable parameters (even less than our TinyVGG model). Though there's also 4,007,548 non-trainable parameters, these will create a base representation of our input images to feed into our <code>classifier</code> layer.</li> </ul> <p>Note: The more trainable parameters a model has, the more compute power/longer it takes to train. Freezing the base layers of our model and leaving it with less trainable parameters means our model should train quite quickly. This is one huge benefit of transfer learning, taking the already learned parameters of a model trained on a problem similar to yours and only tweaking the outputs slightly to suit your problem.</p> In\u00a0[16]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) <p>Wonderful!</p> <p>To train our model, we can use <code>train()</code> function we defined in the 05. PyTorch Going Modular section 04.</p> <p>The <code>train()</code> function is in the <code>engine.py</code> script inside the <code>going_modular</code> directory.</p> <p>Let's see how long it takes to train our model for 5 epochs.</p> <p>Note: We're only going to be training the parameters <code>classifier</code> here as all of the other parameters in our model have been frozen.</p> In\u00a0[17]: Copied! <pre># Set the random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Setup training and save the results\nresults = engine.train(model=model,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=5,\n                       device=device)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set the random seeds torch.manual_seed(42) torch.cuda.manual_seed(42)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Setup training and save the results results = engine.train(model=model,                        train_dataloader=train_dataloader,                        test_dataloader=test_dataloader,                        optimizer=optimizer,                        loss_fn=loss_fn,                        epochs=5,                        device=device)  # End the timer and print out how long it took end_time = timer() print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\") <pre> 20%|\u2588\u2588        | 1/5 [00:01&lt;00:06,  1.74s/it]</pre> <pre>Epoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9131 | test_acc: 0.5398\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:03&lt;00:04,  1.57s/it]</pre> <pre>Epoch: 2 | train_loss: 0.8717 | train_acc: 0.7773 | test_loss: 0.7911 | test_acc: 0.8153\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:04&lt;00:03,  1.50s/it]</pre> <pre>Epoch: 3 | train_loss: 0.7648 | train_acc: 0.7930 | test_loss: 0.7463 | test_acc: 0.8561\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:06&lt;00:01,  1.51s/it]</pre> <pre>Epoch: 4 | train_loss: 0.7109 | train_acc: 0.7539 | test_loss: 0.6372 | test_acc: 0.8655\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:07&lt;00:00,  1.53s/it]</pre> <pre>Epoch: 5 | train_loss: 0.6255 | train_acc: 0.7852 | test_loss: 0.6260 | test_acc: 0.8561\n[INFO] Total training time: 7.635 seconds\n</pre> <pre>\n</pre> <p>Wow!</p> <p>Our model trained quite fast (~5 seconds on my local machine with a NVIDIA TITAN RTX GPU/about 15 seconds on Google Colab with a NVIDIA P100 GPU).</p> <p>And it looks like it smashed our previous model results out of the park!</p> <p>With an <code>efficientnet_b0</code> backbone, our model achieves almost 85%+ accuracy on the test dataset, almost double what we were able to achieve with TinyVGG.</p> <p>Not bad for a model we downloaded with a few lines of code.</p> In\u00a0[18]: Copied! <pre># Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\ntry:\n    from helper_functions import plot_loss_curves\nexcept:\n    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n    with open(\"helper_functions.py\", \"wb\") as f:\n        import requests\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n        f.write(request.content)\n    from helper_functions import plot_loss_curves\n\n# Plot the loss curves of our model\nplot_loss_curves(results)\n</pre> # Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it try:     from helper_functions import plot_loss_curves except:     print(\"[INFO] Couldn't find helper_functions.py, downloading...\")     with open(\"helper_functions.py\", \"wb\") as f:         import requests         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")         f.write(request.content)     from helper_functions import plot_loss_curves  # Plot the loss curves of our model plot_loss_curves(results) <p>Those are some excellent looking loss curves!</p> <p>It looks like the loss for both datasets (train and test) is heading in the right direction.</p> <p>The same with the accuracy values, trending upwards.</p> <p>That goes to show the power of transfer learning. Using a pretrained model often leads to pretty good results with a small amount of data in less time.</p> <p>I wonder what would happen if you tried to train the model for longer? Or if we added more data?</p> <p>Question: Looking at the loss curves, does our model look like it's overfitting or underfitting? Or perhaps neither? Hint: Check out notebook 04. PyTorch Custom Datasets part 8. What should an ideal loss curve look like? for ideas.</p> In\u00a0[19]: Copied! <pre>from typing import List, Tuple\n\nfrom PIL import Image\n\n# 1. Take in a trained model, class names, image path, image size, a transform and target device\ndef pred_and_plot_image(model: torch.nn.Module,\n                        image_path: str, \n                        class_names: List[str],\n                        image_size: Tuple[int, int] = (224, 224),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=device):\n    \n    \n    # 2. Open image\n    img = Image.open(image_path)\n\n    # 3. Create transformation for image (if one doesn't exist)\n    if transform is not None:\n        image_transform = transform\n    else:\n        image_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\n    ### Predict on image ### \n\n    # 4. Make sure the model is on the target device\n    model.to(device)\n\n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n      transformed_image = image_transform(img).unsqueeze(dim=0)\n\n      # 7. Make a prediction on image with an extra dimension and send it to the target device\n      target_image_pred = model(transformed_image.to(device))\n\n    # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 9. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # 10. Plot image with predicted label and probability \n    plt.figure()\n    plt.imshow(img)\n    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n    plt.axis(False);\n</pre> from typing import List, Tuple  from PIL import Image  # 1. Take in a trained model, class names, image path, image size, a transform and target device def pred_and_plot_image(model: torch.nn.Module,                         image_path: str,                          class_names: List[str],                         image_size: Tuple[int, int] = (224, 224),                         transform: torchvision.transforms = None,                         device: torch.device=device):               # 2. Open image     img = Image.open(image_path)      # 3. Create transformation for image (if one doesn't exist)     if transform is not None:         image_transform = transform     else:         image_transform = transforms.Compose([             transforms.Resize(image_size),             transforms.ToTensor(),             transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225]),         ])      ### Predict on image ###       # 4. Make sure the model is on the target device     model.to(device)      # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():       # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])       transformed_image = image_transform(img).unsqueeze(dim=0)        # 7. Make a prediction on image with an extra dimension and send it to the target device       target_image_pred = model(transformed_image.to(device))      # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 9. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)      # 10. Plot image with predicted label and probability      plt.figure()     plt.imshow(img)     plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")     plt.axis(False); <p>What a good looking function!</p> <p>Let's test it out by making predictions on a few random images from the test set.</p> <p>We can get a list of all the test image paths using <code>list(Path(test_dir).glob(\"*/*.jpg\"))</code>, the stars in the <code>glob()</code> method say \"any file matching this pattern\", in other words, any file ending in <code>.jpg</code> (all of our images).</p> <p>And then we can randomly sample a number of these using Python's <code>random.sample(populuation, k)</code> where <code>population</code> is the sequence to sample and <code>k</code> is the number of samples to retrieve.</p> In\u00a0[20]: Copied! <pre># Get a random list of image paths from test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \ntest_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n\n# Make predictions on and plot the images\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=model, \n                        image_path=image_path,\n                        class_names=class_names,\n                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n                        image_size=(224, 224))\n</pre> # Get a random list of image paths from test set import random num_images_to_plot = 3 test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data  test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths                                        k=num_images_to_plot) # randomly select 'k' image paths to pred and plot  # Make predictions on and plot the images for image_path in test_image_path_sample:     pred_and_plot_image(model=model,                          image_path=image_path,                         class_names=class_names,                         # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights                         image_size=(224, 224)) <p>Woohoo!</p> <p>Those predictions look far better than the ones our TinyVGG model was previously making.</p> In\u00a0[21]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = data_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=model,                     image_path=custom_image_path,                     class_names=class_names) <pre>data/04-pizza-dad.jpeg already exists, skipping download.\n</pre> <p>Two thumbs up!</p> <p>Looks like our model got it right again!</p> <p>But this time the prediction probability is higher than the one from TinyVGG (<code>0.373</code>) in 04. PyTorch Custom Datasets section 11.3.</p> <p>This indicates our <code>efficientnet_b0</code> model is more confident in its prediction where as our TinyVGG model was par with just guessing.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#06-pytorch-transfer-learning","title":"06. PyTorch Transfer Learning\u00b6","text":"<p>Note: This notebook uses <code>torchvision</code>'s new multi-weight support API (available in <code>torchvision</code> v0.13+).</p> <p>We've built a few models by hand so far.</p> <p>But their performance has been poor.</p> <p>You might be thinking, is there a well-performing model that already exists for our problem?</p> <p>And in the world of deep learning, the answer is often yes.</p> <p>We'll see how by using a powerful technique called transfer learning.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#what-is-transfer-learning","title":"What is transfer learning?\u00b6","text":"<p>Transfer learning allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.</p> <p>For example, we can take the patterns a computer vision model has learned from datasets such as ImageNet (millions of images of different objects) and use them to power our FoodVision Mini model.</p> <p>Or we could take the patterns from a language model (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.</p> <p>The premise remains: find a well-performing existing model and apply it to your own problem.</p> <p>Example of transfer learning being applied to computer vision and natural language processing (NLP). In the case of computer vision, a computer vision model might learn patterns on millions of images in ImageNet and then use those patterns to infer on another problem. And for NLP, a language model may learn the structure of language by reading all of Wikipedia (and perhaps more) and then apply that knowledge to a different problem.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#why-use-transfer-learning","title":"Why use transfer learning?\u00b6","text":"<p>There are two main benefits to using transfer learning:</p> <ol> <li>Can leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.</li> <li>Can leverage a working model which has already learned patterns on similar data to our own. This often results in achieving great results with less custom data.</li> </ol> <p>We'll be putting these to the test for our FoodVision Mini problem, we'll take a computer vision model pretrained on ImageNet and try to leverage its underlying learned representations for classifying images of pizza, steak and sushi.</p> <p>Both research and practice support the use of transfer learning too.</p> <p>A finding from a recent machine learning research paper recommended practioner's use transfer learning wherever possible.</p> <p></p> <p>A study into the effects of whether training from scratch or using transfer learning was better from a practioner's point of view, found transfer learning to be far more beneficial in terms of cost and time. Source: How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers paper section 6 (conclusion).</p> <p>And Jeremy Howard (founder of fastai) is a big proponent of transfer learning.</p> <p>The things that really make a difference (transfer learning), if we can do better at transfer learning, it\u2019s this world changing thing. Suddenly lots more people can do world-class work with less resources and less data. \u2014 Jeremy Howard on the Lex Fridman Podcast</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#where-to-find-pretrained-models","title":"Where to find pretrained models\u00b6","text":"<p>The world of deep learning is an amazing place.</p> <p>So amazing that many people around the world share their work.</p> <p>Often, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.</p> <p>And there are several places you can find pretrained models to use for your own problems.</p> Location What's there? Link(s) PyTorch domain libraries Each of the PyTorch domain libraries (<code>torchvision</code>, <code>torchtext</code>) come with pretrained models of some form. The models there work right within PyTorch. <code>torchvision.models</code>, <code>torchtext.models</code>, <code>torchaudio.models</code>, <code>torchrec.models</code> HuggingFace Hub A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. https://huggingface.co/models, https://huggingface.co/datasets <code>timm</code> (PyTorch Image Models) library Almost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. https://github.com/rwightman/pytorch-image-models Paperswithcode A collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. https://paperswithcode.com/ <p>With access to such high-quality resources as above, it should be common practice at the start of every deep learning problem you take on to ask, \"Does a pretrained model exist for my problem?\"</p> <p>Exercise: Spend 5-minutes going through <code>torchvision.models</code> as well as the HuggingFace Hub Models page, what do you find? (there's no right answers here, it's just to practice exploring)</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to take a pretrained model from <code>torchvision.models</code> and customise it to work on (and hopefully improve) our FoodVision Mini problem.</p> Topic Contents 0. Getting setup We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. 1. Get data Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our model's results. 2. Create Datasets and DataLoaders We'll use the <code>data_setup.py</code> script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. 3. Get and customise a pretrained model Here we'll download a pretrained model from <code>torchvision.models</code> and customise it to our own problem. 4. Train model Let's see how the new pretrained model goes on our pizza, steak, sushi dataset. We'll use the training functions we created in the previous chapter. 5. Evaluate the model by plotting loss curves How did our first transfer learning model go? Did it overfit or underfit? 6. Make predictions on images from the test set It's one thing to check out a model's evaluation metrics but it's another thing to view its predictions on test samples, let's visualize, visualize, visualize!"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#0-getting-setup","title":"0. Getting setup\u00b6","text":"<p>Let's get started by importing/downloading the required modules for this section.</p> <p>To save us writing extra code, we're going to be leveraging some of the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in the previous section, 05. PyTorch Going Modular.</p> <p>Specifically, we're going to download the <code>going_modular</code> directory from the <code>pytorch-deep-learning</code> repository (if we don't already have it).</p> <p>We'll also get the <code>torchinfo</code> package if it's not available.</p> <p><code>torchinfo</code> will help later on to give us a visual representation of our model.</p> <p>Note: As of June 2022, this notebook uses the nightly versions of <code>torch</code> and <code>torchvision</code> as <code>torchvision</code> v0.13+ is required for using the updated multi-weights API. You can install these using the command below.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#1-get-data","title":"1. Get data\u00b6","text":"<p>Before we can start to use transfer learning, we'll need a dataset.</p> <p>To see how transfer learning compares to our previous attempts at model building, we'll download the same dataset we've been using for FoodVision Mini.</p> <p>Let's write some code to download the <code>pizza_steak_sushi.zip</code> dataset from the course GitHub and then unzip it.</p> <p>We can also make sure if we've already got the data, it doesn't redownload.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#2-create-datasets-and-dataloaders","title":"2. Create Datasets and DataLoaders\u00b6","text":"<p>Since we've downloaded the <code>going_modular</code> directory, we can use the <code>data_setup.py</code> script we created in section 05. PyTorch Going Modular to prepare and setup our DataLoaders.</p> <p>But since we'll be using a pretrained model from <code>torchvision.models</code>, there's a specific transform we need to prepare our images first.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#21-creating-a-transform-for-torchvisionmodels-manual-creation","title":"2.1 Creating a transform for <code>torchvision.models</code> (manual creation)\u00b6","text":"<p>Note: As of <code>torchvision</code> v0.13+, there's an update to how data transforms can be created using <code>torchvision.models</code>. I've called the previous method \"manual creation\" and the new method \"auto creation\". This notebook showcases both.</p> <p>When using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model.</p> <p>Prior to <code>torchvision</code> v0.13+, to create a transform for a pretrained model in <code>torchvision.models</code>, the documentation stated:</p> <p>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.</p> <p>The images have to be loaded in to a range of <code>[0, 1]</code> and then normalized using <code>mean = [0.485, 0.456, 0.406]</code> and <code>std = [0.229, 0.224, 0.225]</code>.</p> <p>You can use the following transform to normalize:</p> <pre><code>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n</code></pre> <p>The good news is, we can achieve the above transformations with a combination of:</p> Transform number Transform required Code to perform transform 1 Mini-batches of size <code>[batch_size, 3, height, width]</code> where height and width are at least 224x224^. <code>torchvision.transforms.Resize()</code> to resize images into <code>[3, 224, 224]</code>^ and <code>torch.utils.data.DataLoader()</code> to create batches of images. 2 Values between 0 &amp; 1. <code>torchvision.transforms.ToTensor()</code> 3 A mean of <code>[0.485, 0.456, 0.406]</code> (values across each colour channel). <code>torchvision.transforms.Normalize(mean=...)</code> to adjust the mean of our images. 4 A standard deviation of <code>[0.229, 0.224, 0.225]</code> (values across each colour channel). <code>torchvision.transforms.Normalize(std=...)</code> to adjust the standard deviation of our images. <p>Note: ^some pretrained models from <code>torchvision.models</code> in different sizes to <code>[3, 224, 224]</code>, for example, some might take them in <code>[3, 240, 240]</code>. For specific input image sizes, see the documentation.</p> <p>Question: Where did the mean and standard deviation values come from? Why do we need to do this?</p> <p>These were calculated from the data. Specifically, the ImageNet dataset by taking the means and standard deviations across a subset of images.</p> <p>We also don't need to do this. Neural networks are usually quite capable of figuring out appropriate data distributions (they'll calculate where the mean and standard deviations need to be on their own) but setting them at the start can help our networks achieve better performance quicker.</p> <p>Let's compose a series of <code>torchvision.transforms</code> to perform the above steps.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#22-creating-a-transform-for-torchvisionmodels-auto-creation","title":"2.2 Creating a transform for <code>torchvision.models</code> (auto creation)\u00b6","text":"<p>As previously stated, when using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model.</p> <p>Above we saw how to manually create a transform for a pretrained model.</p> <p>But as of <code>torchvision</code> v0.13+, an automatic transform creation feature has been added.</p> <p>When you setup a model from <code>torchvision.models</code> and select the pretrained model weights you'd like to use, for example, say we'd like to use:</p> <pre>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n</pre> <p>Where,</p> <ul> <li><code>EfficientNet_B0_Weights</code> is the model architecture weights we'd like to use (there are many differnt model architecture options in <code>torchvision.models</code>).</li> <li><code>DEFAULT</code> means the best available weights (the best performance in ImageNet).<ul> <li>Note: Depending on the model architecture you choose, you may also see other options such as <code>IMAGENET_V1</code> and <code>IMAGENET_V2</code> where generally the higher version number the better. Though if you want the best available, <code>DEFAULT</code> is the easiest option. See the <code>torchvision.models</code> documentation for more.</li> </ul> </li> </ul> <p>Let's try it out.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#3-getting-a-pretrained-model","title":"3. Getting a pretrained model\u00b6","text":"<p>Alright, here comes the fun part!</p> <p>Over the past few notebooks we've been building PyTorch neural networks from scratch.</p> <p>And while that's a good skill to have, our models haven't been performing as well as we'd like.</p> <p>That's where transfer learning comes in.</p> <p>The whole idea of transfer learning is to take an already well-performing model on a problem-space similar to yours and then customising it to your use case.</p> <p>Since we're working on a computer vision problem (image classification with FoodVision Mini), we can find pretrained classification models in <code>torchvision.models</code>.</p> <p>Exploring the documentation, you'll find plenty of common computer vision architecture backbones such as:</p> Architecuture backbone Code ResNet's <code>torchvision.models.resnet18()</code>, <code>torchvision.models.resnet50()</code>... VGG (similar to what we used for TinyVGG) <code>torchvision.models.vgg16()</code> EfficientNet's <code>torchvision.models.efficientnet_b0()</code>, <code>torchvision.models.efficientnet_b1()</code>... VisionTransformer (ViT's) <code>torchvision.models.vit_b_16()</code>, <code>torchvision.models.vit_b_32()</code>... ConvNeXt <code>torchvision.models.convnext_tiny()</code>,  <code>torchvision.models.convnext_small()</code>... More available in <code>torchvision.models</code> <code>torchvision.models...</code>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#31-which-pretrained-model-should-you-use","title":"3.1 Which pretrained model should you use?\u00b6","text":"<p>It depends on your problem/the device you're working with.</p> <p>Generally, the higher number in the model name (e.g. <code>efficientnet_b0()</code> -&gt; <code>efficientnet_b1()</code> -&gt; <code>efficientnet_b7()</code>) means better performance but a larger model.</p> <p>You might think better performance is always better, right?</p> <p>That's true but some better performing models are too big for some devices.</p> <p>For example, say you'd like to run your model on a mobile-device, you'll have to take into account the limited compute resources on the device, thus you'd be looking for a smaller model.</p> <p>But if you've got unlimited compute power, as The Bitter Lesson states, you'd likely take the biggest, most compute hungry model you can.</p> <p>Understanding this performance vs. speed vs. size tradeoff will come with time and practice.</p> <p>For me, I've found a nice balance in the <code>efficientnet_bX</code> models.</p> <p>As of May 2022, Nutrify (the machine learning powered app I'm working on) is powered by an <code>efficientnet_b0</code>.</p> <p>Comma.ai (a company that makes open source self-driving car software) uses an <code>efficientnet_b2</code> to learn a representation of the road.</p> <p>Note: Even though we're using <code>efficientnet_bX</code>, it's important not to get too attached to any one architecture, as they are always changing as new research gets released. Best to experiment, experiment, experiment and see what works for your problem.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#32-setting-up-a-pretrained-model","title":"3.2 Setting up a pretrained model\u00b6","text":"<p>The pretrained model we're going to be using is <code>torchvision.models.efficientnet_b0()</code>.</p> <p>The architecture is from the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.</p> <p>Example of what we're going to create, a pretrained <code>EfficientNet_B0</code> model from <code>torchvision.models</code> with the output layer adjusted for our use case of classifying pizza, steak and sushi images.</p> <p>We can setup the <code>EfficientNet_B0</code> pretrained ImageNet weights using the same code as we used to create the transforms.</p> <pre>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights for ImageNet\n</pre> <p>This means the model has already been trained on millions of images and has a good base representation of image data.</p> <p>The PyTorch version of this pretrained model is capable of achieving ~77.7% accuracy across ImageNet's 1000 classes.</p> <p>We'll also send it to the target device.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#33-getting-a-summary-of-our-model-with-torchinfosummary","title":"3.3 Getting a summary of our model with <code>torchinfo.summary()</code>\u00b6","text":"<p>To learn more about our model, let's use <code>torchinfo</code>'s <code>summary()</code> method.</p> <p>To do so, we'll pass in:</p> <ul> <li><code>model</code> - the model we'd like to get a summary of.</li> <li><code>input_size</code> - the shape of the data we'd like to pass to our model, for the case of <code>efficientnet_b0</code>, the input size is <code>(batch_size, 3, 224, 224)</code>, though other variants of <code>efficientnet_bX</code> have different input sizes.<ul> <li>Note: Many modern models can handle input images of varying sizes thanks to <code>torch.nn.AdaptiveAvgPool2d()</code>, this layer adaptively adjusts the <code>output_size</code> of a given input as required. You can try this out by passing different size input images to <code>summary()</code> or your models.</li> </ul> </li> <li><code>col_names</code> - the various information columns we'd like to see about our model.</li> <li><code>col_width</code> - how wide the columns should be for the summary.</li> <li><code>row_settings</code> - what features to show in a row.</li> </ul>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs","title":"3.4 Freezing the base model and changing the output layer to suit our needs\u00b6","text":"<p>The process of transfer learning usually goes: freeze some base layers of a pretrained model (typically the <code>features</code> section) and then adjust the output layers (also called head/classifier layers) to suit your needs.</p> <p>You can customise the outputs of a pretrained model by changing the output layer(s) to suit your problem. The original <code>torchvision.models.efficientnet_b0()</code> comes with <code>out_features=1000</code> because there are 1000 classes in ImageNet, the dataset it was trained on. However, for our problem, classifying images of pizza, steak and sushi we only need <code>out_features=3</code>.</p> <p>Let's freeze all of the layers/parameters in the <code>features</code> section of our <code>efficientnet_b0</code> model.</p> <p>Note: To freeze layers means to keep them how they are during training. For example, if your model has pretrained layers, to freeze them would be to say, \"don't change any of the patterns in these layers during training, keep them how they are.\" In essence, we'd like to keep the pretrained weights/patterns our model has learned from ImageNet as a backbone and then only change the output layers.</p> <p>We can freeze all of the layers/parameters in the <code>features</code> section by setting the attribute <code>requires_grad=False</code>.</p> <p>For parameters with <code>requires_grad=False</code>, PyTorch doesn't track gradient updates and in turn, these parameters won't be changed by our optimizer during training.</p> <p>In essence, a parameter with <code>requires_grad=False</code> is \"untrainable\" or \"frozen\" in place.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#4-train-model","title":"4. Train model\u00b6","text":"<p>Now we've got a pretrained model that's semi-frozen and has a customised <code>classifier</code>, how about we see transfer learning in action?</p> <p>To begin training, let's create a loss function and an optimizer.</p> <p>Because we're still working with multi-class classification, we'll use <code>nn.CrossEntropyLoss()</code> for the loss function.</p> <p>And we'll stick with <code>torch.optim.Adam()</code> as our optimizer with <code>lr=0.001</code>.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#5-evaluate-model-by-plotting-loss-curves","title":"5. Evaluate model by plotting loss curves\u00b6","text":"<p>Our model looks like it's performing pretty well.</p> <p>Let's plot it's loss curves to see what the training looks like over time.</p> <p>We can plot the loss curves using the function <code>plot_loss_curves()</code> we created in 04. PyTorch Custom Datasets section 7.8.</p> <p>The function is stored in the <code>helper_functions.py</code> script so we'll try to import it and download the script if we don't have it.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set","title":"6. Make predictions on images from the test set\u00b6","text":"<p>It looks like our model performs well quantitatively but how about qualitatively?</p> <p>Let's find out by making some predictions with our model on images from the test set (these aren't seen during training) and plotting them.</p> <p>Visualize, visualize, visualize!</p> <p>One thing we'll have to remember is that for our model to make predictions on an image, the image has to be in same format as the images our model was trained on.</p> <p>This means we'll need to make sure our images have:</p> <ul> <li>Same shape - If our images are different shapes to what our model was trained on, we'll get shape errors.</li> <li>Same datatype - If our images are a different datatype (e.g. <code>torch.int8</code> vs. <code>torch.float32</code>) we'll get datatype errors.</li> <li>Same device - If our images are on a different device to our model, we'll get device errors.</li> <li>Same transformations - If our model is trained on images that have been transformed in certain way (e.g. normalized with a specific mean and standard deviation) and we try and make preidctions on images transformed in a different way, these predictions may be off.</li> </ul> <p>Note: These requirements go for all kinds of data if you're trying to make predictions with a trained model. Data you'd like to predict on should be in the same format as your model was trained on.</p> <p>To do all of this, we'll create a function <code>pred_and_plot_image()</code> to:</p> <ol> <li>Take in a trained model, a list of class names, a filepath to a target image, an image size, a transform and a target device.</li> <li>Open an image with <code>PIL.Image.open()</code>.</li> <li>Create a transform for the image (this will default to the <code>manual_transforms</code> we created above or it could use a transform generated from <code>weights.transforms()</code>).</li> <li>Make sure the model is on the target device.</li> <li>Turn on model eval mode with <code>model.eval()</code> (this turns off layers like <code>nn.Dropout()</code>, so they aren't used for inference) and the inference mode context manager.</li> <li>Transform the target image with the transform made in step 3 and add an extra batch dimension with <code>torch.unsqueeze(dim=0)</code> so our input image has shape <code>[batch_size, color_channels, height, width]</code>.</li> <li>Make a prediction on the image by passing it to the model ensuring it's on the target device.</li> <li>Convert the model's output logits to prediction probabilities with <code>torch.softmax()</code>.</li> <li>Convert model's prediction probabilities to prediction labels with <code>torch.argmax()</code>.</li> <li>Plot the image with <code>matplotlib</code> and set the title to the prediction label from step 9 and prediction probability from step 8.</li> </ol> <p>Note: This is a similar function to 04. PyTorch Custom Datasets section 11.3's <code>pred_and_plot_image()</code> with a few tweaked steps.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#61-making-predictions-on-a-custom-image","title":"6.1 Making predictions on a custom image\u00b6","text":"<p>It looks like our model does well qualitatively on data from the test set.</p> <p>But how about on our own custom image?</p> <p>That's where the real fun of machine learning is!</p> <p>Predicting on your own custom data, outisde of any training or test set.</p> <p>To test our model on a custom image, let's import the old faithful <code>pizza-dad.jpeg</code> image (an image of my dad eating pizza).</p> <p>We'll then pass it to the <code>pred_and_plot_image()</code> function we created above and see what happens.</p>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#main-takeaways","title":"Main takeaways\u00b6","text":"<ul> <li>Transfer learning often allows to you get good results with a relatively small amount of custom data.</li> <li>Knowing the power of transfer learning, it's a good idea to ask at the start of every problem, \"does an existing well-performing model exist for my problem?\"</li> <li>When using a pretrained model, it's important that your custom data be formatted/preprocessed in the same way that the original model was trained on, otherwise you may get degraded performance.</li> <li>The same goes for predicting on custom data, ensure your custom data is in the same format as the data your model was trained on.</li> <li>There are several different places to find pretrained models from the PyTorch domain libraries, HuggingFace Hub and libraries such as <code>timm</code> (PyTorch Image Models).</li> </ul>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 06</li> <li>Example solutions notebook for 06 (try the exercises before looking at this)<ul> <li>See a live video walkthrough of the solutions on YouTube (errors and all)</li> </ul> </li> </ul> <ol> <li>Make predictions on the entire test dataset and plot a confusion matrix for the results of our model compared to the truth labels. Check out 03. PyTorch Computer Vision section 10 for ideas.</li> <li>Get the \"most wrong\" of the predictions on the test dataset and plot the 5 \"most wrong\" images. You can do this by:<ul> <li>Predicting across all of the test dataset, storing the labels and predicted probabilities.</li> <li>Sort the predictions by wrong prediction and then descending predicted probabilities, this will give you the wrong predictions with the highest prediction probabilities, in other words, the \"most wrong\".</li> <li>Plot the top 5 \"most wrong\" images, why do you think the model got these wrong?</li> </ul> </li> <li>Predict on your own image of pizza/steak/sushi - how does the model go? What happens if you predict on an image that isn't pizza/steak/sushi?</li> <li>Train the model from section 4 above for longer (10 epochs should do), what happens to the performance?</li> <li>Train the model from section 4 above with more data, say 20% of the images from Food101 of Pizza, Steak and Sushi images.<ul> <li>You can find the 20% Pizza, Steak, Sushi dataset on the course GitHub. It was created with the notebook <code>extras/04_custom_data_creation.ipynb</code>.</li> </ul> </li> <li>Try a different model from <code>torchvision.models</code> on the Pizza, Steak, Sushi data, how does this model perform?<ul> <li>You'll have to change the size of the classifier layer to suit our problem.</li> <li>You may want to try an EfficientNet with a higher number than our B0, perhaps <code>torchvision.models.efficientnet_b2()</code>?</li> </ul> </li> </ol>"},{"location":"Learning/Pytorch/06_pytorch_transfer_learning/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Look up what \"model fine-tuning\" is and spend 30-minutes researching different methods to perform it with PyTorch. How would we change our code to fine-tine? Tip: fine-tuning usually works best if you have lots of custom data, where as, feature extraction is typically better if you have less custom data.</li> <li>Check out the new/upcoming PyTorch multi-weights API (still in beta at time of writing, May 2022), it's a new way to perform transfer learning in PyTorch. What changes to our code would need to made to use the new API?</li> <li>Try to create your own classifier on two classes of images, for example, you could collect 10 photos of your dog and your friends dog and train a model to classify the two dogs. This would be a good way to practice creating a dataset as well as building a model on that dataset.</li> </ul>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/","title":"07. PyTorch Experiment Tracking","text":"<p>View Source Code | View Slides</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n</pre> <p>Note: If you're using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of <code>torch</code> (0.12+) and <code>torchvision</code> (0.13+).</p> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine <p>Now let's setup device agnostic code.</p> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p> In\u00a0[3]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre># Set seeds\ndef set_seeds(seed: int=42):\n    \"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    # Set the seed for general torch operations\n    torch.manual_seed(seed)\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n    torch.cuda.manual_seed(seed)\n</pre> # Set seeds def set_seeds(seed: int=42):     \"\"\"Sets random sets for torch operations.      Args:         seed (int, optional): Random seed to set. Defaults to 42.     \"\"\"     # Set the seed for general torch operations     torch.manual_seed(seed)     # Set the seed for CUDA torch operations (ones that happen on the GPU)     torch.cuda.manual_seed(seed) In\u00a0[5]: Copied! <pre>import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\ndef download_data(source: str, \n                  destination: str,\n                  remove_source: bool = True) -&gt; Path:\n    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n\n    Args:\n        source (str): A link to a zipped file containing data.\n        destination (str): A target directory to unzip data to.\n        remove_source (bool): Whether to remove the source after downloading and extracting.\n    \n    Returns:\n        pathlib.Path to downloaded data.\n    \n    Example usage:\n        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                      destination=\"pizza_steak_sushi\")\n    \"\"\"\n    # Setup path to data folder\n    data_path = Path(\"data/\")\n    image_path = data_path / destination\n\n    # If the image folder doesn't exist, download it and prepare it... \n    if image_path.is_dir():\n        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n    else:\n        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n        image_path.mkdir(parents=True, exist_ok=True)\n        \n        # Download pizza, steak, sushi data\n        target_file = Path(source).name\n        with open(data_path / target_file, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"[INFO] Downloading {target_file} from {source}...\")\n            f.write(request.content)\n\n        # Unzip pizza, steak, sushi data\n        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n            print(f\"[INFO] Unzipping {target_file} data...\") \n            zip_ref.extractall(image_path)\n\n        # Remove .zip file\n        if remove_source:\n            os.remove(data_path / target_file)\n    \n    return image_path\n\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> import os import zipfile  from pathlib import Path  import requests  def download_data(source: str,                    destination: str,                   remove_source: bool = True) -&gt; Path:     \"\"\"Downloads a zipped dataset from source and unzips to destination.      Args:         source (str): A link to a zipped file containing data.         destination (str): A target directory to unzip data to.         remove_source (bool): Whether to remove the source after downloading and extracting.          Returns:         pathlib.Path to downloaded data.          Example usage:         download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                       destination=\"pizza_steak_sushi\")     \"\"\"     # Setup path to data folder     data_path = Path(\"data/\")     image_path = data_path / destination      # If the image folder doesn't exist, download it and prepare it...      if image_path.is_dir():         print(f\"[INFO] {image_path} directory exists, skipping download.\")     else:         print(f\"[INFO] Did not find {image_path} directory, creating one...\")         image_path.mkdir(parents=True, exist_ok=True)                  # Download pizza, steak, sushi data         target_file = Path(source).name         with open(data_path / target_file, \"wb\") as f:             request = requests.get(source)             print(f\"[INFO] Downloading {target_file} from {source}...\")             f.write(request.content)          # Unzip pizza, steak, sushi data         with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:             print(f\"[INFO] Unzipping {target_file} data...\")              zip_ref.extractall(image_path)          # Remove .zip file         if remove_source:             os.remove(data_path / target_file)          return image_path  image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[5]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>Excellent! Looks like we've got our pizza, steak and sushi images in standard image classification format ready to go.</p> In\u00a0[6]: Copied! <pre># Setup directories\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup directories train_dir = image_path / \"train\" test_dir = image_path / \"test\"  # Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Create transform pipeline manually manual_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])            print(f\"Manually created transforms: {manual_transforms}\")  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=manual_transforms, # use manually created transforms     batch_size=32 )  train_dataloader, test_dataloader, class_names <pre>Manually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)\n</pre> Out[6]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d218e0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d216a0&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[7]: Copied! <pre># Setup dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup pretrained weights (plenty of these available in torchvision.models)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n\n# Get transforms from weights (these are the transforms that were used to obtain the weights)\nautomatic_transforms = weights.transforms() \nprint(f\"Automatically created transforms: {automatic_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=automatic_transforms, # use automatic created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup dirs train_dir = image_path / \"train\" test_dir = image_path / \"test\"  # Setup pretrained weights (plenty of these available in torchvision.models) weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # Get transforms from weights (these are the transforms that were used to obtain the weights) automatic_transforms = weights.transforms()  print(f\"Automatically created transforms: {automatic_transforms}\")  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=automatic_transforms, # use automatic created transforms     batch_size=32 )  train_dataloader, test_dataloader, class_names <pre>Automatically created transforms: ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n</pre> Out[7]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d213a0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d21490&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[8]: Copied! <pre># Note: This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions.\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD \n\n# Download the pretrained weights for EfficientNet_B0\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"\n\n# Setup the model with the pretrained weights and send it to the target device\nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n# View the output of the model\n# model\n</pre> # Note: This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions. # model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD   # Download the pretrained weights for EfficientNet_B0 weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"  # Setup the model with the pretrained weights and send it to the target device model = torchvision.models.efficientnet_b0(weights=weights).to(device)  # View the output of the model # model <p>Wonderful!</p> <p>Now we've got a pretrained model let's turn into a feature extractor model.</p> <p>In essence, we'll freeze the base layers of the model (we'll use these to extract features from our input images) and we'll change the classifier head (output layer) to suit the number of classes we're working with (we've got 3 classes: pizza, steak, sushi).</p> <p>Note: The idea of creating a feature extractor model (what we're doing here) was covered in more depth in 06. PyTorch Transfer Learning section 3.2: Setting up a pretrained model.</p> In\u00a0[9]: Copied! <pre># Freeze all base layers by setting requires_grad attribute to False\nfor param in model.features.parameters():\n    param.requires_grad = False\n    \n# Since we're creating a new layer with random weights (torch.nn.Linear), \n# let's set the seeds\nset_seeds() \n\n# Update the classifier head to suit our problem\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(in_features=1280, \n              out_features=len(class_names),\n              bias=True).to(device))\n</pre> # Freeze all base layers by setting requires_grad attribute to False for param in model.features.parameters():     param.requires_grad = False      # Since we're creating a new layer with random weights (torch.nn.Linear),  # let's set the seeds set_seeds()   # Update the classifier head to suit our problem model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2, inplace=True),     nn.Linear(in_features=1280,                out_features=len(class_names),               bias=True).to(device)) <p>Base layers frozen, classifier head changed, let's get a summary of our model with <code>torchinfo.summary()</code>.</p> In\u00a0[10]: Copied! <pre>from torchinfo import summary\n\n# # Get a summary of the model (uncomment for full output)\n# summary(model, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n#         verbose=0,\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> from torchinfo import summary  # # Get a summary of the model (uncomment for full output) # summary(model,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width) #         verbose=0, #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # ) <p>Output of <code>torchinfo.summary()</code> with our feature extractor EffNetB0 model, notice how the base layers are frozen (not trainable) and the output layers are customized to our own problem.</p> In\u00a0[11]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[12]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\n\n# Create a writer with all default settings\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter  # Create a writer with all default settings writer = SummaryWriter() <p>Now to use the writer, we could write a new training loop or we could adjust the existing <code>train()</code> function we created in 05. PyTorch Going Modular section 4.</p> <p>Let's take the latter option.</p> <p>We'll get the <code>train()</code> function from <code>engine.py</code> and adjust it to use <code>writer</code>.</p> <p>Specifically, we'll add the ability for our <code>train()</code> function to log our model's training and test loss and accuracy values.</p> <p>We can do this with <code>writer.add_scalars(main_tag, tag_scalar_dict)</code>, where:</p> <ul> <li><code>main_tag</code> (string) - the name for the scalars being tracked (e.g. \"Accuracy\")</li> <li><code>tag_scalar_dict</code> (dict) - a dictionary of the values being tracked (e.g. <code>{\"train_loss\": 0.3454}</code>)<ul> <li> <p>Note: The method is called <code>add_scalars()</code> because our loss and accuracy values are generally scalars (single values).</p> </li> </ul> </li> </ul> <p>Once we've finished tracking values, we'll call <code>writer.close()</code> to tell the <code>writer</code> to stop looking for values to track.</p> <p>To start modifying <code>train()</code> we'll also import <code>train_step()</code> and <code>test_step()</code> from <code>engine.py</code>.</p> <p>Note: You can track information about your model almost anywhere in your code. But quite often experiments will be tracked while a model is training (inside a training/testing loop).</p> <p>The <code>torch.utils.tensorboard.SummaryWriter()</code> class also has many different methods to track different things about your model/data, such as <code>add_graph()</code> which tracks the computation graph of your model. For more options, check the <code>SummaryWriter()</code> documentation.</p> In\u00a0[13]: Copied! <pre>from typing import Dict, List\nfrom tqdm.auto import tqdm\n\nfrom going_modular.going_modular.engine import train_step, test_step\n\n# Import train() function from: \n# https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      \n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer,\n                                           device=device)\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        ### New: Experiment tracking ###\n        # Add loss results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n\n        # Add accuracy results to SummaryWriter\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n        \n        # Track the PyTorch model architecture\n        writer.add_graph(model=model, \n                         # Pass in an example input\n                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n    \n    # Close the writer\n    writer.close()\n    \n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n</pre> from typing import Dict, List from tqdm.auto import tqdm  from going_modular.going_modular.engine import train_step, test_step  # Import train() function from:  # https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device) -&gt; Dict[str, List]:     \"\"\"Trains and tests a PyTorch model.      Passes a target PyTorch models through train_step() and test_step()     functions for a number of epochs, training and testing the model     in the same epoch loop.      Calculates, prints and stores evaluation metrics throughout.      Args:       model: A PyTorch model to be trained and tested.       train_dataloader: A DataLoader instance for the model to be trained on.       test_dataloader: A DataLoader instance for the model to be tested on.       optimizer: A PyTorch optimizer to help minimize the loss function.       loss_fn: A PyTorch loss function to calculate loss on both datasets.       epochs: An integer indicating how many epochs to train for.       device: A target device to compute on (e.g. \"cuda\" or \"cpu\").            Returns:       A dictionary of training and testing loss as well as training and       testing accuracy metrics. Each metric has a value in a list for        each epoch.       In the form: {train_loss: [...],                 train_acc: [...],                 test_loss: [...],                 test_acc: [...]}        For example if training for epochs=2:                {train_loss: [2.0616, 1.0537],                 train_acc: [0.3945, 0.3945],                 test_loss: [1.2641, 1.5706],                 test_acc: [0.3400, 0.2973]}      \"\"\"     # Create empty results dictionary     results = {\"train_loss\": [],                \"train_acc\": [],                \"test_loss\": [],                \"test_acc\": []     }      # Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                            dataloader=train_dataloader,                                            loss_fn=loss_fn,                                            optimizer=optimizer,                                            device=device)         test_loss, test_acc = test_step(model=model,                                         dataloader=test_dataloader,                                         loss_fn=loss_fn,                                         device=device)          # Print out what's happening         print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f}\"         )          # Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          ### New: Experiment tracking ###         # Add loss results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)          # Add accuracy results to SummaryWriter         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)                  # Track the PyTorch model architecture         writer.add_graph(model=model,                           # Pass in an example input                          input_to_model=torch.randn(32, 3, 224, 224).to(device))          # Close the writer     writer.close()          ### End new ###      # Return the filled results at the end of the epochs     return results <p>Woohoo!</p> <p>Our <code>train()</code> function is now updated to use a <code>SummaryWriter()</code> instance to track our model's results.</p> <p>How about we try it out for 5 epochs?</p> In\u00a0[14]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated to use writer\nset_seeds()\nresults = train(model=model,\n                train_dataloader=train_dataloader,\n                test_dataloader=test_dataloader,\n                optimizer=optimizer,\n                loss_fn=loss_fn,\n                epochs=5,\n                device=device)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated to use writer set_seeds() results = train(model=model,                 train_dataloader=train_dataloader,                 test_dataloader=test_dataloader,                 optimizer=optimizer,                 loss_fn=loss_fn,                 epochs=5,                 device=device) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8975 | train_acc: 0.6562 | test_loss: 0.7838 | test_acc: 0.8561\nEpoch: 3 | train_loss: 0.8037 | train_acc: 0.7461 | test_loss: 0.6723 | test_acc: 0.8864\nEpoch: 4 | train_loss: 0.6769 | train_acc: 0.8516 | test_loss: 0.6698 | test_acc: 0.8049\nEpoch: 5 | train_loss: 0.7065 | train_acc: 0.7188 | test_loss: 0.6746 | test_acc: 0.7737\n</pre> <p>Note: You might notice the results here are slightly different to what our model got in 06. PyTorch Transfer Learning. The difference comes from using the <code>engine.train()</code> and our modified <code>train()</code> function. Can you guess why? The PyTorch documentation on randomness may help more.</p> <p>Running the cell above we get similar outputs we got in 06. PyTorch Transfer Learning section 4: Train model but the difference is behind the scenes our <code>writer</code> instance has created a <code>runs/</code> directory storing our model's results.</p> <p>For example, the save location might look like:</p> <pre><code>runs/Jun21_00-46-03_daniels_macbook_pro\n</code></pre> <p>Where the default format is <code>runs/CURRENT_DATETIME_HOSTNAME</code>.</p> <p>We'll check these out in a second but just as a reminder, we were previously tracking our model's results in a dictionary.</p> In\u00a0[15]: Copied! <pre># Check out the model results\nresults\n</pre> # Check out the model results results Out[15]: <pre>{'train_loss': [1.0923754647374153,\n  0.8974628075957298,\n  0.803724929690361,\n  0.6769256368279457,\n  0.7064960040152073],\n 'train_acc': [0.3984375, 0.65625, 0.74609375, 0.8515625, 0.71875],\n 'test_loss': [0.9132757981618246,\n  0.7837507526079813,\n  0.6722926497459412,\n  0.6698453426361084,\n  0.6746167540550232],\n 'test_acc': [0.5397727272727273,\n  0.8560606060606061,\n  0.8863636363636364,\n  0.8049242424242425,\n  0.7736742424242425]}</pre> <p>Hmmm, we could format this to be a nice plot but could you imagine keeping track of a bunch of these dictionaries?</p> <p>There has to be a better way...</p> In\u00a0[16]: Copied! <pre># Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n</pre> # Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out) # %load_ext tensorboard # %tensorboard --logdir runs <p>If all went correctly, you should see something like the following:</p> <p>Viewing a single modelling experiment's results for accuracy and loss in TensorBoard.</p> <p>Note: For more information on running TensorBoard in notebooks or in other locations, see the following:</p> <ul> <li>Using TensorBoard in Notebooks guide by TensorFlow</li> <li>Get started with TensorBoard.dev (helpful for uploading your TensorBoard logs to a shareable link)</li> </ul> In\u00a0[17]: Copied! <pre>def create_writer(experiment_name: str, \n                  model_name: str, \n                  extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():\n    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n\n    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n\n    Where timestamp is the current date in YYYY-MM-DD format.\n\n    Args:\n        experiment_name (str): Name of experiment.\n        model_name (str): Name of model.\n        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n\n    Returns:\n        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n\n    Example usage:\n        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n        writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb2\",\n                               extra=\"5_epochs\")\n        # The above is the same as:\n        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n    \"\"\"\n    from datetime import datetime\n    import os\n\n    # Get timestamp of current date (all experiments on certain day live in same folder)\n    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n\n    if extra:\n        # Create log directory path\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n    else:\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n        \n    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n    return SummaryWriter(log_dir=log_dir)\n</pre> def create_writer(experiment_name: str,                    model_name: str,                    extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():     \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.      log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.      Where timestamp is the current date in YYYY-MM-DD format.      Args:         experiment_name (str): Name of experiment.         model_name (str): Name of model.         extra (str, optional): Anything extra to add to the directory. Defaults to None.      Returns:         torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.      Example usage:         # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"         writer = create_writer(experiment_name=\"data_10_percent\",                                model_name=\"effnetb2\",                                extra=\"5_epochs\")         # The above is the same as:         writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")     \"\"\"     from datetime import datetime     import os      # Get timestamp of current date (all experiments on certain day live in same folder)     timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format      if extra:         # Create log directory path         log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)     else:         log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)              print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")     return SummaryWriter(log_dir=log_dir) <p>Beautiful!</p> <p>Now we've got a <code>create_writer()</code> function, let's try it out.</p> In\u00a0[18]: Copied! <pre># Create an example writer\nexample_writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb0\",\n                               extra=\"5_epochs\")\n</pre> # Create an example writer example_writer = create_writer(experiment_name=\"data_10_percent\",                                model_name=\"effnetb0\",                                extra=\"5_epochs\") <pre>[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n</pre> <p>Looking good, now we've got a way to log and trace back our various experiments.</p> In\u00a0[19]: Copied! <pre>from typing import Dict, List\nfrom tqdm.auto import tqdm\n\n# Add writer parameter to train()\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device, \n          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n          ) -&gt; Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Stores metrics to specified writer log_dir if present.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      writer: A SummaryWriter() instance to log model results to.\n\n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n\n        ### New: Use the writer parameter to track experiments ###\n        # See if there's a writer, if so, log to it\n        if writer:\n            # Add results to SummaryWriter\n            writer.add_scalars(main_tag=\"Loss\", \n                               tag_scalar_dict={\"train_loss\": train_loss,\n                                                \"test_loss\": test_loss},\n                               global_step=epoch)\n            writer.add_scalars(main_tag=\"Accuracy\", \n                               tag_scalar_dict={\"train_acc\": train_acc,\n                                                \"test_acc\": test_acc}, \n                               global_step=epoch)\n\n            # Close the writer\n            writer.close()\n        else:\n            pass\n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n</pre> from typing import Dict, List from tqdm.auto import tqdm  # Add writer parameter to train() def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device,            writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer           ) -&gt; Dict[str, List]:     \"\"\"Trains and tests a PyTorch model.      Passes a target PyTorch models through train_step() and test_step()     functions for a number of epochs, training and testing the model     in the same epoch loop.      Calculates, prints and stores evaluation metrics throughout.      Stores metrics to specified writer log_dir if present.      Args:       model: A PyTorch model to be trained and tested.       train_dataloader: A DataLoader instance for the model to be trained on.       test_dataloader: A DataLoader instance for the model to be tested on.       optimizer: A PyTorch optimizer to help minimize the loss function.       loss_fn: A PyTorch loss function to calculate loss on both datasets.       epochs: An integer indicating how many epochs to train for.       device: A target device to compute on (e.g. \"cuda\" or \"cpu\").       writer: A SummaryWriter() instance to log model results to.      Returns:       A dictionary of training and testing loss as well as training and       testing accuracy metrics. Each metric has a value in a list for        each epoch.       In the form: {train_loss: [...],                 train_acc: [...],                 test_loss: [...],                 test_acc: [...]}        For example if training for epochs=2:                {train_loss: [2.0616, 1.0537],                 train_acc: [0.3945, 0.3945],                 test_loss: [1.2641, 1.5706],                 test_acc: [0.3400, 0.2973]}      \"\"\"     # Create empty results dictionary     results = {\"train_loss\": [],                \"train_acc\": [],                \"test_loss\": [],                \"test_acc\": []     }      # Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                           dataloader=train_dataloader,                                           loss_fn=loss_fn,                                           optimizer=optimizer,                                           device=device)         test_loss, test_acc = test_step(model=model,           dataloader=test_dataloader,           loss_fn=loss_fn,           device=device)          # Print out what's happening         print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f}\"         )          # Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)           ### New: Use the writer parameter to track experiments ###         # See if there's a writer, if so, log to it         if writer:             # Add results to SummaryWriter             writer.add_scalars(main_tag=\"Loss\",                                 tag_scalar_dict={\"train_loss\": train_loss,                                                 \"test_loss\": test_loss},                                global_step=epoch)             writer.add_scalars(main_tag=\"Accuracy\",                                 tag_scalar_dict={\"train_acc\": train_acc,                                                 \"test_acc\": test_acc},                                 global_step=epoch)              # Close the writer             writer.close()         else:             pass     ### End new ###      # Return the filled results at the end of the epochs     return results In\u00a0[20]: Copied! <pre># Download 10 percent and 20 percent training data (if necessary)\ndata_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                                     destination=\"pizza_steak_sushi\")\n\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n</pre> # Download 10 percent and 20 percent training data (if necessary) data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                                      destination=\"pizza_steak_sushi\")  data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",                                      destination=\"pizza_steak_sushi_20_percent\") <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n</pre> <p>Data downloaded!</p> <p>Now let's setup the filepaths to data we'll be using for the different experiments.</p> <p>We'll create different training directory paths but we'll only need one testing directory path since all experiments will be using the same test dataset (the test dataset from pizza, steak, sushi 10%).</p> In\u00a0[21]: Copied! <pre># Setup training directory paths\ntrain_dir_10_percent = data_10_percent_path / \"train\"\ntrain_dir_20_percent = data_20_percent_path / \"train\"\n\n# Setup testing directory paths (note: use the same test dataset for both to compare the results)\ntest_dir = data_10_percent_path / \"test\"\n\n# Check the directories\nprint(f\"Training directory 10%: {train_dir_10_percent}\")\nprint(f\"Training directory 20%: {train_dir_20_percent}\")\nprint(f\"Testing directory: {test_dir}\")\n</pre> # Setup training directory paths train_dir_10_percent = data_10_percent_path / \"train\" train_dir_20_percent = data_20_percent_path / \"train\"  # Setup testing directory paths (note: use the same test dataset for both to compare the results) test_dir = data_10_percent_path / \"test\"  # Check the directories print(f\"Training directory 10%: {train_dir_10_percent}\") print(f\"Training directory 20%: {train_dir_20_percent}\") print(f\"Testing directory: {test_dir}\") <pre>Training directory 10%: data/pizza_steak_sushi/train\nTraining directory 20%: data/pizza_steak_sushi_20_percent/train\nTesting directory: data/pizza_steak_sushi/test\n</pre> In\u00a0[22]: Copied! <pre>from torchvision import transforms\n\n# Create a transform to normalize data distribution to be inline with ImageNet\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n\n# Compose transforms into a pipeline\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Resize the images\n    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 &amp; 1\n    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n])\n</pre> from torchvision import transforms  # Create a transform to normalize data distribution to be inline with ImageNet normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]                                  std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]  # Compose transforms into a pipeline simple_transform = transforms.Compose([     transforms.Resize((224, 224)), # 1. Resize the images     transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 &amp; 1     normalize # 3. Normalize the images so their distributions match the ImageNet dataset  ]) <p>Transform ready!</p> <p>Now let's create our DataLoaders using the <code>create_dataloaders()</code> function from <code>data_setup.py</code> we created in 05. PyTorch Going Modular section 2.</p> <p>We'll create the DataLoaders with a batch size of 32.</p> <p>For all of our experiments we'll be using the same <code>test_dataloader</code> (to keep comparisons consistent).</p> In\u00a0[23]: Copied! <pre>BATCH_SIZE = 32\n\n# Create 10% training and test DataLoaders\ntrain_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n    test_dir=test_dir, \n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create 20% training and test data DataLoders\ntrain_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\nprint(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\nprint(f\"Number of classes: {len(class_names)}, class names: {class_names}\")\n</pre> BATCH_SIZE = 32  # Create 10% training and test DataLoaders train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,     test_dir=test_dir,      transform=simple_transform,     batch_size=BATCH_SIZE )  # Create 20% training and test data DataLoders train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,     test_dir=test_dir,     transform=simple_transform,     batch_size=BATCH_SIZE )  # Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments) print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\") print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\") print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\") print(f\"Number of classes: {len(class_names)}, class names: {class_names}\") <pre>Number of batches of size 32 in 10 percent training data: 8\nNumber of batches of size 32 in 20 percent training data: 15\nNumber of batches of size 32 in testing data: 8 (all experiments will use the same test set)\nNumber of classes: 3, class names: ['pizza', 'steak', 'sushi']\n</pre> In\u00a0[24]: Copied! <pre>import torchvision\nfrom torchinfo import summary\n\n# 1. Create an instance of EffNetB2 with pretrained weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n\n# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n\n# 3. Get the number of in_features of the EfficientNetB2 classifier layer\nprint(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")\n</pre> import torchvision from torchinfo import summary  # 1. Create an instance of EffNetB2 with pretrained weights effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)  # # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output) # summary(model=effnetb2,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )   # 3. Get the number of in_features of the EfficientNetB2 classifier layer print(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\") <pre>Number of in_features to final layer of EfficientNetB2: 1408\n</pre> <p>Model summary of EffNetB2 feature extractor model with all layers unfrozen (trainable) and default classifier head from ImageNet pretraining.</p> <p>Now we know the required number of <code>in_features</code> for the EffNetB2 model, let's create a couple of helper functions to setup our EffNetB0 and EffNetB2 feature extractor models.</p> <p>We want these functions to:</p> <ol> <li>Get the base model from <code>torchvision.models</code></li> <li>Freeze the base layers in the model (set <code>requires_grad=False</code>)</li> <li>Set the random seeds (we don't need to do this but since we're running a series of experiments and initalizing a new layer with random weights, we want the randomness to be similar for each experiment)</li> <li>Change the classifier head (to suit our problem)</li> <li>Give the model a name (e.g. \"effnetb0\" for EffNetB0)</li> </ol> In\u00a0[25]: Copied! <pre>import torchvision\nfrom torch import nn\n\n# Get num out features (one for each class pizza, steak, sushi)\nOUT_FEATURES = len(class_names)\n\n# Create an EffNetB0 feature extractor\ndef create_effnetb0():\n    # 1. Get the base mdoel with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.2),\n        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb0\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n\n# Create an EffNetB2 feature extractor\ndef create_effnetb2():\n    # 1. Get the base model with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3),\n        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb2\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n</pre> import torchvision from torch import nn  # Get num out features (one for each class pizza, steak, sushi) OUT_FEATURES = len(class_names)  # Create an EffNetB0 feature extractor def create_effnetb0():     # 1. Get the base mdoel with pretrained weights and send to target device     weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT     model = torchvision.models.efficientnet_b0(weights=weights).to(device)      # 2. Freeze the base model layers     for param in model.features.parameters():         param.requires_grad = False      # 3. Set the seeds     set_seeds()      # 4. Change the classifier head     model.classifier = nn.Sequential(         nn.Dropout(p=0.2),         nn.Linear(in_features=1280, out_features=OUT_FEATURES)     ).to(device)      # 5. Give the model a name     model.name = \"effnetb0\"     print(f\"[INFO] Created new {model.name} model.\")     return model  # Create an EffNetB2 feature extractor def create_effnetb2():     # 1. Get the base model with pretrained weights and send to target device     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     model = torchvision.models.efficientnet_b2(weights=weights).to(device)      # 2. Freeze the base model layers     for param in model.features.parameters():         param.requires_grad = False      # 3. Set the seeds     set_seeds()      # 4. Change the classifier head     model.classifier = nn.Sequential(         nn.Dropout(p=0.3),         nn.Linear(in_features=1408, out_features=OUT_FEATURES)     ).to(device)      # 5. Give the model a name     model.name = \"effnetb2\"     print(f\"[INFO] Created new {model.name} model.\")     return model <p>Those are some nice looking functions!</p> <p>Let's test them out by creating an instance of EffNetB0 and EffNetB2 and checking out their <code>summary()</code>.</p> In\u00a0[26]: Copied! <pre>effnetb0 = create_effnetb0() \n\n# Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output)\n# summary(model=effnetb0, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n</pre> effnetb0 = create_effnetb0()   # Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output) # summary(model=effnetb0,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )  <pre>[INFO] Created new effnetb0 model.\n</pre> <p>Model summary of EffNetB0 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).</p> In\u00a0[27]: Copied! <pre>effnetb2 = create_effnetb2()\n\n# Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n</pre> effnetb2 = create_effnetb2()  # Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output) # summary(model=effnetb2,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )  <pre>[INFO] Created new effnetb2 model.\n</pre> <p>Model summary of EffNetB2 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).</p> <p>Looking at the outputs of the summaries, it seems the EffNetB2 backbone has nearly double the amount of parameters as EffNetB0.</p> Model Total parameters (before freezing/changing head) Total parameters (after freezing/changing head) Total trainable parameters (after freezing/changing head) EfficientNetB0 5,288,548 4,011,391 3,843 EfficientNetB2 9,109,994 7,705,221 4,227 <p>This gives the backbone of the EffNetB2 model more opportunities to form a representation of our pizza, steak and sushi data.</p> <p>However, the trainable parameters for each model (the classifier heads) aren't very different.</p> <p>Will these extra parameters lead to better results?</p> <p>We'll have to wait and see...</p> <p>Note: In the spirit of experimenting, you really could try almost any model from <code>torchvision.models</code> in a similar fashion to what we're doing here. I've only chosen EffNetB0 and EffNetB2 as examples. Perhaps you might want to throw something like <code>torchvision.models.convnext_tiny()</code> or <code>torchvision.models.convnext_small()</code> into the mix.</p> In\u00a0[28]: Copied! <pre># 1. Create epochs list\nnum_epochs = [5, 10]\n\n# 2. Create models list (need to create a new model for each experiment)\nmodels = [\"effnetb0\", \"effnetb2\"]\n\n# 3. Create dataloaders dictionary for various dataloaders\ntrain_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n                     \"data_20_percent\": train_dataloader_20_percent}\n</pre> # 1. Create epochs list num_epochs = [5, 10]  # 2. Create models list (need to create a new model for each experiment) models = [\"effnetb0\", \"effnetb2\"]  # 3. Create dataloaders dictionary for various dataloaders train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,                      \"data_20_percent\": train_dataloader_20_percent} <p>Lists and dictionary created!</p> <p>Now we can write code to iterate through each of the different options and try out each of the different combinations.</p> <p>We'll also save the model at the end of each experiment so later on we can load back in the best model and use it for making predictions.</p> <p>Specifically, let's go through the following steps:</p> <ol> <li>Set the random seeds (so our experiment results are reproducible, in practice, you might run the same experiment across ~3 different seeds and average the results).</li> <li>Keep track of different experiment numbers (this is mostly for pretty print outs).</li> <li>Loop through the <code>train_dataloaders</code> dictionary items for each of the different training DataLoaders.</li> <li>Loop through the list of epoch numbers.</li> <li>Loop through the list of different model names.</li> <li>Create information print outs for the current running experiment (so we know what's happening).</li> <li>Check which model is the target model and create a new EffNetB0 or EffNetB2 instance (we create a new model instance each experiment so all models start from the same standpoint).</li> <li>Create a new loss function (<code>torch.nn.CrossEntropyLoss()</code>) and optimizer (<code>torch.optim.Adam(params=model.parameters(), lr=0.001)</code>) for each new experiment.</li> <li>Train the model with the modified <code>train()</code> function passing the appropriate details to the <code>writer</code> parameter.</li> <li>Save the trained model with an appropriate file name to file with <code>save_model()</code> from <code>utils.py</code>.</li> </ol> <p>We can also use the <code>%%time</code> magic to see how long all of our experiments take together in a single Jupyter/Google Colab cell.</p> <p>Let's do it!</p> In\u00a0[29]: Copied! <pre>%%time\nfrom going_modular.going_modular.utils import save_model\n\n# 1. Set the random seeds\nset_seeds(seed=42)\n\n# 2. Keep track of experiment numbers\nexperiment_number = 0\n\n# 3. Loop through each DataLoader\nfor dataloader_name, train_dataloader in train_dataloaders.items():\n\n    # 4. Loop through each number of epochs\n    for epochs in num_epochs: \n\n        # 5. Loop through each model name and create a new model based on the name\n        for model_name in models:\n\n            # 6. Create information print outs\n            experiment_number += 1\n            print(f\"[INFO] Experiment number: {experiment_number}\")\n            print(f\"[INFO] Model: {model_name}\")\n            print(f\"[INFO] DataLoader: {dataloader_name}\")\n            print(f\"[INFO] Number of epochs: {epochs}\")  \n\n            # 7. Select the model\n            if model_name == \"effnetb0\":\n                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n            else:\n                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n            \n            # 8. Create a new loss and optimizer for every model\n            loss_fn = nn.CrossEntropyLoss()\n            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n            # 9. Train target model with target dataloaders and track experiments\n            train(model=model,\n                  train_dataloader=train_dataloader,\n                  test_dataloader=test_dataloader, \n                  optimizer=optimizer,\n                  loss_fn=loss_fn,\n                  epochs=epochs,\n                  device=device,\n                  writer=create_writer(experiment_name=dataloader_name,\n                                       model_name=model_name,\n                                       extra=f\"{epochs}_epochs\"))\n            \n            # 10. Save the model to file so we can get back the best model\n            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n            save_model(model=model,\n                       target_dir=\"models\",\n                       model_name=save_filepath)\n            print(\"-\"*50 + \"\\n\")\n</pre> %%time from going_modular.going_modular.utils import save_model  # 1. Set the random seeds set_seeds(seed=42)  # 2. Keep track of experiment numbers experiment_number = 0  # 3. Loop through each DataLoader for dataloader_name, train_dataloader in train_dataloaders.items():      # 4. Loop through each number of epochs     for epochs in num_epochs:           # 5. Loop through each model name and create a new model based on the name         for model_name in models:              # 6. Create information print outs             experiment_number += 1             print(f\"[INFO] Experiment number: {experiment_number}\")             print(f\"[INFO] Model: {model_name}\")             print(f\"[INFO] DataLoader: {dataloader_name}\")             print(f\"[INFO] Number of epochs: {epochs}\")                # 7. Select the model             if model_name == \"effnetb0\":                 model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)             else:                 model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)                          # 8. Create a new loss and optimizer for every model             loss_fn = nn.CrossEntropyLoss()             optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)              # 9. Train target model with target dataloaders and track experiments             train(model=model,                   train_dataloader=train_dataloader,                   test_dataloader=test_dataloader,                    optimizer=optimizer,                   loss_fn=loss_fn,                   epochs=epochs,                   device=device,                   writer=create_writer(experiment_name=dataloader_name,                                        model_name=model_name,                                        extra=f\"{epochs}_epochs\"))                          # 10. Save the model to file so we can get back the best model             save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"             save_model(model=model,                        target_dir=\"models\",                        model_name=save_filepath)             print(\"-\"*50 + \"\\n\") <pre>[INFO] Experiment number: 1\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 2\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 3\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\nEpoch: 6 | train_loss: 0.5611 | train_acc: 0.8984 | test_loss: 0.5949 | test_acc: 0.8864\nEpoch: 7 | train_loss: 0.5573 | train_acc: 0.7930 | test_loss: 0.5566 | test_acc: 0.8864\nEpoch: 8 | train_loss: 0.4702 | train_acc: 0.9492 | test_loss: 0.5176 | test_acc: 0.8759\nEpoch: 9 | train_loss: 0.5728 | train_acc: 0.7773 | test_loss: 0.5095 | test_acc: 0.8873\nEpoch: 10 | train_loss: 0.4794 | train_acc: 0.8242 | test_loss: 0.4640 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 4\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\nEpoch: 6 | train_loss: 0.6111 | train_acc: 0.7812 | test_loss: 0.6325 | test_acc: 0.9280\nEpoch: 7 | train_loss: 0.6127 | train_acc: 0.8008 | test_loss: 0.6404 | test_acc: 0.8769\nEpoch: 8 | train_loss: 0.5202 | train_acc: 0.9336 | test_loss: 0.6200 | test_acc: 0.8977\nEpoch: 9 | train_loss: 0.5425 | train_acc: 0.8008 | test_loss: 0.6227 | test_acc: 0.8466\nEpoch: 10 | train_loss: 0.4908 | train_acc: 0.8125 | test_loss: 0.5870 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 5\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 6\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 7\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\nEpoch: 6 | train_loss: 0.3705 | train_acc: 0.8854 | test_loss: 0.3568 | test_acc: 0.9072\nEpoch: 7 | train_loss: 0.3551 | train_acc: 0.9250 | test_loss: 0.3187 | test_acc: 0.9072\nEpoch: 8 | train_loss: 0.3745 | train_acc: 0.8938 | test_loss: 0.3349 | test_acc: 0.8873\nEpoch: 9 | train_loss: 0.2972 | train_acc: 0.9396 | test_loss: 0.3092 | test_acc: 0.9280\nEpoch: 10 | train_loss: 0.3620 | train_acc: 0.8479 | test_loss: 0.2780 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 8\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\nEpoch: 6 | train_loss: 0.3889 | train_acc: 0.9104 | test_loss: 0.4555 | test_acc: 0.8977\nEpoch: 7 | train_loss: 0.3483 | train_acc: 0.9271 | test_loss: 0.4227 | test_acc: 0.9384\nEpoch: 8 | train_loss: 0.3862 | train_acc: 0.8771 | test_loss: 0.4344 | test_acc: 0.9280\nEpoch: 9 | train_loss: 0.3308 | train_acc: 0.8979 | test_loss: 0.4242 | test_acc: 0.9384\nEpoch: 10 | train_loss: 0.3383 | train_acc: 0.8896 | test_loss: 0.3906 | test_acc: 0.9384\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\nCPU times: user 29.5 s, sys: 1min 28s, total: 1min 58s\nWall time: 2min 33s\n</pre> In\u00a0[30]: Copied! <pre># Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n</pre> # Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance) # %load_ext tensorboard # %tensorboard --logdir runs <p>Running the cell above we should get an output similar to the following.</p> <p>Note: Depending on the random seeds you used/hardware you used there's a chance your numbers aren't exactly the same as what's here. This is okay. It's due to the inheret randomness of deep learning. What matters most is the trend. Where your numbers are heading. If they're off by a large amount, perhaps there's something wrong and best to go back and check the code. But if they're off by a small amount (say a couple of decimal places or so), that's okay.</p> <p>Visualizing the test loss values for the different modelling experiments in TensorBoard, you can see that the EffNetB0 model trained for 10 epochs and with 20% of the data achieves the lowest loss. This sticks with the overall trend of the experiments that: more data, larger model and longer training time is generally better.</p> <p>You can also upload your TensorBoard experiment results to tensorboard.dev to host them publically for free.</p> <p>For example, running code similiar to the following:</p> In\u00a0[31]: Copied! <pre># # Upload the results to TensorBoard.dev (uncomment to try it out)\n# !tensorboard dev upload --logdir runs \\\n#     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\\n#     --description \"Comparing results of different model size, training data amount and training time.\"\n</pre> # # Upload the results to TensorBoard.dev (uncomment to try it out) # !tensorboard dev upload --logdir runs \\ #     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\ #     --description \"Comparing results of different model size, training data amount and training time.\" <p>Running the cell above results in the experiments from this notebook being publically viewable at: https://tensorboard.dev/experiment/VySxUYY7Rje0xREYvCvZXA/</p> <p>Note: Beware that anything you upload to tensorboard.dev is publically available for anyone to see. So if you do upload your experiments, be careful they don't contain sensitive information.</p> In\u00a0[32]: Copied! <pre># Setup the best model filepath\nbest_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"\n\n# Instantiate a new instance of EffNetB2 (to load the saved state_dict() to)\nbest_model = create_effnetb2()\n\n# Load the saved best model state_dict()\nbest_model.load_state_dict(torch.load(best_model_path))\n</pre> # Setup the best model filepath best_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"  # Instantiate a new instance of EffNetB2 (to load the saved state_dict() to) best_model = create_effnetb2()  # Load the saved best model state_dict() best_model.load_state_dict(torch.load(best_model_path)) <pre>[INFO] Created new effnetb2 model.\n</pre> Out[32]: <pre>&lt;All keys matched successfully&gt;</pre> <p>Best model loaded!</p> <p>While we're here, let's check its filesize.</p> <p>This is an important consideration later on when deploying the model (incorporating it in an app).</p> <p>If the model is too large, it can be hard to deploy.</p> In\u00a0[33]: Copied! <pre># Check the model file size\nfrom pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\neffnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024)\nprint(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\")\n</pre> # Check the model file size from pathlib import Path  # Get the model size in bytes then convert to megabytes effnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024) print(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\") <pre>EfficientNetB2 feature extractor model size: 29 MB\n</pre> <p>Looks like our best model so far is 29 MB in size. We'll keep this in mind if we wanted to deploy it later on.</p> <p>Time to make and visualize some predictions.</p> <p>We created a <code>pred_and_plot_image()</code> function to use a trained model to make predictions on an image in 06. PyTorch Transfer Learning section 6.</p> <p>And we can reuse this function by importing it from <code>going_modular.going_modular.predictions.py</code> (I put the <code>pred_and_plot_image()</code> function in a script so we could reuse it).</p> <p>So to make predictions on various images the model hasn't seen before, we'll first get a list of all the image filepaths from the 20% pizza, steak, sushi testing dataset and then we'll randomly select a subset of these filepaths to pass to our <code>pred_and_plot_image()</code> function.</p> In\u00a0[34]: Copied! <pre># Import function to make predictions on images and plot them \n# See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Get a random list of 3 images from 20% test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset\ntest_image_path_sample = random.sample(population=test_image_path_list,\n                                       k=num_images_to_plot) # randomly select k number of images\n\n# Iterate through random test image paths, make predictions on them and plot them\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=best_model,\n                        image_path=image_path,\n                        class_names=class_names,\n                        image_size=(224, 224))\n</pre> # Import function to make predictions on images and plot them  # See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set from going_modular.going_modular.predictions import pred_and_plot_image  # Get a random list of 3 images from 20% test set import random num_images_to_plot = 3 test_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset test_image_path_sample = random.sample(population=test_image_path_list,                                        k=num_images_to_plot) # randomly select k number of images  # Iterate through random test image paths, make predictions on them and plot them for image_path in test_image_path_sample:     pred_and_plot_image(model=best_model,                         image_path=image_path,                         class_names=class_names,                         image_size=(224, 224)) <p>Nice!</p> <p>Running the cell above a few times we can see our model performs quite well and often has higher prediction probabilities than previous models we've built.</p> <p>This suggests the model is more confident in the decisions it's making.</p> In\u00a0[35]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = Path(\"data/04-pizza-dad.jpeg\")\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = Path(\"data/04-pizza-dad.jpeg\")  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=model,                     image_path=custom_image_path,                     class_names=class_names) <pre>data/04-pizza-dad.jpeg already exists, skipping download.\n</pre> <p>Woah!</p> <p>Two thumbs again!</p> <p>Our best model predicts \"pizza\" correctly and this time with an even higher prediction probability (0.978) than the first feature extraction model we trained and used in 06. PyTorch Transfer Learning section 6.1.</p> <p>This again suggests our current best model (EffNetB2 feature extractor trained on 20% of the pizza, steak, sushi training data and for 10 epochs) has learned patterns to make it more confident of its decision to predict pizza.</p> <p>I wonder what could improve our model's performance even further?</p> <p>I'll leave that as a challenge for you to investigate.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#07-pytorch-experiment-tracking","title":"07. PyTorch Experiment Tracking\u00b6","text":"<p>Note: This notebook uses <code>torchvision</code>'s new multi-weight support API (available in <code>torchvision</code> v0.13+).</p> <p>We've trained a fair few models now on the journey to making FoodVision Mini (an image classification model to classify images of pizza, steak or sushi).</p> <p>And so far we've keep track of them via Python dictionaries.</p> <p>Or just comparing them by the metric print outs during training.</p> <p>What if you wanted to run a dozen (or more) different models at once?</p> <p>Surely there's a better way...</p> <p>There is.</p> <p>Experiment tracking.</p> <p>And since experiment tracking is so important and integral to machine learning, you can consider this notebook your first milestone project.</p> <p>So welcome to Milestone Project 1: FoodVision Mini Experiment Tracking.</p> <p>We're going to answer the question: how do I track my machine learning experiments?</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#what-is-experiment-tracking","title":"What is experiment tracking?\u00b6","text":"<p>Machine learning and deep learning are very experimental.</p> <p>You have to put on your artist's beret/chef's hat to cook up lots of different models.</p> <p>And you have to put on your scientist's coat to track the results of various combinations of data, model architectures and training regimes.</p> <p>That's where experiment tracking comes in.</p> <p>If you're running lots of different experiments, experiment tracking helps you figure out what works and what doesn't.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#why-track-experiments","title":"Why track experiments?\u00b6","text":"<p>If you're only running a handful of models (like we've done so far), it might be okay just to track their results in print outs and a few dictionaries.</p> <p>However, as the number of experiments you run starts to increase, this naive way of tracking could get out of hand.</p> <p>So if you're following the machine learning practitioner's motto of experiment, experiment, experiment!, you'll want a way to track them.</p> <p>After building a few models and tracking their results, you'll start to notice how quickly it can get out of hand.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#different-ways-to-track-machine-learning-experiments","title":"Different ways to track machine learning experiments\u00b6","text":"<p>There are as many different ways to track machine learning experiments as there is experiments to run.</p> <p>This table covers a few.</p> Method Setup Pros Cons Cost Python dictionaries, CSV files, print outs None Easy to setup, runs in pure Python Hard to keep track of large numbers of experiments Free TensorBoard Minimal, install <code>tensorboard</code> Extensions built into PyTorch, widely recognized and used, easily scales. User-experience not as nice as other options. Free Weights &amp; Biases Experiment Tracking Minimal, install <code>wandb</code>, make an account Incredible user experience, make experiments public, tracks almost anything. Requires external resource outside of PyTorch. Free for personal use MLFlow Minimal, install <code>mlflow</code> and starting tracking Fully open-source MLOps lifecycle management, many integrations. Little bit harder to setup a remote tracking server than other services. Free <p>Various places and techniques you can use to track your machine learning experiments. Note: There are various other options similar to Weights &amp; Biases and open-source options similar to MLflow but I've left them out for brevity. You can find more by searching \"machine learning experiment tracking\".</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to be running several different modelling experiments with various levels of data, model size and training time to try and improve on FoodVision Mini.</p> <p>And due to its tight integration with PyTorch and widespread use, this notebook focuses on using TensorBoard to track our experiments.</p> <p>However, the principles we're going to cover are similar across all of the other tools for experiment tracking.</p> Topic Contents 0. Getting setup We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. 1. Get data Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our FoodVision Mini model's results. 2. Create Datasets and DataLoaders We'll use the <code>data_setup.py</code> script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. 3. Get and customise a pretrained model Just like the last section, 06. PyTorch Transfer Learning we'll download a pretrained model from <code>torchvision.models</code> and customise it to our own problem. 4. Train model amd track results Let's see what it's like to train and track the training results of a single model using TensorBoard. 5. View our model's results in TensorBoard Previously we visualized our model's loss curves with a helper function, now let's see what they look like in TensorBoard. 6. Creating a helper function to track experiments If we're going to be adhering to the machine learner practitioner's motto of experiment, experiment, experiment!, we best create a function that will help us save our modelling experiment results. 7. Setting up a series of modelling experiments Instead of running experiments one by one, how about we write some code to run several experiments at once, with different models, different amounts of data and different training times. 8. View modelling experiments in TensorBoard By this stage we'll have run eight modelling experiments in one go, a fair bit to keep track of, let's see what their results look like in TensorBoard. 9. Load in the best model and make predictions with it The point of experiment tracking is to figure out which model performs the best, let's load in the best performing model and make some predictions with it to visualize, visualize, visualize!."},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#0-getting-setup","title":"0. Getting setup\u00b6","text":"<p>Let's start by downloading all of the modules we'll need for this section.</p> <p>To save us writing extra code, we're going to be leveraging some of the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in section, 05. PyTorch Going Modular.</p> <p>Specifically, we're going to download the <code>going_modular</code> directory from the <code>pytorch-deep-learning</code> repository (if we don't already have it).</p> <p>We'll also get the <code>torchinfo</code> package if it's not available.</p> <p><code>torchinfo</code> will help later on to give us visual summaries of our model(s).</p> <p>And since we're using a newer version of the <code>torchvision</code> package (v0.13 as of June 2022), we'll make sure we've got the latest versions.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds","title":"Create a helper function to set seeds\u00b6","text":"<p>Since we've been setting random seeds a whole bunch throughout previous sections, how about we functionize it?</p> <p>Let's create a function to \"set the seeds\" called <code>set_seeds()</code>.</p> <p>Note: Recall a random seed is a way of flavouring the randomness generated by a computer. They aren't necessary to always set when running machine learning code, however, they help ensure there's an element of reproducibility (the numbers I get with my code are similar to the numbers you get with your code). Outside of an education or experimental setting, random seeds generally aren't required.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#1-get-data","title":"1. Get data\u00b6","text":"<p>As always, before we can run machine learning experiments, we'll need a dataset.</p> <p>We're going to continue trying to improve upon the results we've been getting on FoodVision Mini.</p> <p>In the previous section, 06. PyTorch Transfer Learning, we saw how powerful using a pretrained model and transfer learning could be when classifying images of pizza, steak and sushi.</p> <p>So how about we run some experiments and try to further improve our results?</p> <p>To do so, we'll use similar code to the previous section to download the <code>pizza_steak_sushi.zip</code> (if the data doesn't already exist) except this time its been functionised.</p> <p>This will allow us to use it again later.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#2-create-datasets-and-dataloaders","title":"2. Create Datasets and DataLoaders\u00b6","text":"<p>Now we've got some data, let's turn it into PyTorch DataLoaders.</p> <p>We can do so using the <code>create_dataloaders()</code> function we created in 05. PyTorch Going Modular part 2.</p> <p>And since we'll be using transfer learning and specifically pretrained models from <code>torchvision.models</code>, we'll create a transform to prepare our images correctly.</p> <p>To transform our images in tensors, we can use:</p> <ol> <li>Manually created transforms using <code>torchvision.transforms</code>.</li> <li>Automatically created transforms using <code>torchvision.models.MODEL_NAME.MODEL_WEIGHTS.DEFAULT.transforms()</code>.<ul> <li>Where <code>MODEL_NAME</code> is a specific <code>torchvision.models</code> architecture, <code>MODEL_WEIGHTS</code> is a specific set of pretrained weights and <code>DEFAULT</code> means the \"best available weights\".</li> </ul> </li> </ol> <p>We saw an example of each of these in 06. PyTorch Transfer Learning section 2.</p> <p>Let's see first an example of manually creating a <code>torchvision.transforms</code> pipeline (creating a transforms pipeline this way gives the most customization but can potentially result in performance degradation if the transforms don't match the pretrained model).</p> <p>The main manual transformation we need to be sure of is that all of our images are normalized in ImageNet format (this is because pretrained <code>torchvision.models</code> are all pretrained on ImageNet).</p> <p>We can do this with:</p> <pre>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n</pre>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#21-create-dataloaders-using-manually-created-transforms","title":"2.1 Create DataLoaders using manually created transforms\u00b6","text":""},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#22-create-dataloaders-using-automatically-created-transforms","title":"2.2 Create DataLoaders using automatically created transforms\u00b6","text":"<p>Data transformed and DataLoaders created!</p> <p>Let's now see what the same transformation pipeline looks like but this time by using automatic transforms.</p> <p>We can do this by first instantiating a set of pretrained weights (for example <code>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT</code>)  we'd like to use and calling the <code>transforms()</code> method on it.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#3-getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head","title":"3. Getting a pretrained model, freezing the base layers and changing the classifier head\u00b6","text":"<p>Before we run and track multiple modelling experiments, let's see what it's like to run and track a single one.</p> <p>And since our data is ready, the next thing we'll need is a model.</p> <p>Let's download the pretrained weights for a <code>torchvision.models.efficientnet_b0()</code> model and prepare it for use with our own data.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#4-train-model-and-track-results","title":"4. Train model and track results\u00b6","text":"<p>Model ready to go!</p> <p>Let's get ready to train it by creating a loss function and an optimizer.</p> <p>Since we're working with multiple classes, we'll use <code>torch.nn.CrossEntropyLoss()</code> as the loss function.</p> <p>And we'll stick with <code>torch.optim.Adam()</code> with learning rate of <code>0.001</code> for the optimizer.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#adjust-train-function-to-track-results-with-summarywriter","title":"Adjust <code>train()</code> function to track results with <code>SummaryWriter()</code>\u00b6","text":"<p>Beautiful!</p> <p>All of the pieces of our training code are starting to come together.</p> <p>Let's now add the final piece to track our experiments.</p> <p>Previously, we've tracked our modelling experiments using multiple Python dictionaries (one for each model).</p> <p>But you can imagine this could get out of hand if we were running anything more than a few experiments.</p> <p>Not to worry, there's a better option!</p> <p>We can use PyTorch's <code>torch.utils.tensorboard.SummaryWriter()</code> class to save various parts of our model's training progress to file.</p> <p>By default, the <code>SummaryWriter()</code> class saves various information about our model to a file set by the <code>log_dir</code> parameter.</p> <p>The default location for <code>log_dir</code> is under <code>runs/CURRENT_DATETIME_HOSTNAME</code>, where the <code>HOSTNAME</code> is the name of your computer.</p> <p>But of course, you can change where your experiments are tracked (the filename is as customisable as you'd like).</p> <p>The outputs of the <code>SummaryWriter()</code> are saved in TensorBoard format.</p> <p>TensorBoard is a part of the TensorFlow deep learning library and is an excellent way to visualize different parts of your model.</p> <p>To start tracking our modelling experiments, let's create a default <code>SummaryWriter()</code> instance.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#5-view-our-models-results-in-tensorboard","title":"5. View our model's results in TensorBoard\u00b6","text":"<p>The <code>SummaryWriter()</code> class stores our model's results in a directory called <code>runs/</code> in TensorBoard format by default.</p> <p>TensorBoard is a visualization program created by the TensorFlow team to view and inspect information about models and data.</p> <p>You know what that means?</p> <p>It's time to follow the data visualizer's motto and visualize, visualize, visualize!</p> <p>You can view TensorBoard in a number of ways:</p> Code environment How to view TensorBoard Resource VS Code (notebooks or Python scripts) Press <code>SHIFT + CMD + P</code> to open the Command Palette and search for the command \"Python: Launch TensorBoard\". VS Code Guide on TensorBoard and PyTorch Jupyter and Colab Notebooks Make sure TensorBoard is installed, load it with <code>%load_ext tensorboard</code> and then view your results with <code>%tensorboard --logdir DIR_WITH_LOGS</code>. <code>torch.utils.tensorboard</code> and Get started with TensorBoard <p>You can also upload your experiments to tensorboard.dev to share them publicly with others.</p> <p>Running the following code in a Google Colab or Jupyter Notebook will start an interactive TensorBoard session to view TensorBoard files in the <code>runs/</code> directory.</p> <pre>%load_ext tensorboard # line magic to load TensorBoard\n%tensorboard --logdir runs # run TensorBoard session with the \"runs/\" directory\n</pre>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#6-create-a-helper-function-to-build-summarywriter-instances","title":"6. Create a helper function to build <code>SummaryWriter()</code> instances\u00b6","text":"<p>The <code>SummaryWriter()</code> class logs various information to a directory specified by the <code>log_dir</code> parameter.</p> <p>How about we make a helper function to create a custom directory per experiment?</p> <p>In essence, each experiment gets its own logs directory.</p> <p>For example, say we'd like to track things like:</p> <ul> <li>Experiment date/timestamp - when did the experiment take place?</li> <li>Experiment name - is there something we'd like to call the experiment?</li> <li>Model name - what model was used?</li> <li>Extra - should anything else be tracked?</li> </ul> <p>You could track almost anything here and be as creative as you want but these should be enough to start.</p> <p>Let's create a helper function called <code>create_writer()</code> that produces a <code>SummaryWriter()</code> instance tracking to a custom <code>log_dir</code>.</p> <p>Ideally, we'd like the <code>log_dir</code> to be something like:</p> <p><code>runs/YYYY-MM-DD/experiment_name/model_name/extra</code></p> <p>Where <code>YYYY-MM-DD</code> is the date the experiment was run (you could add the time if you wanted to as well).</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#61-update-the-train-function-to-include-a-writer-parameter","title":"6.1 Update the <code>train()</code> function to include a <code>writer</code> parameter\u00b6","text":"<p>Our <code>create_writer()</code> function works fantastic.</p> <p>How about we give our <code>train()</code> function the ability to take in a <code>writer</code> parameter so we actively update the <code>SummaryWriter()</code> instance we're using each time we call <code>train()</code>.</p> <p>For example, say we're running a series of experiments, calling <code>train()</code> multiple times for multiple different models, it would be good if each experiment used a different <code>writer</code>.</p> <p>One <code>writer</code> per experiment = one logs directory per experiment.</p> <p>To adjust the <code>train()</code> function we'll add a <code>writer</code> parameter to the function and then we'll add some code to see if there's a <code>writer</code> and if so, we'll track our information there.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#7-setting-up-a-series-of-modelling-experiments","title":"7. Setting up a series of modelling experiments\u00b6","text":"<p>It's to step things up a notch.</p> <p>Previously we've been running various experiments and inspecting the results one by one.</p> <p>But what if we could run multiple experiments and then inspect the results all together?</p> <p>You in?</p> <p>C'mon, let's go.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#71-what-kind-of-experiments-should-you-run","title":"7.1 What kind of experiments should you run?\u00b6","text":"<p>That's the million dollar question in machine learning.</p> <p>Because there's really no limit to the experiments you can run.</p> <p>Such a freedom is why machine learning is so exciting and terrifying at the same time.</p> <p>This is where you'll have to put on your scientist coat and remember the machine learning practitioner's motto: experiment, experiment, experiment!</p> <p>Every hyperparameter stands as a starting point for a different experiment:</p> <ul> <li>Change the number of epochs.</li> <li>Change the number of layers/hidden units.</li> <li>Change the amount of data.</li> <li>Change the learning rate.</li> <li>Try different kinds of data augmentation.</li> <li>Choose a different model architecture.</li> </ul> <p>With practice and running many different experiments, you'll start to build an intuition of what might help your model.</p> <p>I say might on purpose because there's no guarantees.</p> <p>But generally, in light of The Bitter Lesson (I've mentioned this twice now because it's an important essay in the world of AI), generally the bigger your model (more learnable parameters) and the more data you have (more opportunities to learn), the better the performance.</p> <p>However, when you're first approaching a machine learning problem: start small and if something works, scale it up.</p> <p>Your first batch of experiments should take no longer than a few seconds to a few minutes to run.</p> <p>The quicker you can experiment, the faster you can work out what doesn't work, in turn, the faster you can work out what does work.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#72-what-experiments-are-we-going-to-run","title":"7.2 What experiments are we going to run?\u00b6","text":"<p>Our goal is to improve the model powering FoodVision Mini without it getting too big.</p> <p>In essence, our ideal model achieves a high level of test set accuracy (90%+) but doesn't take too long to train/perform inference (make predictions).</p> <p>We've got plenty of options but how about we keep things simple?</p> <p>Let's try a combination of:</p> <ol> <li>A different amount of data (10% of Pizza, Steak, Sushi vs. 20%)</li> <li>A different model (<code>torchvision.models.efficientnet_b0</code> vs. <code>torchvision.models.efficientnet_b2</code>)</li> <li>A different training time (5 epochs vs. 10 epochs)</li> </ol> <p>Breaking these down we get:</p> Experiment number Training Dataset Model (pretrained on ImageNet) Number of epochs 1 Pizza, Steak, Sushi 10% percent EfficientNetB0 5 2 Pizza, Steak, Sushi 10% percent EfficientNetB2 5 3 Pizza, Steak, Sushi 10% percent EfficientNetB0 10 4 Pizza, Steak, Sushi 10% percent EfficientNetB2 10 5 Pizza, Steak, Sushi 20% percent EfficientNetB0 5 6 Pizza, Steak, Sushi 20% percent EfficientNetB2 5 7 Pizza, Steak, Sushi 20% percent EfficientNetB0 10 8 Pizza, Steak, Sushi 20% percent EfficientNetB2 10 <p>Notice how we're slowly scaling things up.</p> <p>With each experiment we slowly increase the amount of data, the model size and the length of training.</p> <p>By the end, experiment 8 will be using double the data, double the model size and double the length of training compared to experiment 1.</p> <p>Note: I want to be clear that there truly is no limit to amount of experiments you can run. What we've designed here is only a very small subset of options. However, you can't test everything so best to try a few things to begin with and then follow the ones which work the best.</p> <p>And as a reminder, the datasets we're using are a subset of the Food101 dataset (3 classes, pizza, steak, suhsi, instead of 101) and 10% and 20% of the images rather than 100%. If our experiments work, we could start to run more on more data (though this will take longer to compute). You can see how the datasets were created via the <code>04_custom_data_creation.ipynb</code> notebook.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#73-download-different-datasets","title":"7.3 Download different datasets\u00b6","text":"<p>Before we start running our series of experiments, we need to make sure our datasets are ready.</p> <p>We'll need two forms of a training set:</p> <ol> <li>A training set with 10% of the data of Food101 pizza, steak, sushi images (we've already created this above but we'll do it again for completeness).</li> <li>A training set with 20% of the data of Food101 pizza, steak, sushi images.</li> </ol> <p>For consistency, all experiments will use the same testing dataset (the one from the 10% data split).</p> <p>We'll start by downloading the various datasets we need using the <code>download_data()</code> function we created earlier.</p> <p>Both datasets are available from the course GitHub:</p> <ol> <li>Pizza, steak, sushi 10% training data.</li> <li>Pizza, steak, sushi 20% training data.</li> </ol>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#74-transform-datasets-and-create-dataloaders","title":"7.4 Transform Datasets and create DataLoaders\u00b6","text":"<p>Next we'll create a series of transforms to prepare our images for our model(s).</p> <p>To keep things consistent, we'll manually create a transform (just like we did above) and use the same transform across all of the datasets.</p> <p>The transform will:</p> <ol> <li>Resize all the images (we'll start with 224, 224 but this could be changed).</li> <li>Turn them into tensors with values between 0 &amp; 1.</li> <li>Normalize them in way so their distributions are inline with the ImageNet dataset (we do this because our models from <code>torchvision.models</code> have been pretrained on ImageNet).</li> </ol>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#75-create-feature-extractor-models","title":"7.5 Create feature extractor models\u00b6","text":"<p>Time to start building our models.</p> <p>We're going to create two feature extractor models:</p> <ol> <li><code>torchvision.models.efficientnet_b0()</code> pretrained backbone + custom classifier head (EffNetB0 for short).</li> <li><code>torchvision.models.efficientnet_b2()</code> pretrained backbone + custom classifier head (EffNetB2 for short).</li> </ol> <p>To do this, we'll freeze the base layers (the feature layers) and update the model's classifier heads (output layers) to suit our problem just like we did in 06. PyTorch Transfer Learning section 3.4.</p> <p>We saw in the previous chapter the <code>in_features</code> parameter to the classifier head of EffNetB0 is <code>1280</code> (the backbone turns the input image into a feature vector of size <code>1280</code>).</p> <p>Since EffNetB2 has a different number of layers and parameters, we'll need to adapt it accordingly.</p> <p>Note: Whenever you use a different model, one of the first things you should inspect is the input and output shapes. That way you'll know how you'll have to prepare your input data/update the model to have the correct output shape.</p> <p>We can find the input and output shapes of EffNetB2 using <code>torchinfo.summary()</code> and passing in the <code>input_size=(32, 3, 224, 224)</code> parameter (<code>(32, 3, 224, 224)</code> is equivalent to <code>(batch_size, color_channels, height, width)</code>, i.e we pass in an example of what a single batch of data would be to our model).</p> <p>Note: Many modern models can handle input images of varying sizes thanks to <code>torch.nn.AdaptiveAvgPool2d()</code> layer, this layer adaptively adjusts the <code>output_size</code> of a given input as required. You can try this out by passing different size input images to <code>torchinfo.summary()</code> or to your own models using the layer.</p> <p>To find the required input shape to the final layer of EffNetB2, let's:</p> <ol> <li>Create an instance of <code>torchvision.models.efficientnet_b2(pretrained=True)</code>.</li> <li>See the various input and output shapes by running <code>torchinfo.summary()</code>.</li> <li>Print out the number of <code>in_features</code> by inspecting <code>state_dict()</code> of the classifier portion of EffNetB2 and printing the length of the weight matrix.<ul> <li>Note: You could also just inspect the output of <code>effnetb2.classifier</code>.</li> </ul> </li> </ol>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code","title":"7.6 Create experiments and set up training code\u00b6","text":"<p>We've prepared our data and prepared our models, the time has come to setup some experiments!</p> <p>We'll start by creating two lists and a dictionary:</p> <ol> <li>A list of the number of epochs we'd like to test (<code>[5, 10]</code>)</li> <li>A list of the models we'd like to test (<code>[\"effnetb0\", \"effnetb2\"]</code>)</li> <li>A dictionary of the different training DataLoaders</li> </ol>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#8-view-experiments-in-tensorboard","title":"8. View experiments in TensorBoard\u00b6","text":"<p>Ho, ho!</p> <p>Look at us go!</p> <p>Training eight models in one go?</p> <p>Now that's living up to the motto!</p> <p>Experiment, experiment, experiment!</p> <p>How about we check out the results in TensorBoard?</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it","title":"9. Load in the best model and make predictions with it\u00b6","text":"<p>Looking at the TensorBoard logs for our eight experiments, it seems experiment number eight achieved the best overall results (highest test accuracy, second lowest test loss).</p> <p>This is the experiment that used:</p> <ul> <li>EffNetB2 (double the parameters of EffNetB0)</li> <li>20% pizza, steak, sushi training data (double the original training data)</li> <li>10 epochs (double the original training time)</li> </ul> <p>In essence, our biggest model achieved the best results.</p> <p>Though it wasn't as if these results were far better than the other models.</p> <p>The same model on the same data achieved similar results in half the training time (experiment number 6).</p> <p>This suggests that potentially the most influential parts of our experiments were the number of parameters and the amount of data.</p> <p>Inspecting the results further it seems that generally a model with more parameters (EffNetB2) and more data (20% pizza, steak, sushi training data) performs better (lower test loss and higher test accuracy).</p> <p>More experiments could be done to further test this but for now, let's import our best performing model from experiment eight (saved to: <code>models/07_effnetb2_data_20_percent_10_epochs.pth</code>, you can download this model from the course GitHub) and perform some qualitative evaluations.</p> <p>In other words, let's visualize, visualize, visualize!</p> <p>We can import the best saved model by creating a new instance of EffNetB2 using the <code>create_effnetb2()</code> function and then load in the saved <code>state_dict()</code> with <code>torch.load()</code>.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#91-predict-on-a-custom-image-with-the-best-model","title":"9.1 Predict on a custom image with the best model\u00b6","text":"<p>Making predictions on the test dataset is cool but the real magic of machine learning is making predictions on custom images of your own.</p> <p>So let's import the trusty pizza dad image (a photo of my dad in front of a pizza) we've been using for the past couple of sections and see how our model performs on it.</p>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#main-takeaways","title":"Main takeaways\u00b6","text":"<p>We've now gone full circle on the PyTorch workflow introduced in 01. PyTorch Workflow Fundamentals, we've gotten data ready, we've built and picked a pretrained model, we've used our various helper functions to train and evaluate the model and in this notebook we've improved our FoodVision Mini model by running and tracking a series of experiments.</p> <p></p> <p>You should be proud of yourself, this is no small feat!</p> <p>The main ideas you should take away from this Milestone Project 1 are:</p> <ul> <li>The machine learning practioner's motto: experiment, experiment, experiment! (though we've been doing plenty of this already).</li> <li>In the beginning, keep your experiments small so you can work fast, your first few experiments shouldn't take more than a few seconds to a few minutes to run.</li> <li>The more experiments you do, the quicker you can figure out what doesn't work.</li> <li>Scale up when you find something that works. For example, since we've found a pretty good performing model with EffNetB2 as a feature extractor, perhaps you'd now like to see what happens when you scale it up to the whole Food101 dataset from <code>torchvision.datasets</code>.</li> <li>Programmatically tracking your experiments takes a few steps to set up but it's worth it in the long run so you can figure out what works and what doesn't.<ul> <li>There are many different machine learning experiment trackers out there so explore a few and try them out.</li> </ul> </li> </ul>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#exercises","title":"Exercises\u00b6","text":"<p>Note: These exercises expect the use of <code>torchvision</code> v0.13+ (released July 2022), previous versions may work but will likely have errors.</p> <p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 07</li> <li>Example solutions notebook for 07 (try the exercises before looking at this)<ul> <li>See a live video walkthrough of the solutions on YouTube (errors and all)</li> </ul> </li> </ul> <ol> <li>Pick a larger model from <code>torchvision.models</code> to add to the list of experiments (for example, EffNetB3 or higher).<ul> <li>How does it perform compared to our existing models?</li> </ul> </li> <li>Introduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?<ul> <li>For example, you could have one training DataLoader that uses data augmentation (e.g. <code>train_dataloader_20_percent_aug</code> and <code>train_dataloader_20_percent_no_aug</code>) and then compare the results of two of the same model types training on these two DataLoaders.</li> <li>Note: You may need to alter the <code>create_dataloaders()</code> function to be able to take a transform for the training data and the testing data (because you don't need to perform data augmentation on the test data). See 04. PyTorch Custom Datasets section 6 for examples of using data augmentation or the script below for an example:</li> </ul> </li> </ol> <pre># Note: Data augmentation transform like this should only be performed on training data\ntrain_transform_data_aug = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(),\n    transforms.ToTensor(),\n    normalize\n])\n\n# Helper function to view images in a DataLoader (works with data augmentation transforms or not) \ndef view_dataloader_images(dataloader, n=10):\n    if n &gt; 10:\n        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n        n = 10\n    imgs, labels = next(iter(dataloader))\n    plt.figure(figsize=(16, 8))\n    for i in range(n):\n        # Min max scale the image for display purposes\n        targ_image = imgs[i]\n        sample_min, sample_max = targ_image.min(), targ_image.max()\n        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n\n        # Plot images with appropriate axes information\n        plt.subplot(1, 10, i+1)\n        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n        plt.title(class_names[labels[i]])\n        plt.axis(False)\n\n# Have to update `create_dataloaders()` to handle different augmentations\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\n\nNUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data \n\n# Note: this is an update version of data_setup.create_dataloaders to handle\n# differnt train and test transforms.\ndef create_dataloaders(\n    train_dir, \n    test_dir, \n    train_transform, # add parameter for train transform (transforms on train dataset)\n    test_transform,  # add parameter for test transform (transforms on test dataset)\n    batch_size=32, num_workers=NUM_WORKERS\n):\n    # Use ImageFolder to create dataset(s)\n    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n\n    # Get class names\n    class_names = train_data.classes\n\n    # Turn images into data loaders\n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    test_dataloader = DataLoader(\n        test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    return train_dataloader, test_dataloader, class_names\n</pre> <ol> <li>Scale up the dataset to turn FoodVision Mini into FoodVision Big using the entire Food101 dataset from <code>torchvision.models</code><ul> <li>You could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.</li> <li>If you try more than one model, it would be good to have the model's results tracked.</li> <li>If you load the Food101 dataset from <code>torchvision.models</code>, you'll have to create PyTorch DataLoaders to use it in training.</li> <li>Note: Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train.</li> </ul> </li> </ol>"},{"location":"Learning/Pytorch/07_pytorch_experiment_tracking/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Read The Bitter Lesson blog post by Richard Sutton to get an idea of how many of the latest advancements in AI have come from increased scale (bigger datasets and bigger models) and more general (less meticulously crafted) methods.</li> <li>Go through the PyTorch YouTube/code tutorial for TensorBoard for 20-minutes and see how it compares to the code we've written in this notebook.</li> <li>Perhaps you may want to view and rearrange your model's TensorBoard logs with a DataFrame (so you can sort the results by lowest loss or highest accuracy), there's a guide for this in the TensorBoard documentation.</li> <li>If you like to use VSCode for development using scripts or notebooks (VSCode can now use Jupyter Notebooks natively), you can setup TensorBoard right within VSCode using the  PyTorch Development in VSCode guide.</li> <li>To go further with experiment tracking and see how your PyTorch model is performing from a speed perspective (are there any bottlenecks that could be improved to speed up training?), see the PyTorch documentation for the PyTorch profiler.</li> <li>Made With ML is an outstanding resource for all things machine learning by Goku Mohandas and their guide on experiment tracking contains a fantastic introduction to tracking machine learning experiments with MLflow.</li> </ul>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/","title":"08. PyTorch Paper Replicating","text":"<p>View Source Code | View Slides</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>/home/jupyter-trunglph/.conda/envs/Multimodal_RumorDetection/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <pre>torch version: 2.2.2\ntorchvision version: 0.17.2\n</pre> <p>Note: If you're using Google Colab and the cell above starts to install various software packages, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of <code>torch</code> and <code>torchvision</code>.</p> <p>Now we'll continue with the regular imports, setting up device agnostic code and this time we'll also get the <code>helper_functions.py</code> script from GitHub.</p> <p>The <code>helper_functions.py</code> script contains several functions we created in previous sections:</p> <ul> <li><code>set_seeds()</code> to set the random seeds (created in 07. PyTorch Experiment Tracking section 0).</li> <li><code>download_data()</code> to download a data source given a link (created in 07. PyTorch Experiment Tracking section 1).</li> <li><code>plot_loss_curves()</code> to inspect our model's training results (created in 04. PyTorch Custom Datasets section 7.8)</li> </ul> <p>Note: It may be a better idea for many of the functions in the <code>helper_functions.py</code> script to be merged into <code>going_modular/going_modular/utils.py</code>, perhaps that's an extension you'd like to try.</p> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves <pre>[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\nCloning into 'pytorch-deep-learning'...\nremote: Enumerating objects: 4356, done.\nremote: Counting objects: 100% (321/321), done.\nremote: Compressing objects: 100% (144/144), done.\nremote: Total 4356 (delta 213), reused 252 (delta 176), pack-reused 4035 (from 1)\nReceiving objects: 100% (4356/4356), 654.51 MiB | 6.45 MiB/s, done.\nResolving deltas: 100% (2584/2584), done.\nUpdating files: 100% (248/248), done.\n</pre> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p> In\u00a0[3]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre># Download pizza, steak, sushi images from GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> # Download pizza, steak, sushi images from GitHub image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[4]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>Beautiful! Data downloaded, let's setup the training and test directories.</p> In\u00a0[5]: Copied! <pre># Setup directory paths to train and test images\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n</pre> # Setup directory paths to train and test images train_dir = image_path / \"train\" test_dir = image_path / \"test\" In\u00a0[6]: Copied! <pre># Create image size (from Table 3 in the ViT paper)\nIMG_SIZE = 224\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n])\nprint(f\"Manually created transforms: {manual_transforms}\")\n</pre> # Create image size (from Table 3 in the ViT paper) IMG_SIZE = 224  # Create transform pipeline manually manual_transforms = transforms.Compose([     transforms.Resize((IMG_SIZE, IMG_SIZE)),     transforms.ToTensor(), ]) print(f\"Manually created transforms: {manual_transforms}\") <pre>Manually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n    ToTensor()\n)\n</pre> In\u00a0[7]: Copied! <pre># Set the batch size\nBATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=BATCH_SIZE\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Set the batch size BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=manual_transforms, # use manually created transforms     batch_size=BATCH_SIZE )  train_dataloader, test_dataloader, class_names Out[7]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f91885b94f0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f92663850a0&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[8]: Copied! <pre># Get a batch of images\nimage_batch, label_batch = next(iter(train_dataloader))\n\n# Get a single image from the batch\nimage, label = image_batch[0], label_batch[0]\n\n# View the batch shapes\nimage.shape, label\n</pre> # Get a batch of images image_batch, label_batch = next(iter(train_dataloader))  # Get a single image from the batch image, label = image_batch[0], label_batch[0]  # View the batch shapes image.shape, label Out[8]: <pre>(torch.Size([3, 224, 224]), tensor(2))</pre> <p>Wonderful!</p> <p>Now let's plot the image and its label with <code>matplotlib</code>.</p> In\u00a0[9]: Copied! <pre># Plot image with matplotlib\nplt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels]\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # Plot image with matplotlib plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels] plt.title(class_names[label]) plt.axis(False); <p>Nice!</p> <p>Looks like our images are importing correctly, let's continue with the paper replication.</p> In\u00a0[10]: Copied! <pre># Create example values\nheight = 224 # H (\"The training resolution is 224.\")\nwidth = 224 # W\ncolor_channels = 3 # C\npatch_size = 16 # P\n\n# Calculate N (number of patches)\nnumber_of_patches = int((height * width) / patch_size**2)\nprint(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")\n</pre> # Create example values height = 224 # H (\"The training resolution is 224.\") width = 224 # W color_channels = 3 # C patch_size = 16 # P  # Calculate N (number of patches) number_of_patches = int((height * width) / patch_size**2) print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\") <pre>Number of patches (N) with image height (H=224), width (W=224) and patch size (P=16): 196\n</pre> <p>We've got the number of patches, how about we create the image output size as well?</p> <p>Better yet, let's replicate the input and output shapes of the patch embedding layer.</p> <p>Recall:</p> <ul> <li>Input: The image starts as 2D with size ${H \\times W \\times C}$.</li> <li>Output: The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> In\u00a0[11]: Copied! <pre># Input shape (this is the size of a single image)\nembedding_layer_input_shape = (height, width, color_channels)\n\n# Output shape\nembedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n\nprint(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\nprint(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")\n</pre> # Input shape (this is the size of a single image) embedding_layer_input_shape = (height, width, color_channels)  # Output shape embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)  print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\") print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\") <pre>Input shape (single 2D image): (224, 224, 3)\nOutput shape (single 2D image flattened into patches): (196, 768)\n</pre> <p>Input and output shapes acquired!</p> In\u00a0[12]: Copied! <pre># View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); <p>We want to turn this image into patches of itself inline with Figure 1 of the ViT paper.</p> <p>How about we start by just visualizing the top row of patched pixels?</p> <p>We can do this by indexing on the different image dimensions.</p> In\u00a0[13]: Copied! <pre># Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels)\nimage_permuted = image.permute(1, 2, 0)\n\n# Index to plot the top row of patched pixels\npatch_size = 16\nplt.figure(figsize=(patch_size, patch_size))\nplt.imshow(image_permuted[:patch_size, :, :]);\n</pre> # Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels) image_permuted = image.permute(1, 2, 0)  # Index to plot the top row of patched pixels patch_size = 16 plt.figure(figsize=(patch_size, patch_size)) plt.imshow(image_permuted[:patch_size, :, :]); <p>Now we've got the top row, let's turn it into patches.</p> <p>We can do this by iterating through the number of patches there'd be in the top row.</p> In\u00a0[14]: Copied! <pre># Setup hyperparameters and make sure img_size and patch_size are compatible\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size\nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\nprint(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n\n# Create a series of subplots\nfig, axs = plt.subplots(nrows=1,\n                        ncols=img_size // patch_size, # one column for each patch\n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Iterate through number of patches in the top row\nfor i, patch in enumerate(range(0, img_size, patch_size)):\n    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n    axs[i].set_xlabel(i+1) # set the label\n    axs[i].set_xticks([])\n    axs[i].set_yticks([])\n</pre> # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")  # Create a series of subplots fig, axs = plt.subplots(nrows=1,                         ncols=img_size // patch_size, # one column for each patch                         figsize=(num_patches, num_patches),                         sharex=True,                         sharey=True)  # Iterate through number of patches in the top row for i, patch in enumerate(range(0, img_size, patch_size)):     axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index     axs[i].set_xlabel(i+1) # set the label     axs[i].set_xticks([])     axs[i].set_yticks([]) <pre>Number of patches per row: 14.0\nPatch size: 16 pixels x 16 pixels\n</pre> <p>Those are some nice looking patches!</p> <p>How about we do it for the whole image?</p> <p>This time we'll iterate through the indexs for height and width and plot each patch as it's own subplot.</p> In\u00a0[15]: Copied! <pre># Setup hyperparameters and make sure img_size and patch_size are compatible\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size\nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\nprint(f\"Number of patches per row: {num_patches}\\\n        \\nNumber of patches per column: {num_patches}\\\n        \\nTotal patches: {num_patches*num_patches}\\\n        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n\n# Create a series of subplots\nfig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n                        ncols=img_size // patch_size,\n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Loop through height and width of image\nfor i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n\n        # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))\n        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height\n                                        patch_width:patch_width+patch_size, # iterate through width\n                                        :]) # get all color channels\n\n        # Set up label information, remove the ticks for clarity and set labels to outside\n        axs[i, j].set_ylabel(i+1,\n                             rotation=\"horizontal\",\n                             horizontalalignment=\"right\",\n                             verticalalignment=\"center\")\n        axs[i, j].set_xlabel(j+1)\n        axs[i, j].set_xticks([])\n        axs[i, j].set_yticks([])\n        axs[i, j].label_outer()\n\n# Set a super title\nfig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16)\nplt.show()\n</pre> # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" print(f\"Number of patches per row: {num_patches}\\         \\nNumber of patches per column: {num_patches}\\         \\nTotal patches: {num_patches*num_patches}\\         \\nPatch size: {patch_size} pixels x {patch_size} pixels\")  # Create a series of subplots fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float                         ncols=img_size // patch_size,                         figsize=(num_patches, num_patches),                         sharex=True,                         sharey=True)  # Loop through height and width of image for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height     for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width          # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))         axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height                                         patch_width:patch_width+patch_size, # iterate through width                                         :]) # get all color channels          # Set up label information, remove the ticks for clarity and set labels to outside         axs[i, j].set_ylabel(i+1,                              rotation=\"horizontal\",                              horizontalalignment=\"right\",                              verticalalignment=\"center\")         axs[i, j].set_xlabel(j+1)         axs[i, j].set_xticks([])         axs[i, j].set_yticks([])         axs[i, j].label_outer()  # Set a super title fig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16) plt.show() <pre>Number of patches per row: 14.0        \nNumber of patches per column: 14.0        \nTotal patches: 196.0        \nPatch size: 16 pixels x 16 pixels\n</pre> <p>Image patchified!</p> <p>Woah, that looks cool.</p> <p>Now how do we turn each of these patches into an embedding and convert them into a sequence?</p> <p>Hint: we can use PyTorch layers. Can you guess which?</p> In\u00a0[16]: Copied! <pre>from torch import nn\n\n# Set the patch size\npatch_size=16\n\n# Create the Conv2d layer with hyperparameters from the ViT paper\nconv2d = nn.Conv2d(in_channels=3, # number of color channels\n                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n                   stride=patch_size,\n                   padding=0)\n</pre> from torch import nn  # Set the patch size patch_size=16  # Create the Conv2d layer with hyperparameters from the ViT paper conv2d = nn.Conv2d(in_channels=3, # number of color channels                    out_channels=768, # from Table 1: Hidden size D, this is the embedding size                    kernel_size=patch_size, # could also use (patch_size, patch_size)                    stride=patch_size,                    padding=0) <p>Now we've got a convoluational layer, let's see what happens when we pass a single image through it.</p> In\u00a0[17]: Copied! <pre># View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); In\u00a0[18]: Copied! <pre># Pass the image through the convolutional layer\nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels)\nprint(image_out_of_conv.shape)\n</pre> # Pass the image through the convolutional layer image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels) print(image_out_of_conv.shape) <pre>torch.Size([1, 768, 14, 14])\n</pre> <p>Passing our image through the convolutional layer turns it into a series of 768 (this is the embedding size or $D$) feature/activation maps.</p> <p>So its output shape can be read as:</p> <pre>torch.Size([1, 768, 14, 14]) -&gt; [batch_size, embedding_dim, feature_map_height, feature_map_width]\n</pre> <p>Let's visualize five random feature maps and see what they look like.</p> In\u00a0[19]: Copied! <pre># Plot random 5 convolutional feature maps\nimport random\nrandom_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\nprint(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n\n# Create plot\nfig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n\n# Plot random image feature maps\nfor i, idx in enumerate(random_indexes):\n    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);\n</pre> # Plot random 5 convolutional feature maps import random random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")  # Create plot fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))  # Plot random image feature maps for i, idx in enumerate(random_indexes):     image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer     axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())     axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]); <pre>Showing random convolutional feature maps from indexes: [674, 631, 496, 118, 161]\n</pre> <p>Notice how the feature maps all kind of represent the original image, after visualizing a few more you can start to see the different major outlines and some major features.</p> <p>The important thing to note is that these features may change over time as the neural network learns.</p> <p>And because of these, these feature maps can be considered a learnable embedding of our image.</p> <p>Let's check one out in numerical form.</p> In\u00a0[20]: Copied! <pre># Get a single feature map in tensor form\nsingle_feature_map = image_out_of_conv[:, 0, :, :]\nsingle_feature_map, single_feature_map.requires_grad\n</pre> # Get a single feature map in tensor form single_feature_map = image_out_of_conv[:, 0, :, :] single_feature_map, single_feature_map.requires_grad Out[20]: <pre>(tensor([[[-0.4969, -0.5042, -0.3972, -0.4130, -0.3962, -0.3981, -0.3880,\n           -0.3763, -0.3764, -0.3615, -0.3352, -0.3145, -0.3044, -0.2902],\n          [-0.4909, -0.3708, -0.2844, -0.2801, -0.3556, -0.4164, -0.3867,\n           -0.3487, -0.3447, -0.3526, -0.3410, -0.3187, -0.2963, -0.2764],\n          [-0.1972, -0.1913, -0.1672, -0.2462, -0.1263, -0.4333, -0.4545,\n           -0.2683, -0.2262, -0.3561, -0.3767, -0.4285, -0.4269, -0.4051],\n          [-0.2234, -0.1573, -0.1751, -0.3561, -0.3060, -0.1518, -0.2663,\n           -0.2502, -0.1224, -0.1306, -0.1323, -0.2491, -0.2363, -0.2521],\n          [-0.1682, -0.1161, -0.1290, -0.3115, -0.2833, -0.1815, -0.2370,\n           -0.1411, -0.1128, -0.1486, -0.2010, -0.2417, -0.1820, -0.2156],\n          [-0.0982, -0.1053, -0.1531, -0.1384, -0.1593, -0.1974, -0.1333,\n           -0.1280, -0.1576, -0.1891, -0.1161, -0.1662, -0.2544, -0.2740],\n          [-0.2348, -0.1427, -0.1145, -0.1608, -0.1771, -0.1199, -0.1313,\n           -0.1276, -0.1736, -0.1082, -0.1434, -0.2854, -0.1056, -0.1518],\n          [-0.2009, -0.1707, -0.3431, -0.6486, -0.5940, -0.4131, -0.2521,\n           -0.1849, -0.1630, -0.1082, -0.1600, -0.2933, -0.2543, -0.2077],\n          [-0.1320, -0.1585, -0.3377, -0.6092, -0.5409, -0.2658, -0.3988,\n           -0.4087, -0.1068, -0.1686, -0.1685, -0.1450, -0.2747, -0.3932],\n          [-0.1529, -0.1958, -0.2093, -0.7470, -0.4375, -0.3852, -0.2338,\n           -0.4072, -0.2000, -0.1176, -0.3359, -0.1182, -0.2355, -0.4446],\n          [-0.1901, -0.3891, -0.5073, -0.6136, -0.4269, -0.3604, -0.3356,\n           -0.4258, -0.2694, -0.0864, -0.0942, -0.1309, -0.3707, -0.3089],\n          [-0.6471, -0.4759, -0.4846, -0.4851, -0.5892, -0.3724, -0.3602,\n           -0.3281, -0.2334, -0.0873, -0.0968, -0.2450, -0.1206, -0.2354],\n          [-0.0993, -0.6716, -0.5356, -0.4815, -0.4414, -0.4094, -0.2823,\n           -0.1802, -0.0802, -0.0993, -0.2263, -0.1503, -0.1455, -0.5511],\n          [-0.1376, -0.3307, -0.5020, -0.1709, -0.1229, -0.0842, -0.0759,\n           -0.0758, -0.0871, -0.0948, -0.0985, -0.0926, -0.2630, -0.0857]]],\n        grad_fn=&lt;SliceBackward0&gt;),\n True)</pre> <p>The <code>grad_fn</code> output of the <code>single_feature_map</code> and the <code>requires_grad=True</code> attribute means PyTorch is tracking the gradients of this feature map and it will be updated by gradient descent during training.</p> In\u00a0[21]: Copied! <pre># Current tensor shape\nprint(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\")\n</pre> # Current tensor shape print(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\") <pre>Current tensor shape: torch.Size([1, 768, 14, 14]) -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\n</pre> <p>Well we've got the 768 part ( $(P^{2} \\cdot C)$ ) but we still need the number of patches ($N$).</p> <p>Reading back through section 3.1 of the ViT paper it says (bold mine):</p> <p>As a special case, the patches can have spatial size $1 \\times 1$, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension.</p> <p>Flattening the spatial dimensions of the feature map hey?</p> <p>What layer do we have in PyTorch that can flatten?</p> <p>How about <code>torch.nn.Flatten()</code>?</p> <p>But we don't want to flatten the whole tensor, we only want to flatten the \"spatial dimensions of the feature map\".</p> <p>Which in our case is the <code>feature_map_height</code> and <code>feature_map_width</code> dimensions of <code>image_out_of_conv</code>.</p> <p>So how about we create a <code>torch.nn.Flatten()</code> layer to only flatten those dimensions, we can use the <code>start_dim</code> and <code>end_dim</code> parameters to set that up?</p> In\u00a0[22]: Copied! <pre># Create flatten layer\nflatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n                     end_dim=3) # flatten feature_map_width (dimension 3)\n</pre> # Create flatten layer flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)                      end_dim=3) # flatten feature_map_width (dimension 3) <p>Nice! Now let's put it all together!</p> <p>We'll:</p> <ol> <li>Take a single image.</li> <li>Put in through the convolutional layer (<code>conv2d</code>) to turn the image into 2D feature maps (patch embeddings).</li> <li>Flatten the 2D feature map into a single sequence.</li> </ol> In\u00a0[23]: Copied! <pre># 1. View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\nprint(f\"Original image shape: {image.shape}\")\n\n# 2. Turn image into feature maps\nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\nprint(f\"Image feature map shape: {image_out_of_conv.shape}\")\n\n# 3. Flatten the feature maps\nimage_out_of_conv_flattened = flatten(image_out_of_conv)\nprint(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")\n</pre> # 1. View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); print(f\"Original image shape: {image.shape}\")  # 2. Turn image into feature maps image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors print(f\"Image feature map shape: {image_out_of_conv.shape}\")  # 3. Flatten the feature maps image_out_of_conv_flattened = flatten(image_out_of_conv) print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\") <pre>Original image shape: torch.Size([3, 224, 224])\nImage feature map shape: torch.Size([1, 768, 14, 14])\nFlattened image feature map shape: torch.Size([1, 768, 196])\n</pre> <p>Woohoo! It looks like our <code>image_out_of_conv_flattened</code> shape is very close to our desired output shape:</p> <ul> <li>Desired output (flattened 2D patches): (196, 768) -&gt; ${N \\times\\left(P^{2} \\cdot C\\right)}$</li> <li>Current shape: (1, 768, 196)</li> </ul> <p>The only difference is our current shape has a batch size and the dimensions are in a different order to the desired output.</p> <p>How could we fix this?</p> <p>Well, how about we rearrange the dimensions?</p> <p>We can do so with <code>torch.Tensor.permute()</code> just like we do when rearranging image tensors to plot them with matplotlib.</p> <p>Let's try.</p> In\u00a0[24]: Copied! <pre># Get flattened image patch embeddings in right shape\nimage_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C]\nprint(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\")\n</pre> # Get flattened image patch embeddings in right shape image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C] print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\") <pre>Patch embedding sequence shape: torch.Size([1, 196, 768]) -&gt; [batch_size, num_patches, embedding_size]\n</pre> <p>Yes!!!</p> <p>We've now matched the desired input and output shapes for the patch embedding layer of the ViT architecture using a couple of PyTorch layers.</p> <p>How about we visualize one of the flattened feature maps?</p> In\u00a0[25]: Copied! <pre># Get a single flattened feature map\nsingle_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n\n# Plot the flattened feature map visually\nplt.figure(figsize=(22, 22))\nplt.imshow(single_flattened_feature_map.detach().numpy())\nplt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\nplt.axis(False);\n</pre> # Get a single flattened feature map single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)  # Plot the flattened feature map visually plt.figure(figsize=(22, 22)) plt.imshow(single_flattened_feature_map.detach().numpy()) plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\") plt.axis(False); <p>Hmm, the flattened feature map doesn't look like much visually, but that's not what we're concerned about, this is what will be the output of the patching embedding layer and the input to the rest of the ViT architecture.</p> <p>Note: The original Transformer architecture was designed to work with text. The Vision Transformer architecture (ViT) had the goal of using the original Transformer for images. This is why the input to the ViT architecture is processed in the way it is. We're essentially taking a 2D image and formatting it so it appears as a 1D sequence of text.</p> <p>How about we view the flattened feature map in tensor form?</p> In\u00a0[26]: Copied! <pre># See the flattened feature map as a tensor\nsingle_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape\n</pre> # See the flattened feature map as a tensor single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape Out[26]: <pre>(tensor([[-0.4969, -0.5042, -0.3972, -0.4130, -0.3962, -0.3981, -0.3880, -0.3763,\n          -0.3764, -0.3615, -0.3352, -0.3145, -0.3044, -0.2902, -0.4909, -0.3708,\n          -0.2844, -0.2801, -0.3556, -0.4164, -0.3867, -0.3487, -0.3447, -0.3526,\n          -0.3410, -0.3187, -0.2963, -0.2764, -0.1972, -0.1913, -0.1672, -0.2462,\n          -0.1263, -0.4333, -0.4545, -0.2683, -0.2262, -0.3561, -0.3767, -0.4285,\n          -0.4269, -0.4051, -0.2234, -0.1573, -0.1751, -0.3561, -0.3060, -0.1518,\n          -0.2663, -0.2502, -0.1224, -0.1306, -0.1323, -0.2491, -0.2363, -0.2521,\n          -0.1682, -0.1161, -0.1290, -0.3115, -0.2833, -0.1815, -0.2370, -0.1411,\n          -0.1128, -0.1486, -0.2010, -0.2417, -0.1820, -0.2156, -0.0982, -0.1053,\n          -0.1531, -0.1384, -0.1593, -0.1974, -0.1333, -0.1280, -0.1576, -0.1891,\n          -0.1161, -0.1662, -0.2544, -0.2740, -0.2348, -0.1427, -0.1145, -0.1608,\n          -0.1771, -0.1199, -0.1313, -0.1276, -0.1736, -0.1082, -0.1434, -0.2854,\n          -0.1056, -0.1518, -0.2009, -0.1707, -0.3431, -0.6486, -0.5940, -0.4131,\n          -0.2521, -0.1849, -0.1630, -0.1082, -0.1600, -0.2933, -0.2543, -0.2077,\n          -0.1320, -0.1585, -0.3377, -0.6092, -0.5409, -0.2658, -0.3988, -0.4087,\n          -0.1068, -0.1686, -0.1685, -0.1450, -0.2747, -0.3932, -0.1529, -0.1958,\n          -0.2093, -0.7470, -0.4375, -0.3852, -0.2338, -0.4072, -0.2000, -0.1176,\n          -0.3359, -0.1182, -0.2355, -0.4446, -0.1901, -0.3891, -0.5073, -0.6136,\n          -0.4269, -0.3604, -0.3356, -0.4258, -0.2694, -0.0864, -0.0942, -0.1309,\n          -0.3707, -0.3089, -0.6471, -0.4759, -0.4846, -0.4851, -0.5892, -0.3724,\n          -0.3602, -0.3281, -0.2334, -0.0873, -0.0968, -0.2450, -0.1206, -0.2354,\n          -0.0993, -0.6716, -0.5356, -0.4815, -0.4414, -0.4094, -0.2823, -0.1802,\n          -0.0802, -0.0993, -0.2263, -0.1503, -0.1455, -0.5511, -0.1376, -0.3307,\n          -0.5020, -0.1709, -0.1229, -0.0842, -0.0759, -0.0758, -0.0871, -0.0948,\n          -0.0985, -0.0926, -0.2630, -0.0857]], grad_fn=&lt;SelectBackward0&gt;),\n True,\n torch.Size([1, 196]))</pre> <p>Beautiful!</p> <p>We've turned our single 2D image into a 1D learnable embedding vector (or \"Linear Projection of Flattned Patches\" in Figure 1 of the ViT paper).</p> In\u00a0[27]: Copied! <pre># 1. Create a class which subclasses nn.Module\nclass PatchEmbedding(nn.Module):\n    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n\n    Args:\n        in_channels (int): Number of color channels for the input images. Defaults to 3.\n        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n    \"\"\"\n    # 2. Initialize the class with appropriate variables\n    def __init__(self,\n                 in_channels:int=3,\n                 patch_size:int=16,\n                 embedding_dim:int=768):\n        super().__init__()\n\n        # 3. Create a layer to turn an image into patches\n        self.patcher = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=embedding_dim,\n                                 kernel_size=patch_size,\n                                 stride=patch_size,\n                                 padding=0)\n\n        # 4. Create a layer to flatten the patch feature maps into a single dimension\n        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n                                  end_dim=3)\n\n    # 5. Define the forward method\n    def forward(self, x):\n        # Create assertion to check that inputs are the correct shape\n        image_resolution = x.shape[-1]\n        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n\n        # Perform the forward pass\n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched)\n        # 6. Make sure the output shape has the right order\n        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C]\n</pre> # 1. Create a class which subclasses nn.Module class PatchEmbedding(nn.Module):     \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.      Args:         in_channels (int): Number of color channels for the input images. Defaults to 3.         patch_size (int): Size of patches to convert input image into. Defaults to 16.         embedding_dim (int): Size of embedding to turn image into. Defaults to 768.     \"\"\"     # 2. Initialize the class with appropriate variables     def __init__(self,                  in_channels:int=3,                  patch_size:int=16,                  embedding_dim:int=768):         super().__init__()          # 3. Create a layer to turn an image into patches         self.patcher = nn.Conv2d(in_channels=in_channels,                                  out_channels=embedding_dim,                                  kernel_size=patch_size,                                  stride=patch_size,                                  padding=0)          # 4. Create a layer to flatten the patch feature maps into a single dimension         self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector                                   end_dim=3)      # 5. Define the forward method     def forward(self, x):         # Create assertion to check that inputs are the correct shape         image_resolution = x.shape[-1]         assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"          # Perform the forward pass         x_patched = self.patcher(x)         x_flattened = self.flatten(x_patched)         # 6. Make sure the output shape has the right order         return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C] <p><code>PatchEmbedding</code> layer created!</p> <p>Let's try it out on a single image.</p> In\u00a0[28]: Copied! <pre>set_seeds()\n\n# Create an instance of patch embedding layer\npatchify = PatchEmbedding(in_channels=3,\n                          patch_size=16,\n                          embedding_dim=768)\n\n# Pass a single image through\nprint(f\"Input image shape: {image.unsqueeze(0).shape}\")\npatch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\nprint(f\"Output patch embedding shape: {patch_embedded_image.shape}\")\n</pre> set_seeds()  # Create an instance of patch embedding layer patchify = PatchEmbedding(in_channels=3,                           patch_size=16,                           embedding_dim=768)  # Pass a single image through print(f\"Input image shape: {image.unsqueeze(0).shape}\") patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error print(f\"Output patch embedding shape: {patch_embedded_image.shape}\") <pre>Input image shape: torch.Size([1, 3, 224, 224])\nOutput patch embedding shape: torch.Size([1, 196, 768])\n</pre> <p>Beautiful!</p> <p>The output shape matches the ideal input and output shapes we'd like to see from the patch embedding layer:</p> <ul> <li>Input: The image starts as 2D with size ${H \\times W \\times C}$.</li> <li>Output: The image gets converted to a 1D sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> <p>Where:</p> <ul> <li>$(H, W)$ is the resolution of the original image.</li> <li>$C$ is the number of channels.</li> <li>$(P, P)$ is the resolution of each image patch (patch size).</li> <li>$N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.</li> </ul> <p>We've now replicated the patch embedding for equation 1 but not the class token/position embedding.</p> <p>We'll get to these later on.</p> <p></p> <p>Our <code>PatchEmbedding</code> class (right) replicates the patch embedding of the ViT architecture from Figure 1 and Equation 1 from the ViT paper (left). However, the learnable class embedding and position embeddings haven't been created yet. These will come soon.</p> <p>Let's now get a summary of our <code>PatchEmbedding</code> layer.</p> In\u00a0[29]: Copied! <pre># Create random input sizes\nrandom_input_image = (1, 3, 224, 224)\nrandom_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n\n# # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\n# summary(PatchEmbedding(),\n#         input_size=random_input_image, # try swapping this for \"random_input_image_error\"\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> # Create random input sizes random_input_image = (1, 3, 224, 224) random_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size  # # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output) # summary(PatchEmbedding(), #         input_size=random_input_image, # try swapping this for \"random_input_image_error\" #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) In\u00a0[30]: Copied! <pre># View the patch embedding and patch embedding shape\nprint(patch_embedded_image)\nprint(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # View the patch embedding and patch embedding shape print(patch_embedded_image) print(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[-0.8509,  0.1935, -0.0733,  ...,  0.4927, -0.4795,  0.3345],\n         [-0.7782, -0.0290, -0.0858,  ...,  0.5058, -0.4762,  0.3578],\n         [-0.7469,  0.0840, -0.0677,  ...,  0.4260, -0.4161,  0.2774],\n         ...,\n         [-0.1418,  0.0333, -0.0511,  ...,  0.0897, -0.0767,  0.0492],\n         [-0.5927,  0.0719,  0.0266,  ...,  0.2245, -0.1302, -0.2618],\n         [-0.1805,  0.0180, -0.0556,  ...,  0.0914, -0.1476,  0.0110]]],\n       grad_fn=&lt;PermuteBackward0&gt;)\nPatch embedding shape: torch.Size([1, 196, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>To \"prepend a learnable embedding to the sequence of embedded patches\" we need to create a learnable embedding in the shape of the <code>embedding_dimension</code> ($D$) and then add it to the <code>number_of_patches</code> dimension.</p> <p>Or in pseudocode:</p> <pre>patch_embedding = [image_patch_1, image_patch_2, image_patch_3...]\nclass_token = learnable_embedding\npatch_embedding_with_class_token = torch.cat((class_token, patch_embedding), dim=1)\n</pre> <p>Notice the concatenation (<code>torch.cat()</code>) happens on <code>dim=1</code> (the <code>number_of_patches</code> dimension).</p> <p>Let's create a learnable embedding for the class token.</p> <p>To do so, we'll get the batch size and embedding dimension shape and then we'll create a <code>torch.ones()</code> tensor in the shape <code>[batch_size, 1, embedding_dimension]</code>.</p> <p>And we'll make the tensor learnable by passing it to <code>nn.Parameter()</code> with <code>requires_grad=True</code>.</p> In\u00a0[31]: Copied! <pre># Get the batch size and embedding dimension\nbatch_size = patch_embedded_image.shape[0]\nembedding_dimension = patch_embedded_image.shape[-1]\n\n# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n                           requires_grad=True) # make sure the embedding is learnable\n\n# Show the first 10 examples of the class_token\nprint(class_token[:, :, :10])\n\n# Print the class_token shape\nprint(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\")\n</pre> # Get the batch size and embedding dimension batch_size = patch_embedded_image.shape[0] embedding_dimension = patch_embedded_image.shape[-1]  # Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D) class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]                            requires_grad=True) # make sure the embedding is learnable  # Show the first 10 examples of the class_token print(class_token[:, :, :10])  # Print the class_token shape print(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\") <pre>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)\nClass token shape: torch.Size([1, 1, 768]) -&gt; [batch_size, number_of_tokens, embedding_dimension]\n</pre> <p>Note: Here we're only creating the class token embedding as <code>torch.ones()</code> for demonstration purposes, in reality, you'd likely create the class token embedding with <code>torch.randn()</code> (since machine learning is all about harnessing the power of controlled randomness, you generally start with a random number and improve it over time).</p> <p>See how the <code>number_of_tokens</code> dimension of <code>class_token</code> is <code>1</code> since we only want to prepend one class token value to the start of the patch embedding sequence.</p> <p>Now we've got the class token embedding, let's prepend it to our sequence of image patches, <code>patch_embedded_image</code>.</p> <p>We can do so using <code>torch.cat()</code> and set <code>dim=1</code> (so <code>class_token</code>'s <code>number_of_tokens</code> dimension is preprended to <code>patch_embedded_image</code>'s <code>number_of_patches</code> dimension).</p> In\u00a0[32]: Copied! <pre># Add the class token embedding to the front of the patch embedding\npatch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n                                                      dim=1) # concat on first dimension\n\n# Print the sequence of patch embeddings with the prepended class token embedding\nprint(patch_embedded_image_with_class_embedding)\nprint(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Add the class token embedding to the front of the patch embedding patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),                                                       dim=1) # concat on first dimension  # Print the sequence of patch embeddings with the prepended class token embedding print(patch_embedded_image_with_class_embedding) print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n         [-0.8509,  0.1935, -0.0733,  ...,  0.4927, -0.4795,  0.3345],\n         [-0.7782, -0.0290, -0.0858,  ...,  0.5058, -0.4762,  0.3578],\n         ...,\n         [-0.1418,  0.0333, -0.0511,  ...,  0.0897, -0.0767,  0.0492],\n         [-0.5927,  0.0719,  0.0266,  ...,  0.2245, -0.1302, -0.2618],\n         [-0.1805,  0.0180, -0.0556,  ...,  0.0914, -0.1476,  0.0110]]],\n       grad_fn=&lt;CatBackward0&gt;)\nSequence of patch embeddings with class token prepended shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>Nice! Learnable class token prepended!</p> <p></p> <p>Reviewing what we've done to create the learnable class token, we start with a sequence of image patch embeddings created by <code>PatchEmbedding()</code> on single image, we then created a learnable class token with one value for each of the embedding dimensions and then prepended it to the original sequence of patch embeddings. Note: Using <code>torch.ones()</code> to create the learnable class token is mostly for demonstration purposes only, in practice, you'd likely create it with <code>torch.randn()</code>.</p> In\u00a0[33]: Copied! <pre># View the sequence of patch embeddings with the prepended class embedding\npatch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape\n</pre> # View the sequence of patch embeddings with the prepended class embedding patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape Out[33]: <pre>(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n          [-0.8509,  0.1935, -0.0733,  ...,  0.4927, -0.4795,  0.3345],\n          [-0.7782, -0.0290, -0.0858,  ...,  0.5058, -0.4762,  0.3578],\n          ...,\n          [-0.1418,  0.0333, -0.0511,  ...,  0.0897, -0.0767,  0.0492],\n          [-0.5927,  0.0719,  0.0266,  ...,  0.2245, -0.1302, -0.2618],\n          [-0.1805,  0.0180, -0.0556,  ...,  0.0914, -0.1476,  0.0110]]],\n        grad_fn=&lt;CatBackward0&gt;),\n torch.Size([1, 197, 768]))</pre> <p>Equation 1 states that the position embeddings ($\\mathbf{E}_{\\text {pos }}$) should have the shape $(N + 1) \\times D$:</p> <p>$$\\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}$$</p> <p>Where:</p> <ul> <li>$N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer (number of patches).</li> <li>$D$ is the size of the patch embeddings, different values for $D$ can be found in Table 1 (embedding dimension).</li> </ul> <p>Luckily we've got both of these values already.</p> <p>So let's make a learnable 1D embedding with <code>torch.ones()</code> to create $\\mathbf{E}_{\\text {pos }}$.</p> In\u00a0[34]: Copied! <pre># Calculate N (number of patches)\nnumber_of_patches = int((height * width) / patch_size**2)\n\n# Get embedding dimension\nembedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n\n# Create the learnable 1D position embedding\nposition_embedding = nn.Parameter(torch.ones(1,\n                                             number_of_patches+1,\n                                             embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\nprint(position_embedding[:, :10, :10])\nprint(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Calculate N (number of patches) number_of_patches = int((height * width) / patch_size**2)  # Get embedding dimension embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]  # Create the learnable 1D position embedding position_embedding = nn.Parameter(torch.ones(1,                                              number_of_patches+1,                                              embedding_dimension),                                   requires_grad=True) # make sure it's learnable  # Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding print(position_embedding[:, :10, :10]) print(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)\nPosition embeddding shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>Note: Only creating the position embedding as <code>torch.ones()</code> for demonstration purposes, in reality, you'd likely create the position embedding with <code>torch.randn()</code> (start with a random number and improve via gradient descent).</p> <p>Position embeddings created!</p> <p>Let's add them to our sequence of patch embeddings with a prepended class token.</p> In\u00a0[35]: Copied! <pre># Add the position embedding to the patch and class token embedding\npatch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\nprint(patch_and_position_embedding)\nprint(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Add the position embedding to the patch and class token embedding patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding print(patch_and_position_embedding) print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[2.0000, 2.0000, 2.0000,  ..., 2.0000, 2.0000, 2.0000],\n         [0.1491, 1.1935, 0.9267,  ..., 1.4927, 0.5205, 1.3345],\n         [0.2218, 0.9710, 0.9142,  ..., 1.5058, 0.5238, 1.3578],\n         ...,\n         [0.8582, 1.0333, 0.9489,  ..., 1.0897, 0.9233, 1.0492],\n         [0.4073, 1.0719, 1.0266,  ..., 1.2245, 0.8698, 0.7382],\n         [0.8195, 1.0180, 0.9444,  ..., 1.0914, 0.8524, 1.0110]]],\n       grad_fn=&lt;AddBackward0&gt;)\nPatch embeddings, class token prepended and positional embeddings added shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>Notice how the values of each of the elements in the embedding tensor increases by 1 (this is because of the position embeddings being created with <code>torch.ones()</code>).</p> <p>Note: We could put both the class token embedding and position embedding into their own layer if we wanted to. But we'll see later on in section 8 how they can be incorporated into the overall ViT architecture's <code>forward()</code> method.</p> <p></p> <p>The workflow we've used for adding the position embeddings to the sequence of patch embeddings and class token. Note: <code>torch.ones()</code> only used to create embeddings for illustration purposes, in practice, you'd likely use <code>torch.randn()</code> to start with a random number.</p> In\u00a0[36]: Copied! <pre>set_seeds()\n\n# 1. Set patch size\npatch_size = 16\n\n# 2. Print shape of original image tensor and get the image dimensions\nprint(f\"Image tensor shape: {image.shape}\")\nheight, width = image.shape[1], image.shape[2]\n\n# 3. Get image tensor and add batch dimension\nx = image.unsqueeze(0)\nprint(f\"Input image with batch dimension shape: {x.shape}\")\n\n# 4. Create patch embedding layer\npatch_embedding_layer = PatchEmbedding(in_channels=3,\n                                       patch_size=patch_size,\n                                       embedding_dim=768)\n\n# 5. Pass image through patch embedding layer\npatch_embedding = patch_embedding_layer(x)\nprint(f\"Patching embedding shape: {patch_embedding.shape}\")\n\n# 6. Create class token embedding\nbatch_size = patch_embedding.shape[0]\nembedding_dimension = patch_embedding.shape[-1]\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n                           requires_grad=True) # make sure it's learnable\nprint(f\"Class token embedding shape: {class_token.shape}\")\n\n# 7. Prepend class token embedding to patch embedding\npatch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\nprint(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n\n# 8. Create position embedding\nnumber_of_patches = int((height * width) / patch_size**2)\nposition_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# 9. Add position embedding to patch embedding with class token\npatch_and_position_embedding = patch_embedding_class_token + position_embedding\nprint(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n</pre> set_seeds()  # 1. Set patch size patch_size = 16  # 2. Print shape of original image tensor and get the image dimensions print(f\"Image tensor shape: {image.shape}\") height, width = image.shape[1], image.shape[2]  # 3. Get image tensor and add batch dimension x = image.unsqueeze(0) print(f\"Input image with batch dimension shape: {x.shape}\")  # 4. Create patch embedding layer patch_embedding_layer = PatchEmbedding(in_channels=3,                                        patch_size=patch_size,                                        embedding_dim=768)  # 5. Pass image through patch embedding layer patch_embedding = patch_embedding_layer(x) print(f\"Patching embedding shape: {patch_embedding.shape}\")  # 6. Create class token embedding batch_size = patch_embedding.shape[0] embedding_dimension = patch_embedding.shape[-1] class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),                            requires_grad=True) # make sure it's learnable print(f\"Class token embedding shape: {class_token.shape}\")  # 7. Prepend class token embedding to patch embedding patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1) print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")  # 8. Create position embedding number_of_patches = int((height * width) / patch_size**2) position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),                                   requires_grad=True) # make sure it's learnable  # 9. Add position embedding to patch embedding with class token patch_and_position_embedding = patch_embedding_class_token + position_embedding print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\") <pre>Image tensor shape: torch.Size([3, 224, 224])\nInput image with batch dimension shape: torch.Size([1, 3, 224, 224])\nPatching embedding shape: torch.Size([1, 196, 768])\nClass token embedding shape: torch.Size([1, 1, 768])\nPatch embedding with class token shape: torch.Size([1, 197, 768])\nPatch and position embedding shape: torch.Size([1, 197, 768])\n</pre> <p>Woohoo!</p> <p>From a single image to patch and position embeddings in a single cell of code.</p> <p></p> <p>Mapping equation 1 from the ViT paper to our PyTorch code. This is the essence of paper replicating, taking a research paper and turning it into usable code.</p> <p>Now we've got a way to encode our images and pass them to the Transformer Encoder in Figure 1 of the ViT paper.</p> <p>Animating the entire ViT workflow: from patch embeddings to transformer encoder to MLP head.</p> <p>From a code perspective, creating the patch embedding is probably the largest section of replicating the ViT paper.</p> <p>Many of the other parts of the ViT paper such as the Multi-Head Attention and Norm layers can be created using existing PyTorch layers.</p> <p>Onwards!</p> In\u00a0[37]: Copied! <pre># 1. Create a class that inherits from nn.Module\nclass MultiheadSelfAttentionBlock(nn.Module):\n    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n    \"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n        super().__init__()\n\n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n\n        # 4. Create the Multi-Head Attention (MSA) layer\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                    num_heads=num_heads,\n                                                    dropout=attn_dropout,\n                                                    batch_first=True) # does our batch dimension come first?\n\n    # 5. Create a forward() method to pass the data throguh the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        attn_output, _ = self.multihead_attn(query=x, # query embeddings\n                                             key=x, # key embeddings\n                                             value=x, # value embeddings\n                                             need_weights=False) # do we need the weights or just the layer outputs?\n        return attn_output\n</pre> # 1. Create a class that inherits from nn.Module class MultiheadSelfAttentionBlock(nn.Module):     \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).     \"\"\"     # 2. Initialize the class with hyperparameters from Table 1     def __init__(self,                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks         super().__init__()          # 3. Create the Norm layer (LN)         self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)          # 4. Create the Multi-Head Attention (MSA) layer         self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,                                                     num_heads=num_heads,                                                     dropout=attn_dropout,                                                     batch_first=True) # does our batch dimension come first?      # 5. Create a forward() method to pass the data throguh the layers     def forward(self, x):         x = self.layer_norm(x)         attn_output, _ = self.multihead_attn(query=x, # query embeddings                                              key=x, # key embeddings                                              value=x, # value embeddings                                              need_weights=False) # do we need the weights or just the layer outputs?         return attn_output <p>Note: Unlike Figure 1, our <code>MultiheadSelfAttentionBlock</code> doesn't include a skip or residual connection (\"$+\\mathbf{z}_{\\ell-1}$\" in equation 2), we'll include this when we create the entire Transformer Encoder later on in section 7.1.</p> <p>MSABlock created!</p> <p>Let's try it out by create an instance of our <code>MultiheadSelfAttentionBlock</code> and passing through the <code>patch_and_position_embedding</code> variable we created in section 4.8.</p> In\u00a0[38]: Copied! <pre># Create an instance of MSABlock\nmultihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1\n                                                             num_heads=12) # from Table 1\n\n# Pass patch and position image embedding through MSABlock\npatched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\nprint(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\nprint(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")\n</pre> # Create an instance of MSABlock multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1                                                              num_heads=12) # from Table 1  # Pass patch and position image embedding through MSABlock patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding) print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\") print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\") <pre>Input shape of MSA block: torch.Size([1, 197, 768])\nOutput shape MSA block: torch.Size([1, 197, 768])\n</pre> <p>Notice how the input and output shape of our data stays the same when it goes through the MSA block.</p> <p>This doesn't mean the data doesn't change as it goes through.</p> <p>You could try printing the input and output tensor to see how it changes (though this change will be across <code>1 * 197 * 768</code> values and could be hard to visualize).</p> <p></p> <p>***Left:** Vision Transformer architecture from Figure 1 with Multi-Head Attention and LayerNorm layers highlighted, these layers make up equation 2 from section 3.1 of the paper. Right: Replicating equation 2 (without the skip connection on the end) using PyTorch layers.*</p> <p>We've now officially replicated equation 2 (except for the residual connection on the end but we'll get to this in section 7.1)!</p> <p>Onto the next!</p> In\u00a0[39]: Copied! <pre># 1. Create a class that inherits from nn.Module\nclass MLPBlock(nn.Module):\n    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n        super().__init__()\n\n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n\n        # 4. Create the Multilayer perceptron (MLP) layer(s)\n        self.mlp = nn.Sequential(\n            nn.Linear(in_features=embedding_dim,\n                      out_features=mlp_size),\n            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n                      out_features=embedding_dim), # take back to embedding_dim\n            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n        )\n\n    # 5. Create a forward() method to pass the data throguh the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        return x\n</pre> # 1. Create a class that inherits from nn.Module class MLPBlock(nn.Module):     \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  dropout:float=0.1): # Dropout from Table 3 for ViT-Base         super().__init__()          # 3. Create the Norm layer (LN)         self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)          # 4. Create the Multilayer perceptron (MLP) layer(s)         self.mlp = nn.Sequential(             nn.Linear(in_features=embedding_dim,                       out_features=mlp_size),             nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"             nn.Dropout(p=dropout),             nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above                       out_features=embedding_dim), # take back to embedding_dim             nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"         )      # 5. Create a forward() method to pass the data throguh the layers     def forward(self, x):         x = self.layer_norm(x)         x = self.mlp(x)         return x <p>Note: Unlike Figure 1, our <code>MLPBlock()</code> doesn't include a skip or residual connection (\"$+\\mathbf{z}_{\\ell}^{\\prime}$\" in equation 3), we'll include this when we create the entire Transformer encoder later on.</p> <p>MLPBlock class created!</p> <p>Let's try it out by create an instance of our <code>MLPBlock</code> and passing through the <code>patched_image_through_msa_block</code> variable we created in section 5.3.</p> In\u00a0[40]: Copied! <pre># Create an instance of MLPBlock\nmlp_block = MLPBlock(embedding_dim=768, # from Table 1\n                     mlp_size=3072, # from Table 1\n                     dropout=0.1) # from Table 3\n\n# Pass output of MSABlock through MLPBlock\npatched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\nprint(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\nprint(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")\n</pre> # Create an instance of MLPBlock mlp_block = MLPBlock(embedding_dim=768, # from Table 1                      mlp_size=3072, # from Table 1                      dropout=0.1) # from Table 3  # Pass output of MSABlock through MLPBlock patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block) print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\") print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\") <pre>Input shape of MLP block: torch.Size([1, 197, 768])\nOutput shape MLP block: torch.Size([1, 197, 768])\n</pre> <p>Notice how the input and output shape of our data again stays the same when it goes in and out of the MLP block.</p> <p>However, the shape does change when the data gets passed through the <code>nn.Linear()</code> layers within the MLP block (expanded to MLP size from Table 1 and then compressed back to Hidden size $D$ from Table 1).</p> <p></p> <p>Left: Vision Transformer architecture from Figure 1 with MLP and Norm layers highlighted, these layers make up equation 3 from section 3.1 of the paper. Right: Replicating equation 3 (without the skip connection on the end) using PyTorch layers.</p> <p>Ho ho!</p> <p>Equation 3 replicated (except for the residual connection on the end but we'll get to this in section 7.1)!</p> <p>Now we've got equation's 2 and 3 in PyTorch code, let's now put them together to create the Transformer Encoder.</p> In\u00a0[41]: Copied! <pre># 1. Create a class that inherits from nn.Module\nclass TransformerEncoderBlock(nn.Module):\n    \"\"\"Creates a Transformer Encoder block.\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n                 attn_dropout:float=0): # Amount of dropout for attention layers\n        super().__init__()\n\n        # 3. Create MSA block (equation 2)\n        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n                                                     num_heads=num_heads,\n                                                     attn_dropout=attn_dropout)\n\n        # 4. Create MLP block (equation 3)\n        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n                                   mlp_size=mlp_size,\n                                   dropout=mlp_dropout)\n\n    # 5. Create a forward() method\n    def forward(self, x):\n\n        # 6. Create residual connection for MSA block (add the input to the output)\n        x =  self.msa_block(x) + x\n\n        # 7. Create residual connection for MLP block (add the input to the output)\n        x = self.mlp_block(x) + x\n\n        return x\n</pre> # 1. Create a class that inherits from nn.Module class TransformerEncoderBlock(nn.Module):     \"\"\"Creates a Transformer Encoder block.\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base                  attn_dropout:float=0): # Amount of dropout for attention layers         super().__init__()          # 3. Create MSA block (equation 2)         self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,                                                      num_heads=num_heads,                                                      attn_dropout=attn_dropout)          # 4. Create MLP block (equation 3)         self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,                                    mlp_size=mlp_size,                                    dropout=mlp_dropout)      # 5. Create a forward() method     def forward(self, x):          # 6. Create residual connection for MSA block (add the input to the output)         x =  self.msa_block(x) + x          # 7. Create residual connection for MLP block (add the input to the output)         x = self.mlp_block(x) + x          return x <p>Beautiful!</p> <p>Transformer Encoder block created!</p> <p></p> <p>***Left:** Figure 1 from the ViT paper with the Transformer Encoder of the ViT architecture highlighted. Right: Transformer Encoder mapped to equation 2 and 3 of the ViT paper, the Transformer Encoder is comprised of alternating blocks of equation 2 (Multi-Head Attention) and equation 3 (Multilayer perceptron).*</p> <p>See how we're starting to piece together the overall architecture like legos, coding one brick (or equation) at a time.</p> <p></p> <p>Mapping the ViT Transformer Encoder to code.</p> <p>You might've noticed that Table 1 from the ViT paper has a Layers column. This refers to the number of Transformer Encoder blocks in the specific ViT architecure.</p> <p>In our case, for ViT-Base, we'll be stacking together 12 of these Transformer Encoder blocks to form the backbone of our architecture (we'll get to this in section 8).</p> <p>Let's get a <code>torchinfo.summary()</code> of passing an input of shape <code>(1, 197, 768) -&gt; (batch_size, num_patches, embedding_dimension)</code> to our Transformer Encoder block.</p> In\u00a0[42]: Copied! <pre># Create an instance of TransformerEncoderBlock\ntransformer_encoder_block = TransformerEncoderBlock()\n\n# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n# summary(model=transformer_encoder_block,\n#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> # Create an instance of TransformerEncoderBlock transformer_encoder_block = TransformerEncoderBlock()  # # Print an input and output summary of our Transformer Encoder (uncomment for full output) # summary(model=transformer_encoder_block, #         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension) #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p>Woah! Check out all those parameters!</p> <p>You can see our input changing shape as it moves through all of the various layers in the MSA block and MLP block of the Transformer Encoder block before finally returning to its original shape at the very end.</p> <p>Note: Just because our input to the Transformer Encoder block has the same shape at the output of the block doesn't mean the values weren't manipulated, the whole goal of the Transformer Encoder block (and stacking them together) is to learn a deep representation of the input using the various layers in between.</p> In\u00a0[43]: Copied! <pre># Create the same as above with torch.nn.TransformerEncoderLayer()\ntorch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base\n                                                             nhead=12, # Heads from Table 1 for ViT-Base\n                                                             dim_feedforward=3072, # MLP size from Table 1 for ViT-Base\n                                                             dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n                                                             activation=\"gelu\", # GELU non-linear activation\n                                                             batch_first=True, # Do our batches come first?\n                                                             norm_first=True) # Normalize first or after MSA/MLP layers?\n\ntorch_transformer_encoder_layer\n</pre> # Create the same as above with torch.nn.TransformerEncoderLayer() torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base                                                              nhead=12, # Heads from Table 1 for ViT-Base                                                              dim_feedforward=3072, # MLP size from Table 1 for ViT-Base                                                              dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base                                                              activation=\"gelu\", # GELU non-linear activation                                                              batch_first=True, # Do our batches come first?                                                              norm_first=True) # Normalize first or after MSA/MLP layers?  torch_transformer_encoder_layer Out[43]: <pre>TransformerEncoderLayer(\n  (self_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (linear1): Linear(in_features=768, out_features=3072, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear2): Linear(in_features=3072, out_features=768, bias=True)\n  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (dropout1): Dropout(p=0.1, inplace=False)\n  (dropout2): Dropout(p=0.1, inplace=False)\n)</pre> <p>To inspect it further, let's get a summary with <code>torchinfo.summary()</code>.</p> In\u00a0[44]: Copied! <pre># # Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output)\n# summary(model=torch_transformer_encoder_layer,\n#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> # # Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output) # summary(model=torch_transformer_encoder_layer, #         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension) #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p>The output of the summary is slightly different to ours due to how <code>torch.nn.TransformerEncoderLayer()</code> constructs its layer.</p> <p>But the layers it uses, number of parameters and input and output shapes are the same.</p> <p>You might be thinking, \"if we could create the Transformer Encoder so quickly with PyTorch layers, why did we bother reproducing equation 2 and 3?\"</p> <p>The answer is: practice.</p> <p>Now we've replicated a series of equations and layers from a paper, if you need to change the layers and try something different you can.</p> <p>But there are benefits of using the PyTorch pre-built layers, such as:</p> <ul> <li>Less prone to errors - Generally, if a layer makes it into the PyTorch standard library, its been tested and tried to work.</li> <li>Potentially better performance - As of July 2022 and PyTorch 1.12, the PyTorch implemented version of <code>torch.nn.TransformerEncoderLayer()</code> can see a speedup of more than 2x on many common workloads.</li> </ul> <p>Finally, since the ViT architecture uses several Transformer Layers stacked on top of each for the full architecture (Table 1 shows 12 Layers in the case of ViT-Base), you can do this with <code>torch.nn.TransformerEncoder(encoder_layer, num_layers)</code> where:</p> <ul> <li><code>encoder_layer</code> - The target Transformer Encoder layer created with <code>torch.nn.TransformerEncoderLayer()</code>.</li> <li><code>num_layers</code> - The number of Transformer Encoder layers to stack together.</li> </ul> In\u00a0[45]: Copied! <pre># 1. Create a ViT class that inherits from nn.Module\nclass ViT(nn.Module):\n    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n                 in_channels:int=3, # Number of channels in input image\n                 patch_size:int=16, # Patch size\n                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 attn_dropout:float=0, # Dropout for attention projection\n                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n                 num_classes:int=1000): # Default for ImageNet but can customize this\n        super().__init__() # don't forget the super().__init__()!\n\n        # 3. Make the image size is divisble by the patch size\n        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n\n        # 4. Calculate number of patches (height * width/patch^2)\n        self.num_patches = (img_size * img_size) // patch_size**2\n\n        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n                                            requires_grad=True)\n\n        # 6. Create learnable position embedding\n        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n                                               requires_grad=True)\n\n        # 7. Create embedding dropout value\n        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n\n        # 8. Create patch embedding layer\n        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n                                              patch_size=patch_size,\n                                              embedding_dim=embedding_dim)\n\n        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n        # Note: The \"*\" means \"all\"\n        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n                                                                            num_heads=num_heads,\n                                                                            mlp_size=mlp_size,\n                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n\n        # 10. Create classifier head\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim,\n                      out_features=num_classes)\n        )\n\n    # 11. Create a forward() method\n    def forward(self, x):\n\n        # 12. Get batch size\n        batch_size = x.shape[0]\n\n        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n\n        # 14. Create patch embedding (equation 1)\n        x = self.patch_embedding(x)\n\n        # 15. Concat class embedding and patch embedding (equation 1)\n        x = torch.cat((class_token, x), dim=1)\n\n        # 16. Add position embedding to patch embedding (equation 1)\n        x = self.position_embedding + x\n\n        # 17. Run embedding dropout (Appendix B.1)\n        x = self.embedding_dropout(x)\n\n        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)\n        x = self.transformer_encoder(x)\n\n        # 19. Put 0 index logit through classifier (equation 4)\n        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n\n        return x\n</pre> # 1. Create a ViT class that inherits from nn.Module class ViT(nn.Module):     \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  img_size:int=224, # Training resolution from Table 3 in ViT paper                  in_channels:int=3, # Number of channels in input image                  patch_size:int=16, # Patch size                  num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  attn_dropout:float=0, # Dropout for attention projection                  mlp_dropout:float=0.1, # Dropout for dense/MLP layers                  embedding_dropout:float=0.1, # Dropout for patch and position embeddings                  num_classes:int=1000): # Default for ImageNet but can customize this         super().__init__() # don't forget the super().__init__()!          # 3. Make the image size is divisble by the patch size         assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"          # 4. Calculate number of patches (height * width/patch^2)         self.num_patches = (img_size * img_size) // patch_size**2          # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)         self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),                                             requires_grad=True)          # 6. Create learnable position embedding         self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),                                                requires_grad=True)          # 7. Create embedding dropout value         self.embedding_dropout = nn.Dropout(p=embedding_dropout)          # 8. Create patch embedding layer         self.patch_embedding = PatchEmbedding(in_channels=in_channels,                                               patch_size=patch_size,                                               embedding_dim=embedding_dim)          # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())         # Note: The \"*\" means \"all\"         self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,                                                                             num_heads=num_heads,                                                                             mlp_size=mlp_size,                                                                             mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])          # 10. Create classifier head         self.classifier = nn.Sequential(             nn.LayerNorm(normalized_shape=embedding_dim),             nn.Linear(in_features=embedding_dim,                       out_features=num_classes)         )      # 11. Create a forward() method     def forward(self, x):          # 12. Get batch size         batch_size = x.shape[0]          # 13. Create class token embedding and expand it to match the batch size (equation 1)         class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)          # 14. Create patch embedding (equation 1)         x = self.patch_embedding(x)          # 15. Concat class embedding and patch embedding (equation 1)         x = torch.cat((class_token, x), dim=1)          # 16. Add position embedding to patch embedding (equation 1)         x = self.position_embedding + x          # 17. Run embedding dropout (Appendix B.1)         x = self.embedding_dropout(x)          # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)         x = self.transformer_encoder(x)          # 19. Put 0 index logit through classifier (equation 4)         x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index          return x <ol> <li>\ud83d\udd7a\ud83d\udc83\ud83e\udd73 Woohoo!!! We just built a vision transformer!</li> </ol> <p>What an effort!</p> <p>Slowly but surely we created layers and blocks, inputs and outputs and put them all together to build our own ViT!</p> <p>Let's create a quick demo to showcase what's happening with the class token embedding being expanded over the batch dimensions.</p> In\u00a0[46]: Copied! <pre># Example of creating the class embedding and expanding over a batch dimension\nbatch_size = 32\nclass_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\nclass_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n\n# Print out the change in shapes\nprint(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")\nprint(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")\n</pre> # Example of creating the class embedding and expanding over a batch dimension batch_size = 32 class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"  # Print out the change in shapes print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\") print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\") <pre>Shape of class token embedding single: torch.Size([1, 1, 768])\nShape of class token embedding expanded: torch.Size([32, 1, 768])\n</pre> <p>Notice how the first dimension gets expanded to the batch size and the other dimensions stay the same (because they're inferred by the \"<code>-1</code>\" dimensions in <code>.expand(batch_size, -1, -1)</code>).</p> <p>Alright time to test out <code>ViT()</code> class.</p> <p>Let's create a random tensor in the same shape as a single image, pass to an instance of <code>ViT</code> and see what happens.</p> In\u00a0[47]: Copied! <pre>set_seeds()\n\n# Create a random tensor with same shape as a single image\nrandom_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n\n# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\nvit = ViT(num_classes=len(class_names))\n\n# Pass the random image tensor to our ViT instance\nvit(random_image_tensor)\n</pre> set_seeds()  # Create a random tensor with same shape as a single image random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)  # Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi) vit = ViT(num_classes=len(class_names))  # Pass the random image tensor to our ViT instance vit(random_image_tensor) Out[47]: <pre>tensor([[-0.2377,  0.7360,  1.2137]], grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>Outstanding!</p> <p>It looks like our random image tensor made it all the way through our ViT architecture and it's outputting three logit values (one for each class).</p> <p>And because our <code>ViT</code> class has plenty of parameters we could customize the <code>img_size</code>, <code>patch_size</code> or <code>num_classes</code> if we wanted to.</p> In\u00a0[48]: Copied! <pre>from torchinfo import summary\n\n# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n# summary(model=vit,\n#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> from torchinfo import summary  # # Print a summary of our custom ViT model using torchinfo (uncomment for actual output) # summary(model=vit, #         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width) #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # ) <p>Now those are some nice looking layers!</p> <p>Checkout the total number of parameters too, 85,800,963, our biggest model yet!</p> <p>The number is very close to PyTorch's pretrained ViT-Base with patch size 16 at <code>torch.vision.models.vit_b_16()</code> with 86,567,656 total parameters (though this number of parameters is for the 1000 classes in ImageNet).</p> <p>Exercise: Try changing the <code>num_classes</code> parameter of our <code>ViT()</code> model to 1000 and then creating another summary with <code>torchinfo.summary()</code> and see if the number of parameters lines up between our code and <code>torchvision.models.vit_b_16()</code>.</p> In\u00a0[49]: Copied! <pre>from going_modular.going_modular import engine\n\n# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper\noptimizer = torch.optim.Adam(params=vit.parameters(),\n                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)\n                             weight_decay=0.3) # from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n\n# Setup the loss function for multi-class classification\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Set the seeds\nset_seeds()\n\n# Train the model and save the training results to a dictionary\nresults = engine.train(model=vit,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=10,\n                       device=device)\n</pre> from going_modular.going_modular import engine  # Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper optimizer = torch.optim.Adam(params=vit.parameters(),                              lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k                              betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)                              weight_decay=0.3) # from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k  # Setup the loss function for multi-class classification loss_fn = torch.nn.CrossEntropyLoss()  # Set the seeds set_seeds()  # Train the model and save the training results to a dictionary results = engine.train(model=vit,                        train_dataloader=train_dataloader,                        test_dataloader=test_dataloader,                        optimizer=optimizer,                        loss_fn=loss_fn,                        epochs=10,                        device=device) <pre> 10%|\u2588         | 1/10 [00:03&lt;00:32,  3.56s/it]</pre> <pre>Epoch: 1 | train_loss: 5.1782 | train_acc: 0.3633 | test_loss: 4.9653 | test_acc: 0.2604\n</pre> <pre> 20%|\u2588\u2588        | 2/10 [00:06&lt;00:26,  3.31s/it]</pre> <pre>Epoch: 2 | train_loss: 2.1727 | train_acc: 0.3008 | test_loss: 1.4031 | test_acc: 0.2604\n</pre> <pre> 30%|\u2588\u2588\u2588       | 3/10 [00:09&lt;00:22,  3.24s/it]</pre> <pre>Epoch: 3 | train_loss: 1.1747 | train_acc: 0.4414 | test_loss: 1.8320 | test_acc: 0.2604\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:13&lt;00:19,  3.21s/it]</pre> <pre>Epoch: 4 | train_loss: 1.2742 | train_acc: 0.2773 | test_loss: 1.3042 | test_acc: 0.1979\n</pre> <pre> 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:16&lt;00:15,  3.20s/it]</pre> <pre>Epoch: 5 | train_loss: 1.1426 | train_acc: 0.3047 | test_loss: 1.1761 | test_acc: 0.2604\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:19&lt;00:12,  3.18s/it]</pre> <pre>Epoch: 6 | train_loss: 1.1823 | train_acc: 0.3086 | test_loss: 1.2052 | test_acc: 0.1979\n</pre> <pre> 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:22&lt;00:09,  3.09s/it]</pre> <pre>Epoch: 7 | train_loss: 1.2615 | train_acc: 0.2852 | test_loss: 1.1434 | test_acc: 0.1979\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:25&lt;00:05,  2.99s/it]</pre> <pre>Epoch: 8 | train_loss: 1.1012 | train_acc: 0.4062 | test_loss: 1.1849 | test_acc: 0.2604\n</pre> <pre> 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:27&lt;00:02,  2.94s/it]</pre> <pre>Epoch: 9 | train_loss: 1.1487 | train_acc: 0.3047 | test_loss: 1.0172 | test_acc: 0.5417\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:31&lt;00:00,  3.11s/it]</pre> <pre>Epoch: 10 | train_loss: 1.2539 | train_acc: 0.2617 | test_loss: 1.3794 | test_acc: 0.2604\n</pre> <pre>\n</pre> <p>Wonderful!</p> <p>Our ViT model has come to life!</p> <p>Though the results on our pizza, steak and sushi dataset don't look too good.</p> <p>Perhaps it's because we're missing a few things?</p> In\u00a0[50]: Copied! <pre>from helper_functions import plot_loss_curves\n\n# Plot our ViT model's loss curves\nplot_loss_curves(results)\n</pre> from helper_functions import plot_loss_curves  # Plot our ViT model's loss curves plot_loss_curves(results) <p>Hmm, it looks like our model's loss curves are all over the place.</p> <p>At least the loss looks like it's heading the right direction but the accuracy curves don't really show much promise.</p> <p>These results are likely because of the difference in data resources and training regime of our ViT model versus the ViT paper.</p> <p>It seems our model is severly underfitting (not achieving the results we'd like it to).</p> <p>How about we see if we can fix that by bringing in a pretrained ViT model?</p> In\u00a0[51]: Copied! <pre># The following requires torch v0.12+ and torchvision v0.13+\nimport torch\nimport torchvision\nprint(torch.__version__)\nprint(torchvision.__version__)\n</pre> # The following requires torch v0.12+ and torchvision v0.13+ import torch import torchvision print(torch.__version__) print(torchvision.__version__) <pre>2.2.2\n0.17.2\n</pre> <p>Then we'll setup device-agonistc code.</p> In\u00a0[52]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[52]: <pre>'cuda'</pre> <p>Finally, we'll get the pretrained ViT-Base with patch size 16 from <code>torchvision.models</code> and prepare it for our FoodVision Mini use case by turning it into a feature extractor transfer learning model.</p> <p>Specifically, we'll:</p> <ol> <li>Get the pretrained weights for ViT-Base trained on ImageNet-1k from <code>torchvision.models.ViT_B_16_Weights.DEFAULT</code> (<code>DEFAULT</code> stands for best available).</li> <li>Setup a ViT model instance via <code>torchvision.models.vit_b_16</code>, pass it the pretrained weights step 1 and send it to the target device.</li> <li>Freeze all of the parameters in the base ViT model created in step 2 by setting their <code>requires_grad</code> attribute to <code>False</code>.</li> <li>Update the classifier head of the ViT model created in step 2 to suit our own problem by changing the number of <code>out_features</code> to our number of classes (pizza, steak, sushi).</li> </ol> <p>We covered steps like this in 06. PyTorch Transfer Learning section 3.2: Setting up a pretrained model and section 3.4: Freezing the base model and changing the output layer to suit our needs.</p> In\u00a0[53]: Copied! <pre># 1. Get pretrained weights for ViT-Base\npretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, \"DEFAULT\" means best available\n\n# 2. Setup a ViT model instance with pretrained weights\npretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n\n# 3. Freeze the base parameters\nfor parameter in pretrained_vit.parameters():\n    parameter.requires_grad = False\n\n# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\nset_seeds()\npretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n# pretrained_vit # uncomment for model output\n</pre> # 1. Get pretrained weights for ViT-Base pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, \"DEFAULT\" means best available  # 2. Setup a ViT model instance with pretrained weights pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)  # 3. Freeze the base parameters for parameter in pretrained_vit.parameters():     parameter.requires_grad = False  # 4. Change the classifier head (set the seeds to ensure same initialization with linear head) set_seeds() pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device) # pretrained_vit # uncomment for model output <pre>Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /home/jupyter-trunglph/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 330M/330M [00:30&lt;00:00, 11.4MB/s] \n</pre> <p>Pretrained ViT feature extractor model created!</p> <p>Let's now check it out by printing a <code>torchinfo.summary()</code>.</p> In\u00a0[54]: Copied! <pre># # Print a summary using torchinfo (uncomment for actual output)\n# summary(model=pretrained_vit,\n#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> # # Print a summary using torchinfo (uncomment for actual output) # summary(model=pretrained_vit, #         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width) #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # ) <p></p> <p>Woohoo!</p> <p>Notice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen).</p> <p>And the total number of parameters, 85,800,963, is the same as our custom made ViT model above.</p> <p>But the number of trainable parameters for <code>pretrained_vit</code> is much, much lower than our custom <code>vit</code> at only 2,307 compared to 85,800,963 (in our custom <code>vit</code>, since we're training from scratch, all parameters are trainable).</p> <p>This means the pretrained model should train a lot faster, we could potentially even use a larger batch size since less parameter updates are going to be taking up memory.</p> In\u00a0[55]: Copied! <pre>from helper_functions import download_data\n\n# Download pizza, steak, sushi images from GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> from helper_functions import download_data  # Download pizza, steak, sushi images from GitHub image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[55]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>And now we'll setup the training and test directory paths.</p> In\u00a0[56]: Copied! <pre># Setup train and test directory paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\ntrain_dir, test_dir\n</pre> # Setup train and test directory paths train_dir = image_path / \"train\" test_dir = image_path / \"test\" train_dir, test_dir Out[56]: <pre>(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))</pre> <p>Finally, we'll transform our images into tensors and turn the tensors into DataLoaders.</p> <p>Since we're using a pretrained model form <code>torchvision.models</code> we can call the <code>transforms()</code> method on it to get its required transforms.</p> <p>Remember, if you're going to use a pretrained model, it's generally important to ensure your own custom data is transformed/formatted in the same way the data the original model was trained on.</p> <p>We covered this method of \"automatic\" transform creation in 06. PyTorch Transfer Learning section 2.2.</p> In\u00a0[57]: Copied! <pre># Get automatic transforms from pretrained ViT weights\npretrained_vit_transforms = pretrained_vit_weights.transforms()\nprint(pretrained_vit_transforms)\n</pre> # Get automatic transforms from pretrained ViT weights pretrained_vit_transforms = pretrained_vit_weights.transforms() print(pretrained_vit_transforms) <pre>ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n</pre> <p>And now we've got transforms ready, we can turn our images into DataLoaders using the <code>data_setup.create_dataloaders()</code> method we created in 05. PyTorch Going Modular section 2.</p> <p>Since we're using a feature extractor model (less trainable parameters), we could increase the batch size to a higher value (if we set it to 1024, we'd be mimicing an improvement found in Better plain ViT baselines for ImageNet-1k, a paper which improves upon the original ViT paper and suggested extra reading). But since we only have ~200 training samples total, we'll stick with 32.</p> In\u00a0[58]: Copied! <pre># Setup dataloaders\ntrain_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                     test_dir=test_dir,\n                                                                                                     transform=pretrained_vit_transforms,\n                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n</pre> # Setup dataloaders train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                                      test_dir=test_dir,                                                                                                      transform=pretrained_vit_transforms,                                                                                                      batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)  In\u00a0[59]: Copied! <pre>from going_modular.going_modular import engine\n\n# Create optimizer and loss function\noptimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n                             lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Train the classifier head of the pretrained ViT feature extractor model\nset_seeds()\npretrained_vit_results = engine.train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_pretrained,\n                                      test_dataloader=test_dataloader_pretrained,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=10,\n                                      device=device)\n</pre> from going_modular.going_modular import engine  # Create optimizer and loss function optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),                              lr=1e-3) loss_fn = torch.nn.CrossEntropyLoss()  # Train the classifier head of the pretrained ViT feature extractor model set_seeds() pretrained_vit_results = engine.train(model=pretrained_vit,                                       train_dataloader=train_dataloader_pretrained,                                       test_dataloader=test_dataloader_pretrained,                                       optimizer=optimizer,                                       loss_fn=loss_fn,                                       epochs=10,                                       device=device) <pre> 10%|\u2588         | 1/10 [00:02&lt;00:23,  2.56s/it]</pre> <pre>Epoch: 1 | train_loss: 0.7663 | train_acc: 0.7188 | test_loss: 0.5435 | test_acc: 0.8769\n</pre> <pre> 20%|\u2588\u2588        | 2/10 [00:05&lt;00:20,  2.56s/it]</pre> <pre>Epoch: 2 | train_loss: 0.3436 | train_acc: 0.9453 | test_loss: 0.3257 | test_acc: 0.8977\n</pre> <pre> 30%|\u2588\u2588\u2588       | 3/10 [00:07&lt;00:17,  2.56s/it]</pre> <pre>Epoch: 3 | train_loss: 0.2068 | train_acc: 0.9492 | test_loss: 0.2698 | test_acc: 0.9186\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:10&lt;00:15,  2.56s/it]</pre> <pre>Epoch: 4 | train_loss: 0.1557 | train_acc: 0.9609 | test_loss: 0.2414 | test_acc: 0.9186\n</pre> <pre> 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:12&lt;00:12,  2.56s/it]</pre> <pre>Epoch: 5 | train_loss: 0.1244 | train_acc: 0.9727 | test_loss: 0.2271 | test_acc: 0.8977\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:15&lt;00:10,  2.57s/it]</pre> <pre>Epoch: 6 | train_loss: 0.1210 | train_acc: 0.9766 | test_loss: 0.2122 | test_acc: 0.9280\n</pre> <pre> 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:17&lt;00:07,  2.56s/it]</pre> <pre>Epoch: 7 | train_loss: 0.0933 | train_acc: 0.9766 | test_loss: 0.2342 | test_acc: 0.8883\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:20&lt;00:05,  2.55s/it]</pre> <pre>Epoch: 8 | train_loss: 0.0793 | train_acc: 0.9844 | test_loss: 0.2268 | test_acc: 0.9081\n</pre> <pre> 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:23&lt;00:02,  2.57s/it]</pre> <pre>Epoch: 9 | train_loss: 0.1084 | train_acc: 0.9883 | test_loss: 0.2064 | test_acc: 0.9384\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:25&lt;00:00,  2.57s/it]</pre> <pre>Epoch: 10 | train_loss: 0.0646 | train_acc: 0.9922 | test_loss: 0.1795 | test_acc: 0.9176\n</pre> <pre>\n</pre> <p>Holy cow!</p> <p>Looks like our pretrained ViT feature extractor performed far better than our custom ViT model trained from scratch (in the same amount of time).</p> <p>Let's get visual.</p> In\u00a0[60]: Copied! <pre># Plot the loss curves\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(pretrained_vit_results)\n</pre> # Plot the loss curves from helper_functions import plot_loss_curves  plot_loss_curves(pretrained_vit_results) <p>Woah!</p> <p>Those are some close to textbook looking (really good) loss curves (check out 04. PyTorch Custom Datasets section 8 for what an ideal loss curve should look like).</p> <p>That's the power of transfer learning!</p> <p>We managed to get outstanding results with the same model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it.</p> <p>What do you think?</p> <p>Would our feature extractor model improve more if you kept training it?</p> In\u00a0[61]: Copied! <pre># Save the model\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=pretrained_vit,\n                 target_dir=\"models\",\n                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")\n</pre> # Save the model from going_modular.going_modular import utils  utils.save_model(model=pretrained_vit,                  target_dir=\"models\",                  model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\") <pre>[INFO] Saving model to: models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\n</pre> <p>And since we're thinking about deploying this model, it'd be good to know the size of it (in megabytes or MB).</p> <p>Since we want our Food Vision Mini application to run fast, generally a smaller model with good performance will be better than a larger model with great performance.</p> <p>We can check the size of our model in bytes using the <code>st_size</code> attribute of Python's <code>pathlib.Path().stat()</code> method whilst passing it our model's filepath name.</p> <p>We can then scale the size in bytes to megabytes.</p> In\u00a0[62]: Copied! <pre>from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")\n</pre> from pathlib import Path  # Get the model size in bytes then convert to megabytes pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\") <pre>Pretrained ViT feature extractor model size: 327 MB\n</pre> <p>Hmm, looks like our ViT feature extractor model for Food Vision Mini turned out to be about 327 MB in size.</p> <p>How does this compare to the EffNetB2 feature extractor model in 07. PyTorch Experiment Tracking section 9?</p> Model Model size (MB) Test loss Test accuracy EffNetB2 feature extractor^ 29 ~0.3906 ~0.9384 ViT feature extractor 327 ~0.1084 ~0.9384 <p>Note: ^ the EffNetB2 model in reference was trained with 20% of pizza, steak and sushi data (double the amount of images) rather than the ViT feature extractor which was trained with 10% of pizza, steak and sushi data. An exercise would be to train the ViT feature extractor model on the same amount of data and see how much the results improve.</p> <p>The EffNetB2 model is ~11x smaller than the ViT model with similiar results for test loss and accuracy.</p> <p>However, the ViT model's results may improve more when trained with the same data (20% pizza, steak and sushi data).</p> <p>But in terms of deployment, if we were comparing these two models, something we'd need to consider is whether the extra accuracy from the ViT model is worth the ~11x increase in model size?</p> <p>Perhaps such a large model would take longer to load/run and wouldn't provide as good an experience as EffNetB2 which performs similarly but at a much reduced size.</p> In\u00a0[63]: Copied! <pre>import requests\n\n# Import function to make predictions on images and plot them\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Setup custom image path\ncustom_image_path = image_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=pretrained_vit,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> import requests  # Import function to make predictions on images and plot them from going_modular.going_modular.predictions import pred_and_plot_image  # Setup custom image path custom_image_path = image_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=pretrained_vit,                     image_path=custom_image_path,                     class_names=class_names) <pre>Downloading data/pizza_steak_sushi/04-pizza-dad.jpeg...\n</pre> <p>Two thumbs up!</p> <p>Congratulations!</p> <p>We've gone all the way from research paper to usable model code on our own custom images!</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#08-pytorch-paper-replicating","title":"08. PyTorch Paper Replicating\u00b6","text":"<p>Welcome to Milestone Project 2: PyTorch Paper Replicating!</p> <p>In this project, we're going to be replicating a machine learning research paper and creating a Vision Transformer (ViT) from scratch using PyTorch.</p> <p>We'll then see how ViT, a state-of-the-art computer vision architecture, performs on our FoodVision Mini problem.</p> <p>For Milestone Project 2 we're going to focus on recreating the Vision Transformer (ViT) computer vision architecture and applying it to our FoodVision Mini problem to classify different images of pizza, steak and sushi.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#what-is-paper-replicating","title":"What is paper replicating?\u00b6","text":"<p>It's no secret machine learning is advancing fast.</p> <p>Many of these advances get published in machine learning research papers.</p> <p>And the goal of paper replicating is to replicate these advances with code so you can use the techniques for your own problem.</p> <p>For example, let's say a new model architecture gets released that performs better than any other architecture before on various benchmarks, wouldn't it be nice to try that architecture on your own problems?</p> <p>Machine learning paper replicating involves turning a machine learning paper comprised of images/diagrams, math and text into usable code and in our case, usable PyTorch code. Diagram, math equations and text from the ViT paper.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#what-is-a-machine-learning-research-paper","title":"What is a machine learning research paper?\u00b6","text":"<p>A machine learning research paper is a scientific paper that details findings of a research group on a specific area.</p> <p>The contents of a machine learning research paper can vary from paper to paper but they generally follow the structure:</p> Section Contents Abstract An overview/summary of the paper's main findings/contributions. Introduction What's the paper's main problem and details of previous methods used to try and solve it. Method How did the researchers go about conducting their research? For example, what model(s), data sources, training setups were used? Results What are the outcomes of the paper? If a new type of model or training setup was used, how did the results of findings compare to previous works? (this is where experiment tracking comes in handy) Conclusion What are the limitations of the suggested methods? What are some next steps for the research community? References What resources/other papers did the researchers look at to build their own body of work? Appendix Are there any extra resources/findings to look at that weren't included in any of the above sections?"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#why-replicate-a-machine-learning-research-paper","title":"Why replicate a machine learning research paper?\u00b6","text":"<p>A machine learning research paper is often a presentation of months of work and experiments done by some of the best machine learning teams in the world condensed into a few pages of text.</p> <p>And if these experiments lead to better results in an area related to the problem you're working on, it'd be nice to check them out.</p> <p>Also, replicating the work of others is a fantastic way to practice your skills.</p> <p></p> <p>George Hotz is founder of comma.ai, a self-driving car company and livestreams machine learning coding on Twitch and those videos get posted in full to YouTube. I pulled this quote from one of his livestreams. The \"\u066d\" is to note that machine learning engineering often involves the extra step(s) of preprocessing data and making your models available for others to use (deployment).</p> <p>When you first start trying to replicate research papers, you'll likely be overwhelmed.</p> <p>That's normal.</p> <p>Research teams spend weeks, months and sometimes years creating these works so it makes sense if it takes you sometime to even read let alone reproduce the works.</p> <p>Replicating research is such a tough problem, phenomenal machine learning libraries and tools such as, HuggingFace, PyTorch Image Models (<code>timm</code> library) and fast.ai have been born out of making machine learning research more accessible.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#where-can-you-find-code-examples-for-machine-learning-research-papers","title":"Where can you find code examples for machine learning research papers?\u00b6","text":"<p>One of the first things you'll notice when it comes to machine learning research is: there's a lot of it.</p> <p>So beware, trying to stay on top of it is like trying to outrun a hamster wheel.</p> <p>Follow your interest, pick a few things that stand out to you.</p> <p>In saying this, there are several places to find and read machine learning research papers (and code):</p> Resource What is it? arXiv Pronounced \"archive\", arXiv is a free and open resource for reading technical articles on everything from physics to computer science (inlcuding machine learning). AK Twitter The AK Twitter account publishes machine learning research highlights, often with live demos almost every day. I don't understand 9/10 posts but I find it fun to explore every so often. Papers with Code A curated collection of trending, active and greatest machine learning papers, many of which include code resources attached. Also includes a collection of common machine learning datasets, benchmarks and current state-of-the-art models. lucidrains' <code>vit-pytorch</code> GitHub repository Less of a place to find research papers and more of an example of what paper replicating with code on a larger-scale and with a specific focus looks like. The <code>vit-pytorch</code> repository is a collection of Vision Transformer model architectures from various research papers replicated with PyTorch code (much of the inspiration for this notebook was gathered from this repository). <p>Note: This list is far from exhaustive. I only list a few places, the ones I use most frequently personally. So beware the bias. However, I've noticed that even this short list often sully satisfies my needs for knowing what's going on in the field. Any more and I might go crazy.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Rather than talk about replicating a paper, we're going to get hands-on and actually replicate a paper.</p> <p>The process for replicating all papers will be slightly different but by seeing what it's like to do one, we'll get the momentum to do more.</p> <p>More specifically, we're going to be replicating the machine learning research paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  (ViT paper) with PyTorch.</p> <p>The Transformer neural network architecture was originally introduced in the machine learning research paper Attention is all you need.</p> <p>And the original Transformer architecture was designed to work on one-dimensional (1D) sequences of text.</p> <p>A Transformer architecture is generally considered to be any neural network that uses the attention mechanism) as its primary learning layer. Similar to a how a convolutional neural network (CNN) uses convolutions as its primary learning layer.</p> <p>Like the name suggests, the Vision Transformer (ViT) architecture was designed to adapt the original Transformer architecture to vision problem(s) (classification being the first and since many others have followed).</p> <p>The original Vision Transformer has been through several iterations over the past couple of years, however, we're going to focus on replicating the original, otherwise known as the \"vanilla Vision Transformer\". Because if you can recreate the original, you can adapt to the others.</p> <p>We're going to be focusing on building the ViT architecture as per the original ViT paper and applying it to FoodVision Mini.</p> Topic Contents 0. Getting setup We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. 1. Get data Let's get the pizza, steak and sushi image classification dataset we've been using and build a Vision Transformer to try and improve FoodVision Mini model's results. 2. Create Datasets and DataLoaders We'll use the <code>data_setup.py</code> script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. 3. Replicating the ViT paper: an overview Replicating a machine learning research paper can be bit a fair challenge, so before we jump in, let's break the ViT paper down into smaller chunks, so we can replicate the paper chunk by chunk. 4. Equation 1: The Patch Embedding The ViT architecture is comprised of four main equations, the first being the patch and position embedding. Or turning an image into a sequence of learnable patches. 5. Equation 2: Multi-Head Attention (MSA) The self-attention/multi-head self-attention (MSA) mechanism is at the heart of every Transformer architecture, including the ViT architecture, let's create an MSA block using PyTorch's in-built layers. 6. Equation 3: Multilayer Perceptron (MLP) The ViT architecture uses a multilayer perceptron as part of its Transformer Encoder and for its output layer. Let's start by creating an MLP for the Transformer Encoder. 7. Creating the Transformer Encoder A Transformer Encoder is typically comprised of alternating layers of MSA (equation 2) and MLP (equation 3) joined together via residual connections. Let's create one by stacking the layers we created in sections 5 &amp; 6 on top of each other. 8. Putting it all together to create ViT We've got all the pieces of the puzzle to create the ViT architecture, let's put them all together into a single class we can call as our model. 9. Setting up training code for our ViT model Training our custom ViT implementation is similar to all of the other model's we've trained previously. And thanks to our <code>train()</code> function in <code>engine.py</code> we can start training with a few lines of code. 10. Using a pretrained ViT from <code>torchvision.models</code> Training a large model like ViT usually takes a fair amount of data. Since we're only working with a small amount of pizza, steak and sushi images, let's see if we can leverage the power of transfer learning to improve our performance. 11. Make predictions on a custom image The magic of machine learning is seeing it work on your own data, so let's take our best performing model and put FoodVision Mini to the test on the infamous pizza-dad image (a photo of my dad eating pizza). <p>Note: Despite the fact we're going to be focused on replicating the ViT paper, avoid getting too bogged down on a particular paper as newer better methods will often come along, quickly, so the skill should be to remain curious whilst building the fundamental skills of turning math and words on a page into working code.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#terminology","title":"Terminology\u00b6","text":"<p>There are going to be a fair few acronyms throughout this notebook.</p> <p>In light of this, here are some definitions:</p> <ul> <li>ViT - Stands for Vision Transformer (the main neural network architecture we're going to be focused on replicating).</li> <li>ViT paper - Short hand for the original machine learning research paper that introduced the ViT architecture, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, anytime ViT paper is mentioned, you can be assured it is referencing this paper.</li> </ul>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#0-getting-setup","title":"0. Getting setup\u00b6","text":"<p>As we've done previously, let's make sure we've got all of the modules we'll need for this section.</p> <p>We'll import the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in 05. PyTorch Going Modular.</p> <p>To do so, we'll download <code>going_modular</code> directory from the <code>pytorch-deep-learning</code> repository (if we don't already have it).</p> <p>We'll also get the <code>torchinfo</code> package if it's not available.</p> <p><code>torchinfo</code> will help later on to give us a visual representation of our model.</p> <p>And since later on we'll be using <code>torchvision</code> v0.13 package (available as of July 2022), we'll make sure we've got the latest versions.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#1-get-data","title":"1. Get Data\u00b6","text":"<p>Since we're continuing on with FoodVision Mini, let's download the pizza, steak and sushi image dataset we've been using.</p> <p>To do so we can use the <code>download_data()</code> function from <code>helper_functions.py</code> that we created in 07. PyTorch Experiment Tracking section 1.</p> <p>We'll <code>source</code> to the raw GitHub link of the <code>pizza_steak_sushi.zip</code> data and the <code>destination</code> to <code>pizza_steak_sushi</code>.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#2-create-datasets-and-dataloaders","title":"2. Create Datasets and DataLoaders\u00b6","text":"<p>Now we've got some data, let's now turn it into <code>DataLoader</code>'s.</p> <p>To do so we can use the <code>create_dataloaders()</code> function in <code>data_setup.py</code>.</p> <p>First, we'll create a transform to prepare our images.</p> <p>This where one of the first references to the ViT paper will come in.</p> <p>In Table 3, the training resolution is mentioned as being 224 (height=224, width=224).</p> <p></p> <p>You can often find various hyperparameter settings listed in a table. In this case we're still preparing our data, so we're mainly concerned with things like image size and batch size. Source: Table 3 in ViT paper.</p> <p>So we'll make sure our transform resizes our images appropriately.</p> <p>And since we'll be training our model from scratch (no transfer learning to begin with), we won't provide a <code>normalize</code> transform like we did in 06. PyTorch Transfer Learning section 2.1.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#21-prepare-transforms-for-images","title":"2.1 Prepare transforms for images\u00b6","text":""},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#22-turn-images-into-dataloaders","title":"2.2 Turn images into <code>DataLoader</code>'s\u00b6","text":"<p>Transforms created!</p> <p>Let's now create our <code>DataLoader</code>'s.</p> <p>The ViT paper states the use of a batch size of 4096 which is 128x the size of the batch size we've been using (32).</p> <p>However, we're going to stick with a batch size of 32.</p> <p>Why?</p> <p>Because some hardware (including the free tier of Google Colab) may not be able to handle a batch size of 4096.</p> <p>Having a batch size of 4096 means that 4096 images need to fit into the GPU memory at a time.</p> <p>This works when you've got the hardware to handle it like a research team from Google often does but when you're running on a single GPU (such as using Google Colab), making sure things work with smaller batch size first is a good idea.</p> <p>An extension of this project could be to try a higher batch size value and see what happens.</p> <p>Note: We're using the <code>pin_memory=True</code> parameter in the <code>create_dataloaders()</code> function to speed up computation. <code>pin_memory=True</code> avoids unnecessary copying of memory between the CPU and GPU memory by \"pinning\" examples that have been seen before. Though the benefits of this will likely be seen with larger dataset sizes (our FoodVision Mini dataset is quite small). However, setting <code>pin_memory=True</code> doesn't always improve performance (this is another one of those we're scenarios in machine learning where some things work sometimes and don't other times), so best to experiment, experiment, experiment. See the PyTorch <code>torch.utils.data.DataLoader</code> documentation or Making Deep Learning Go Brrrr from First Principles by Horace He for more.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#23-visualize-a-single-image","title":"2.3 Visualize a single image\u00b6","text":"<p>Now we've loaded our data, let's visualize, visualize, visualize!</p> <p>An important step in the ViT paper is preparing the images into patches.</p> <p>We'll get to what this means in section 4 but for now, let's view a single image and its label.</p> <p>To do so, let's get a single image and label from a batch of data and inspect their shapes.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview","title":"3. Replicating the ViT paper: an overview\u00b6","text":"<p>Before we write any more code, let's discuss what we're doing.</p> <p>We'd like to replicate the ViT paper for our own problem, FoodVision Mini.</p> <p>So our model inputs are: images of pizza, steak and sushi.</p> <p>And our ideal model outputs are: predicted labels of pizza, steak or sushi.</p> <p>No different to what we've been doing throughout the previous sections.</p> <p>The question is: how do we go from our inputs to the desired outputs?</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#31-inputs-and-outputs-layers-and-blocks","title":"3.1 Inputs and outputs, layers and blocks\u00b6","text":"<p>ViT is a deep learning neural network architecture.</p> <p>And any neural network architecture is generally comprised of layers.</p> <p>And a collection of layers is often referred to as a block.</p> <p>And stacking many blocks together is what gives us the whole architecture.</p> <p>A layer takes an input (say an image tensor), performs some kind of function on it (for example what's in the layer's <code>forward()</code> method) and then returns an output.</p> <p>So if a single layer takes an input and gives an output, then a collection of layers or a block also takes an input and gives an output.</p> <p>Let's make this concrete:</p> <ul> <li>Layer - takes an input, performs a function on it, returns an output.</li> <li>Block - a collection of layers, takes an input, performs a series of functions on it, returns an output.</li> <li>Architecture (or model) - a collection of blocks, takes an input, performs a series of functions on it, returns an output.</li> </ul> <p>This ideology is what we're going to be using to replicate the ViT paper.</p> <p>We're going to take it layer by layer, block by block, function by function putting the pieces of the puzzle together like Lego to get our desired overall architecture.</p> <p>The reason we do this is because looking at a whole research paper can be intimidating.</p> <p>So for a better understanding, we'll break it down, starting with the inputs and outputs of single layer and working up to the inputs and outputs of the whole model.</p> <p>A modern deep learning architecture is usually collection of layers and blocks. Where layers take an input (data as a numerical representation) and manipulate it using some kind of function (for example, the self-attention formula pictured above, however, this function could be almost anything) and then output it. Blocks are generally stacks of layers on top of each other doing a similar thing to a single layer but multiple times.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#32-getting-specific-whats-vit-made-of","title":"3.2 Getting specific: What's ViT made of?\u00b6","text":"<p>There are many little details about the ViT model sprinkled throughout the paper.</p> <p>Finding them all is like one big treasure hunt!</p> <p>Remember, a research paper is often months of work compressed into a few pages so it's understandable for it to take of practice to replicate.</p> <p>However, the main three resources we'll be looking at for the architecture design are:</p> <ol> <li>Figure 1 - This gives an overview of the model in a graphical sense, you could almost recreate the architecture with this figure alone.</li> <li>Four equations in section 3.1 - These equations give a little bit more of a mathematical grounding to the coloured blocks in Figure 1.</li> <li>Table 1 - This table shows the various hyperparameter settings (such as number of layers and number of hidden units) for different ViT model variants. We'll be focused on the smallest version, ViT-Base.</li> </ol>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#321-exploring-figure-1","title":"3.2.1 Exploring Figure 1\u00b6","text":"<p>Let's start by going through Figure 1 of the ViT Paper.</p> <p>The main things we'll be paying attention to are:</p> <ol> <li>Layers - takes an input, performs an operation or function on the input, produces an output.</li> <li>Blocks - a collection of layers, which in turn also takes an input and produces an output.</li> </ol> <p></p> <p>Figure 1 from the ViT Paper showcasing the different inputs, outputs, layers and blocks that create the architecture. Our goal will be to replicate each of these using PyTorch code.</p> <p>The ViT architecture is comprised of several stages:</p> <ul> <li>Patch + Position Embedding (inputs) - Turns the input image into a sequence of image patches and adds a position number to specify in what order the patch comes in.</li> <li>Linear projection of flattened patches (Embedded Patches) - The image patches get turned into an embedding, the benefit of using an embedding rather than just the image values is that an embedding is a learnable representation (typically in the form of a vector) of the image that can improve with training.</li> <li>Norm - This is short for \"Layer Normalization\" or \"LayerNorm\", a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the PyTorch layer <code>torch.nn.LayerNorm()</code>.</li> <li>Multi-Head Attention - This is a Multi-Headed Self-Attention layer or \"MSA\" for short. You can create an MSA layer via the PyTorch layer <code>torch.nn.MultiheadAttention()</code>.</li> <li>MLP (or Multilayer perceptron) - A MLP can often refer to any collection of feedforward layers (or in PyTorch's case, a collection of layers with a <code>forward()</code> method). In the ViT Paper, the authors refer to the MLP as \"MLP block\" and it contains two <code>torch.nn.Linear()</code> layers with a <code>torch.nn.GELU()</code> non-linearity activation in between them (section 3.1) and a <code>torch.nn.Dropout()</code> layer after each (Appendex B.1).</li> <li>Transformer Encoder - The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the \"+\" symbols) meaning the layer's inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother.</li> <li>MLP Head - This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we're working on image classification, you could also call this the \"classifier head\". The structure of the MLP Head is similar to the MLP block.</li> </ul> <p>You might notice that many of the pieces of the ViT architecture can be created with existing PyTorch layers.</p> <p>This is because of how PyTorch is designed, it's one of the main purposes of PyTorch to create reusable neural network layers for both researchers and machine learning practitioners.</p> <p>Question: Why not code everything from scratch?</p> <p>You could definitely do that by reproducing all of the math equations from the paper with custom PyTorch layers and that would certainly be an educative exercise, however, using pre-existing PyTorch layers is usually favoured as pre-existing layers have often been extensively tested and performance checked to make sure they run correctly and fast.</p> <p>Note: We're going to be focused on writing PyTorch code to create these layers. For the background on what each of these layers does, I'd suggest reading the ViT Paper in full or reading the linked resources for each layer.</p> <p>Let's take Figure 1 and adapt it to our FoodVision Mini problem of classifying images of food into pizza, steak or sushi.</p> <p></p> <p>Figure 1 from the ViT Paper adapted for use with FoodVision Mini. An image of food goes in (pizza), the image gets turned into patches and then projected to an embedding. The embedding then travels through the various layers and blocks and (hopefully) the class \"pizza\" is returned.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#322-exploring-the-four-equations","title":"3.2.2 Exploring the Four Equations\u00b6","text":"<p>The next main part(s) of the ViT paper we're going to look at are the four equations in section 3.1.</p> <p></p> <p>These four equations represent the math behind the four major parts of the ViT architecture.</p> <p>Section 3.1 describes each of these (some of the text has been omitted for brevity, bolded text is mine):</p> Equation number Description from ViT paper section 3.1 1 ...The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings... Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings... 2 The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski &amp; Auli, 2019). 3 Same as equation 2. 4 Similar to BERT's [ class ] token, we prepend a learnable embedding to the sequence of embedded patches $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_{L}^{0}\\right)$ serves as the image representation $\\mathbf{y}$ (Eq. 4)... <p>Let's map these descriptions to the ViT architecture in Figure 1.</p> <p></p> <p>Connecting Figure 1 from the ViT paper to the four equations from section 3.1 describing the math behind each of the layers/blocks.</p> <p>There's a lot happening in the image above but following the coloured lines and arrows reveals the main concepts of the ViT architecture.</p> <p>How about we break down each equation further (it will be our goal to recreate these with code)?</p> <p>In all equations (except equation 4), \"$\\mathbf{z}$\" is the raw output of a particular layer:</p> <ol> <li>$\\mathbf{z}_{0}$ is \"z zero\" (this is the output of the initial patch embedding layer).</li> <li>$\\mathbf{z}_{\\ell}^{\\prime}$ is \"z of a particular layer prime\" (or an intermediary value of z).</li> <li>$\\mathbf{z}_{\\ell}$ is \"z of a particular layer\".</li> </ol> <p>And $\\mathbf{y}$ is the overall output of the architecture.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#323-equation-1-overview","title":"3.2.3 Equation 1 overview\u00b6","text":"<p>$$ \\begin{aligned} \\mathbf{z}_{0} &amp;=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, &amp; &amp; \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{aligned} $$</p> <p>This equation deals with the class token, patch embedding and position embedding ($\\mathbf{E}$ is for embedding) of the input image.</p> <p>In vector form, the embedding might look something like:</p> <pre>x_input = [class_token, image_patch_1, image_patch_2, image_patch_3...] + [class_token_position, image_patch_1_position, image_patch_2_position, image_patch_3_position...]\n</pre> <p>Where each of the elements in the vector is learnable (their <code>requires_grad=True</code>).</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#324-equation-2-overview","title":"3.2.4 Equation 2 overview\u00b6","text":"<p>$$ \\begin{aligned} \\mathbf{z}_{\\ell}^{\\prime} &amp;=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$</p> <p>This says that for every layer from $1$ through to $L$ (the total number of layers), there's a Multi-Head Attention layer (MSA) wrapping a LayerNorm layer (LN).</p> <p>The addition on the end is the equivalent of adding the input to the output and forming a skip/residual connection.</p> <p>We'll call this layer the \"MSA block\".</p> <p>In pseudocode, this might look like:</p> <pre>x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n</pre> <p>Notice the skip connection on the end (adding the input of the layers to the output of the layers).</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#325-equation-3-overview","title":"3.2.5 Equation 3 overview\u00b6","text":"<p>$$ \\begin{aligned} \\mathbf{z}_{\\ell} &amp;=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, &amp; &amp; \\ell=1 \\ldots L \\\\ \\end{aligned} $$</p> <p>This says that for every layer from $1$ through to $L$ (the total number of layers), there's also a Multilayer Perceptron layer (MLP) wrapping a LayerNorm layer (LN).</p> <p>The addition on the end is showing the presence of a skip/residual connection.</p> <p>We'll call this layer the \"MLP block\".</p> <p>In pseudocode, this might look like:</p> <pre>x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n</pre> <p>Notice the skip connection on the end (adding the input of the layers to the output of the layers).</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#326-equation-4-overview","title":"3.2.6 Equation 4 overview\u00b6","text":"<p>$$ \\begin{aligned} \\mathbf{y} &amp;=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) &amp; &amp; \\end{aligned} $$</p> <p>This says for the last layer $L$, the output $y$ is the 0 index token of $z$ wrapped in a LayerNorm layer (LN).</p> <p>Or in our case, the 0 index of <code>x_output_MLP_block</code>:</p> <pre>y = Linear_layer(LN_layer(x_output_MLP_block[0]))\n</pre> <p>Of course there are some simplifications above but we'll take care of those when we start to write PyTorch code for each section.</p> <p>Note: The above section covers alot of information. But don't forget if something doesn't make sense, you can always research it further. By asking questions like \"what is a residual connection?\".</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#327-exploring-table-1","title":"3.2.7 Exploring Table 1\u00b6","text":"<p>The final piece of the ViT architecture puzzle we'll focus on (for now) is Table 1.</p> Model Layers Hidden size $D$ MLP size Heads Params ViT-Base 12 768 3072 12 $86M$ ViT-Large 24 1024 4096 16 $307M$ ViT-Huge 32 1280 5120 16 $632M$ Table 1: Details of Vision Transformer model variants. Source: ViT paper. <p>This table showcasing the various hyperparameters of each of the ViT architectures.</p> <p>You can see the numbers gradually increase from ViT-Base to ViT-Huge.</p> <p>We're going to focus on replicating ViT-Base (start small and scale up when necessary) but we'll be writing code that could easily scale up to the larger variants.</p> <p>Breaking the hyperparameters down:</p> <ul> <li>Layers - How many Transformer Encoder blocks are there? (each of these will contain a MSA block and MLP block)</li> <li>Hidden size $D$ - This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, the better results. However, a larger embedding comes at the cost of more compute.</li> <li>MLP size - What are the number of hidden units in the MLP layers?</li> <li>Heads - How many heads are there in the Multi-Head Attention layers?</li> <li>Params - What are the total number of parameters of the model? Generally, more parameters leads to better performance but at the cost of more compute. You'll notice even ViT-Base has far more parameters than any other model we've used so far.</li> </ul> <p>We'll use these values as the hyperparameter settings for our ViT architecture.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#33-my-workflow-for-replicating-papers","title":"3.3 My workflow for replicating papers\u00b6","text":"<p>When I start working on replicating a paper, I go through the following steps:</p> <ol> <li>Read the whole paper end-to-end once (to get an idea of the main concepts).</li> <li>Go back through each section and see how they line up with each other and start thinking about how they might be turned into code (just like above).</li> <li>Repeat step 2 until I've got a fairly good outline.</li> <li>Use mathpix.com (a very handy tool) to turn any sections of the paper into markdown/LaTeX to put into notebooks.</li> <li>Replicate the simplest version of the model possible.</li> <li>If I get stuck, look up other examples.</li> </ol> <p></p> <p>Turning the four equations from the ViT paper into editable LaTeX/markdown using mathpix.com.</p> <p>We've already gone through the first few steps above (and if you haven't read the full paper yet, I'd encourage you to give it a go) but what we'll be focusing on next is step 5: replicating the simplest version of the model possible.</p> <p>This is why we're starting with ViT-Base.</p> <p>Replicating the smallest version of the architecture possible, get it working and then we can scale up if we wanted to.</p> <p>Note: If you've never read a research paper before, many of the above steps can be intimidating. But don't worry, like anything, your skills at reading and replicating papers will improve with practice. Don't forget, a research paper is often months of work by many people compressed into a few pages. So trying to replicate it on your own is no small feat.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#4-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding","title":"4. Equation 1: Split data into patches and creating the class, position and patch embedding\u00b6","text":"<p>I remember one of my machine learning engineer friends used to say \"it's all about the embedding.\"</p> <p>As in, if you can represent your data in a good, learnable way (as embeddings are learnable representations), chances are, a learning algorithm will be able to perform well on them.</p> <p>With that being said, let's start by creating the class, position and patch embeddings for the ViT architecture.</p> <p>We'll start with the patch embedding.</p> <p>This means we'll be turning our input images in a sequence of patches and then embedding those patches.</p> <p>Recall that an embedding is a learnable representation of some form and is often a vector.</p> <p>The term learnable is important because this means the numerical representation of an input image (that the model sees) can be improved over time.</p> <p>We'll begin by following the opening paragraph of section 3.1 of the ViT paper (bold mine):</p> <p>The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.</p> <p>And size we're dealing with image shapes, let's keep in mind the line from Table 3 of the ViT paper:</p> <p>Training resolution is 224.</p> <p>Let's break down the text above.</p> <ul> <li>$D$ is the size of the patch embeddings, different values for $D$ for various sized ViT models can be found in Table 1.</li> <li>The image starts as 2D with size ${H \\times W \\times C}$.<ul> <li>$(H, W)$ is the resolution of the original image (height, width).</li> <li>$C$ is the number of channels.</li> </ul> </li> <li>The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.<ul> <li>$(P, P)$ is the resolution of each image patch (patch size).</li> <li>$N=H W / P^{2}$ is the resulting number of patches, which also serves as the input sequence length for the Transformer.</li> </ul> </li> </ul> <p></p> <p>Mapping the patch and position embedding portion of the ViT architecture from Figure 1 to Equation 1. The opening paragraph of section 3.1 describes the different input and output shapes of the patch embedding layer.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#41-calculating-patch-embedding-input-and-output-shapes-by-hand","title":"4.1 Calculating patch embedding input and output shapes by hand\u00b6","text":"<p>How about we start by calculating these input and output shape values by hand?</p> <p>To do so, let's create some variables to mimic each of the terms (such as $H$, $W$ etc) above.</p> <p>We'll use a patch size ($P$) of 16 since it's the best performing version of ViT-Base uses (see column \"ViT-B/16\" of Table 5 in the ViT paper for more).</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#42-turning-a-single-image-into-patches","title":"4.2 Turning a single image into patches\u00b6","text":"<p>Now we know the ideal input and output shapes for our patch embedding layer, let's move towards making it.</p> <p>What we're doing is breaking down the overall architecture into smaller pieces, focusing on the inputs and outputs of individual layers.</p> <p>So how do we create the patch embedding layer?</p> <p>We'll get to that shortly, first, let's visualize, visualize, visualize! what it looks like to turn an image into patches.</p> <p>Let's start with our single image.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#43-creating-image-patches-with-torchnnconv2d","title":"4.3 Creating image patches with <code>torch.nn.Conv2d()</code>\u00b6","text":"<p>We've seen what an image looks like when it gets turned into patches, now let's start moving towards replicating the patch embedding layers with PyTorch.</p> <p>To visualize our single image we wrote code to loop through the different height and width dimensions of a single image and plot individual patches.</p> <p>This operation is very similar to the convolutional operation we saw in 03. PyTorch Computer Vision section 7.1: Stepping through <code>nn.Conv2d()</code>.</p> <p>In fact, the authors of the ViT paper mention in section 3.1 that the patch embedding is achievable with a convolutional neural network (CNN):</p> <p>Hybrid Architecture. As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection $\\mathbf{E}$ (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size $1 \\times 1$, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above.</p> <p>The \"feature map\" they're refering to are the weights/activations produced by a convolutional layer passing over a given image.</p> <p></p> <p>By setting the <code>kernel_size</code> and <code>stride</code> parameters of a <code>torch.nn.Conv2d()</code> layer equal to the <code>patch_size</code>, we can effectively get a layer that splits our image into patches and creates a learnable embedding (referred to as a \"Linear Projection\" in the ViT paper) of each patch.</p> <p>Remember our ideal input and output shapes for the patch embedding layer?</p> <ul> <li>Input: The image starts as 2D with size ${H \\times W \\times C}$.</li> <li>Output: The image gets converted to a 1D sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> <p>Or for an image size of 224 and patch size of 16:</p> <ul> <li>Input (2D image): (224, 224, 3) -&gt; (height, width, color channels)</li> <li>Output (flattened 2D patches): (196, 768) -&gt; (number of patches, embedding dimension)</li> </ul> <p>We can recreate these with:</p> <ul> <li><code>torch.nn.Conv2d()</code> for turning our image into patches of CNN feature maps.</li> <li><code>torch.nn.Flatten()</code> for flattening the spatial dimensions of the feature map.</li> </ul> <p>Let's start with the <code>torch.nn.Conv2d()</code> layer.</p> <p>We can replicate the creation of patches by setting the <code>kernel_size</code> and <code>stride</code> equal to <code>patch_size</code>.</p> <p>This means each convolutional kernel will be of size <code>(patch_size x patch_size)</code> or if <code>patch_size=16</code>, <code>(16 x 16)</code> (the equivalent of one whole patch).</p> <p>And each step or <code>stride</code> of the convolutional kernel will be <code>patch_size</code> pixels long or <code>16</code> pixels long (equivalent of stepping to the next patch).</p> <p>We'll set <code>in_channels=3</code> for the number of color channels in our image and we'll set <code>out_channels=768</code>, the same as the $D$ value in Table 1 for ViT-Base (this is the embedding dimension, each image will be embedded into a learnable vector of size 768).</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#44-flattening-the-patch-embedding-with-torchnnflatten","title":"4.4 Flattening the patch embedding with <code>torch.nn.Flatten()</code>\u00b6","text":"<p>We've turned our image into patch embeddings but they're still in 2D format.</p> <p>How do we get them into the desired output shape of the patch embedding layer of the ViT model?</p> <ul> <li>Desired output (1D sequence of flattened 2D patches): (196, 768) -&gt; (number of patches, embedding dimension) -&gt; ${N \\times\\left(P^{2} \\cdot C\\right)}$</li> </ul> <p>Let's check the current shape.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#45-turning-the-vit-patch-embedding-layer-into-a-pytorch-module","title":"4.5 Turning the ViT patch embedding layer into a PyTorch module\u00b6","text":"<p>Time to put everything we've done for creating the patch embedding into a single PyTorch layer.</p> <p>We can do so by subclassing <code>nn.Module</code> and creating a small PyTorch \"model\" to do all of the steps above.</p> <p>Specifically we'll:</p> <ol> <li>Create a class called <code>PatchEmbedding</code> which subclasses <code>nn.Module</code> (so it can be used a PyTorch layer).</li> <li>Initialize the class with the parameters <code>in_channels=3</code>, <code>patch_size=16</code> (for ViT-Base) and <code>embedding_dim=768</code> (this is $D$ for ViT-Base from Table 1).</li> <li>Create a layer to turn an image into patches using <code>nn.Conv2d()</code> (just like in 4.3 above).</li> <li>Create a layer to flatten the patch feature maps into a single dimension (just like in 4.4 above).</li> <li>Define a <code>forward()</code> method to take an input and pass it through the layers created in 3 and 4.</li> <li>Make sure the output shape reflects the required output shape of the ViT architecture (${N \\times\\left(P^{2} \\cdot C\\right)}$).</li> </ol> <p>Let's do it!</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#46-creating-the-class-token-embedding","title":"4.6 Creating the class token embedding\u00b6","text":"<p>Okay we've made the image patch embedding, time to get to work on the class token embedding.</p> <p>Or $\\mathbf{x}_\\text {class }$ from equation 1.</p> <p></p> <p>Left: Figure 1 from the ViT paper with the \"classification token\" or <code>[class]</code> embedding token we're going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the learnable class embedding token.</p> <p>Reading the second paragraph of section 3.1 from the ViT paper, we see the following description:</p> <p>Similar to BERT's <code>[ class ]</code> token, we prepend a learnable embedding to the sequence of embedded patches $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_{L}^{0}\\right)$ serves as the image representation $\\mathbf{y}$ (Eq. 4).</p> <p>Note: BERT (Bidirectional Encoder Representations from Transformers) is one of the original machine learning research papers to use the Transformer architecture to achieve outstanding results on natural language processing (NLP) tasks and is where the idea of having a <code>[ class ]</code> token at the start of a sequence originated, class being a description for the \"classification\" class the sequence belonged to.</p> <p>So we need to \"preprend a learnable embedding to the sequence of embedded patches\".</p> <p>Let's start by viewing our sequence of embedded patches tensor (created in section 4.5) and its shape.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#47-creating-the-position-embedding","title":"4.7 Creating the position embedding\u00b6","text":"<p>Well, we've got the class token embedding and the patch embedding, now how might we create the position embedding?</p> <p>Or $\\mathbf{E}_{\\text {pos }}$ from equation 1 where $E$ stands for \"embedding\".</p> <p></p> <p>Left: Figure 1 from the ViT paper with the position embedding we're going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the position embedding.</p> <p>Let's find out more by reading section 3.1 of the ViT paper (bold mine):</p> <p>Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.</p> <p>By \"retain positional information\" the authors mean they want the architecture to know what \"order\" the patches come in. As in, patch two comes after patch one and patch three comes after patch two and on and on.</p> <p>This positional information can be important when considering what's in an image (without positional information an a flattened sequence could be seen as having no order and thus no patch relates to any other patch).</p> <p>To start creating the position embeddings, let's view our current embeddings.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#48-putting-it-all-together-from-image-to-embedding","title":"4.8 Putting it all together: from image to embedding\u00b6","text":"<p>Alright, we've come a long way in terms of turning our input images into an embedding and replicating equation 1 from section 3.1 of the ViT paper:</p> <p>$$ \\begin{aligned} \\mathbf{z}_{0} &amp;=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, &amp; &amp; \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{aligned} $$</p> <p>Let's now put everything together in a single code cell and go from input image ($\\mathbf{x}$) to output embedding ($\\mathbf{z}_0$).</p> <p>We can do so by:</p> <ol> <li>Setting the patch size (we'll use <code>16</code> as it's widely used throughout the paper and for ViT-Base).</li> <li>Getting a single image, printing its shape and storing its height and width.</li> <li>Adding a batch dimension to the single image so it's compatible with our <code>PatchEmbedding</code> layer.</li> <li>Creating a <code>PatchEmbedding</code> layer (the one we made in section 4.5) with a <code>patch_size=16</code> and <code>embedding_dim=768</code> (from Table 1 for ViT-Base).</li> <li>Passing the single image through the <code>PatchEmbedding</code> layer in 4 to create a sequence of patch embeddings.</li> <li>Creating a class token embedding like in section 4.6.</li> <li>Prepending the class token embedding to the patch embeddings created in step 5.</li> <li>Creating a position embedding like in section 4.7.</li> <li>Adding the position embedding to the class token and patch embeddings created in step 7.</li> </ol> <p>We'll also make sure to set the random seeds with <code>set_seeds()</code> and print out the shapes of different tensors along the way.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#5-equation-2-multi-head-attention-msa","title":"5. Equation 2: Multi-Head Attention (MSA)\u00b6","text":"<p>We've got our input data patchified and embedded, now let's move onto the next part of the ViT architecture.</p> <p>To start, we'll break down the Transformer Encoder section into two parts (start small and increase when necessary).</p> <p>The first being equation 2 and the second being equation 3.</p> <p>Recall equation 2 states:</p> <p>$$ \\begin{aligned} \\mathbf{z}_{\\ell}^{\\prime} &amp;=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$</p> <p>This indicates a Multi-Head Attention (MSA) layer wrapped in a LayerNorm (LN) layer with a residual connection (the input to the layer gets added to the output of the layer).</p> <p>We'll refer to equation 2 as the \"MSA block\".</p> <p>***Left:** Figure 1 from the ViT paper with Multi-Head Attention and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. Right: Mapping the Multi-Head Self Attention (MSA) layer, Norm layer and residual connection to their respective parts of equation 2 in the ViT paper.*</p> <p>Many layers you find in research papers are already implemented in modern deep learning frameworks such as PyTorch.</p> <p>In saying this, to replicate these layers and residual connection with PyTorch code we can use:</p> <ul> <li>Multi-Head Self Attention (MSA) - <code>torch.nn.MultiheadAttention()</code>.</li> <li>Norm (LN or LayerNorm) - <code>torch.nn.LayerNorm()</code>.</li> <li>Residual connection - add the input to output (we'll see this later on when we create the full Transformer Encoder block in section 7.1).</li> </ul>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#51-the-layernorm-ln-layer","title":"5.1 The LayerNorm (LN) layer\u00b6","text":"<p>Layer Normalization (<code>torch.nn.LayerNorm()</code> or Norm or LayerNorm or LN) normalizes an input over the last dimension.</p> <p>You can find the formal definition of <code>torch.nn.LayerNorm()</code> in the PyTorch documentation.</p> <p>PyTorch's <code>torch.nn.LayerNorm()</code>'s main parameter is <code>normalized_shape</code> which we can set to be equal to the dimension size we'd like to noramlize over (in our case it'll be $D$ or <code>768</code> for ViT-Base).</p> <p>What does it do?</p> <p>Layer Normalization helps improve training time and model generalization (ability to adapt to unseen data).</p> <p>I like to think of any kind of normalization as \"getting the data into a similar format\" or \"getting data samples into a similar distribution\".</p> <p>Imagine trying to walk up (or down) a set of stairs all with differing heights and lengths.</p> <p>It'd take some adjustment on each step right?</p> <p>And what you learn for each step wouldn't necessary help with the next one since they all differ, increasing the time it takes you to navigate the stairs.</p> <p>Normalization (including Layer Normalization) is the equivalent of making all the stairs the same height and length except the stairs are your data samples.</p> <p>So just like you can walk up (or down) stairs with similar heights and lengths much easier than those with unequal heights and widths, neural networks can optimize over data samples with similar distributions (similar mean and standard-deviations) easier than those with varying distributions.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#52-the-multi-head-self-attention-msa-layer","title":"5.2 The Multi-Head Self Attention (MSA) layer\u00b6","text":"<p>The power of the self-attention and multi-head attention (self-attention applied multiple times) were revealed in the form of the original Transformer architecture introduced in the Attention is all you need research paper.</p> <p>Originally designed for text inputs, the original self-attention mechanism takes a sequence of words and then calculates which word should pay more \"attention\" to another word.</p> <p>In other words, in the sentence \"the dog jumped over the fence\", perhaps the word \"dog\" relates strongly to \"jumped\" and \"fence\".</p> <p>This is simplified but the premise remains for images.</p> <p>Since our input is a sequence of image patches rather than words, self-attention and in turn multi-head attention will calculate which patch of an image is most related to another patch, eventually forming a learned representation of an image.</p> <p>But what's most important is that the layer does this on it's own given the data (we don't tell it what patterns to learn).</p> <p>And if the learned representation the layers form using MSA are good, we'll see the results in our model's performance.</p> <p>There are many resources online to learn more about the Transformer architeture and attention mechanism online such as Jay Alammar's wonderful Illustrated Transformer post and Illustrated Attention post.</p> <p>We're going to focus more on coding an existing PyTorch MSA implementation than creating our own.</p> <p>However, you can find the formal defintion of the ViT paper's MSA implementation is defined in Appendix A:</p> <p>***Left:** Vision Transformer architecture overview from Figure 1 of the ViT paper. Right: Definitions of equation 2, section 3.1 and Appendix A of the ViT paper highlighted to reflect their respective parts in Figure 1.*</p> <p>The image above highlights the triple embedding input to the MSA layer.</p> <p>This is known as query, key, value input or qkv for short which is fundamental to the self-attention mechanism.</p> <p>In our case, the triple embedding input will be three versions of the output of the Norm layer, one for query, key and value.</p> <p>Or three versions of our layer-normalized image patch and position embeddings created in section 4.8.</p> <p>We can implement the MSA layer in PyTorch with <code>torch.nn.MultiheadAttention()</code> with the parameters:</p> <ul> <li><code>embed_dim</code> - the embedding dimension from Table 1 (Hidden size $D$).</li> <li><code>num_heads</code> - how many attention heads to use (this is where the term \"multihead\" comes from), this value is also in Table 1 (Heads).</li> <li><code>dropout</code> - whether or not to apply dropout to the attention layer (according to Appendix B.1, dropout isn't used after the qkv-projections).</li> <li><code>batch_first</code> - does our batch dimension come first? (yes it does)</li> </ul>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#53-replicating-equation-2-with-pytorch-layers","title":"5.3 Replicating Equation 2 with PyTorch layers\u00b6","text":"<p>Let's put everything we've discussed about the LayerNorm (LN) and Multi-Head Attention (MSA) layers in equation 2 into practice.</p> <p>To do so, we'll:</p> <ol> <li>Create a class called <code>MultiheadSelfAttentionBlock</code> that inherits from <code>torch.nn.Module</code>.</li> <li>Initialize the class with hyperparameters from Table 1 of the ViT paper for the ViT-Base model.</li> <li>Create a layer normalization (LN) layer with <code>torch.nn.LayerNorm()</code> with the <code>normalized_shape</code> parameter the same as our embedding dimension ($D$ from Table 1).</li> <li>Create a multi-head attention (MSA) layer with the appropriate <code>embed_dim</code>, <code>num_heads</code>, <code>dropout</code> and <code>batch_first</code> parameters.</li> <li>Create a <code>forward()</code> method for our class passing the in the inputs through the LN layer and MSA layer.</li> </ol>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#6-equation-3-multilayer-perceptron-mlp","title":"6. Equation 3: Multilayer Perceptron (MLP)\u00b6","text":"<p>We're on a roll here!</p> <p>Let's keep it going and replicate equation 3:</p> <p>$$ \\begin{aligned} \\mathbf{z}_{\\ell} &amp;=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$</p> <p>Here MLP stands for \"multilayer perceptron\" and LN stands for \"layer normalization\" (as discussed above).</p> <p>And the addition on the end is the skip/residual connection.</p> <p>We'll refer to equation 3 as the \"MLP block\" of the Transformer encoder (notice how we're continuing the trend of breaking down the architecture into smaller chunks).</p> <p>***Left:** Figure 1 from the ViT paper with MLP and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. Right: Mapping the multilayer perceptron (MLP) layer, Norm layer (LN) and residual connection to their respective parts of equation 3 in the ViT paper.*</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#61-the-mlp-layers","title":"6.1 The MLP layer(s)\u00b6","text":"<p>The term MLP is quite broad as it can refer to almost any combination of multiple layers (hence the \"multi\" in multilayer perceptron).</p> <p>But it generally follows the pattern of:</p> <p><code>linear layer -&gt; non-linear layer -&gt; linear layer -&gt; non-linear layer</code></p> <p>In the the case of the ViT paper, the MLP structure is defined in section 3.1:</p> <p>The MLP contains two layers with a GELU non-linearity.</p> <p>Where \"two layers\" refers to linear layers (<code>torch.nn.Linear()</code> in PyTorch) and \"GELU non-linearity\" is the GELU  (Gaussian Error Linear Units) non-linear activation function (<code>torch.nn.GELU()</code> in PyTorch).</p> <p>Note: A linear layer (<code>torch.nn.Linear()</code>) can sometimes also be referred to as a \"dense layer\" or \"feedforward layer\". Some papers even use all three terms to describe the same thing (as in the ViT paper).</p> <p>Another sneaky detail about the MLP block doesn't appear until Appendix B.1 (Training):</p> <p>Table 3 summarizes our training setups for our different models. ...Dropout, when used, is applied after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings.</p> <p>This means that every linear layer (or dense layer) in the MLP block has a dropout layer (<code>torch.nn.Dropout()</code> in PyTorch).</p> <p>The value of which can be found in Table 3 of the ViT paper (for ViT-Base, <code>dropout=0.1</code>).</p> <p>Knowing this, the structure of our MLP block will be:</p> <p><code>layer norm -&gt; linear layer -&gt; non-linear layer -&gt; dropout -&gt; linear layer -&gt; dropout</code></p> <p>With hyperparameter values for the linear layers available from Table 1 (MLP size is the number of hidden units between the linear layers and hidden size $D$ is the output size of the MLP block).</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#62-replicating-equation-3-with-pytorch-layers","title":"6.2 Replicating Equation 3 with PyTorch layers\u00b6","text":"<p>Let's put everything we've discussed about the LayerNorm (LN) and MLP (MSA) layers in equation 3 into practice.</p> <p>To do so, we'll:</p> <ol> <li>Create a class called <code>MLPBlock</code> that inherits from <code>torch.nn.Module</code>.</li> <li>Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.</li> <li>Create a layer normalization (LN) layer with <code>torch.nn.LayerNorm()</code> with the <code>normalized_shape</code> parameter the same as our embedding dimension ($D$ from Table 1).</li> <li>Create a sequential series of MLP layers(s) using <code>torch.nn.Linear()</code>, <code>torch.nn.Dropout()</code> and <code>torch.nn.GELU()</code> with appropriate hyperparameter values from Table 1 and Table 3.</li> <li>Create a <code>forward()</code> method for our class passing the in the inputs through the LN layer and MLP layer(s).</li> </ol>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#7-create-the-transformer-encoder","title":"7. Create the Transformer Encoder\u00b6","text":"<p>Time to stack together our <code>MultiheadSelfAttentionBlock</code> (equation 2) and <code>MLPBlock</code> (equation 3) and create the Transformer Encoder of the ViT architecture.</p> <p>In deep learning, an \"encoder\" or \"auto encoder\" generally refers to a stack of layers that \"encodes\" an input (turns it into some form of numerical representation).</p> <p>In our case, the Transformer Encoder will encode our patched image embedding into a learned representation using a series of alternating layers of MSA blocks and MLP blocks, as per section 3.1 of the ViT Paper:</p> <p>The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski &amp; Auli, 2019).</p> <p>We've created MSA and MLP blocks but what about the residual connections?</p> <p>Residual connections (also called skip connections), were first introduced in the paper Deep Residual Learning for Image Recognition and are achieved by adding a layer(s) input to its subsequent output.</p> <p>Where the subsequence output might be one or more layers later.</p> <p>In the case of the ViT architecture, the residual connection means the input of the MSA block is added back to the output of the MSA block before it passes to the MLP block.</p> <p>And the same thing happens with the MLP block before it goes onto the next Transformer Encoder block.</p> <p>Or in pseudocode:</p> <p><code>x_input -&gt; MSA_block -&gt; [MSA_block_output + x_input] -&gt; MLP_block -&gt; [MLP_block_output + MSA_block_output + x_input] -&gt; ...</code></p> <p>What does this do?</p> <p>One of the main ideas behind residual connections is that they prevent weight values and gradient updates from getting too small and thus allow deeper networks and in turn allow deeper representations to be learned.</p> <p>Note: The iconic computer vision architecture \"ResNet\" is named so because of the introduction of residual connections. You can find many pretrained versions of ResNet architectures in <code>torchvision.models</code>.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#71-creating-a-transformer-encoder-by-combining-our-custom-made-layers","title":"7.1 Creating a Transformer Encoder by combining our custom made layers\u00b6","text":"<p>Enough talk, let's see this in action and make a ViT Transformer Encoder with PyTorch by combining our previously created layers.</p> <p>To do so, we'll:</p> <ol> <li>Create a class called <code>TransformerEncoderBlock</code> that inherits from <code>torch.nn.Module</code>.</li> <li>Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.</li> <li>Instantiate a MSA block for equation 2 using our <code>MultiheadSelfAttentionBlock</code> from section 5.2 with the appropriate parameters.</li> <li>Instantiate a MLP block for equation 3 using our <code>MLPBlock</code> from section 6.2 with the appropriate parameters.</li> <li>Create a <code>forward()</code> method for our <code>TransformerEncoderBlock</code> class.</li> <li>Create a residual connection for the MSA block (for equation 2).</li> <li>Create a residual connection for the MLP block (for equation 3).</li> </ol>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#72-creating-a-transformer-encoder-with-pytorchs-transformer-layers","title":"7.2 Creating a Transformer Encoder with PyTorch's Transformer layers\u00b6","text":"<p>So far we've built the components of and the Transformer Encoder layer itself ourselves.</p> <p>But because of their rise in popularity and effectiveness, PyTorch now has in-built Transformer layers as part of <code>torch.nn</code>.</p> <p>For example, we can recreate the <code>TransformerEncoderBlock</code> we just created using <code>torch.nn.TransformerEncoderLayer()</code> and setting the same hyperparameters as above.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#8-putting-it-all-together-to-create-vit","title":"8. Putting it all together to create ViT\u00b6","text":"<p>Alright, alright, alright, we've come a long way!</p> <p>But now it's time to do the exciting thing of putting together all of the pieces of the puzzle.</p> <p>We're going to combine all of the blocks we've created to replicate the full ViT architecture.</p> <p>From the patch and positional embedding to the Transformer Encoder(s) to the MLP Head.</p> <p>But wait, we haven't created equation 4 yet...</p> <p>$$ \\begin{aligned} \\mathbf{y} &amp;=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) &amp; &amp; \\end{aligned} $$</p> <p>Don't worry, we can put equation 4 into our overall ViT architecture class.</p> <p>All we need is a <code>torch.nn.LayerNorm()</code> layer and a <code>torch.nn.Linear()</code> layer to convert the 0th index ($\\mathbf{z}_{L}^{0}$) of the Transformer Encoder logit outputs to the target number of classes we have.</p> <p>To create the full architecture, we'll also need to stack a number of our <code>TransformerEncoderBlock</code>s on top of each other, we can do this by passing a list of them to <code>torch.nn.Sequential()</code> (this will make a sequential range of <code>TransformerEncoderBlock</code>s).</p> <p>We'll focus on the ViT-Base hyperparameters from Table 1 but our code should be adaptable to other ViT variants.</p> <p>Creating ViT will be our biggest code block yet but we can do it!</p> <p>Finally, to bring our own implementation of ViT to life, let's:</p> <ol> <li>Create a class called <code>ViT</code> that inherits from <code>torch.nn.Module</code>.</li> <li>Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.</li> <li>Make sure the image size is divisible by the patch size (the image should be split into even patches).</li> <li>Calculate the number of patches using the formula $N=H W / P^{2}$, where $H$ is the image height, $W$ is the image width and $P$ is the patch size.</li> <li>Create a learnable class embedding token (equation 1) as done above in section 4.6.</li> <li>Create a learnable position embedding vector (equation 1) as done above in section 4.7.</li> <li>Setup the embedding dropout layer as discussed in Appendix B.1 of the ViT paper.</li> <li>Create the patch embedding layer using the <code>PatchEmbedding</code> class as above in section 4.5.</li> <li>Create a series of Transformer Encoder blocks by passing a list of <code>TransformerEncoderBlock</code>s created in section 7.1 to <code>torch.nn.Sequential()</code> (equations 2 &amp; 3).</li> <li>Create the MLP head (also called classifier head or equation 4) by passing a <code>torch.nn.LayerNorm()</code> (LN) layer and a <code>torch.nn.Linear(out_features=num_classes)</code> layer (where <code>num_classes</code> is the target number of classes) linear layer to <code>torch.nn.Sequential()</code>.</li> <li>Create a <code>forward()</code> method that accepts an input.</li> <li>Get the batch size of the input (the first dimension of the shape).</li> <li>Create the patching embedding using the layer created in step 8 (equation 1).</li> <li>Create the class token embedding using the layer created in step 5 and expand it across the number of batches found in step 11 using <code>torch.Tensor.expand()</code> (equation 1).</li> <li>Concatenate the class token embedding created in step 13 to the first dimension of the patch embedding created in step 12 using <code>torch.cat()</code> (equation 1).</li> <li>Add the position embedding created in step 6 to the patch and class token embedding created in step 14 (equation 1).</li> <li>Pass the patch and position embedding through the dropout layer created in step 7.</li> <li>Pass the patch and position embedding from step 16 through the stack of Transformer Encoder layers created in step 9 (equations 2 &amp; 3).</li> <li>Pass index 0 of the output of the stack of Transformer Encoder layers from step 17 through the classifier head created in step 10 (equation 4).</li> <li>Dance and shout woohoo!!! We just built a Vision Transformer!</li> </ol> <p>You ready?</p> <p>Let's go.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#81-getting-a-visual-summary-of-our-vit-model","title":"8.1 Getting a visual summary of our ViT model\u00b6","text":"<p>We handcrafted our own version of the ViT architecture and seen that a random image tensor can flow all the way through it.</p> <p>How about we use <code>torchinfo.summary()</code> to get a visual overview of the input and output shapes of all the layers in our model?</p> <p>Note: The ViT paper states the use of a batch size of 4096 for training, however, this requires a far bit of CPU/GPU compute memory to handle (the larger the batch size the more memory required). So to make sure we don't get memory errors, we'll stick with a batch size of 32. You could always increase this later if you have access to hardware with more memory.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#9-setting-up-training-code-for-our-vit-model","title":"9. Setting up training code for our ViT model\u00b6","text":"<p>Ok time for the easy part.</p> <p>Training!</p> <p>Why easy?</p> <p>Because we've got most of what we need ready to go, from our model (<code>vit</code>) to our DataLoaders (<code>train_dataloader</code>, <code>test_dataloader</code>) to the training functions we created in 05. PyTorch Going Modular section 4.</p> <p>To train our model we can import the <code>train()</code> function from <code>going_modular.going_modular.engine</code>.</p> <p>All we need is a loss function and an optimizer.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#91-creating-an-optimizer","title":"9.1 Creating an optimizer\u00b6","text":"<p>Searching the ViT paper for \"optimizer\", section 4.1 on Training &amp; Fine-tuning states:</p> <p>Training &amp; Fine-tuning. We train all models, including ResNets, using Adam (Kingma &amp; Ba, 2015 ) with $\\beta_{1}=0.9, \\beta_{2}=0.999$, a batch size of 4096 and apply a high weight decay of $0.1$, which we found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common practices, Adam works slightly better than SGD for ResNets in our setting).</p> <p>So we can see they chose to use the \"Adam\" optimizer (<code>torch.optim.Adam()</code>) rather than SGD (stochastic gradient descent, <code>torch.optim.SGD()</code>).</p> <p>The authors set Adam's $\\beta$ (beta) values to $\\beta_{1}=0.9, \\beta_{2}=0.999$, these are the default values for the <code>betas</code> parameter in <code>torch.optim.Adam(betas=(0.9, 0.999))</code>.</p> <p>They also state the use of weight decay (slowly reducing the values of the weights during optimization to prevent overfitting), we can set this with the <code>weight_decay</code> parameter in <code>torch.optim.Adam(weight_decay=0.3)</code> (according to the setting of ViT-* trained on ImageNet-1k).</p> <p>We'll set the learning rate of the optimizer to 0.003 as per Table 3 (according to the setting of ViT-* trained on ImageNet-1k).</p> <p>And as discussed previously, we're going to use a lower batch size than 4096 due to hardware limitations (if you have a large GPU, feel free to increase this).</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#92-creating-a-loss-function","title":"9.2 Creating a loss function\u00b6","text":"<p>Strangely, searching the ViT paper for \"loss\" or \"loss function\" or \"criterion\" returns no results.</p> <p>However, since the target problem we're working with is multi-class classification (the same for the ViT paper), we'll use <code>torch.nn.CrossEntropyLoss()</code>.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#93-training-our-vit-model","title":"9.3 Training our ViT model\u00b6","text":"<p>Okay, now we know what optimizer and loss function we're going to use, let's setup the training code for training our ViT.</p> <p>We'll start by importing the <code>engine.py</code> script from <code>going_modular.going_modular</code> then we'll setup the optimizer and loss function and finally we'll use the <code>train()</code> function from <code>engine.py</code> to train our ViT model for 10 epochs (we're using a smaller number of epochs than the ViT paper to make sure everything works).</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#94-what-our-training-setup-is-missing","title":"9.4 What our training setup is missing\u00b6","text":"<p>The original ViT architecture achieves good results on several image classification benchmarks (on par or better than many state-of-the-art results when it was released).</p> <p>However, our results (so far) aren't as good.</p> <p>There's a few reasons this could be but the main one is scale.</p> <p>The original ViT paper uses a far larger amount of data than ours (in deep learning, more data is generally always a good thing) and a longer training schedule (see Table 3).</p> Hyperparameter value ViT Paper Our implementation Number of training images 1.3M (ImageNet-1k), 14M (ImageNet-21k), 303M (JFT) 225 Epochs 7 (for largest dataset), 90, 300 (for ImageNet) 10 Batch size 4096 32 Learning rate warmup 10k steps (Table 3) None Learning rate decay Linear/Cosine (Table 3) None Gradient clipping Global norm 1 (Table 3) None <p>Even though our ViT architecture is the same as the paper, the results from the ViT paper were achieved using far more data and a more elaborate training scheme than ours.</p> <p>Because of the size of the ViT architecture and its high number of parameters (increased learning capabilities), and amount of data it uses (increased learning opportunities), many of the techniques used in the ViT paper training scheme such as learning rate warmup, learning rate decay and gradient clipping are specifically designed to prevent overfitting (regularization).</p> <p>Note: For any technique you're unsure of, you can often quickly find an example by searching \"pytorch TECHNIQUE NAME\", for exmaple, say you wanted to learn about learning rate warmup and what it does, you could search \"pytorch learning rate warmup\".</p> <p>Good news is, there are many pretrained ViT models (using vast amounts of data) available online, we'll see one in action in section 10.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#95-plot-the-loss-curves-of-our-vit-model","title":"9.5 Plot the loss curves of our ViT model\u00b6","text":"<p>We've trained our ViT model and seen the results as numbers on a page.</p> <p>But let's now follow the data explorer's motto of visualize, visualize, visualize!</p> <p>And one of the best things to visualize for a model is its loss curves.</p> <p>To check out our ViT model's loss curves, we can use the <code>plot_loss_curves</code> function from <code>helper_functions.py</code> we created in 04. PyTorch Custom Datasets section 7.8.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset","title":"10. Using a pretrained ViT from <code>torchvision.models</code> on the same dataset\u00b6","text":"<p>We've discussed the benefits of using pretrained models in 06. PyTorch Transfer Learning.</p> <p>But since we've now trained our own ViT from scratch and achieved less than optimal results, the benefits of transfer learning (using a pretrained model) really shine.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#101-why-use-a-pretrained-model","title":"10.1 Why use a pretrained model?\u00b6","text":"<p>An important note on many modern machine learning research papers is that much of the results are obtained with large datasets and vast compute resources.</p> <p>And in modern day machine learning, the original fully trained ViT would likely not be considered a \"super large\" training setup (models are continually getting bigger and bigger).</p> <p>Reading the ViT paper section 4.2:</p> <p>Finally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days.</p> <p>As of July 2022, the price for renting a TPUv3 (Tensor Processing Unit version 3) with 8 cores on Google Cloud is $8 USD per hour.</p> <p>To rent one for 30 straight days would cost $5,760 USD.</p> <p>This cost (monetary and time) may be viable for some larger research teams or enterprises but for many people it's not.</p> <p>So having a pretrained model available through resources like <code>torchvision.models</code>, the <code>timm</code> (Torch Image Models) library, the HuggingFace Hub or even from the authors of the papers themselves (there's a growing trend for machine learning researchers to release the code and pretrained models from their research papers, I'm a big fan of this trend, many of these resources can be found on Paperswithcode.com).</p> <p>If you're focused on leveraging the benefits of a specific model architecture rather than creating your custom architecture, I'd highly recommend using a pretrained model.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#102-getting-a-pretrained-vit-model-and-creating-a-feature-extractor","title":"10.2 Getting a pretrained ViT model and creating a feature extractor\u00b6","text":"<p>We can get a pretrained ViT model from <code>torchvision.models</code>.</p> <p>We'll go from the top by first making sure we've got the right versions of <code>torch</code> and <code>torchvision</code>.</p> <p>Note: The following code requires <code>torch</code> v0.12+ and <code>torchvision</code> v0.13+ to use the latest <code>torchvision</code> model weights API.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#103-preparing-data-for-the-pretrained-vit-model","title":"10.3 Preparing data for the pretrained ViT model\u00b6","text":"<p>We downloaded and created DataLoaders for our own ViT model back in section 2.</p> <p>So we don't necessarily need to do it again.</p> <p>But in the name of practice, let's download some image data (pizza, steak and sushi images for Food Vision Mini), setup train and test directories and then transform the images into tensors and DataLoaders.</p> <p>We can download pizza, steak and sushi images from the course GitHub and the <code>download_data()</code> function we creating in 07. PyTorch Experiment Tracking section 1.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#104-train-feature-extractor-vit-model","title":"10.4 Train feature extractor ViT model\u00b6","text":"<p>Feature extractor model ready, DataLoaders ready, time to train!</p> <p>As before we'll use the Adam optimizer (<code>torch.optim.Adam()</code>) with a learning rate of <code>1e-3</code> and <code>torch.nn.CrossEntropyLoss()</code> as the loss function.</p> <p>Our <code>engine.train()</code> function we created in 05. PyTorch Going Modular section 4 will take care of the rest.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#105-plot-feature-extractor-vit-model-loss-curves","title":"10.5 Plot feature extractor ViT model loss curves\u00b6","text":"<p>Our pretrained ViT feature model numbers look good on the training and test sets.</p> <p>How do the loss curves look?</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size","title":"10.6 Save feature extractor ViT model and check file size\u00b6","text":"<p>It looks like our ViT feature extractor model is performing quite well for our Food Vision Mini problem.</p> <p>Perhaps we might want to try deploying it and see how it goes in production (in this case, deploying means putting our trained model in an application someone could use, say taking photos on their smartphone of food and seeing if our model thinks its pizza, steak or sushi).</p> <p>To do so we can first save our model with the <code>utils.save_model()</code> function we created in 05. PyTorch Going Modular section 5.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#11-make-predictions-on-a-custom-image","title":"11. Make predictions on a custom image\u00b6","text":"<p>And finally, we'll finish with the ultimate test, predicting on our own custom data.</p> <p>Let's download the pizza dad image (a photo of my dad eating pizza) and use our ViT feature extractor to predict on it.</p> <p>To do we, let's can use the <code>pred_and_plot()</code> function we created in 06. PyTorch Transfer Learning section 6, for convenience, I saved this function to <code>going_modular.going_modular.predictions.py</code> on the course GitHub.</p>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#main-takeaways","title":"Main takeaways\u00b6","text":"<ul> <li>With the explosion of machine learning, new research papers detailing advancements come out every day. And it's impossible to keep up with it all but you can narrow things down to your own use case, such as what we did here, replicating a computer vision paper for FoodVision Mini.</li> <li>Machine learning research papers often contain months of research by teams of smart people compressed into a few pages (so teasing out all the details and replicating the paper in full can be a bit of challenge).</li> <li>The goal of paper replicating is to turn machine learning research papers (text and math) into usable code.<ul> <li>With this being said, many machine learning research teams are starting to publish code with their papers and one of the best places to see this is at Paperswithcode.com</li> </ul> </li> <li>Breaking a machine learning research paper into inputs and outputs (what goes in and out of each layer/block/model?) and layers (how does each layer manipulate the input?) and blocks (a collection of layers) and replicating each part step by step (like we've done in this notebook) can be very helpful for understanding.</li> <li>Pretrained models are available for many state of the art model architectures and with the power of transfer learning, these often perform very well with little data.</li> <li>Larger models generally perform better but have a larger footprint too (they take up more storage space and can take longer to perform inference).<ul> <li>A big question is: deployment wise, is the extra performance of a larger model worth it/aligned with the use case?</li> </ul> </li> </ul>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#exercises","title":"Exercises\u00b6","text":"<p>Note: These exercises expect the use of <code>torchvision</code> v0.13+ (released July 2022), previous versions may work but will likely have errors.</p> <p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 08.</li> <li>Example solutions notebook for 08 (try the exercises before looking at this).<ul> <li>See a live video walkthrough of the solutions on YouTube (errors and all).</li> </ul> </li> </ul> <ol> <li>Replicate the ViT architecture we created with in-built PyTorch transformer layers.<ul> <li>You'll want to look into replacing our <code>TransformerEncoderBlock()</code> class with <code>torch.nn.TransformerEncoderLayer()</code> (these contain the same layers as our custom blocks).</li> <li>You can stack <code>torch.nn.TransformerEncoderLayer()</code>'s on top of each other with <code>torch.nn.TransformerEncoder()</code>.</li> </ul> </li> <li>Turn the custom ViT architecture we created into a Python script, for example, <code>vit.py</code>.<ul> <li>You should be able to import an entire ViT model using something like<code>from vit import ViT</code>.</li> </ul> </li> <li>Train a pretrained ViT feature extractor model (like the one we made in 08. PyTorch Paper Replicating section 10) on 20% of the pizza, steak and sushi data like the dataset we used in 07. PyTorch Experiment Tracking section 7.3.<ul> <li>See how it performs compared to the EffNetB2 model we compared it to in 08. PyTorch Paper Replicating section 10.6.</li> </ul> </li> <li>Try repeating the steps from excercise 3 but this time use the \"<code>ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1</code>\" pretrained weights from <code>torchvision.models.vit_b_16()</code>.<ul> <li>Note: ViT pretrained with SWAG weights has a minimum input image size of <code>(384, 384)</code> (the pretrained ViT in exercise 3 has a minimum input size of <code>(224, 224)</code>), though this is accessible in the weights <code>.transforms()</code> method.</li> </ul> </li> <li>Our custom ViT model architecture closely mimics that of the ViT paper, however, our training recipe misses a few things. Research some of the following topics from Table 3 in the ViT paper that we miss and write a sentence about each and how it might help with training:<ul> <li>ImageNet-21k pretraining (more data).</li> <li>Learning rate warmup.</li> <li>Learning rate decay.</li> <li>Gradient clipping.</li> </ul> </li> </ol>"},{"location":"Learning/Pytorch/08_pytorch_paper_replicating/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>There have been several iterations and tweaks to the Vision Transformer since its original release and the most concise and best performing (as of July 2022) can be viewed in Better plain ViT baselines for ImageNet-1k. Despite of the upgrades, we stuck with replicating a \"vanilla Vision Transformer\" in this notebook because if you understand the structure of the original, you can bridge to different iterations.</li> <li>The <code>vit-pytorch</code> repository on GitHub by lucidrains is one of the most extensive resources of different ViT architectures implemented in PyTorch. It's a phenomenal reference and one I used often to create the materials we've been through in this chapter.</li> <li>PyTorch have their own implementation of the ViT architecture on GitHub, it's used as the basis of the pretrained ViT models in <code>torchvision.models</code>.</li> <li>Jay Alammar has fantastic illustrations and explanations on his blog of the attention mechanism (the foundation of Transformer models) and Transformer models.</li> <li>Adrish Dey has a fantastic write up of Layer Normalization (a main component of the ViT architecture) can help neural network training.</li> <li>The self-attention (and multi-head self-attention) mechanism is at the heart of the ViT architecture as well as many other Transformer architectures, it was originally introduced in the Attention is all you need paper.</li> <li>Yannic Kilcher's YouTube channel is a sensational resource for visual paper walkthroughs, you can see his videos for the following papers:<ul> <li>Attention is all you need (the paper that introduced the Transformer architecture).</li> <li>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (the paper that introduced the ViT architecture).</li> </ul> </li> </ul>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/","title":"00. Getting started with TensorFlow: A guide to the fundamentals","text":"In\u00a0[1]: Copied! <pre># Create timestamp\nimport datetime\n\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> # Create timestamp import datetime  print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>Notebook last run (end-to-end): 2024-04-18 13:54:27.276504\n</pre> In\u00a0[2]: Copied! <pre># Import TensorFlow\nimport tensorflow as tf\nprint(tf.__version__) # find the version number (should be 2.x+)\n</pre> # Import TensorFlow import tensorflow as tf print(tf.__version__) # find the version number (should be 2.x+) <pre>2.17.0-dev20240226\n</pre> In\u00a0[3]: Copied! <pre># Create a scalar (rank 0 tensor)\nscalar = tf.constant(7)\nscalar\n</pre> # Create a scalar (rank 0 tensor) scalar = tf.constant(7) scalar Out[3]: <pre>&lt;tf.Tensor: shape=(), dtype=int32, numpy=7&gt;</pre> <p>A scalar is known as a rank 0 tensor. Because it has no dimensions (it's just a number).</p> <p>\ud83d\udd11 Note: For now, you don't need to know too much about the different ranks of tensors (but we will see more on this later). The important point is knowing tensors can have an unlimited range of dimensions (the exact amount will depend on what data you're representing).</p> In\u00a0[4]: Copied! <pre># Check the number of dimensions of a tensor (ndim stands for number of dimensions)\nscalar.ndim\n</pre> # Check the number of dimensions of a tensor (ndim stands for number of dimensions) scalar.ndim Out[4]: <pre>0</pre> In\u00a0[5]: Copied! <pre># Create a vector (more than 0 dimensions)\nvector = tf.constant([10, 10])\nvector\n</pre> # Create a vector (more than 0 dimensions) vector = tf.constant([10, 10]) vector Out[5]: <pre>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([10, 10])&gt;</pre> In\u00a0[6]: Copied! <pre># Check the number of dimensions of our vector tensor\nvector.ndim\n</pre> # Check the number of dimensions of our vector tensor vector.ndim Out[6]: <pre>1</pre> In\u00a0[7]: Copied! <pre># Create a matrix (more than 1 dimension)\nmatrix = tf.constant([[10, 7],\n                      [7, 10]])\nmatrix\n</pre> # Create a matrix (more than 1 dimension) matrix = tf.constant([[10, 7],                       [7, 10]]) matrix Out[7]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[10,  7],\n       [ 7, 10]])&gt;</pre> In\u00a0[8]: Copied! <pre>matrix.ndim\n</pre> matrix.ndim Out[8]: <pre>2</pre> <p>By default, TensorFlow creates tensors with either an <code>int32</code> or <code>float32</code> datatype.</p> <p>This is known as 32-bit precision (the higher the number, the more precise the number, the more space it takes up on your computer).</p> In\u00a0[9]: Copied! <pre># Create another matrix and define the datatype\nanother_matrix = tf.constant([[10., 7.],\n                              [3., 2.],\n                              [8., 9.]], dtype=tf.float16) # specify the datatype with 'dtype'\nanother_matrix\n</pre> # Create another matrix and define the datatype another_matrix = tf.constant([[10., 7.],                               [3., 2.],                               [8., 9.]], dtype=tf.float16) # specify the datatype with 'dtype' another_matrix Out[9]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=float16, numpy=\narray([[10.,  7.],\n       [ 3.,  2.],\n       [ 8.,  9.]], dtype=float16)&gt;</pre> In\u00a0[10]: Copied! <pre># Even though another_matrix contains more numbers, its dimensions stay the same\nanother_matrix.ndim\n</pre> # Even though another_matrix contains more numbers, its dimensions stay the same another_matrix.ndim Out[10]: <pre>2</pre> In\u00a0[11]: Copied! <pre># How about a tensor? (more than 2 dimensions, although, all of the above items are also technically tensors)\ntensor = tf.constant([[[1, 2, 3],\n                       [4, 5, 6]],\n                      [[7, 8, 9],\n                       [10, 11, 12]],\n                      [[13, 14, 15],\n                       [16, 17, 18]]])\ntensor\n</pre> # How about a tensor? (more than 2 dimensions, although, all of the above items are also technically tensors) tensor = tf.constant([[[1, 2, 3],                        [4, 5, 6]],                       [[7, 8, 9],                        [10, 11, 12]],                       [[13, 14, 15],                        [16, 17, 18]]]) tensor Out[11]: <pre>&lt;tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]],\n\n       [[13, 14, 15],\n        [16, 17, 18]]])&gt;</pre> In\u00a0[12]: Copied! <pre>tensor.ndim\n</pre> tensor.ndim Out[12]: <pre>3</pre> <p>This is known as a rank 3 tensor (3-dimensions), however a tensor can have an arbitrary (unlimited) amount of dimensions.</p> <p>For example, you might turn a series of images into tensors with shape (224, 224, 3, 32), where:</p> <ul> <li>224, 224 (the first 2 dimensions) are the height and width of the images in pixels.</li> <li>3 is the number of colour channels of the image (red, green blue).</li> <li>32 is the batch size (the number of images a neural network sees at any one time).</li> </ul> <p>All of the above variables we've created are actually tensors. But you may also hear them referred to as their different names (the ones we gave them):</p> <ul> <li>scalar: a single number.</li> <li>vector: a number with direction (e.g. wind speed with direction).</li> <li>matrix: a 2-dimensional array of numbers.</li> <li>tensor: an n-dimensional arrary of numbers (where n can be any number, a 0-dimension tensor is a scalar, a 1-dimension tensor is a vector).</li> </ul> <p>To add to the confusion, the terms matrix and tensor are often used interchangably.</p> <p>Going forward since we're using TensorFlow, everything we refer to and use will be tensors.</p> <p>For more on the mathematical difference between scalars, vectors and matrices see the visual algebra post by Math is Fun.</p> <p></p> In\u00a0[14]: Copied! <pre># Create the same tensor with tf.Variable() and tf.constant()\nchangeable_tensor = tf.Variable([10, 7])\nunchangeable_tensor = tf.constant([10, 7])\nchangeable_tensor, unchangeable_tensor\n</pre> # Create the same tensor with tf.Variable() and tf.constant() changeable_tensor = tf.Variable([10, 7]) unchangeable_tensor = tf.constant([10, 7]) changeable_tensor, unchangeable_tensor Out[14]: <pre>(&lt;tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([10,  7])&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([10,  7])&gt;)</pre> <p>Now let's try to change one of the elements of the changable tensor.</p> In\u00a0[15]: Copied! <pre># Will error (requires the .assign() method)\nchangeable_tensor[0] = 7\nchangeable_tensor\n</pre> # Will error (requires the .assign() method) changeable_tensor[0] = 7 changeable_tensor <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[15], line 2\n      1 # Will error (requires the .assign() method)\n----&gt; 2 changeable_tensor[0] = 7\n      3 changeable_tensor\n\nTypeError: 'ResourceVariable' object does not support item assignment</pre> <p>To change an element of a <code>tf.Variable()</code> tensor requires the <code>assign()</code> method.</p> In\u00a0[16]: Copied! <pre># Won't error\nchangeable_tensor[0].assign(7)\nchangeable_tensor\n</pre> # Won't error changeable_tensor[0].assign(7) changeable_tensor Out[16]: <pre>&lt;tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([7, 7])&gt;</pre> <p>Now let's try to change a value in a <code>tf.constant()</code> tensor.</p> In\u00a0[17]: Copied! <pre># Will error (can't change tf.constant())\nunchangeable_tensor[0].assign(7)\nunchangleable_tensor\n</pre> # Will error (can't change tf.constant()) unchangeable_tensor[0].assign(7) unchangleable_tensor <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[17], line 2\n      1 # Will error (can't change tf.constant())\n----&gt; 2 unchangeable_tensor[0].assign(7)\n      3 unchangleable_tensor\n\nFile D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:260, in Tensor.__getattr__(self, name)\n    252 if name in {\"T\", \"astype\", \"ravel\", \"transpose\", \"reshape\", \"clip\", \"size\",\n    253             \"tolist\", \"data\"}:\n    254   # TODO(wangpeng): Export the enable_numpy_behavior knob\n    255   raise AttributeError(\n    256       f\"{type(self).__name__} object has no attribute '{name}'. \" + \"\"\"\n    257     If you are looking for numpy-related methods, please run the following:\n    258     tf.experimental.numpy.experimental_enable_numpy_behavior()\n    259   \"\"\")\n--&gt; 260 self.__getattribute__(name)\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'assign'</pre> <p>Which one should you use? <code>tf.constant()</code> or <code>tf.Variable()</code>?</p> <p>It will depend on what your problem requires. However, most of the time, TensorFlow will automatically choose for you (when loading data or modelling data).</p> In\u00a0[18]: Copied! <pre># Create two random (but the same) tensors\nrandom_1 = tf.random.Generator.from_seed(42) # set the seed for reproducibility\nrandom_1 = random_1.normal(shape=(3, 2)) # create tensor from a normal distribution \nrandom_2 = tf.random.Generator.from_seed(42)\nrandom_2 = random_2.normal(shape=(3, 2))\n\n# Are they equal?\nrandom_1, random_2, random_1 == random_2\n</pre> # Create two random (but the same) tensors random_1 = tf.random.Generator.from_seed(42) # set the seed for reproducibility random_1 = random_1.normal(shape=(3, 2)) # create tensor from a normal distribution  random_2 = tf.random.Generator.from_seed(42) random_2 = random_2.normal(shape=(3, 2))  # Are they equal? random_1, random_2, random_1 == random_2 Out[18]: <pre>(&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n array([[-0.7565803 , -0.06854702],\n        [ 0.07595026, -1.2573844 ],\n        [-0.23193763, -1.8107855 ]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n array([[-0.7565803 , -0.06854702],\n        [ 0.07595026, -1.2573844 ],\n        [-0.23193763, -1.8107855 ]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy=\n array([[ True,  True],\n        [ True,  True],\n        [ True,  True]])&gt;)</pre> <p>The random tensors we've made are actually pseudorandom numbers (they appear as random, but really aren't).</p> <p>If we set a seed we'll get the same random numbers (if you've ever used NumPy, this is similar to <code>np.random.seed(42)</code>).</p> <p>Setting the seed says, \"hey, create some random numbers, but flavour them with X\" (X is the seed).</p> <p>What do you think will happen when we change the seed?</p> In\u00a0[19]: Copied! <pre># Create two random (and different) tensors\nrandom_3 = tf.random.Generator.from_seed(42)\nrandom_3 = random_3.normal(shape=(3, 2))\nrandom_4 = tf.random.Generator.from_seed(11)\nrandom_4 = random_4.normal(shape=(3, 2))\n\n# Check the tensors and see if they are equal\nrandom_3, random_4, random_1 == random_3, random_3 == random_4\n</pre> # Create two random (and different) tensors random_3 = tf.random.Generator.from_seed(42) random_3 = random_3.normal(shape=(3, 2)) random_4 = tf.random.Generator.from_seed(11) random_4 = random_4.normal(shape=(3, 2))  # Check the tensors and see if they are equal random_3, random_4, random_1 == random_3, random_3 == random_4 Out[19]: <pre>(&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n array([[-0.7565803 , -0.06854702],\n        [ 0.07595026, -1.2573844 ],\n        [-0.23193763, -1.8107855 ]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n array([[ 0.27305737, -0.29925638],\n        [-0.3652325 ,  0.61883307],\n        [-1.0130816 ,  0.28291714]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy=\n array([[ True,  True],\n        [ True,  True],\n        [ True,  True]])&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy=\n array([[False, False],\n        [False, False],\n        [False, False]])&gt;)</pre> <p>What if you wanted to shuffle the order of a tensor?</p> <p>Wait, why would you want to do that?</p> <p>Let's say you working with 15,000 images of cats and dogs and the first 10,000 images of were of cats and the next 5,000 were of dogs. This order could effect how a neural network learns (it may overfit by learning the order of the data), instead, it might be a good idea to move your data around.</p> In\u00a0[20]: Copied! <pre># Shuffle a tensor (valuable for when you want to shuffle your data)\nnot_shuffled = tf.constant([[10, 7],\n                            [3, 4],\n                            [2, 5]])\n# Gets different results each time\ntf.random.shuffle(not_shuffled)\n</pre> # Shuffle a tensor (valuable for when you want to shuffle your data) not_shuffled = tf.constant([[10, 7],                             [3, 4],                             [2, 5]]) # Gets different results each time tf.random.shuffle(not_shuffled) Out[20]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[ 3,  4],\n       [ 2,  5],\n       [10,  7]])&gt;</pre> In\u00a0[21]: Copied! <pre># Shuffle in the same order every time using the seed parameter (won't acutally be the same)\ntf.random.shuffle(not_shuffled, seed=42)\n</pre> # Shuffle in the same order every time using the seed parameter (won't acutally be the same) tf.random.shuffle(not_shuffled, seed=42) Out[21]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[ 2,  5],\n       [ 3,  4],\n       [10,  7]])&gt;</pre> <p>Wait... why didn't the numbers come out the same?</p> <p>It's due to rule #4 of the <code>tf.random.set_seed()</code> documentation.</p> <p>\"4. If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence.\"</p> <p><code>tf.random.set_seed(42)</code> sets the global seed, and the <code>seed</code> parameter in <code>tf.random.shuffle(seed=42)</code> sets the operation seed.</p> <p>Because, \"Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed.\"</p> In\u00a0[22]: Copied! <pre># Shuffle in the same order every time\n\n# Set the global random seed\ntf.random.set_seed(42)\n\n# Set the operation random seed\ntf.random.shuffle(not_shuffled, seed=42)\n</pre> # Shuffle in the same order every time  # Set the global random seed tf.random.set_seed(42)  # Set the operation random seed tf.random.shuffle(not_shuffled, seed=42) Out[22]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[10,  7],\n       [ 3,  4],\n       [ 2,  5]])&gt;</pre> In\u00a0[23]: Copied! <pre># Set the global random seed\ntf.random.set_seed(42) # if you comment this out you'll get different results\n\n# Set the operation random seed\ntf.random.shuffle(not_shuffled)\n</pre> # Set the global random seed tf.random.set_seed(42) # if you comment this out you'll get different results  # Set the operation random seed tf.random.shuffle(not_shuffled) Out[23]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[ 3,  4],\n       [ 2,  5],\n       [10,  7]])&gt;</pre> In\u00a0[24]: Copied! <pre># Make a tensor of all ones\ntf.ones(shape=(3, 2))\n</pre> # Make a tensor of all ones tf.ones(shape=(3, 2)) Out[24]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[1., 1.],\n       [1., 1.],\n       [1., 1.]], dtype=float32)&gt;</pre> In\u00a0[25]: Copied! <pre># Make a tensor of all zeros\ntf.zeros(shape=(3, 2))\n</pre> # Make a tensor of all zeros tf.zeros(shape=(3, 2)) Out[25]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[0., 0.],\n       [0., 0.],\n       [0., 0.]], dtype=float32)&gt;</pre> <p>You can also turn NumPy arrays in into tensors.</p> <p>Remember, the main difference between tensors and NumPy arrays is that tensors can be run on GPUs.</p> <p>\ud83d\udd11 Note: A matrix or tensor is typically represented by a capital letter (e.g. <code>X</code> or <code>A</code>) where as a vector is typically represented by a lowercase letter (e.g. <code>y</code> or <code>b</code>).</p> In\u00a0[26]: Copied! <pre>import numpy as np\nnumpy_A = np.arange(1, 25, dtype=np.int32) # create a NumPy array between 1 and 25\nA = tf.constant(numpy_A,  \n                shape=[2, 4, 3]) # note: the shape total (2*4*3) has to match the number of elements in the array\nnumpy_A, A\n</pre> import numpy as np numpy_A = np.arange(1, 25, dtype=np.int32) # create a NumPy array between 1 and 25 A = tf.constant(numpy_A,                   shape=[2, 4, 3]) # note: the shape total (2*4*3) has to match the number of elements in the array numpy_A, A Out[26]: <pre>(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n        18, 19, 20, 21, 22, 23, 24]),\n &lt;tf.Tensor: shape=(2, 4, 3), dtype=int32, numpy=\n array([[[ 1,  2,  3],\n         [ 4,  5,  6],\n         [ 7,  8,  9],\n         [10, 11, 12]],\n \n        [[13, 14, 15],\n         [16, 17, 18],\n         [19, 20, 21],\n         [22, 23, 24]]])&gt;)</pre> In\u00a0[27]: Copied! <pre># Create a rank 4 tensor (4 dimensions)\nrank_4_tensor = tf.zeros([2, 3, 4, 5])\nrank_4_tensor\n</pre> # Create a rank 4 tensor (4 dimensions) rank_4_tensor = tf.zeros([2, 3, 4, 5]) rank_4_tensor Out[27]: <pre>&lt;tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=\narray([[[[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]]], dtype=float32)&gt;</pre> In\u00a0[30]: Copied! <pre>rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor)\n</pre> rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor) Out[30]: <pre>(TensorShape([2, 3, 4, 5]), 4, &lt;tf.Tensor: shape=(), dtype=int32, numpy=120&gt;)</pre> In\u00a0[32]: Copied! <pre># Get various attributes of tensor\nprint(\"Datatype of every element:\", rank_4_tensor.dtype)\nprint(\"Number of dimensions (rank):\", rank_4_tensor.ndim)\nprint(\"Shape of tensor:\", rank_4_tensor.shape)\nprint(\"Elements along axis 0 of tensor:\", rank_4_tensor.shape[0])\nprint(\"Elements along last axis of tensor:\", rank_4_tensor.shape[-1])\nprint(\"Total number of elements (2*3*4*5):\", tf.size(rank_4_tensor).numpy()) # .numpy() converts to NumPy array\n</pre> # Get various attributes of tensor print(\"Datatype of every element:\", rank_4_tensor.dtype) print(\"Number of dimensions (rank):\", rank_4_tensor.ndim) print(\"Shape of tensor:\", rank_4_tensor.shape) print(\"Elements along axis 0 of tensor:\", rank_4_tensor.shape[0]) print(\"Elements along last axis of tensor:\", rank_4_tensor.shape[-1]) print(\"Total number of elements (2*3*4*5):\", tf.size(rank_4_tensor).numpy()) # .numpy() converts to NumPy array <pre>Datatype of every element: &lt;dtype: 'float32'&gt;\nNumber of dimensions (rank): 4\nShape of tensor: (2, 3, 4, 5)\nElements along axis 0 of tensor: 2\nElements along last axis of tensor: 5\nTotal number of elements (2*3*4*5): 120\n</pre> <p>You can also index tensors just like Python lists.</p> In\u00a0[33]: Copied! <pre># Get the first 2 items of each dimension\nrank_4_tensor[:2, :2, :2, :2]\n</pre> # Get the first 2 items of each dimension rank_4_tensor[:2, :2, :2, :2] Out[33]: <pre>&lt;tf.Tensor: shape=(2, 2, 2, 2), dtype=float32, numpy=\narray([[[[0., 0.],\n         [0., 0.]],\n\n        [[0., 0.],\n         [0., 0.]]],\n\n\n       [[[0., 0.],\n         [0., 0.]],\n\n        [[0., 0.],\n         [0., 0.]]]], dtype=float32)&gt;</pre> In\u00a0[34]: Copied! <pre># Get the dimension from each index except for the final one\nrank_4_tensor[:1, :1, :1, :]\n</pre> # Get the dimension from each index except for the final one rank_4_tensor[:1, :1, :1, :] Out[34]: <pre>&lt;tf.Tensor: shape=(1, 1, 1, 5), dtype=float32, numpy=array([[[[0., 0., 0., 0., 0.]]]], dtype=float32)&gt;</pre> In\u00a0[35]: Copied! <pre># Create a rank 2 tensor (2 dimensions)\nrank_2_tensor = tf.constant([[10, 7],\n                             [3, 4]])\n\n# Get the last item of each row\nrank_2_tensor[:, -1]\n</pre> # Create a rank 2 tensor (2 dimensions) rank_2_tensor = tf.constant([[10, 7],                              [3, 4]])  # Get the last item of each row rank_2_tensor[:, -1] Out[35]: <pre>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([7, 4])&gt;</pre> <p>You can also add dimensions to your tensor whilst keeping the same information present using <code>tf.newaxis</code>.</p> In\u00a0[36]: Copied! <pre># Add an extra dimension (to the end)\nrank_3_tensor = rank_2_tensor[..., tf.newaxis] # in Python \"...\" means \"all dimensions prior to\"\nrank_2_tensor, rank_3_tensor # shape (2, 2), shape (2, 2, 1)\n</pre> # Add an extra dimension (to the end) rank_3_tensor = rank_2_tensor[..., tf.newaxis] # in Python \"...\" means \"all dimensions prior to\" rank_2_tensor, rank_3_tensor # shape (2, 2), shape (2, 2, 1) Out[36]: <pre>(&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n array([[10,  7],\n        [ 3,  4]])&gt;,\n &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\n array([[[10],\n         [ 7]],\n \n        [[ 3],\n         [ 4]]])&gt;)</pre> <p>You can achieve the same using <code>tf.expand_dims()</code>.</p> In\u00a0[37]: Copied! <pre>tf.expand_dims(rank_2_tensor, axis=-1) # \"-1\" means last axis\n</pre> tf.expand_dims(rank_2_tensor, axis=-1) # \"-1\" means last axis Out[37]: <pre>&lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\narray([[[10],\n        [ 7]],\n\n       [[ 3],\n        [ 4]]])&gt;</pre> In\u00a0[38]: Copied! <pre># You can add values to a tensor using the addition operator\ntensor = tf.constant([[10, 7], [3, 4]])\ntensor + 10\n</pre> # You can add values to a tensor using the addition operator tensor = tf.constant([[10, 7], [3, 4]]) tensor + 10 Out[38]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[20, 17],\n       [13, 14]])&gt;</pre> <p>Since we used <code>tf.constant()</code>, the original tensor is unchanged (the addition gets done on a copy).</p> In\u00a0[39]: Copied! <pre># Original tensor unchanged\ntensor\n</pre> # Original tensor unchanged tensor Out[39]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[10,  7],\n       [ 3,  4]])&gt;</pre> <p>Other operators also work.</p> In\u00a0[40]: Copied! <pre># Multiplication (known as element-wise multiplication)\ntensor * 10\n</pre> # Multiplication (known as element-wise multiplication) tensor * 10 Out[40]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[100,  70],\n       [ 30,  40]])&gt;</pre> In\u00a0[41]: Copied! <pre># Subtraction\ntensor - 10\n</pre> # Subtraction tensor - 10 Out[41]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 0, -3],\n       [-7, -6]])&gt;</pre> <p>You can also use the equivalent TensorFlow function. Using the TensorFlow function (where possible) has the advantage of being sped up later down the line when running as part of a TensorFlow graph.</p> In\u00a0[42]: Copied! <pre># Use the tensorflow function equivalent of the '*' (multiply) operator\ntf.multiply(tensor, 10)\n</pre> # Use the tensorflow function equivalent of the '*' (multiply) operator tf.multiply(tensor, 10) Out[42]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[100,  70],\n       [ 30,  40]])&gt;</pre> In\u00a0[43]: Copied! <pre># The original tensor is still unchanged\ntensor\n</pre> # The original tensor is still unchanged tensor Out[43]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[10,  7],\n       [ 3,  4]])&gt;</pre> In\u00a0[44]: Copied! <pre># Matrix multiplication in TensorFlow\nprint(tensor)\ntf.matmul(tensor, tensor)\n</pre> # Matrix multiplication in TensorFlow print(tensor) tf.matmul(tensor, tensor) <pre>tf.Tensor(\n[[10  7]\n [ 3  4]], shape=(2, 2), dtype=int32)\n</pre> Out[44]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[121,  98],\n       [ 42,  37]])&gt;</pre> In\u00a0[45]: Copied! <pre># Matrix multiplication with Python operator '@'\ntensor @ tensor\n</pre> # Matrix multiplication with Python operator '@' tensor @ tensor Out[45]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[121,  98],\n       [ 42,  37]])&gt;</pre> <p>Both of these examples work because our <code>tensor</code> variable is of shape (2, 2).</p> <p>What if we created some tensors which had mismatched shapes?</p> In\u00a0[46]: Copied! <pre># Create (3, 2) tensor\nX = tf.constant([[1, 2],\n                 [3, 4],\n                 [5, 6]])\n\n# Create another (3, 2) tensor\nY = tf.constant([[7, 8],\n                 [9, 10],\n                 [11, 12]])\nX, Y\n</pre> # Create (3, 2) tensor X = tf.constant([[1, 2],                  [3, 4],                  [5, 6]])  # Create another (3, 2) tensor Y = tf.constant([[7, 8],                  [9, 10],                  [11, 12]]) X, Y Out[46]: <pre>(&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n array([[1, 2],\n        [3, 4],\n        [5, 6]])&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n array([[ 7,  8],\n        [ 9, 10],\n        [11, 12]])&gt;)</pre> In\u00a0[47]: Copied! <pre># Try to matrix multiply them (will error)\nX @ Y\n</pre> # Try to matrix multiply them (will error) X @ Y <pre>\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\nCell In[47], line 2\n      1 # Try to matrix multiply them (will error)\n----&gt; 2 X @ Y\n\nFile D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n    151 except Exception as e:\n    152   filtered_tb = _process_traceback_frames(e.__traceback__)\n--&gt; 153   raise e.with_traceback(filtered_tb) from None\n    154 finally:\n    155   del filtered_tb\n\nFile D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5983, in raise_from_not_ok_status(e, name)\n   5981 def raise_from_not_ok_status(e, name) -&gt; NoReturn:\n   5982   e.message += (\" name: \" + str(name if name is not None else \"\"))\n-&gt; 5983   raise core._status_to_exception(e) from None\n\nInvalidArgumentError: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [3,2], In[1]: [3,2] [Op:MatMul] name: </pre> <p>Trying to matrix multiply two tensors with the shape <code>(3, 2)</code> errors because the inner dimensions don't match.</p> <p>We need to either:</p> <ul> <li>Reshape X to <code>(2, 3)</code> so it's <code>(2, 3) @ (3, 2)</code>.</li> <li>Reshape Y to <code>(3, 2)</code> so it's <code>(3, 2) @ (2, 3)</code>.</li> </ul> <p>We can do this with either:</p> <ul> <li><code>tf.reshape()</code> - allows us to reshape a tensor into a defined shape.</li> <li><code>tf.transpose()</code> - switches the dimensions of a given tensor.</li> </ul> <p></p> <p>Let's try <code>tf.reshape()</code> first.</p> In\u00a0[48]: Copied! <pre># Example of reshape (3, 2) -&gt; (2, 3)\ntf.reshape(Y, shape=(2, 3))\n</pre> # Example of reshape (3, 2) -&gt; (2, 3) tf.reshape(Y, shape=(2, 3)) Out[48]: <pre>&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[ 7,  8,  9],\n       [10, 11, 12]])&gt;</pre> In\u00a0[49]: Copied! <pre># Try matrix multiplication with reshaped Y\nX @ tf.reshape(Y, shape=(2, 3))\n</pre> # Try matrix multiplication with reshaped Y X @ tf.reshape(Y, shape=(2, 3)) Out[49]: <pre>&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[ 27,  30,  33],\n       [ 61,  68,  75],\n       [ 95, 106, 117]])&gt;</pre> <p>It worked, let's try the same with a reshaped <code>X</code>, except this time we'll use <code>tf.transpose()</code> and <code>tf.matmul()</code>.</p> In\u00a0[50]: Copied! <pre># Example of transpose (3, 2) -&gt; (2, 3)\ntf.transpose(X)\n</pre> # Example of transpose (3, 2) -&gt; (2, 3) tf.transpose(X) Out[50]: <pre>&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 3, 5],\n       [2, 4, 6]])&gt;</pre> In\u00a0[51]: Copied! <pre># Try matrix multiplication \ntf.matmul(tf.transpose(X), Y)\n</pre> # Try matrix multiplication  tf.matmul(tf.transpose(X), Y) Out[51]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 89,  98],\n       [116, 128]])&gt;</pre> In\u00a0[52]: Copied! <pre># You can achieve the same result with parameters\ntf.matmul(a=X, b=Y, transpose_a=True, transpose_b=False)\n</pre> # You can achieve the same result with parameters tf.matmul(a=X, b=Y, transpose_a=True, transpose_b=False) Out[52]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 89,  98],\n       [116, 128]])&gt;</pre> <p>Notice the difference in the resulting shapes when tranposing <code>X</code> or reshaping <code>Y</code>.</p> <p>This is because of the 2nd rule mentioned above:</p> <ul> <li><code>(3, 2) @ (2, 3)</code> -&gt; <code>(3, 3)</code> done with <code>X @ tf.reshape(Y, shape=(2, 3))</code></li> <li><code>(2, 3) @ (3, 2)</code> -&gt; <code>(2, 2)</code> done with <code>tf.matmul(tf.transpose(X), Y)</code></li> </ul> <p>This kind of data manipulation is a reminder: you'll spend a lot of your time in machine learning and working with neural networks reshaping data (in the form of tensors) to prepare it to be used with various operations (such as feeding it to a model).</p> In\u00a0[60]: Copied! <pre># Perform the dot product on X and Y (requires X to be transposed)\ntf.tensordot(tf.transpose(X), Y, axes=1)\n</pre> # Perform the dot product on X and Y (requires X to be transposed) tf.tensordot(tf.transpose(X), Y, axes=1) Out[60]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 89,  98],\n       [116, 128]])&gt;</pre> <p>You might notice that although using both <code>reshape</code> and <code>tranpose</code> work, you get different results when using each.</p> <p>Let's see an example, first with <code>tf.transpose()</code> then with <code>tf.reshape()</code>.</p> In\u00a0[61]: Copied! <pre># Perform matrix multiplication between X and Y (transposed)\ntf.matmul(X, tf.transpose(Y))\n</pre> # Perform matrix multiplication between X and Y (transposed) tf.matmul(X, tf.transpose(Y)) Out[61]: <pre>&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[ 23,  29,  35],\n       [ 53,  67,  81],\n       [ 83, 105, 127]])&gt;</pre> In\u00a0[62]: Copied! <pre># Perform matrix multiplication between X and Y (reshaped)\ntf.matmul(X, tf.reshape(Y, (2, 3)))\n</pre> # Perform matrix multiplication between X and Y (reshaped) tf.matmul(X, tf.reshape(Y, (2, 3))) Out[62]: <pre>&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[ 27,  30,  33],\n       [ 61,  68,  75],\n       [ 95, 106, 117]])&gt;</pre> <p>Hmm... they result in different values.</p> <p>Which is strange because when dealing with <code>Y</code> (a <code>(3x2)</code> matrix), reshaping to <code>(2, 3)</code> and tranposing it result in the same shape.</p> In\u00a0[63]: Copied! <pre># Check shapes of Y, reshaped Y and tranposed Y\nY.shape, tf.reshape(Y, (2, 3)).shape, tf.transpose(Y).shape\n</pre> # Check shapes of Y, reshaped Y and tranposed Y Y.shape, tf.reshape(Y, (2, 3)).shape, tf.transpose(Y).shape Out[63]: <pre>(TensorShape([3, 2]), TensorShape([2, 3]), TensorShape([2, 3]))</pre> <p>But calling <code>tf.reshape()</code> and <code>tf.transpose()</code> on <code>Y</code> don't necessarily result in the same values.</p> In\u00a0[64]: Copied! <pre># Check values of Y, reshape Y and tranposed Y\nprint(\"Normal Y:\")\nprint(Y, \"\\n\") # \"\\n\" for newline\n\nprint(\"Y reshaped to (2, 3):\")\nprint(tf.reshape(Y, (2, 3)), \"\\n\")\n\nprint(\"Y transposed:\")\nprint(tf.transpose(Y))\n</pre> # Check values of Y, reshape Y and tranposed Y print(\"Normal Y:\") print(Y, \"\\n\") # \"\\n\" for newline  print(\"Y reshaped to (2, 3):\") print(tf.reshape(Y, (2, 3)), \"\\n\")  print(\"Y transposed:\") print(tf.transpose(Y)) <pre>Normal Y:\ntf.Tensor(\n[[ 7  8]\n [ 9 10]\n [11 12]], shape=(3, 2), dtype=int32) \n\nY reshaped to (2, 3):\ntf.Tensor(\n[[ 7  8  9]\n [10 11 12]], shape=(2, 3), dtype=int32) \n\nY transposed:\ntf.Tensor(\n[[ 7  9 11]\n [ 8 10 12]], shape=(2, 3), dtype=int32)\n</pre> <p>As you can see, the outputs of <code>tf.reshape()</code> and <code>tf.transpose()</code> when called on <code>Y</code>, even though they have the same shape, are different.</p> <p>This can be explained by the default behaviour of each method:</p> <ul> <li><code>tf.reshape()</code> - change the shape of the given tensor (first) and then insert values in order they appear (in our case, 7, 8, 9, 10, 11, 12).</li> <li><code>tf.transpose()</code> - swap the order of the axes, by default the last axis becomes the first, however the order can be changed using the <code>perm</code> parameter.</li> </ul> <p>So which should you use?</p> <p>Again, most of the time these operations (when they need to be run, such as during the training a neural network, will be implemented for you).</p> <p>But generally, whenever performing a matrix multiplication and the shapes of two matrices don't line up, you will transpose (not reshape) one of them in order to line them up.</p> In\u00a0[68]: Copied! <pre># Create a new tensor with default datatype (float32)\nB = tf.constant([1.7, 7.4])\n\n# Create a new tensor with default datatype (int32)\nC = tf.constant([1, 7])\nB, C\n</pre> # Create a new tensor with default datatype (float32) B = tf.constant([1.7, 7.4])  # Create a new tensor with default datatype (int32) C = tf.constant([1, 7]) B, C Out[68]: <pre>(&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.7, 7.4], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 7])&gt;)</pre> In\u00a0[69]: Copied! <pre># Change from float32 to float16 (reduced precision)\nB = tf.cast(B, dtype=tf.float16)\nB\n</pre> # Change from float32 to float16 (reduced precision) B = tf.cast(B, dtype=tf.float16) B Out[69]: <pre>&lt;tf.Tensor: shape=(2,), dtype=float16, numpy=array([1.7, 7.4], dtype=float16)&gt;</pre> In\u00a0[70]: Copied! <pre># Change from int32 to float32\nC = tf.cast(C, dtype=tf.float32)\nC\n</pre> # Change from int32 to float32 C = tf.cast(C, dtype=tf.float32) C Out[70]: <pre>&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 7.], dtype=float32)&gt;</pre> In\u00a0[71]: Copied! <pre># Create tensor with negative values\nD = tf.constant([-7, -10])\nD\n</pre> # Create tensor with negative values D = tf.constant([-7, -10]) D Out[71]: <pre>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ -7, -10])&gt;</pre> In\u00a0[72]: Copied! <pre># Get the absolute values\ntf.abs(D)\n</pre> # Get the absolute values tf.abs(D) Out[72]: <pre>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 7, 10])&gt;</pre> In\u00a0[73]: Copied! <pre># Create a tensor with 50 random values between 0 and 100\nE = tf.constant(np.random.randint(low=0, high=100, size=50))\nE\n</pre> # Create a tensor with 50 random values between 0 and 100 E = tf.constant(np.random.randint(low=0, high=100, size=50)) E Out[73]: <pre>&lt;tf.Tensor: shape=(50,), dtype=int32, numpy=\narray([ 3, 98, 57, 41, 33, 29, 95, 93, 61, 71, 64,  2,  3, 60, 93, 22, 50,\n       47, 35, 60, 26, 52, 38, 18, 64, 89, 98, 38, 78, 51,  4, 92, 45, 39,\n       77, 43, 92, 49, 26, 84,  3, 33, 70,  2, 43,  4, 87, 66, 21, 92])&gt;</pre> In\u00a0[74]: Copied! <pre># Find the minimum\ntf.reduce_min(E)\n</pre> # Find the minimum tf.reduce_min(E) Out[74]: <pre>&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;</pre> In\u00a0[75]: Copied! <pre># Find the maximum\ntf.reduce_max(E)\n</pre> # Find the maximum tf.reduce_max(E) Out[75]: <pre>&lt;tf.Tensor: shape=(), dtype=int32, numpy=98&gt;</pre> In\u00a0[76]: Copied! <pre># Find the mean\ntf.reduce_mean(E)\n</pre> # Find the mean tf.reduce_mean(E) Out[76]: <pre>&lt;tf.Tensor: shape=(), dtype=int32, numpy=50&gt;</pre> In\u00a0[77]: Copied! <pre># Find the sum\ntf.reduce_sum(E)\n</pre> # Find the sum tf.reduce_sum(E) Out[77]: <pre>&lt;tf.Tensor: shape=(), dtype=int32, numpy=2541&gt;</pre> <p>You can also find the standard deviation (<code>tf.reduce_std()</code>) and variance (<code>tf.reduce_variance()</code>) of elements in a tensor using similar methods.</p> In\u00a0[78]: Copied! <pre># Create a tensor with 50 values between 0 and 1\nF = tf.constant(np.random.random(50))\nF\n</pre> # Create a tensor with 50 values between 0 and 1 F = tf.constant(np.random.random(50)) F Out[78]: <pre>&lt;tf.Tensor: shape=(50,), dtype=float64, numpy=\narray([0.33963509, 0.9672647 , 0.43272587, 0.27532735, 0.00204467,\n       0.25983684, 0.25954456, 0.1435441 , 0.97031337, 0.26929533,\n       0.14032421, 0.9082024 , 0.47425121, 0.77826693, 0.81446725,\n       0.67483163, 0.79334419, 0.87420022, 0.18274336, 0.31707395,\n       0.24255584, 0.31444067, 0.05813784, 0.23171223, 0.65060714,\n       0.80601657, 0.27223314, 0.00410714, 0.92168893, 0.35474602,\n       0.69438562, 0.85685611, 0.35194301, 0.00165329, 0.93576894,\n       0.08478894, 0.30160467, 0.88888045, 0.38154264, 0.62620644,\n       0.28666214, 0.00113414, 0.8597382 , 0.73687862, 0.26745934,\n       0.43648347, 0.28677194, 0.43218163, 0.05640713, 0.68271093])&gt;</pre> In\u00a0[79]: Copied! <pre># Find the maximum element position of F\ntf.argmax(F)\n</pre> # Find the maximum element position of F tf.argmax(F) Out[79]: <pre>&lt;tf.Tensor: shape=(), dtype=int64, numpy=8&gt;</pre> In\u00a0[80]: Copied! <pre># Find the minimum element position of F\ntf.argmin(F)\n</pre> # Find the minimum element position of F tf.argmin(F) Out[80]: <pre>&lt;tf.Tensor: shape=(), dtype=int64, numpy=41&gt;</pre> In\u00a0[81]: Copied! <pre># Find the maximum element position of F\nprint(f\"The maximum value of F is at position: {tf.argmax(F).numpy()}\") \nprint(f\"The maximum value of F is: {tf.reduce_max(F).numpy()}\") \nprint(f\"Using tf.argmax() to index F, the maximum value of F is: {F[tf.argmax(F)].numpy()}\")\nprint(f\"Are the two max values the same (they should be)? {F[tf.argmax(F)].numpy() == tf.reduce_max(F).numpy()}\")\n</pre> # Find the maximum element position of F print(f\"The maximum value of F is at position: {tf.argmax(F).numpy()}\")  print(f\"The maximum value of F is: {tf.reduce_max(F).numpy()}\")  print(f\"Using tf.argmax() to index F, the maximum value of F is: {F[tf.argmax(F)].numpy()}\") print(f\"Are the two max values the same (they should be)? {F[tf.argmax(F)].numpy() == tf.reduce_max(F).numpy()}\") <pre>The maximum value of F is at position: 8\nThe maximum value of F is: 0.9703133683461462\nUsing tf.argmax() to index F, the maximum value of F is: 0.9703133683461462\nAre the two max values the same (they should be)? True\n</pre> In\u00a0[82]: Copied! <pre># Create a rank 5 (5 dimensions) tensor of 50 numbers between 0 and 100\nG = tf.constant(np.random.randint(0, 100, 50), shape=(1, 1, 1, 1, 50))\nG.shape, G.ndim\n</pre> # Create a rank 5 (5 dimensions) tensor of 50 numbers between 0 and 100 G = tf.constant(np.random.randint(0, 100, 50), shape=(1, 1, 1, 1, 50)) G.shape, G.ndim Out[82]: <pre>(TensorShape([1, 1, 1, 1, 50]), 5)</pre> In\u00a0[83]: Copied! <pre># Squeeze tensor G (remove all 1 dimensions)\nG_squeezed = tf.squeeze(G)\nG_squeezed.shape, G_squeezed.ndim\n</pre> # Squeeze tensor G (remove all 1 dimensions) G_squeezed = tf.squeeze(G) G_squeezed.shape, G_squeezed.ndim Out[83]: <pre>(TensorShape([50]), 1)</pre> In\u00a0[84]: Copied! <pre># Create a list of indices\nsome_list = [0, 1, 2, 3]\n\n# One hot encode them\ntf.one_hot(some_list, depth=4)\n</pre> # Create a list of indices some_list = [0, 1, 2, 3]  # One hot encode them tf.one_hot(some_list, depth=4) Out[84]: <pre>&lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]], dtype=float32)&gt;</pre> <p>You can also specify values for <code>on_value</code> and <code>off_value</code> instead of the default <code>0</code> and <code>1</code>.</p> In\u00a0[85]: Copied! <pre># Specify custom values for on and off encoding\ntf.one_hot(some_list, depth=4, on_value=\"We're live!\", off_value=\"Offline\")\n</pre> # Specify custom values for on and off encoding tf.one_hot(some_list, depth=4, on_value=\"We're live!\", off_value=\"Offline\") Out[85]: <pre>&lt;tf.Tensor: shape=(4, 4), dtype=string, numpy=\narray([[b\"We're live!\", b'Offline', b'Offline', b'Offline'],\n       [b'Offline', b\"We're live!\", b'Offline', b'Offline'],\n       [b'Offline', b'Offline', b\"We're live!\", b'Offline'],\n       [b'Offline', b'Offline', b'Offline', b\"We're live!\"]], dtype=object)&gt;</pre> In\u00a0[86]: Copied! <pre># Create a new tensor\nH = tf.constant(np.arange(1, 10))\nH\n</pre> # Create a new tensor H = tf.constant(np.arange(1, 10)) H Out[86]: <pre>&lt;tf.Tensor: shape=(9,), dtype=int32, numpy=array([1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;</pre> In\u00a0[87]: Copied! <pre># Square it\ntf.square(H)\n</pre> # Square it tf.square(H) Out[87]: <pre>&lt;tf.Tensor: shape=(9,), dtype=int32, numpy=array([ 1,  4,  9, 16, 25, 36, 49, 64, 81])&gt;</pre> In\u00a0[88]: Copied! <pre># Find the squareroot (will error), needs to be non-integer\ntf.sqrt(H)\n</pre> # Find the squareroot (will error), needs to be non-integer tf.sqrt(H) <pre>\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\nCell In[88], line 2\n      1 # Find the squareroot (will error), needs to be non-integer\n----&gt; 2 tf.sqrt(H)\n\nFile D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88, in weak_tensor_unary_op_wrapper.&lt;locals&gt;.wrapper(*args, **kwargs)\n     86 def wrapper(*args, **kwargs):\n     87   if not ops.is_auto_dtype_conversion_enabled():\n---&gt; 88     return op(*args, **kwargs)\n     89   bound_arguments = signature.bind(*args, **kwargs)\n     90   bound_arguments.apply_defaults()\n\nFile D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n    151 except Exception as e:\n    152   filtered_tb = _process_traceback_frames(e.__traceback__)\n--&gt; 153   raise e.with_traceback(filtered_tb) from None\n    154 finally:\n    155   del filtered_tb\n\nFile D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5983, in raise_from_not_ok_status(e, name)\n   5981 def raise_from_not_ok_status(e, name) -&gt; NoReturn:\n   5982   e.message += (\" name: \" + str(name if name is not None else \"\"))\n-&gt; 5983   raise core._status_to_exception(e) from None\n\nInvalidArgumentError: Value for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\n\t; NodeDef: {{node Sqrt}}; Op&lt;name=Sqrt; signature=x:T -&gt; y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:Sqrt] name: </pre> In\u00a0[89]: Copied! <pre># Change H to float32\nH = tf.cast(H, dtype=tf.float32)\nH\n</pre> # Change H to float32 H = tf.cast(H, dtype=tf.float32) H Out[89]: <pre>&lt;tf.Tensor: shape=(9,), dtype=float32, numpy=array([1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)&gt;</pre> In\u00a0[90]: Copied! <pre># Find the square root\ntf.sqrt(H)\n</pre> # Find the square root tf.sqrt(H) Out[90]: <pre>&lt;tf.Tensor: shape=(9,), dtype=float32, numpy=\narray([1.       , 1.4142135, 1.7320508, 2.       , 2.236068 , 2.4494898,\n       2.6457512, 2.828427 , 3.       ], dtype=float32)&gt;</pre> In\u00a0[91]: Copied! <pre># Find the log (input also needs to be float)\ntf.math.log(H)\n</pre> # Find the log (input also needs to be float) tf.math.log(H) Out[91]: <pre>&lt;tf.Tensor: shape=(9,), dtype=float32, numpy=\narray([0.       , 0.6931472, 1.0986123, 1.3862944, 1.609438 , 1.7917595,\n       1.9459102, 2.0794415, 2.1972246], dtype=float32)&gt;</pre> In\u00a0[92]: Copied! <pre># Create a variable tensor\nI = tf.Variable(np.arange(0, 5))\nI\n</pre> # Create a variable tensor I = tf.Variable(np.arange(0, 5)) I Out[92]: <pre>&lt;tf.Variable 'Variable:0' shape=(5,) dtype=int32, numpy=array([0, 1, 2, 3, 4])&gt;</pre> In\u00a0[93]: Copied! <pre># Assign the final value a new value of 50\nI.assign([0, 1, 2, 3, 50])\n</pre> # Assign the final value a new value of 50 I.assign([0, 1, 2, 3, 50]) Out[93]: <pre>&lt;tf.Variable 'UnreadVariable' shape=(5,) dtype=int32, numpy=array([ 0,  1,  2,  3, 50])&gt;</pre> In\u00a0[94]: Copied! <pre># The change happens in place (the last value is now 50, not 4)\nI\n</pre> # The change happens in place (the last value is now 50, not 4) I Out[94]: <pre>&lt;tf.Variable 'Variable:0' shape=(5,) dtype=int32, numpy=array([ 0,  1,  2,  3, 50])&gt;</pre> In\u00a0[95]: Copied! <pre># Add 10 to every element in I\nI.assign_add([10, 10, 10, 10, 10])\n</pre> # Add 10 to every element in I I.assign_add([10, 10, 10, 10, 10]) Out[95]: <pre>&lt;tf.Variable 'UnreadVariable' shape=(5,) dtype=int32, numpy=array([10, 11, 12, 13, 60])&gt;</pre> In\u00a0[96]: Copied! <pre># Again, the change happens in place\nI\n</pre> # Again, the change happens in place I Out[96]: <pre>&lt;tf.Variable 'Variable:0' shape=(5,) dtype=int32, numpy=array([10, 11, 12, 13, 60])&gt;</pre> In\u00a0[97]: Copied! <pre># Create a tensor from a NumPy array\nJ = tf.constant(np.array([3., 7., 10.]))\nJ\n</pre> # Create a tensor from a NumPy array J = tf.constant(np.array([3., 7., 10.])) J Out[97]: <pre>&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 3.,  7., 10.])&gt;</pre> In\u00a0[98]: Copied! <pre># Convert tensor J to NumPy with np.array()\nnp.array(J), type(np.array(J))\n</pre> # Convert tensor J to NumPy with np.array() np.array(J), type(np.array(J)) Out[98]: <pre>(array([ 3.,  7., 10.]), numpy.ndarray)</pre> In\u00a0[99]: Copied! <pre># Convert tensor J to NumPy with .numpy()\nJ.numpy(), type(J.numpy())\n</pre> # Convert tensor J to NumPy with .numpy() J.numpy(), type(J.numpy()) Out[99]: <pre>(array([ 3.,  7., 10.]), numpy.ndarray)</pre> <p>By default tensors have <code>dtype=float32</code>, where as NumPy arrays have <code>dtype=float64</code>.</p> <p>This is because neural networks (which are usually built with TensorFlow) can generally work very well with less precision (32-bit rather than 64-bit).</p> In\u00a0[100]: Copied! <pre># Create a tensor from NumPy and from an array\nnumpy_J = tf.constant(np.array([3., 7., 10.])) # will be float64 (due to NumPy)\ntensor_J = tf.constant([3., 7., 10.]) # will be float32 (due to being TensorFlow default)\nnumpy_J.dtype, tensor_J.dtype\n</pre> # Create a tensor from NumPy and from an array numpy_J = tf.constant(np.array([3., 7., 10.])) # will be float64 (due to NumPy) tensor_J = tf.constant([3., 7., 10.]) # will be float32 (due to being TensorFlow default) numpy_J.dtype, tensor_J.dtype Out[100]: <pre>(tf.float64, tf.float32)</pre> In\u00a0[101]: Copied! <pre># Create a simple function\ndef function(x, y):\n  return x ** 2 + y\n\nx = tf.constant(np.arange(0, 10))\ny = tf.constant(np.arange(10, 20))\nfunction(x, y)\n</pre> # Create a simple function def function(x, y):   return x ** 2 + y  x = tf.constant(np.arange(0, 10)) y = tf.constant(np.arange(10, 20)) function(x, y) Out[101]: <pre>&lt;tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 10,  12,  16,  22,  30,  40,  52,  66,  82, 100])&gt;</pre> In\u00a0[102]: Copied! <pre># Create the same function and decorate it with tf.function\n@tf.function\ndef tf_function(x, y):\n  return x ** 2 + y\n\nx = np.arange(0, 10)\ny = np.arange(10, 20)\ntf_function(x, y)\n</pre> # Create the same function and decorate it with tf.function @tf.function def tf_function(x, y):   return x ** 2 + y  x = np.arange(0, 10) y = np.arange(10, 20) tf_function(x, y) Out[102]: <pre>&lt;tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 10,  12,  16,  22,  30,  40,  52,  66,  82, 100])&gt;</pre> <p>If you noticed no difference between the above two functions (the decorated one and the non-decorated one) you'd be right.</p> <p>Much of the difference happens behind the scenes. One of the main ones being potential code speed-ups where possible.</p> In\u00a0[103]: Copied! <pre>print(tf.config.list_physical_devices('GPU'))\n</pre> print(tf.config.list_physical_devices('GPU')) <pre>[]\n</pre> <p>If the above outputs an empty array (or nothing), it means you don't have access to a GPU (or at least TensorFlow can't find it).</p> <p>If you're running in Google Colab, you can access a GPU by going to Runtime -&gt; Change Runtime Type -&gt; Select GPU (note: after doing this your notebook will restart and any variables you've saved will be lost).</p> <p>Once you've changed your runtime type, run the cell below.</p> In\u00a0[104]: Copied! <pre>import tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))\n</pre> import tensorflow as tf print(tf.config.list_physical_devices('GPU')) <pre>[]\n</pre> <p>If you've got access to a GPU, the cell above should output something like:</p> <p><code>[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]</code></p> <p>You can also find information about your GPU using <code>!nvidia-smi</code>.</p> In\u00a0[105]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Thu Apr 18 16:24:12 2024       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 517.48       Driver Version: 517.48       CUDA Version: 11.7     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n| N/A   45C    P8     2W /  N/A |      0MiB /  4096MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</pre> <p>\ud83d\udd11 Note: If you have access to a GPU, TensorFlow will automatically use it whenever possible.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#00-getting-started-with-tensorflow-a-guide-to-the-fundamentals","title":"00. Getting started with TensorFlow: A guide to the fundamentals\u00b6","text":""},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#what-is-tensorflow","title":"What is TensorFlow?\u00b6","text":"<p>TensorFlow is an open-source end-to-end machine learning library for preprocessing data, modelling data and serving models (getting them into the hands of others).</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#why-use-tensorflow","title":"Why use TensorFlow?\u00b6","text":"<p>Rather than building machine learning and deep learning models from scratch, it's more likely you'll use a library such as TensorFlow. This is because it contains many of the most common machine learning functions you'll want to use.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>TensorFlow is vast. But the main premise is simple: turn data into numbers (tensors) and build machine learning algorithms to find patterns in them.</p> <p>In this notebook we cover some of the most fundamental TensorFlow operations, more specificially:</p> <ul> <li>Introduction to tensors (creating tensors)</li> <li>Getting information from tensors (tensor attributes)</li> <li>Manipulating tensors (tensor operations)</li> <li>Tensors and NumPy</li> <li>Using @tf.function (a way to speed up your regular Python functions)</li> <li>Using GPUs with TensorFlow</li> <li>Exercises to try</li> </ul> <p>Things to note:</p> <ul> <li>Many of the conventions here will happen automatically behind the scenes (when you build a model) but it's worth knowing so if you see any of these things, you know what's happening.</li> <li>For any TensorFlow function you see, it's important to be able to check it out in the documentation, for example, going to the Python API docs for all functions and searching for what you need: https://www.tensorflow.org/api_docs/python/ (don't worry if this seems overwhelming at first, with enough practice, you'll get used to navigating the documentaiton).</li> </ul>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#introduction-to-tensors","title":"Introduction to Tensors\u00b6","text":"<p>If you've ever used NumPy, tensors are kind of like NumPy arrays (we'll see more on this later).</p> <p>For the sake of this notebook and going forward, you can think of a tensor as a multi-dimensional numerical representation (also referred to as n-dimensional, where n can be any number) of something. Where something can be almost anything you can imagine:</p> <ul> <li>It could be numbers themselves (using tensors to represent the price of houses).</li> <li>It could be an image (using tensors to represent the pixels of an image).</li> <li>It could be text (using tensors to represent words).</li> <li>Or it could be some other form of information (or data) you want to represent with numbers.</li> </ul> <p>The main difference between tensors and NumPy arrays (also an n-dimensional array of numbers) is that tensors can be used on GPUs (graphical processing units) and TPUs (tensor processing units).</p> <p>The benefit of being able to run on GPUs and TPUs is faster computation, this means, if we wanted to find patterns in the numerical representations of our data, we can generally find them faster using GPUs and TPUs.</p> <p>Okay, we've been talking enough about tensors, let's see them.</p> <p>The first thing we'll do is import TensorFlow under the common alias <code>tf</code>.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#creating-tensors-with-tfconstant","title":"Creating Tensors with <code>tf.constant()</code>\u00b6","text":"<p>As mentioned before, in general, you usually won't create tensors yourself. This is because TensorFlow has modules built-in (such as <code>tf.io</code> and <code>tf.data</code>) which are able to read your data sources and automatically convert them to tensors and then later on, neural network models will process these for us.</p> <p>But for now, because we're getting familar with tensors themselves and how to manipulate them, we'll see how we can create them ourselves.</p> <p>We'll begin by using <code>tf.constant()</code>.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#creating-tensors-with-tfvariable","title":"Creating Tensors with <code>tf.Variable()</code>\u00b6","text":"<p>You can also (although you likely rarely will, because often, when working with data, tensors are created for you automatically) create tensors using <code>tf.Variable()</code>.</p> <p>The difference between <code>tf.Variable()</code> and <code>tf.constant()</code> is tensors created with <code>tf.constant()</code> are immutable (can't be changed, can only be used to create a new tensor), where as, tensors created with <code>tf.Variable()</code> are mutable (can be changed).</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#creating-random-tensors","title":"Creating random tensors\u00b6","text":"<p>Random tensors are tensors of some abitrary size which contain random numbers.</p> <p>Why would you want to create random tensors?</p> <p>This is what neural networks use to intialize their weights (patterns) that they're trying to learn in the data.</p> <p>For example, the process of a neural network learning often involves taking a random n-dimensional array of numbers and refining them until they represent some kind of pattern (a compressed way to represent the original data).</p> <p>How a network learns A network learns by starting with random patterns (1) then going through demonstrative examples of data (2) whilst trying to update its random patterns to represent the examples (3).</p> <p>We can create random tensors by using the <code>tf.random.Generator</code> class.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#other-ways-to-make-tensors","title":"Other ways to make tensors\u00b6","text":"<p>Though you might rarely use these (remember, many tensor operations are done behind the scenes for you), you can use <code>tf.ones()</code> to create a tensor of all ones and <code>tf.zeros()</code> to create a tensor of all zeros.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#getting-information-from-tensors-shape-rank-size","title":"Getting information from tensors (shape, rank, size)\u00b6","text":"<p>There will be times when you'll want to get different pieces of information from your tensors, in particuluar, you should know the following tensor vocabulary:</p> <ul> <li>Shape: The length (number of elements) of each of the dimensions of a tensor.</li> <li>Rank: The number of tensor dimensions. A scalar has rank 0, a vector has rank 1, a matrix is rank 2, a tensor has rank n.</li> <li>Axis or Dimension: A particular dimension of a tensor.</li> <li>Size: The total number of items in the tensor.</li> </ul> <p>You'll use these especially when you're trying to line up the shapes of your data to the shapes of your model. For example, making sure the shape of your image tensors are the same shape as your models input layer.</p> <p>We've already seen one of these before using the <code>ndim</code> attribute. Let's see the rest.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#manipulating-tensors-tensor-operations","title":"Manipulating tensors (tensor operations)\u00b6","text":"<p>Finding patterns in tensors (numberical representation of data) requires manipulating them.</p> <p>Again, when building models in TensorFlow, much of this pattern discovery is done for you.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#basic-operations","title":"Basic operations\u00b6","text":"<p>You can perform many of the basic mathematical operations directly on tensors using Pyhton operators such as, <code>+</code>, <code>-</code>, <code>*</code>.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#matrix-mutliplication","title":"Matrix mutliplication\u00b6","text":"<p>One of the most common operations in machine learning algorithms is matrix multiplication.</p> <p>TensorFlow implements this matrix multiplication functionality in the <code>tf.matmul()</code> method.</p> <p>The main two rules for matrix multiplication to remember are:</p> <ol> <li>The inner dimensions must match:</li> </ol> <ul> <li><code>(3, 5) @ (3, 5)</code> won't work</li> <li><code>(5, 3) @ (3, 5)</code> will work</li> <li><code>(3, 5) @ (5, 3)</code> will work</li> </ul> <ol> <li>The resulting matrix has the shape of the outer dimensions:</li> </ol> <ul> <li><code>(5, 3) @ (3, 5)</code> -&gt; <code>(5, 5)</code></li> <li><code>(3, 5) @ (5, 3)</code> -&gt; <code>(3, 3)</code></li> </ul> <p>\ud83d\udd11 Note: '<code>@</code>' in Python is the symbol for matrix multiplication.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#the-dot-product","title":"The dot product\u00b6","text":"<p>Multiplying matrices by eachother is also referred to as the dot product.</p> <p>You can perform the <code>tf.matmul()</code> operation using <code>tf.tensordot()</code>.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#matrix-multiplication-tidbits","title":"Matrix multiplication tidbits\u00b6","text":"<ul> <li>If we transposed <code>Y</code>, it would be represented as $\\mathbf{Y}^\\mathsf{T}$ (note the capital T for tranpose).</li> <li>Get an illustrative view of matrix multiplication by Math is Fun.</li> <li>Try a hands-on demo of matrix multiplcation: http://matrixmultiplication.xyz/ (shown below).</li> </ul>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#changing-the-datatype-of-a-tensor","title":"Changing the datatype of a tensor\u00b6","text":"<p>Sometimes you'll want to alter the default datatype of your tensor.</p> <p>This is common when you want to compute using less precision (e.g. 16-bit floating point numbers vs. 32-bit floating point numbers).</p> <p>Computing with less precision is useful on devices with less computing capacity such as mobile devices (because the less bits, the less space the computations require).</p> <p>You can change the datatype of a tensor using <code>tf.cast()</code>.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#getting-the-absolute-value","title":"Getting the absolute value\u00b6","text":"<p>Sometimes you'll want the absolute values (all values are positive) of elements in your tensors.</p> <p>To do so, you can use <code>tf.abs()</code>.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#finding-the-min-max-mean-sum-aggregation","title":"Finding the min, max, mean, sum (aggregation)\u00b6","text":"<p>You can quickly aggregate (perform a calculation on a whole tensor) tensors to find things like the minimum value, maximum value, mean and sum of all the elements.</p> <p>To do so, aggregation methods typically have the syntax <code>reduce()_[action]</code>, such as:</p> <ul> <li><code>tf.reduce_min()</code> - find the minimum value in a tensor.</li> <li><code>tf.reduce_max()</code> - find the maximum value in a tensor (helpful for when you want to find the highest prediction probability).</li> <li><code>tf.reduce_mean()</code> - find the mean of all elements in a tensor.</li> <li><code>tf.reduce_sum()</code> - find the sum of all elements in a tensor.</li> <li>Note: typically, each of these is under the <code>math</code> module, e.g. <code>tf.math.reduce_min()</code> but you can use the alias <code>tf.reduce_min()</code>.</li> </ul> <p>Let's see them in action.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#finding-the-positional-maximum-and-minimum","title":"Finding the positional maximum and minimum\u00b6","text":"<p>How about finding the position a tensor where the maximum value occurs?</p> <p>This is helpful when you want to line up your labels (say <code>['Green', 'Blue', 'Red']</code>) with your prediction probabilities tensor (e.g. <code>[0.98, 0.01, 0.01]</code>).</p> <p>In this case, the predicted label (the one with the highest prediction probability) would be <code>'Green'</code>.</p> <p>You can do the same for the minimum (if required) with the following:</p> <ul> <li><code>tf.argmax()</code> - find the position of the maximum element in a given tensor.</li> <li><code>tf.argmin()</code> - find the position of the minimum element in a given tensor.</li> </ul>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#squeezing-a-tensor-removing-all-single-dimensions","title":"Squeezing a tensor (removing all single dimensions)\u00b6","text":"<p>If you need to remove single-dimensions from a tensor (dimensions with size 1), you can use <code>tf.squeeze()</code>.</p> <ul> <li><code>tf.squeeze()</code> - remove all dimensions of 1 from a tensor.</li> </ul>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#one-hot-encoding","title":"One-hot encoding\u00b6","text":"<p>If you have a tensor of indicies and would like to one-hot encode it, you can use <code>tf.one_hot()</code>.</p> <p>You should also specify the <code>depth</code> parameter (the level which you want to one-hot encode to).</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#squaring-log-square-root","title":"Squaring, log, square root\u00b6","text":"<p>Many other common mathematical operations you'd like to perform at some stage, probably exist.</p> <p>Let's take a look at:</p> <ul> <li><code>tf.square()</code> - get the square of every value in a tensor.</li> <li><code>tf.sqrt()</code> - get the squareroot of every value in a tensor (note: the elements need to be floats or this will error).</li> <li><code>tf.math.log()</code> - get the natural log of every value in a tensor (elements need to floats).</li> </ul>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#manipulating-tfvariable-tensors","title":"Manipulating <code>tf.Variable</code> tensors\u00b6","text":"<p>Tensors created with <code>tf.Variable()</code> can be changed in place using methods such as:</p> <ul> <li><code>.assign()</code> - assign a different value to a particular index of a variable tensor.</li> <li><code>.add_assign()</code> - add to an existing value and reassign it at a particular index of a variable tensor.</li> </ul>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#tensors-and-numpy","title":"Tensors and NumPy\u00b6","text":"<p>We've seen some examples of tensors interact with NumPy arrays, such as, using NumPy arrays to create tensors.</p> <p>Tensors can also be converted to NumPy arrays using:</p> <ul> <li><code>np.array()</code> - pass a tensor to convert to an ndarray (NumPy's main datatype).</li> <li><code>tensor.numpy()</code> - call on a tensor to convert to an ndarray.</li> </ul> <p>Doing this is helpful as it makes tensors iterable as well as allows us to use any of NumPy's methods on them.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#using-tffunction","title":"Using <code>@tf.function</code>\u00b6","text":"<p>In your TensorFlow adventures, you might come across Python functions which have the decorator <code>@tf.function</code>.</p> <p>If you aren't sure what Python decorators do, read RealPython's guide on them.</p> <p>But in short, decorators modify a function in one way or another.</p> <p>In the <code>@tf.function</code> decorator case, it turns a Python function into a callable TensorFlow graph. Which is a fancy way of saying, if you've written your own Python function, and you decorate it with <code>@tf.function</code>, when you export your code (to potentially run on another device), TensorFlow will attempt to convert it into a fast(er) version of itself (by making it part of a computation graph).</p> <p>For more on this, read the Better performnace with tf.function guide.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#finding-access-to-gpus","title":"Finding access to GPUs\u00b6","text":"<p>We've mentioned GPUs plenty of times throughout this notebook.</p> <p>So how do you check if you've got one available?</p> <p>You can check if you've got access to a GPU using <code>tf.config.list_physical_devices()</code>.</p>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Create a vector, scalar, matrix and tensor with values of your choosing using <code>tf.constant()</code>.</li> <li>Find the shape, rank and size of the tensors you created in 1.</li> <li>Create two tensors containing random values between 0 and 1 with shape <code>[5, 300]</code>.</li> <li>Multiply the two tensors you created in 3 using matrix multiplication.</li> <li>Multiply the two tensors you created in 3 using dot product.</li> <li>Create a tensor with random values between 0 and 1 with shape <code>[224, 224, 3]</code>.</li> <li>Find the min and max values of the tensor you created in 6.</li> <li>Created a tensor with random values of shape <code>[1, 224, 224, 3]</code> then squeeze it to change the shape to <code>[224, 224, 3]</code>.</li> <li>Create a tensor with shape <code>[10]</code> using your own choice of values, then find the index which has the maximum value.</li> <li>One-hot encode the tensor you created in 9.</li> </ol>"},{"location":"Learning/Tensorflow/00_tensorflow_fundamentals/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<ul> <li>Read through the list of TensorFlow Python APIs, pick one we haven't gone through in this notebook, reverse engineer it (write out the documentation code for yourself) and figure out what it does.</li> <li>Try to create a series of tensor functions to calculate your most recent grocery bill (it's okay if you don't use the names of the items, just the price in numerical form).<ul> <li>How would you calculate your grocery bill for the month and for the year using tensors?</li> </ul> </li> <li>Go through the TensorFlow 2.x quick start for beginners tutorial (be sure to type out all of the code yourself, even if you don't understand it).<ul> <li>Are there any functions we used in here that match what's used in there? Which are the same? Which haven't you seen before?</li> </ul> </li> <li>Watch the video \"What's a tensor?\" - a great visual introduction to many of the concepts we've covered in this notebook.</li> </ul>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/","title":"01. Neural Network Regression with TensorFlow","text":"In\u00a0[1]: Copied! <pre>import tensorflow as tf\nprint(tf.__version__) # check the version (should be 2.x+)\n\nimport datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> import tensorflow as tf print(tf.__version__) # check the version (should be 2.x+)  import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>2.17.0-dev20240226\nNotebook last run (end-to-end): 2024-04-18 17:55:36.483369\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create features\nX = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n\n# Create labels\ny = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n\n# Visualize it\nplt.scatter(X, y);\n</pre> import numpy as np import matplotlib.pyplot as plt  # Create features X = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])  # Create labels y = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])  # Visualize it plt.scatter(X, y); <p>Before we do any modelling, can you calculate the pattern between <code>X</code> and <code>y</code>?</p> <p>For example, say I asked you, based on this data what the <code>y</code> value would be if <code>X</code> was 17.0?</p> <p>Or how about if <code>X</code> was -10.0?</p> <p>This kind of pattern discovery is the essence of what we'll be building neural networks to do for us.</p> In\u00a0[3]: Copied! <pre># Example input and output shapes of a regression model\nhouse_info = tf.constant([\"bedroom\", \"bathroom\", \"garage\"])\nhouse_price = tf.constant([939700])\nhouse_info, house_price\n</pre> # Example input and output shapes of a regression model house_info = tf.constant([\"bedroom\", \"bathroom\", \"garage\"]) house_price = tf.constant([939700]) house_info, house_price Out[3]: <pre>(&lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b'bedroom', b'bathroom', b'garage'], dtype=object)&gt;,\n &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([939700])&gt;)</pre> In\u00a0[4]: Copied! <pre>house_info.shape\n</pre> house_info.shape Out[4]: <pre>TensorShape([3])</pre> In\u00a0[5]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create features (using tensors)\nX = tf.constant([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n\n# Create labels (using tensors)\ny = tf.constant([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n\n# Visualize it\nplt.scatter(X, y);\n</pre> import numpy as np import matplotlib.pyplot as plt  # Create features (using tensors) X = tf.constant([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])  # Create labels (using tensors) y = tf.constant([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])  # Visualize it plt.scatter(X, y); <p>Our goal here will be to use <code>X</code> to predict <code>y</code>.</p> <p>So our input will be <code>X</code> and our output will be <code>y</code>.</p> <p>Knowing this, what do you think our input and output shapes will be?</p> <p>Let's take a look.</p> In\u00a0[6]: Copied! <pre># Take a single example of X\ninput_shape = X[0].shape \n\n# Take a single example of y\noutput_shape = y[0].shape\n\ninput_shape, output_shape # these are both scalars (no shape)\n</pre> # Take a single example of X input_shape = X[0].shape   # Take a single example of y output_shape = y[0].shape  input_shape, output_shape # these are both scalars (no shape) Out[6]: <pre>(TensorShape([]), TensorShape([]))</pre> <p>Huh?</p> <p>From this it seems our inputs and outputs have no shape?</p> <p>How could that be?</p> <p>It's because no matter what kind of data we pass to our model, it's always going to take as input and return as output some kind of tensor.</p> <p>But in our case because of our dataset (only 2 small lists of numbers), we're looking at a special kind of tensor, more specifically a rank 0 tensor or a scalar.</p> In\u00a0[7]: Copied! <pre># Let's take a look at the single examples invidually\nX[0], y[0]\n</pre> # Let's take a look at the single examples invidually X[0], y[0] Out[7]: <pre>(&lt;tf.Tensor: shape=(), dtype=float32, numpy=-7.0&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;)</pre> <p>In our case, we're trying to build a model to predict the pattern between <code>X[0]</code> equalling <code>-7.0</code> and <code>y[0]</code> equalling <code>3.0</code>.</p> <p>So now we get our answer, we're trying to use 1 <code>X</code> value to predict 1 <code>y</code> value.</p> <p>You might be thinking, \"this seems pretty complicated for just predicting a straight line...\".</p> <p>And you'd be right.</p> <p>But the concepts we're covering here, the concepts of input and output shapes to a model are fundamental.</p> <p>In fact, they're probably two of the things you'll spend the most time on when you work with neural networks: making sure your input and outputs are in the correct shape.</p> <p>If it doesn't make sense now, we'll see plenty more examples later on (soon you'll notice the input and output shapes can be almost anything you can imagine).</p> <p> If you were working on building a machine learning algorithm for predicting housing prices, your inputs may be number of bedrooms, number of bathrooms and number of garages, giving you an input shape of 3 (3 different features). And since you're trying to predict the price of the house, your output shape would be 1.</p> In\u00a0[8]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model using the Sequential API\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error\n              optimizer=tf.keras.optimizers.SGD(), # SGD is short for stochastic gradient descent\n              metrics=[\"mae\"])\n\n# Fit the model\n# model.fit(X, y, epochs=5) # this will break with TensorFlow 2.7.0+\nmodel.fit(tf.expand_dims(X, axis=-1), y, epochs=5)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model using the Sequential API model = tf.keras.Sequential([   tf.keras.layers.Dense(1) ])  # Compile the model model.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error               optimizer=tf.keras.optimizers.SGD(), # SGD is short for stochastic gradient descent               metrics=[\"mae\"])  # Fit the model # model.fit(X, y, epochs=5) # this will break with TensorFlow 2.7.0+ model.fit(tf.expand_dims(X, axis=-1), y, epochs=5) <pre>Epoch 1/5\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 382ms/step - loss: 12.8714 - mae: 12.8714\nEpoch 2/5\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 33ms/step - loss: 12.7389 - mae: 12.7389\nEpoch 3/5\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 12.6064 - mae: 12.6064\nEpoch 4/5\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 12.4739 - mae: 12.4739\nEpoch 5/5\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 27ms/step - loss: 12.3414 - mae: 12.3414\n</pre> Out[8]: <pre>&lt;keras.src.callbacks.history.History at 0x1d925c69e80&gt;</pre> <p>Boom!</p> <p>We've just trained a model to figure out the patterns between <code>X</code> and <code>y</code>.</p> <p>How do you think it went?</p> In\u00a0[9]: Copied! <pre># Check out X and y\nX, y\n</pre> # Check out X and y X, y Out[9]: <pre>(&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)&gt;)</pre> <p>What do you think the outcome should be if we passed our model an <code>X</code> value of 17.0?</p> In\u00a0[10]: Copied! <pre># Make a prediction with the model\nmodel.predict(tf.constant([17.0]))\n</pre> # Make a prediction with the model model.predict(tf.constant([17.0])) <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 42ms/step\n</pre> Out[10]: <pre>array([[6.0782204]], dtype=float32)</pre> <p>It doesn't go very well... it should've output something close to 27.0.</p> <p>\ud83e\udd14 Question: What's Keras? I thought we were working with TensorFlow but every time we write TensorFlow code, <code>keras</code> comes after <code>tf</code> (e.g. <code>tf.keras.layers.Dense()</code>)?</p> <p>Before TensorFlow 2.0+, Keras was an API designed to be able to build deep learning models with ease. Since TensorFlow 2.0+, its functionality has been tightly integrated within the TensorFlow library.</p> In\u00a0[11]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model (same as above)\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(1)\n])\n\n# Compile model (same as above)\nmodel.compile(loss=tf.keras.losses.mae,\n              optimizer=tf.keras.optimizers.SGD(),\n              metrics=[\"mae\"])\n\n# Fit model (this time we'll train for longer)\nmodel.fit(tf.expand_dims(X, axis=-1), y, epochs=100) # train for 100 epochs not 10\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model (same as above) model = tf.keras.Sequential([   tf.keras.layers.Dense(1) ])  # Compile model (same as above) model.compile(loss=tf.keras.losses.mae,               optimizer=tf.keras.optimizers.SGD(),               metrics=[\"mae\"])  # Fit model (this time we'll train for longer) model.fit(tf.expand_dims(X, axis=-1), y, epochs=100) # train for 100 epochs not 10 <pre>Epoch 1/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 443ms/step - loss: 18.1138 - mae: 18.1138\nEpoch 2/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 28ms/step - loss: 17.8326 - mae: 17.8326\nEpoch 3/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 27ms/step - loss: 17.5513 - mae: 17.5513\nEpoch 4/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 17.2701 - mae: 17.2701\nEpoch 5/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 16.9888 - mae: 16.9888\nEpoch 6/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 16.7076 - mae: 16.7076\nEpoch 7/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 16.4263 - mae: 16.4263\nEpoch 8/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 16.1451 - mae: 16.1451\nEpoch 9/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 15.8638 - mae: 15.8638\nEpoch 10/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 28ms/step - loss: 15.5826 - mae: 15.5826\nEpoch 11/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 28ms/step - loss: 15.3013 - mae: 15.3013\nEpoch 12/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 15.0201 - mae: 15.0201\nEpoch 13/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 33ms/step - loss: 14.7809 - mae: 14.7809\nEpoch 14/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 14.6484 - mae: 14.6484\nEpoch 15/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 82ms/step - loss: 14.5159 - mae: 14.5159\nEpoch 16/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 75ms/step - loss: 14.3834 - mae: 14.3834\nEpoch 17/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 30ms/step - loss: 14.2509 - mae: 14.2509\nEpoch 18/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 30ms/step - loss: 14.1184 - mae: 14.1184\nEpoch 19/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 27ms/step - loss: 13.9859 - mae: 13.9859\nEpoch 20/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 30ms/step - loss: 13.8534 - mae: 13.8534\nEpoch 21/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 28ms/step - loss: 13.7209 - mae: 13.7209\nEpoch 22/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step - loss: 13.5884 - mae: 13.5884\nEpoch 23/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 30ms/step - loss: 13.4559 - mae: 13.4559\nEpoch 24/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 28ms/step - loss: 13.3234 - mae: 13.3234\nEpoch 25/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step - loss: 13.1909 - mae: 13.1909\nEpoch 26/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step - loss: 13.0584 - mae: 13.0584\nEpoch 27/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 37ms/step - loss: 12.9259 - mae: 12.9259\nEpoch 28/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 12.7934 - mae: 12.7934\nEpoch 29/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 36ms/step - loss: 12.6609 - mae: 12.6609\nEpoch 30/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 40ms/step - loss: 12.5284 - mae: 12.5284\nEpoch 31/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 42ms/step - loss: 12.3959 - mae: 12.3959\nEpoch 32/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 33ms/step - loss: 12.2634 - mae: 12.2634\nEpoch 33/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 36ms/step - loss: 12.1309 - mae: 12.1309\nEpoch 34/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 36ms/step - loss: 11.9984 - mae: 11.9984\nEpoch 35/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 34ms/step - loss: 11.8659 - mae: 11.8659\nEpoch 36/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 35ms/step - loss: 11.7334 - mae: 11.7334\nEpoch 37/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 11.6009 - mae: 11.6009\nEpoch 38/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 40ms/step - loss: 11.4684 - mae: 11.4684\nEpoch 39/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 11.3359 - mae: 11.3359\nEpoch 40/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 34ms/step - loss: 11.2034 - mae: 11.2034\nEpoch 41/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 37ms/step - loss: 11.0709 - mae: 11.0709\nEpoch 42/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 34ms/step - loss: 10.9384 - mae: 10.9384\nEpoch 43/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 46ms/step - loss: 10.8059 - mae: 10.8059\nEpoch 44/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 41ms/step - loss: 10.6734 - mae: 10.6734\nEpoch 45/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 27ms/step - loss: 10.5409 - mae: 10.5409\nEpoch 46/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 39ms/step - loss: 10.4084 - mae: 10.4084\nEpoch 47/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 36ms/step - loss: 10.2759 - mae: 10.2759\nEpoch 48/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 27ms/step - loss: 10.1434 - mae: 10.1434\nEpoch 49/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step - loss: 10.0109 - mae: 10.0109\nEpoch 50/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 33ms/step - loss: 9.8784 - mae: 9.8784\nEpoch 51/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 9.7459 - mae: 9.7459\nEpoch 52/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step - loss: 9.6134 - mae: 9.6134\nEpoch 53/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step - loss: 9.4809 - mae: 9.4809\nEpoch 54/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 28ms/step - loss: 9.3484 - mae: 9.3484\nEpoch 55/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 35ms/step - loss: 9.2159 - mae: 9.2159\nEpoch 56/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 33ms/step - loss: 9.0834 - mae: 9.0834\nEpoch 57/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 30ms/step - loss: 8.9509 - mae: 8.9509\nEpoch 58/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 28ms/step - loss: 8.8184 - mae: 8.8184\nEpoch 59/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 8.6859 - mae: 8.6859\nEpoch 60/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 8.5534 - mae: 8.5534\nEpoch 61/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 28ms/step - loss: 8.4209 - mae: 8.4209\nEpoch 62/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 28ms/step - loss: 8.2884 - mae: 8.2884\nEpoch 63/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 35ms/step - loss: 8.1559 - mae: 8.1559\nEpoch 64/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 34ms/step - loss: 8.0234 - mae: 8.0234\nEpoch 65/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 7.8909 - mae: 7.8909\nEpoch 66/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 55ms/step - loss: 7.7584 - mae: 7.7584\nEpoch 67/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 49ms/step - loss: 7.6259 - mae: 7.6259\nEpoch 68/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 46ms/step - loss: 7.4934 - mae: 7.4934\nEpoch 69/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 45ms/step - loss: 7.3609 - mae: 7.3609\nEpoch 70/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 47ms/step - loss: 7.2284 - mae: 7.2284\nEpoch 71/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 39ms/step - loss: 7.0959 - mae: 7.0959\nEpoch 72/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 41ms/step - loss: 6.9900 - mae: 6.9900\nEpoch 73/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 34ms/step - loss: 6.9844 - mae: 6.9844\nEpoch 74/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 6.9788 - mae: 6.9788\nEpoch 75/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 34ms/step - loss: 6.9731 - mae: 6.9731\nEpoch 76/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 6.9675 - mae: 6.9675\nEpoch 77/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 30ms/step - loss: 6.9619 - mae: 6.9619\nEpoch 78/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 40ms/step - loss: 6.9563 - mae: 6.9563\nEpoch 79/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 6.9506 - mae: 6.9506\nEpoch 80/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 28ms/step - loss: 6.9450 - mae: 6.9450\nEpoch 81/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 30ms/step - loss: 6.9394 - mae: 6.9394\nEpoch 82/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 6.9338 - mae: 6.9338\nEpoch 83/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 6.9281 - mae: 6.9281\nEpoch 84/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 6.9225 - mae: 6.9225\nEpoch 85/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step - loss: 6.9169 - mae: 6.9169\nEpoch 86/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 42ms/step - loss: 6.9113 - mae: 6.9113\nEpoch 87/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 6.9056 - mae: 6.9056\nEpoch 88/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 6.9000 - mae: 6.9000\nEpoch 89/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step - loss: 6.8944 - mae: 6.8944\nEpoch 90/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 35ms/step - loss: 6.8888 - mae: 6.8888\nEpoch 91/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 34ms/step - loss: 6.8831 - mae: 6.8831\nEpoch 92/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 41ms/step - loss: 6.8775 - mae: 6.8775\nEpoch 93/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 68ms/step - loss: 6.8719 - mae: 6.8719\nEpoch 94/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 51ms/step - loss: 6.8663 - mae: 6.8663\nEpoch 95/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 47ms/step - loss: 6.8606 - mae: 6.8606\nEpoch 96/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 30ms/step - loss: 6.8550 - mae: 6.8550\nEpoch 97/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step - loss: 6.8494 - mae: 6.8494\nEpoch 98/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step - loss: 6.8438 - mae: 6.8438\nEpoch 99/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step - loss: 6.8381 - mae: 6.8381\nEpoch 100/100\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 33ms/step - loss: 6.8325 - mae: 6.8325\n</pre> Out[11]: <pre>&lt;keras.src.callbacks.history.History at 0x1d925c38680&gt;</pre> <p>You might've noticed the loss value decrease from before (and keep decreasing as the number of epochs gets higher).</p> <p>What do you think this means for when we make a prediction with our model?</p> <p>How about we try predict on 17.0 again?</p> In\u00a0[12]: Copied! <pre># Remind ourselves of what X and y are\nX, y\n</pre> # Remind ourselves of what X and y are X, y Out[12]: <pre>(&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)&gt;)</pre> In\u00a0[13]: Copied! <pre># Try and predict what y would be if X was 17.0\nmodel.predict(tf.constant([17.0])) # the right answer is 27.0 (y = X + 10)\n</pre> # Try and predict what y would be if X was 17.0 model.predict(tf.constant([17.0])) # the right answer is 27.0 (y = X + 10) <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 52ms/step\n</pre> Out[13]: <pre>array([[29.34386]], dtype=float32)</pre> <p>Much better!</p> <p>We got closer this time. But we could still be better.</p> <p>Now we've trained a model, how could we evaluate it?</p> In\u00a0[14]: Copied! <pre># Make a bigger dataset\nX = np.arange(-100, 100, 4)\nX\n</pre> # Make a bigger dataset X = np.arange(-100, 100, 4) X Out[14]: <pre>array([-100,  -96,  -92,  -88,  -84,  -80,  -76,  -72,  -68,  -64,  -60,\n        -56,  -52,  -48,  -44,  -40,  -36,  -32,  -28,  -24,  -20,  -16,\n        -12,   -8,   -4,    0,    4,    8,   12,   16,   20,   24,   28,\n         32,   36,   40,   44,   48,   52,   56,   60,   64,   68,   72,\n         76,   80,   84,   88,   92,   96])</pre> In\u00a0[15]: Copied! <pre># Make labels for the dataset (adhering to the same pattern as before)\ny = np.arange(-90, 110, 4)\ny\n</pre> # Make labels for the dataset (adhering to the same pattern as before) y = np.arange(-90, 110, 4) y Out[15]: <pre>array([-90, -86, -82, -78, -74, -70, -66, -62, -58, -54, -50, -46, -42,\n       -38, -34, -30, -26, -22, -18, -14, -10,  -6,  -2,   2,   6,  10,\n        14,  18,  22,  26,  30,  34,  38,  42,  46,  50,  54,  58,  62,\n        66,  70,  74,  78,  82,  86,  90,  94,  98, 102, 106])</pre> <p>Since $y=X+10$, we could make the labels like so:</p> In\u00a0[16]: Copied! <pre># Same result as above\ny = X + 10\ny\n</pre> # Same result as above y = X + 10 y Out[16]: <pre>array([-90, -86, -82, -78, -74, -70, -66, -62, -58, -54, -50, -46, -42,\n       -38, -34, -30, -26, -22, -18, -14, -10,  -6,  -2,   2,   6,  10,\n        14,  18,  22,  26,  30,  34,  38,  42,  46,  50,  54,  58,  62,\n        66,  70,  74,  78,  82,  86,  90,  94,  98, 102, 106])</pre> In\u00a0[17]: Copied! <pre># Check how many samples we have\nlen(X)\n</pre> # Check how many samples we have len(X) Out[17]: <pre>50</pre> In\u00a0[18]: Copied! <pre># Split data into train and test sets\nX_train = X[:40] # first 40 examples (80% of data)\ny_train = y[:40]\n\nX_test = X[40:] # last 10 examples (20% of data)\ny_test = y[40:]\n\nlen(X_train), len(X_test)\n</pre> # Split data into train and test sets X_train = X[:40] # first 40 examples (80% of data) y_train = y[:40]  X_test = X[40:] # last 10 examples (20% of data) y_test = y[40:]  len(X_train), len(X_test) Out[18]: <pre>(40, 10)</pre> In\u00a0[19]: Copied! <pre>plt.figure(figsize=(10, 7))\n# Plot training data in blue\nplt.scatter(X_train, y_train, c='b', label='Training data')\n# Plot test data in green\nplt.scatter(X_test, y_test, c='g', label='Testing data')\n# Show the legend\nplt.legend();\n</pre> plt.figure(figsize=(10, 7)) # Plot training data in blue plt.scatter(X_train, y_train, c='b', label='Training data') # Plot test data in green plt.scatter(X_test, y_test, c='g', label='Testing data') # Show the legend plt.legend(); <p>Beautiful! Any time you can visualize your data, your model, your anything, it's a good idea.</p> <p>With this graph in mind, what we'll be trying to do is build a model which learns the pattern in the blue dots (<code>X_train</code>) to draw the green dots (<code>X_test</code>).</p> <p>Time to build a model. We'll make the exact same one from before (the one we trained for longer).</p> In\u00a0[20]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model (same as above)\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(1)\n])\n\n# Compile model (same as above)\nmodel.compile(loss=tf.keras.losses.mae,\n              optimizer=tf.keras.optimizers.SGD(),\n              metrics=[\"mae\"])\n\n# Fit model (same as above)\n#model.fit(X_train, y_train, epochs=100) # commented out on purpose (not fitting it just yet)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model (same as above) model = tf.keras.Sequential([   tf.keras.layers.Dense(1) ])  # Compile model (same as above) model.compile(loss=tf.keras.losses.mae,               optimizer=tf.keras.optimizers.SGD(),               metrics=[\"mae\"])  # Fit model (same as above) #model.fit(X_train, y_train, epochs=100) # commented out on purpose (not fitting it just yet) In\u00a0[21]: Copied! <pre># Doesn't work (model not fit/built)\nmodel.summary()\n</pre> # Doesn't work (model not fit/built) model.summary() <pre>Model: \"sequential_2\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 dense_2 (Dense)                      \u2502 ?                           \u2502     0 (unbuilt) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 0 (0.00 B)\n</pre> <pre> Trainable params: 0 (0.00 B)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <p>Ahh, the cell above errors because we haven't fit or built our model.</p> <p>We also haven't told it what input shape it should be expecting.</p> <p>Remember above, how we discussed the input shape was just one number?</p> <p>We can let our model know the input shape of our data using the <code>input_shape</code> parameter to the first layer (usually if <code>input_shape</code> isn't defined, Keras tries to figure it out automatically).</p> In\u00a0[22]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model (same as above)\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(1, input_shape=[1]) # define the input_shape to our model\n])\n\n# Compile model (same as above)\nmodel.compile(loss=tf.keras.losses.mae,\n              optimizer=tf.keras.optimizers.SGD(),\n              metrics=[\"mae\"])\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model (same as above) model = tf.keras.Sequential([   tf.keras.layers.Dense(1, input_shape=[1]) # define the input_shape to our model ])  # Compile model (same as above) model.compile(loss=tf.keras.losses.mae,               optimizer=tf.keras.optimizers.SGD(),               metrics=[\"mae\"]) <pre>D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n</pre> In\u00a0[23]: Copied! <pre># This will work after specifying the input shape\nmodel.summary()\n</pre> # This will work after specifying the input shape model.summary() <pre>Model: \"sequential_3\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 dense_3 (Dense)                      \u2502 (None, 1)                   \u2502               2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 2 (8.00 B)\n</pre> <pre> Trainable params: 2 (8.00 B)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <p>Calling <code>summary()</code> on our model shows us the layers it contains, the output shape and the number of parameters.</p> <ul> <li>Total params - total number of parameters in the model.</li> <li>Trainable parameters - these are the parameters (patterns) the model can update as it trains.</li> <li>Non-trainable parameters - these parameters aren't updated during training (this is typical when you bring in the already learned patterns from other models during transfer learning).</li> </ul> <p>\ud83d\udcd6 Resource: For a more in-depth overview of the trainable parameters within a layer, check out MIT's introduction to deep learning video.</p> <p>\ud83d\udee0 Exercise: Try playing around with the number of hidden units in the <code>Dense</code> layer (e.g. <code>Dense(2)</code>, <code>Dense(3)</code>). How does this change the Total/Trainable params? Investigate what's causing the change.</p> <p>For now, all you need to think about these parameters is that their learnable patterns in the data.</p> <p>Let's fit our model to the training data.</p> In\u00a0[24]: Copied! <pre># Fit the model to the training data\nmodel.fit(X_train, y_train, epochs=100, verbose=0) # verbose controls how much gets output\n</pre> # Fit the model to the training data model.fit(X_train, y_train, epochs=100, verbose=0) # verbose controls how much gets output Out[24]: <pre>&lt;keras.src.callbacks.history.History at 0x1d9275bd8b0&gt;</pre> In\u00a0[25]: Copied! <pre># Check the model summary\nmodel.summary()\n</pre> # Check the model summary model.summary() <pre>Model: \"sequential_3\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 dense_3 (Dense)                      \u2502 (None, 1)                   \u2502               2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 4 (20.00 B)\n</pre> <pre> Trainable params: 2 (8.00 B)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <pre> Optimizer params: 2 (12.00 B)\n</pre> <p>Alongside summary, you can also view a 2D plot of the model using <code>plot_model()</code>.</p> In\u00a0[26]: Copied! <pre># from plot_model import plot_model\n\n# plot_model(model, show_shapes=True)\n</pre> # from plot_model import plot_model  # plot_model(model, show_shapes=True) <p>In our case, the model we used only has an input and an output but visualizing more complicated models can be very helpful for debugging.</p> In\u00a0[27]: Copied! <pre># Make predictions\ny_preds = model.predict(X_test)\n</pre> # Make predictions y_preds = model.predict(X_test) <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 40ms/step\n</pre> In\u00a0[28]: Copied! <pre># View the predictions\ny_preds\n</pre> # View the predictions y_preds Out[28]: <pre>array([[55.658394],\n       [59.274494],\n       [62.890594],\n       [66.5067  ],\n       [70.1228  ],\n       [73.7389  ],\n       [77.355   ],\n       [80.97111 ],\n       [84.587204],\n       [88.20331 ]], dtype=float32)</pre> <p>Okay, we get a list of numbers but how do these compare to the ground truth labels?</p> <p>Let's build a plotting function to find out.</p> <p>\ud83d\udd11 Note: If you think you're going to be visualizing something a lot, it's a good idea to functionize it so you can use it later.</p> In\u00a0[29]: Copied! <pre>def plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=y_preds):\n  \"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", label=\"Training data\")\n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"g\", label=\"Testing data\")\n  # Plot the predictions in red (predictions were made on the test data)\n  plt.scatter(test_data, predictions, c=\"r\", label=\"Predictions\")\n  # Show the legend\n  plt.legend();\n</pre> def plot_predictions(train_data=X_train,                       train_labels=y_train,                       test_data=X_test,                       test_labels=y_test,                       predictions=y_preds):   \"\"\"   Plots training data, test data and compares predictions.   \"\"\"   plt.figure(figsize=(10, 7))   # Plot training data in blue   plt.scatter(train_data, train_labels, c=\"b\", label=\"Training data\")   # Plot test data in green   plt.scatter(test_data, test_labels, c=\"g\", label=\"Testing data\")   # Plot the predictions in red (predictions were made on the test data)   plt.scatter(test_data, predictions, c=\"r\", label=\"Predictions\")   # Show the legend   plt.legend(); In\u00a0[30]: Copied! <pre>plot_predictions(train_data=X_train,\n                 train_labels=y_train,\n                 test_data=X_test,\n                 test_labels=y_test,\n                 predictions=y_preds)\n</pre> plot_predictions(train_data=X_train,                  train_labels=y_train,                  test_data=X_test,                  test_labels=y_test,                  predictions=y_preds) <p>From the plot we can see our predictions aren't totally outlandish but they definitely aren't anything special either.</p> In\u00a0[31]: Copied! <pre># Evaluate the model on the test set\nmodel.evaluate(X_test, y_test)\n</pre> # Evaluate the model on the test set model.evaluate(X_test, y_test) <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 93ms/step - loss: 16.0691 - mae: 16.0691\n</pre> Out[31]: <pre>[16.069149017333984, 16.069149017333984]</pre> <p>In our case, since we used MAE for the loss function as well as MAE for the metrics, <code>model.evaulate()</code> returns them both.</p> <p>TensorFlow also has built in functions for MSE and MAE.</p> <p>For many evaluation functions, the premise is the same: compare predictions to the ground truth labels.</p> In\u00a0[32]: Copied! <pre># Calculate the mean absolute error\nmae = tf.metrics.mean_absolute_error(y_true=y_test, \n                                     y_pred=y_preds)\nmae\n</pre> # Calculate the mean absolute error mae = tf.metrics.mean_absolute_error(y_true=y_test,                                       y_pred=y_preds) mae Out[32]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([14.341606 , 14.725506 , 15.1094055, 15.493301 , 15.877197 ,\n       16.2611   , 16.644997 , 17.028893 , 17.412796 , 17.796692 ],\n      dtype=float32)&gt;</pre> <p>Huh? That's strange, MAE should be a single output.</p> <p>Instead, we get 10 values.</p> <p>This is because our <code>y_test</code> and <code>y_preds</code> tensors are different shapes.</p> In\u00a0[33]: Copied! <pre># Check the test label tensor values\ny_test\n</pre> # Check the test label tensor values y_test Out[33]: <pre>array([ 70,  74,  78,  82,  86,  90,  94,  98, 102, 106])</pre> In\u00a0[34]: Copied! <pre># Check the predictions tensor values (notice the extra square brackets)\ny_preds\n</pre> # Check the predictions tensor values (notice the extra square brackets) y_preds Out[34]: <pre>array([[55.658394],\n       [59.274494],\n       [62.890594],\n       [66.5067  ],\n       [70.1228  ],\n       [73.7389  ],\n       [77.355   ],\n       [80.97111 ],\n       [84.587204],\n       [88.20331 ]], dtype=float32)</pre> In\u00a0[35]: Copied! <pre># Check the tensor shapes\ny_test.shape, y_preds.shape\n</pre> # Check the tensor shapes y_test.shape, y_preds.shape Out[35]: <pre>((10,), (10, 1))</pre> <p>Remember how we discussed dealing with different input and output shapes is one the most common issues you'll come across, this is one of those times.</p> <p>But not to worry.</p> <p>We can fix it using <code>squeeze()</code>, it'll remove the the <code>1</code> dimension from our <code>y_preds</code> tensor, making it the same shape as <code>y_test</code>.</p> <p>\ud83d\udd11 Note: If you're comparing two tensors, it's important to make sure they're the right shape(s) (you won't always have to manipulate the shapes, but always be on the look out, many errors are the result of mismatched tensors, especially mismatched input and output shapes).</p> In\u00a0[36]: Copied! <pre># Shape before squeeze()\ny_preds.shape\n</pre> # Shape before squeeze() y_preds.shape Out[36]: <pre>(10, 1)</pre> In\u00a0[37]: Copied! <pre># Shape after squeeze()\ny_preds.squeeze().shape\n</pre> # Shape after squeeze() y_preds.squeeze().shape Out[37]: <pre>(10,)</pre> In\u00a0[38]: Copied! <pre># What do they look like?\ny_test, y_preds.squeeze()\n</pre> # What do they look like? y_test, y_preds.squeeze() Out[38]: <pre>(array([ 70,  74,  78,  82,  86,  90,  94,  98, 102, 106]),\n array([55.658394, 59.274494, 62.890594, 66.5067  , 70.1228  , 73.7389  ,\n        77.355   , 80.97111 , 84.587204, 88.20331 ], dtype=float32))</pre> <p>Okay, now we know how to make our <code>y_test</code> and <code>y_preds</code> tenors the same shape, let's use our evaluation metrics.</p> In\u00a0[39]: Copied! <pre># Calcuate the MAE\nmae = tf.metrics.mean_absolute_error(y_true=y_test, \n                                     y_pred=y_preds.squeeze()) # use squeeze() to make same shape\nmae\n</pre> # Calcuate the MAE mae = tf.metrics.mean_absolute_error(y_true=y_test,                                       y_pred=y_preds.squeeze()) # use squeeze() to make same shape mae Out[39]: <pre>&lt;tf.Tensor: shape=(), dtype=float32, numpy=16.069149&gt;</pre> In\u00a0[40]: Copied! <pre># Calculate the MSE\nmse = tf.metrics.mean_squared_error(y_true=y_test,\n                                    y_pred=y_preds.squeeze())\nmse\n</pre> # Calculate the MSE mse = tf.metrics.mean_squared_error(y_true=y_test,                                     y_pred=y_preds.squeeze()) mse Out[40]: <pre>&lt;tf.Tensor: shape=(), dtype=float32, numpy=259.4334&gt;</pre> <p>We can also calculate the MAE using pure TensorFlow functions.</p> In\u00a0[41]: Copied! <pre># Returns the same as tf.metrics.mean_absolute_error()\ntf.reduce_mean(tf.abs(y_test-y_preds.squeeze()))\n</pre> # Returns the same as tf.metrics.mean_absolute_error() tf.reduce_mean(tf.abs(y_test-y_preds.squeeze())) Out[41]: <pre>&lt;tf.Tensor: shape=(), dtype=float64, numpy=16.06914939880371&gt;</pre> <p>Again, it's a good idea to functionize anything you think you might use over again (or find yourself using over and over again).</p> <p>Let's make functions for our evaluation metrics.</p> In\u00a0[42]: Copied! <pre>def mae(y_test, y_pred):\n  \"\"\"\n  Calculuates mean absolute error between y_test and y_preds.\n  \"\"\"\n  return tf.metrics.mean_absolute_error(y_test,\n                                        y_pred)\n  \ndef mse(y_test, y_pred):\n  \"\"\"\n  Calculates mean squared error between y_test and y_preds.\n  \"\"\"\n  return tf.metrics.mean_squared_error(y_test,\n                                       y_pred)\n</pre> def mae(y_test, y_pred):   \"\"\"   Calculuates mean absolute error between y_test and y_preds.   \"\"\"   return tf.metrics.mean_absolute_error(y_test,                                         y_pred)    def mse(y_test, y_pred):   \"\"\"   Calculates mean squared error between y_test and y_preds.   \"\"\"   return tf.metrics.mean_squared_error(y_test,                                        y_pred) In\u00a0[43]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Replicate original model\nmodel_1 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel_1.compile(loss=tf.keras.losses.mae,\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['mae'])\n\n# Fit the model\nmodel_1.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100)\n</pre> # Set random seed tf.random.set_seed(42)  # Replicate original model model_1 = tf.keras.Sequential([   tf.keras.layers.Dense(1) ])  # Compile the model model_1.compile(loss=tf.keras.losses.mae,                 optimizer=tf.keras.optimizers.SGD(),                 metrics=['mae'])  # Fit the model model_1.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100) <pre>Epoch 1/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 9ms/step - loss: 10.2953 - mae: 9.9377 \nEpoch 2/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.8617 - mae: 8.2765 \nEpoch 3/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.5937 - mae: 8.9518 \nEpoch 4/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.4829 - mae: 7.9909 \nEpoch 5/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.1120 - mae: 9.6343\nEpoch 6/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.4625 - mae: 7.9731 \nEpoch 7/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 10.0965 - mae: 9.6227\nEpoch 8/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.4421 - mae: 7.9552 \nEpoch 9/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.0809 - mae: 9.6110\nEpoch 10/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.4218 - mae: 7.9374 \nEpoch 11/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.0653 - mae: 9.5994\nEpoch 12/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.4014 - mae: 7.9195 \nEpoch 13/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.0497 - mae: 9.5878\nEpoch 14/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.3811 - mae: 7.9017 \nEpoch 15/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.0342 - mae: 9.5762\nEpoch 16/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.3607 - mae: 7.8838 \nEpoch 17/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 10.0186 - mae: 9.5646\nEpoch 18/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.3403 - mae: 7.8660 \nEpoch 19/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 10.0030 - mae: 9.5529\nEpoch 20/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.3200 - mae: 7.8481 \nEpoch 21/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.9874 - mae: 9.5413 \nEpoch 22/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.2996 - mae: 7.8303 \nEpoch 23/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 9.9719 - mae: 9.5297 \nEpoch 24/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.2792 - mae: 7.8124 \nEpoch 25/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.9563 - mae: 9.5181 \nEpoch 26/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 8.2589 - mae: 7.7946 \nEpoch 27/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 9.9407 - mae: 9.5065 \nEpoch 28/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.2385 - mae: 7.7767 \nEpoch 29/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 9.9252 - mae: 9.4948 \nEpoch 30/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.2181 - mae: 7.7589 \nEpoch 31/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.9096 - mae: 9.4832 \nEpoch 32/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.1978 - mae: 7.7410 \nEpoch 33/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 9.8940 - mae: 9.4716 \nEpoch 34/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.1774 - mae: 7.7232 \nEpoch 35/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 9.8784 - mae: 9.4600 \nEpoch 36/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.4692 - mae: 7.8304 \nEpoch 37/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 9.1815 - mae: 8.4801 \nEpoch 38/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 7.7880 - mae: 7.4489 \nEpoch 39/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.6967 - mae: 10.4088 \nEpoch 40/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 7.4362 - mae: 7.2642 \nEpoch 41/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.5099 - mae: 11.5898 \nEpoch 42/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.0906 - mae: 7.6280 \nEpoch 43/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.7864 - mae: 9.3580 \nEpoch 44/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.0702 - mae: 7.6101 \nEpoch 45/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 9.7708 - mae: 9.3463 \nEpoch 46/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.0499 - mae: 7.5923 \nEpoch 47/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 9.7553 - mae: 9.3347 \nEpoch 48/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 8.3417 - mae: 7.6995 \nEpoch 49/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 9.0562 - mae: 8.3521 \nEpoch 50/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 7.6608 - mae: 7.3184 \nEpoch 51/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.5790 - mae: 10.2907 \nEpoch 52/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 7.3098 - mae: 7.1348 \nEpoch 53/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.3956 - mae: 11.4761 \nEpoch 54/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 7.9630 - mae: 7.4970 \nEpoch 55/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 9.6632 - mae: 9.2327 \nEpoch 56/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 7.9427 - mae: 7.4792 \nEpoch 57/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.6477 - mae: 9.2211 \nEpoch 58/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 7.9223 - mae: 7.4613 \nEpoch 59/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 9.6321 - mae: 9.2095 \nEpoch 60/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.2142 - mae: 7.5687 \nEpoch 61/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.9309 - mae: 8.2240 \nEpoch 62/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 7.5336 - mae: 7.1879 \nEpoch 63/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.4614 - mae: 10.1726\nEpoch 64/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 7.1834 - mae: 7.0053 \nEpoch 65/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.2813 - mae: 11.3625 \nEpoch 66/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 7.8355 - mae: 7.3661 \nEpoch 67/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 9.5401 - mae: 9.1074 \nEpoch 68/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 7.8151 - mae: 7.3482 \nEpoch 69/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.5245 - mae: 9.0958 \nEpoch 70/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 7.7948 - mae: 7.3304 \nEpoch 71/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.7113 - mae: 9.1656 \nEpoch 72/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 6.9956 - mae: 6.8799 \nEpoch 73/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.2257 - mae: 9.5715 \nEpoch 74/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.3729 - mae: 11.3429 \nEpoch 75/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.7856 - mae: 12.0771 \nEpoch 76/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.0629 - mae: 7.4401 \nEpoch 77/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.7985 - mae: 8.1201 \nEpoch 78/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 7.3635 - mae: 7.0348 \nEpoch 79/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.3574 - mae: 10.1055\nEpoch 80/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 7.0038 - mae: 6.8399 \nEpoch 81/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.3377 - mae: 11.3717 \nEpoch 82/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 6.9912 - mae: 6.8153 \nEpoch 83/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.2812 - mae: 11.2899 \nEpoch 84/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 6.7521 - mae: 6.7014 \nEpoch 85/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 8.6482 - mae: 7.7535 \nEpoch 86/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 9.0388 - mae: 9.2223 \nEpoch 87/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 9.9745 - mae: 10.9090  \nEpoch 88/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 8.6563 - mae: 7.8633 \nEpoch 89/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.0469 - mae: 9.3320 \nEpoch 90/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.0929 - mae: 11.0629 \nEpoch 91/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.7475 - mae: 12.1116 \nEpoch 92/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 7.9262 - mae: 7.3464 \nEpoch 93/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 8.6927 - mae: 8.0665 \nEpoch 94/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 7.5365 - mae: 7.0466 \nEpoch 95/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 11ms/step - loss: 9.4372 - mae: 8.8613\nEpoch 96/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 7.2300 - mae: 6.8334 \nEpoch 97/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.7767 - mae: 10.6792  \nEpoch 98/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 8.5304 - mae: 7.6621 \nEpoch 99/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 8.9210 - mae: 9.1309 \nEpoch 100/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 9.8744 - mae: 10.8247  \n</pre> Out[43]: <pre>&lt;keras.src.callbacks.history.History at 0x1d928f0e060&gt;</pre> In\u00a0[44]: Copied! <pre># Make and plot predictions for model_1\ny_preds_1 = model_1.predict(X_test)\nplot_predictions(predictions=y_preds_1)\n</pre> # Make and plot predictions for model_1 y_preds_1 = model_1.predict(X_test) plot_predictions(predictions=y_preds_1) <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 58ms/step\n</pre> In\u00a0[45]: Copied! <pre># Calculate model_1 metrics\nmae_1 = mae(y_test, y_preds_1.squeeze()).numpy()\nmse_1 = mse(y_test, y_preds_1.squeeze()).numpy()\nmae_1, mse_1\n</pre> # Calculate model_1 metrics mae_1 = mae(y_test, y_preds_1.squeeze()).numpy() mse_1 = mse(y_test, y_preds_1.squeeze()).numpy() mae_1, mse_1 Out[45]: <pre>(13.23732, 175.69809)</pre> <p>Build <code>model_2</code></p> <p>This time we'll add an extra dense layer (so now our model will have 2 layers) whilst keeping everything else the same.</p> In\u00a0[46]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Replicate model_1 and add an extra layer\nmodel_2 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Dense(1) # add a second layer\n])\n\n# Compile the model\nmodel_2.compile(loss=tf.keras.losses.mae,\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['mae'])\n\n# Fit the model\nmodel_2.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100, verbose=1) # set verbose to 0 for less output\n</pre> # Set random seed tf.random.set_seed(42)  # Replicate model_1 and add an extra layer model_2 = tf.keras.Sequential([   tf.keras.layers.Dense(1),   tf.keras.layers.Dense(1) # add a second layer ])  # Compile the model model_2.compile(loss=tf.keras.losses.mae,                 optimizer=tf.keras.optimizers.SGD(),                 metrics=['mae'])  # Fit the model model_2.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100, verbose=1) # set verbose to 0 for less output <pre>Epoch 1/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 11ms/step - loss: 40.3570 - mae: 48.5746 \nEpoch 2/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 13.8935 - mae: 13.9815 \nEpoch 3/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 12.8610 - mae: 13.0419 \nEpoch 4/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 15.4269 - mae: 15.1253 \nEpoch 5/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.5182 - mae: 10.3873\nEpoch 6/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 11.9887 - mae: 11.3604 \nEpoch 7/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 12.4901 - mae: 12.3939 \nEpoch 8/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 13.7106 - mae: 13.7781 \nEpoch 9/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 12.6984 - mae: 12.8543 \nEpoch 10/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 15.2709 - mae: 14.9481 \nEpoch 11/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.3849 - mae: 10.2368\nEpoch 12/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.8633 - mae: 11.2205 \nEpoch 13/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 12.3729 - mae: 12.2677 \nEpoch 14/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 13.6145 - mae: 13.6830 \nEpoch 15/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 12.6006 - mae: 12.7629 \nEpoch 16/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 15.2379 - mae: 14.9409 \nEpoch 17/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.3052 - mae: 10.1758\nEpoch 18/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.7935 - mae: 11.1804 \nEpoch 19/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 12.3139 - mae: 12.2506 \nEpoch 20/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.2243 - mae: 13.9952 \nEpoch 21/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 11.7648 - mae: 11.1722 \nEpoch 22/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 12.2910 - mae: 12.2549 \nEpoch 23/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 14.7829 - mae: 14.2712 \nEpoch 24/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.0433 - mae: 9.7591\nEpoch 25/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.5325 - mae: 10.7671 \nEpoch 26/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 12.0542 - mae: 11.8418 \nEpoch 27/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 13.3082 - mae: 13.2874 \nEpoch 28/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 12.3193 - mae: 12.4138 \nEpoch 29/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 15.5180 - mae: 14.8908 \nEpoch 30/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 11.0053 - mae: 9.7761\nEpoch 31/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 11.5061 - mae: 10.8093 \nEpoch 32/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 12.0405 - mae: 11.9114 \nEpoch 33/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 9ms/step - loss: 13.9948 - mae: 13.7252 \nEpoch 34/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.5332 - mae: 10.9144 \nEpoch 35/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 12.0768 - mae: 12.0362 \nEpoch 36/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 14.6945 - mae: 14.2187 \nEpoch 37/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 9ms/step - loss: 10.8666 - mae: 9.6085\nEpoch 38/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.3757 - mae: 10.6606 \nEpoch 39/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.9193 - mae: 11.7836 \nEpoch 40/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.4738 - mae: 13.8969 \nEpoch 41/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.7332 - mae: 9.4020\nEpoch 42/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 11.2434 - mae: 10.4576\nEpoch 43/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.7885 - mae: 11.5848 \nEpoch 44/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.3093 - mae: 13.6640 \nEpoch 45/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.6226 - mae: 9.2417\nEpoch 46/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 11.1354 - mae: 10.3038\nEpoch 47/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.6835 - mae: 11.4383 \nEpoch 48/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 14.1997 - mae: 13.5175 \nEpoch 49/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.5340 - mae: 9.1265\nEpoch 50/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.0508 - mae: 10.1977\nEpoch 51/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.6033 - mae: 11.3425 \nEpoch 52/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 14.1434 - mae: 13.4553 \nEpoch 53/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.4670 - mae: 9.0550\nEpoch 54/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.9891 - mae: 10.1384\nEpoch 55/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.5475 - mae: 11.2965 \nEpoch 56/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.1415 - mae: 13.4769 \nEpoch 57/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.0059 - mae: 10.2200\nEpoch 58/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.5723 - mae: 11.3953 \nEpoch 59/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 14.8371 - mae: 13.9459 \nEpoch 60/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.3723 - mae: 8.9702\nEpoch 61/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.9040 - mae: 10.0751\nEpoch 62/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.4731 - mae: 11.2571 \nEpoch 63/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.7402 - mae: 13.8095 \nEpoch 64/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.2880 - mae: 8.8599\nEpoch 65/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 10.8232 - mae: 9.9732\nEpoch 66/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.3964 - mae: 11.1647 \nEpoch 67/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 14.6966 - mae: 13.7567 \nEpoch 68/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.2229 - mae: 8.7890\nEpoch 69/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.7629 - mae: 9.9134\nEpoch 70/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 11.3415 - mae: 11.1172 \nEpoch 71/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.7051 - mae: 13.7858 \nEpoch 72/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 10.1765 - mae: 8.7566\nEpoch 73/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 10.7226 - mae: 9.8947\nEpoch 74/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 11.3079 - mae: 11.1136 \nEpoch 75/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 15.2599 - mae: 14.1015 \nEpoch 76/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 7.6807 - mae: 6.8457 \nEpoch 77/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 17.6053 - mae: 16.8410 \nEpoch 78/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 9.6484 - mae: 7.9942 \nEpoch 79/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 14.8872 - mae: 15.1902 \nEpoch 80/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 15.0601 - mae: 15.7561 \nEpoch 81/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 13.1340 - mae: 11.4834 \nEpoch 82/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 15.6436 - mae: 17.1007 \nEpoch 83/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 14.1623 - mae: 13.1673 \nEpoch 84/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 15.7980 - mae: 17.4707 \nEpoch 85/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.1975 - mae: 13.5455 \nEpoch 86/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 15.9466 - mae: 17.8262 \nEpoch 87/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 14.2759 - mae: 13.9260 \nEpoch 88/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 14.7984 - mae: 14.9875 \nEpoch 89/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.9393 - mae: 15.5386 \nEpoch 90/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 15.1694 - mae: 16.1068 \nEpoch 91/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 13.6207 - mae: 12.0001 \nEpoch 92/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 15.5182 - mae: 16.9160 \nEpoch 93/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.1112 - mae: 12.9668 \nEpoch 94/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 15.6657 - mae: 17.2686 \nEpoch 95/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.1455 - mae: 13.3261 \nEpoch 96/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 15.8068 - mae: 17.6061 \nEpoch 97/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 14.1782 - mae: 13.6699 \nEpoch 98/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 15.9416 - mae: 17.9290 \nEpoch 99/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 14.5488 - mae: 14.1407 \nEpoch 100/100\n2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 15.9239 - mae: 17.9067 \n</pre> Out[46]: <pre>&lt;keras.src.callbacks.history.History at 0x1d928dbb320&gt;</pre> In\u00a0[47]: Copied! <pre># Make and plot predictions for model_2\ny_preds_2 = model_2.predict(X_test)\nplot_predictions(predictions=y_preds_2)\n</pre> # Make and plot predictions for model_2 y_preds_2 = model_2.predict(X_test) plot_predictions(predictions=y_preds_2) <pre>WARNING:tensorflow:5 out of the last 5 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x000001D9291EA3E0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 43ms/step\n</pre> <p>Woah, that's looking better already! And all it took was an extra layer.</p> In\u00a0[48]: Copied! <pre># Calculate model_2 metrics\nmae_2 = mae(y_test, y_preds_2.squeeze()).numpy()\nmse_2 = mse(y_test, y_preds_2.squeeze()).numpy()\nmae_2, mse_2\n</pre> # Calculate model_2 metrics mae_2 = mae(y_test, y_preds_2.squeeze()).numpy() mse_2 = mse(y_test, y_preds_2.squeeze()).numpy() mae_2, mse_2 Out[48]: <pre>(34.643097, 1215.5195)</pre> <p>Build <code>model_3</code></p> <p>For our 3rd model, we'll keep everything the same as <code>model_2</code> except this time we'll train for longer (500 epochs instead of 100).</p> <p>This will give our model more of a chance to learn the patterns in the data.</p> In\u00a0[49]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Replicate model_2\nmodel_3 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel_3.compile(loss=tf.keras.losses.mae,\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['mae'])\n\n# Fit the model (this time for 500 epochs, not 100)\nmodel_3.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=500, verbose=0) # set verbose to 0 for less output\n</pre> # Set random seed tf.random.set_seed(42)  # Replicate model_2 model_3 = tf.keras.Sequential([   tf.keras.layers.Dense(1),   tf.keras.layers.Dense(1) ])  # Compile the model model_3.compile(loss=tf.keras.losses.mae,                 optimizer=tf.keras.optimizers.SGD(),                 metrics=['mae'])  # Fit the model (this time for 500 epochs, not 100) model_3.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=500, verbose=0) # set verbose to 0 for less output Out[49]: <pre>&lt;keras.src.callbacks.history.History at 0x1d9289798b0&gt;</pre> In\u00a0[50]: Copied! <pre># Make and plot predictions for model_3\ny_preds_3 = model_3.predict(X_test)\nplot_predictions(predictions=y_preds_3)\n</pre> # Make and plot predictions for model_3 y_preds_3 = model_3.predict(X_test) plot_predictions(predictions=y_preds_3) <pre>WARNING:tensorflow:6 out of the last 6 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x000001D92AEC6020&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 45ms/step\n</pre> <p>Strange, we trained for longer but our model performed worse?</p> <p>As it turns out, our model might've trained too long and has thus resulted in worse results (we'll see ways to prevent training for too long later on).</p> In\u00a0[51]: Copied! <pre># Calculate model_3 metrics\nmae_3 = mae(y_test, y_preds_3.squeeze()).numpy()\nmse_3 = mse(y_test, y_preds_3.squeeze()).numpy()\nmae_3, mse_3\n</pre> # Calculate model_3 metrics mae_3 = mae(y_test, y_preds_3.squeeze()).numpy() mse_3 = mse(y_test, y_preds_3.squeeze()).numpy() mae_3, mse_3 Out[51]: <pre>(45.998524, 2154.496)</pre> In\u00a0[52]: Copied! <pre>model_results = [[\"model_1\", mae_1, mse_1],\n                 [\"model_2\", mae_2, mse_2],\n                 [\"model_3\", mae_3, mae_3]]\n</pre> model_results = [[\"model_1\", mae_1, mse_1],                  [\"model_2\", mae_2, mse_2],                  [\"model_3\", mae_3, mae_3]] In\u00a0[53]: Copied! <pre>import pandas as pd\nall_results = pd.DataFrame(model_results, columns=[\"model\", \"mae\", \"mse\"])\nall_results\n</pre> import pandas as pd all_results = pd.DataFrame(model_results, columns=[\"model\", \"mae\", \"mse\"]) all_results Out[53]: model mae mse 0 model_1 13.237320 175.698090 1 model_2 34.643097 1215.519531 2 model_3 45.998524 45.998524 <p>From our experiments, it looks like <code>model_2</code> performed the best.</p> <p>And now, you might be thinking, \"wow, comparing models is tedious...\" and it definitely can be, we've only compared 3 models here.</p> <p>But this is part of what machine learning modelling is about, trying many different combinations of models and seeing which performs best.</p> <p>Each model you build is a small experiment.</p> <p>\ud83d\udd11 Note: One of your main goals should be to minimize the time between your experiments. The more experiments you do, the more things you'll figure out which don't work and in turn, get closer to figuring out what does work. Remember the machine learning practitioner's motto: \"experiment, experiment, experiment\".</p> <p>Another thing you'll also find is what you thought may work (such as training a model for longer) may not always work and the exact opposite is also often the case.</p> In\u00a0[54]: Copied! <pre># Save a model using the SavedModel format\n# model_2.save('best_model_SavedModel_format')\n</pre> # Save a model using the SavedModel format # model_2.save('best_model_SavedModel_format') In\u00a0[55]: Copied! <pre># Check it out - outputs a protobuf binary file (.pb) as well as other files\n# !ls best_model_SavedModel_format\n</pre> # Check it out - outputs a protobuf binary file (.pb) as well as other files # !ls best_model_SavedModel_format <p>Now let's save the model in the HDF5 format, we'll use the same method but with a different filename.</p> In\u00a0[56]: Copied! <pre># Save a model using the HDF5 format\nmodel_2.save(\"best_model_HDF5_format.h5\") # note the addition of '.h5' on the end\n</pre> # Save a model using the HDF5 format model_2.save(\"best_model_HDF5_format.h5\") # note the addition of '.h5' on the end <pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n</pre> In\u00a0[57]: Copied! <pre># Check it out\n!ls best_model_HDF5_format.h5\n</pre> # Check it out !ls best_model_HDF5_format.h5 <pre>best_model_HDF5_format.h5\n</pre> In\u00a0[58]: Copied! <pre># Load a model from the SavedModel format\n# loaded_saved_model = tf.keras.models.load_model(\"best_model_SavedModel_format\")\n# loaded_saved_model.summary()\n</pre> # Load a model from the SavedModel format # loaded_saved_model = tf.keras.models.load_model(\"best_model_SavedModel_format\") # loaded_saved_model.summary() <p>Now let's test it out.</p> In\u00a0[62]: Copied! <pre># Compare model_2 with the SavedModel version (should return True)\nmodel_2_preds = model_2.predict(X_test)\n# saved_model_preds = loaded_saved_model.predict(X_test)\n# mae(y_test, saved_model_preds.squeeze()).numpy() == mae(y_test, model_2_preds.squeeze()).numpy()\n</pre> # Compare model_2 with the SavedModel version (should return True) model_2_preds = model_2.predict(X_test) # saved_model_preds = loaded_saved_model.predict(X_test) # mae(y_test, saved_model_preds.squeeze()).numpy() == mae(y_test, model_2_preds.squeeze()).numpy() <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step\n</pre> <p>Loading in from the HDF5 is much the same.</p> In\u00a0[63]: Copied! <pre># Load a model from the HDF5 format\nloaded_h5_model = tf.keras.models.load_model(\"best_model_HDF5_format.h5\")\nloaded_h5_model.summary()\n</pre> # Load a model from the HDF5 format loaded_h5_model = tf.keras.models.load_model(\"best_model_HDF5_format.h5\") loaded_h5_model.summary() <pre>WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n</pre> <pre>Model: \"sequential_5\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 dense_5 (Dense)                      \u2502 (None, 1)                   \u2502               2 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_6 (Dense)                      \u2502 (None, 1)                   \u2502               2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 6 (28.00 B)\n</pre> <pre> Trainable params: 4 (16.00 B)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <pre> Optimizer params: 2 (12.00 B)\n</pre> In\u00a0[64]: Copied! <pre># Compare model_2 with the loaded HDF5 version (should return True)\nh5_model_preds = loaded_h5_model.predict(X_test)\nmae(y_test, h5_model_preds.squeeze()).numpy() == mae(y_test, model_2_preds.squeeze()).numpy()\n</pre> # Compare model_2 with the loaded HDF5 version (should return True) h5_model_preds = loaded_h5_model.predict(X_test) mae(y_test, h5_model_preds.squeeze()).numpy() == mae(y_test, model_2_preds.squeeze()).numpy() <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 40ms/step\n</pre> Out[64]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre># Download the model (or any file) from Google Colab\nfrom google.colab import files\nfiles.download(\"best_model_HDF5_format.h5\")\n</pre> # Download the model (or any file) from Google Colab from google.colab import files files.download(\"best_model_HDF5_format.h5\") In\u00a0[66]: Copied! <pre># Import required libraries\nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</pre> # Import required libraries import tensorflow as tf import pandas as pd import matplotlib.pyplot as plt In\u00a0[67]: Copied! <pre># Read in the insurance dataset\ninsurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n</pre> # Read in the insurance dataset insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\") In\u00a0[68]: Copied! <pre># Check out the insurance dataset\ninsurance.head()\n</pre> # Check out the insurance dataset insurance.head() Out[68]: age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 16884.92400 1 18 male 33.770 1 no southeast 1725.55230 2 28 male 33.000 3 no southeast 4449.46200 3 33 male 22.705 0 no northwest 21984.47061 4 32 male 28.880 0 no northwest 3866.85520 <p>We're going to have to turn the non-numerical columns into numbers (because a neural network can't handle non-numerical inputs).</p> <p>To do so, we'll use the <code>get_dummies()</code> method in pandas.</p> <p>It converts categorical variables (like the <code>sex</code>, <code>smoker</code> and <code>region</code> columns) into numerical variables using one-hot encoding.</p> In\u00a0[69]: Copied! <pre># Turn all categories into numbers\ninsurance_one_hot = pd.get_dummies(insurance, dtype=int)\ninsurance_one_hot.head() # view the converted columns\n</pre> # Turn all categories into numbers insurance_one_hot = pd.get_dummies(insurance, dtype=int) insurance_one_hot.head() # view the converted columns Out[69]: age bmi children charges sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest 0 19 27.900 0 16884.92400 1 0 0 1 0 0 0 1 1 18 33.770 1 1725.55230 0 1 1 0 0 0 1 0 2 28 33.000 3 4449.46200 0 1 1 0 0 0 1 0 3 33 22.705 0 21984.47061 0 1 1 0 0 1 0 0 4 32 28.880 0 3866.85520 0 1 1 0 0 1 0 0 <p>Now we'll split data into features (<code>X</code>) and labels (<code>y</code>).</p> In\u00a0[70]: Copied! <pre># Create X &amp; y values\nX = insurance_one_hot.drop(\"charges\", axis=1)\ny = insurance_one_hot[\"charges\"]\n</pre> # Create X &amp; y values X = insurance_one_hot.drop(\"charges\", axis=1) y = insurance_one_hot[\"charges\"] In\u00a0[71]: Copied! <pre># View features\nX.head()\n</pre> # View features X.head() Out[71]: age bmi children sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest 0 19 27.900 0 1 0 0 1 0 0 0 1 1 18 33.770 1 0 1 1 0 0 0 1 0 2 28 33.000 3 0 1 1 0 0 0 1 0 3 33 22.705 0 0 1 1 0 0 1 0 0 4 32 28.880 0 0 1 1 0 0 1 0 0 <p>And create training and test sets. We could do this manually, but to make it easier, we'll leverage the already available <code>train_test_split</code> function available from Scikit-Learn.</p> In\u00a0[72]: Copied! <pre># Create training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2, \n                                                    random_state=42) # set random state for reproducible splits\n</pre> # Create training and test sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2,                                                      random_state=42) # set random state for reproducible splits <p>Now we can build and fit a model (we'll make it the same as <code>model_2</code>).</p> In\u00a0[73]: Copied! <pre># Convert X and y to numpy array\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n</pre> # Convert X and y to numpy array X_train = np.array(X_train) y_train = np.array(y_train) In\u00a0[74]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a new model (same as model_2)\ninsurance_model = tf.keras.Sequential([\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Dense(1)\n])\n\n# Compile the model\ninsurance_model.compile(loss=tf.keras.losses.mae,\n                        optimizer=tf.keras.optimizers.SGD(),\n                        metrics=['mae'])\n\n# Fit the model\ninsurance_model.fit(X_train, y_train, epochs=100)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a new model (same as model_2) insurance_model = tf.keras.Sequential([   tf.keras.layers.Dense(1),   tf.keras.layers.Dense(1) ])  # Compile the model insurance_model.compile(loss=tf.keras.losses.mae,                         optimizer=tf.keras.optimizers.SGD(),                         metrics=['mae'])  # Fit the model insurance_model.fit(X_train, y_train, epochs=100) <pre>Epoch 1/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 10001.6504 - mae: 10003.8008   \nEpoch 2/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7677.3208 - mae: 7678.9966 \nEpoch 3/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7499.9785 - mae: 7501.6816 \nEpoch 4/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7131.3779 - mae: 7132.8643 \nEpoch 5/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7260.0825 - mae: 7261.4038 \nEpoch 6/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 994us/step - loss: 7563.4419 - mae: 7565.1353\nEpoch 7/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 7645.0957 - mae: 7645.4868\nEpoch 8/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 7826.6533 - mae: 7826.3794 \nEpoch 9/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7664.6987 - mae: 7666.4385 \nEpoch 10/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7340.5503 - mae: 7341.8989 \nEpoch 11/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7838.0249 - mae: 7838.3022 \nEpoch 12/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7671.2559 - mae: 7673.0312 \nEpoch 13/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7689.4292 - mae: 7691.3120 \nEpoch 14/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7364.9795 - mae: 7364.3979 \nEpoch 15/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7607.3086 - mae: 7607.0210 \nEpoch 16/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7353.7129 - mae: 7355.3892 \nEpoch 17/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7586.5000 - mae: 7588.1470 \nEpoch 18/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7415.3809 - mae: 7417.1250 \nEpoch 19/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7483.4326 - mae: 7482.9375 \nEpoch 20/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7292.5049 - mae: 7293.6855 \nEpoch 21/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7162.9189 - mae: 7164.5073 \nEpoch 22/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7111.6421 - mae: 7113.2026 \nEpoch 23/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7456.3916 - mae: 7457.7910 \nEpoch 24/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7365.7212 - mae: 7367.5229 \nEpoch 25/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7279.4243 - mae: 7281.1890 \nEpoch 26/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7449.4932 - mae: 7451.3252 \nEpoch 27/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7599.8701 - mae: 7601.7549 \nEpoch 28/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7583.5103 - mae: 7585.4800 \nEpoch 29/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7538.4189 - mae: 7538.3672 \nEpoch 30/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7374.6274 - mae: 7373.7964 \nEpoch 31/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7512.4541 - mae: 7514.2744 \nEpoch 32/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7169.3174 - mae: 7170.9644 \nEpoch 33/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7549.1431 - mae: 7548.4082 \nEpoch 34/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7474.6357 - mae: 7474.4302 \nEpoch 35/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7465.6431 - mae: 7464.9517 \nEpoch 36/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7196.7998 - mae: 7198.3965 \nEpoch 37/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7352.3115 - mae: 7351.4849 \nEpoch 38/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7142.7383 - mae: 7144.3521 \nEpoch 39/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7503.1768 - mae: 7504.5981 \nEpoch 40/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7118.6587 - mae: 7119.8096 \nEpoch 41/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7112.3354 - mae: 7114.0605 \nEpoch 42/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7462.2832 - mae: 7461.5244 \nEpoch 43/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7346.6943 - mae: 7348.2617 \nEpoch 44/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7415.4521 - mae: 7417.4438 \nEpoch 45/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7500.2046 - mae: 7502.2544 \nEpoch 46/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7337.9058 - mae: 7336.8755 \nEpoch 47/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7528.6450 - mae: 7530.6118 \nEpoch 48/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7063.4771 - mae: 7062.5732 \nEpoch 49/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7507.3882 - mae: 7508.8213 \nEpoch 50/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7314.5952 - mae: 7315.7446 \nEpoch 51/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7212.9307 - mae: 7213.8398 \nEpoch 52/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 7003.8774 - mae: 7005.1030\nEpoch 53/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 7612.7798 - mae: 7612.1279\nEpoch 54/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7400.9927 - mae: 7402.2505 \nEpoch 55/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7205.1304 - mae: 7206.3662 \nEpoch 56/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7654.4155 - mae: 7653.8765 \nEpoch 57/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7498.5645 - mae: 7500.0942 \nEpoch 58/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7039.6548 - mae: 7041.3203 \nEpoch 59/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7052.3594 - mae: 7054.0659 \nEpoch 60/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7401.7954 - mae: 7402.9146 \nEpoch 61/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7427.4692 - mae: 7428.5938 \nEpoch 62/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7220.0957 - mae: 7221.0854 \nEpoch 63/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7227.1289 - mae: 7228.6035 \nEpoch 64/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7358.0630 - mae: 7359.9058 \nEpoch 65/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7313.3682 - mae: 7314.6367 \nEpoch 66/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7126.7109 - mae: 7127.9390 \nEpoch 67/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7636.0039 - mae: 7637.9336 \nEpoch 68/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7545.4844 - mae: 7546.5649 \nEpoch 69/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7309.5737 - mae: 7310.5527 \nEpoch 70/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 964us/step - loss: 7227.6304 - mae: 7228.5708\nEpoch 71/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 990us/step - loss: 7336.1177 - mae: 7337.3599\nEpoch 72/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7519.1953 - mae: 7517.5044 \nEpoch 73/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 896us/step - loss: 7305.5000 - mae: 7306.3354\nEpoch 74/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7256.1470 - mae: 7257.1094 \nEpoch 75/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7418.2930 - mae: 7419.3623 \nEpoch 76/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7111.0195 - mae: 7111.5967 \nEpoch 77/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7270.0205 - mae: 7271.0908 \nEpoch 78/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7286.1211 - mae: 7288.2881 \nEpoch 79/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 7291.9937 - mae: 7293.2183 \nEpoch 80/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 7030.7515 - mae: 7031.7642\nEpoch 81/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7248.4795 - mae: 7250.4644 \nEpoch 82/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 955us/step - loss: 7159.0327 - mae: 7157.6870\nEpoch 83/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7266.6235 - mae: 7265.4092 \nEpoch 84/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7252.4639 - mae: 7251.0972 \nEpoch 85/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7100.6050 - mae: 7102.5430 \nEpoch 86/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1000us/step - loss: 6894.0845 - mae: 6892.3042\nEpoch 87/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 963us/step - loss: 7253.1851 - mae: 7253.9346\nEpoch 88/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 985us/step - loss: 6853.4917 - mae: 6854.4634\nEpoch 89/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7407.5293 - mae: 7408.5239 \nEpoch 90/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7160.0776 - mae: 7161.1372 \nEpoch 91/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7438.4497 - mae: 7439.7275 \nEpoch 92/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7405.0752 - mae: 7406.1782 \nEpoch 93/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7163.6968 - mae: 7165.8018 \nEpoch 94/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 6872.2222 - mae: 6871.1763 \nEpoch 95/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7041.5376 - mae: 7042.3149 \nEpoch 96/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7134.1865 - mae: 7135.0161 \nEpoch 97/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 6960.7451 - mae: 6962.7158 \nEpoch 98/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 6972.2271 - mae: 6972.9800 \nEpoch 99/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7359.0928 - mae: 7360.9976 \nEpoch 100/100\n34/34 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 7591.9121 - mae: 7593.5176 \n</pre> Out[74]: <pre>&lt;keras.src.callbacks.history.History at 0x1d92d75b710&gt;</pre> In\u00a0[75]: Copied! <pre># Check the results of the insurance model\ninsurance_model.evaluate(X_test, y_test)\n</pre> # Check the results of the insurance model insurance_model.evaluate(X_test, y_test) <pre>9/9 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 8473.0605 - mae: 8437.1006 \n</pre> Out[75]: <pre>[8610.08203125, 8430.283203125]</pre> <p>Our model didn't perform very well, let's try a bigger model.</p> <p>We'll try 3 things:</p> <ul> <li>Increasing the number of layers (2 -&gt; 3).</li> <li>Increasing the number of units in each layer (except for the output layer).</li> <li>Changing the optimizer (from SGD to Adam).</li> </ul> <p>Everything else will stay the same.</p> In\u00a0[76]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Add an extra layer and increase number of units\ninsurance_model_2 = tf.keras.Sequential([\n  tf.keras.layers.Dense(100), # 100 units\n  tf.keras.layers.Dense(10), # 10 units\n  tf.keras.layers.Dense(1) # 1 unit (important for output layer)\n])\n\n# Compile the model\ninsurance_model_2.compile(loss=tf.keras.losses.mae,\n                          optimizer=tf.keras.optimizers.Adam(), # Adam works but SGD doesn't \n                          metrics=['mae'])\n\n# Fit the model and save the history (we can plot this)\nhistory = insurance_model_2.fit(X_train, y_train, epochs=100, verbose=0)\n</pre> # Set random seed tf.random.set_seed(42)  # Add an extra layer and increase number of units insurance_model_2 = tf.keras.Sequential([   tf.keras.layers.Dense(100), # 100 units   tf.keras.layers.Dense(10), # 10 units   tf.keras.layers.Dense(1) # 1 unit (important for output layer) ])  # Compile the model insurance_model_2.compile(loss=tf.keras.losses.mae,                           optimizer=tf.keras.optimizers.Adam(), # Adam works but SGD doesn't                            metrics=['mae'])  # Fit the model and save the history (we can plot this) history = insurance_model_2.fit(X_train, y_train, epochs=100, verbose=0) In\u00a0[77]: Copied! <pre># Evaluate our larger model\ninsurance_model_2.evaluate(X_test, y_test)\n</pre> # Evaluate our larger model insurance_model_2.evaluate(X_test, y_test) <pre>9/9 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 4680.3613 - mae: 4654.7310  \n</pre> Out[77]: <pre>[4663.1640625, 4535.0126953125]</pre> <p>Much better! Using a larger model and the Adam optimizer results in almost half the error as the previous model.</p> <p>\ud83d\udd11 Note: For many problems, the Adam optimizer is a great starting choice. See Andrei Karpathy's \"Adam is safe\" point from A Recipe for Training Neural Networks for more.</p> <p>Let's check out the loss curves of our model, we should see a downward trend.</p> In\u00a0[78]: Copied! <pre># Plot history (also known as a loss curve)\npd.DataFrame(history.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\");\n</pre> # Plot history (also known as a loss curve) pd.DataFrame(history.history).plot() plt.ylabel(\"loss\") plt.xlabel(\"epochs\"); <p>From this, it looks like our model's loss (and MAE) were both still decreasing (in our case, MAE and loss are the same, hence the lines in the plot overlap eachother).</p> <p>What this tells us is the loss might go down if we try training it for longer.</p> <p>\ud83e\udd14 Question: How long should you train for?</p> <p>It depends on what problem you're working on. Sometimes training won't take very long, other times it'll take longer than you expect. A common method is to set your model training for a very long time (e.g. 1000's of epochs) but set it up with an EarlyStopping callback so it stops automatically when it stops improving. We'll see this in another module.</p> <p>Let's train the same model as above for a little longer. We can do this but calling fit on it again.</p> In\u00a0[79]: Copied! <pre># Try training for a little longer (100 more epochs)\nhistory_2 = insurance_model_2.fit(X_train, y_train, epochs=100, verbose=0)\n</pre> # Try training for a little longer (100 more epochs) history_2 = insurance_model_2.fit(X_train, y_train, epochs=100, verbose=0) <p>How did the extra training go?</p> In\u00a0[80]: Copied! <pre># Evaluate the model trained for 200 total epochs\ninsurance_model_2_loss, insurance_model_2_mae = insurance_model_2.evaluate(X_test, y_test)\ninsurance_model_2_loss, insurance_model_2_mae\n</pre> # Evaluate the model trained for 200 total epochs insurance_model_2_loss, insurance_model_2_mae = insurance_model_2.evaluate(X_test, y_test) insurance_model_2_loss, insurance_model_2_mae <pre>9/9 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 3468.0479 - mae: 3454.6157 \n</pre> Out[80]: <pre>(3457.428955078125, 3390.26806640625)</pre> <p>Boom! Training for an extra 100 epochs we see about a 10% decrease in error.</p> <p>How does the visual look?</p> In\u00a0[81]: Copied! <pre># Plot the model trained for 200 total epochs loss curves\npd.DataFrame(history_2.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\"); # note: epochs will only show 100 since we overrid the history variable\n</pre> # Plot the model trained for 200 total epochs loss curves pd.DataFrame(history_2.history).plot() plt.ylabel(\"loss\") plt.xlabel(\"epochs\"); # note: epochs will only show 100 since we overrid the history variable In\u00a0[82]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Read in the insurance dataset\ninsurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n</pre> import pandas as pd import matplotlib.pyplot as plt import tensorflow as tf  # Read in the insurance dataset insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\") In\u00a0[83]: Copied! <pre># Check out the data\ninsurance.head()\n</pre> # Check out the data insurance.head() Out[83]: age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 16884.92400 1 18 male 33.770 1 no southeast 1725.55230 2 28 male 33.000 3 no southeast 4449.46200 3 33 male 22.705 0 no northwest 21984.47061 4 32 male 28.880 0 no northwest 3866.85520 <p>Now, just as before, we need to transform the non-numerical columns into numbers and this time we'll also be normalizing the numerical columns with different ranges (to make sure they're all between 0 and 1).</p> <p>To do this, we're going to use a few classes from Scikit-Learn:</p> <ul> <li><code>make_column_transformer</code> - build a multi-step data preprocessing function for the folllowing transformations:<ul> <li><code>MinMaxScaler</code> - make sure all numerical columns are normalized (between 0 and 1).</li> <li><code>OneHotEncoder</code> - one hot encode the non-numerical columns.</li> </ul> </li> </ul> <p>Let's see them in action.</p> In\u00a0[84]: Copied! <pre>from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\n# Create column transformer (this will help us normalize/preprocess our data)\nct = make_column_transformer(\n    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # get all values between 0 and 1\n    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"])\n)\n\n# Create X &amp; y\nX = insurance.drop(\"charges\", axis=1)\ny = insurance[\"charges\"]\n\n# Build our train and test sets (use random state to ensure same split as before)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit column transformer on the training data only (doing so on test data would result in data leakage)\nct.fit(X_train)\n\n# Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)\nX_train_normal = ct.transform(X_train)\nX_test_normal = ct.transform(X_test)\n</pre> from sklearn.compose import make_column_transformer from sklearn.preprocessing import MinMaxScaler, OneHotEncoder  # Create column transformer (this will help us normalize/preprocess our data) ct = make_column_transformer(     (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # get all values between 0 and 1     (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"]) )  # Create X &amp; y X = insurance.drop(\"charges\", axis=1) y = insurance[\"charges\"]  # Build our train and test sets (use random state to ensure same split as before) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Fit column transformer on the training data only (doing so on test data would result in data leakage) ct.fit(X_train)  # Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder) X_train_normal = ct.transform(X_train) X_test_normal = ct.transform(X_test) <p>Now we've normalized it and one-hot encoding it, what does our data look like now?</p> In\u00a0[85]: Copied! <pre># Non-normalized and non-one-hot encoded data example\nX_train.loc[0]\n</pre> # Non-normalized and non-one-hot encoded data example X_train.loc[0] Out[85]: <pre>age                19\nsex            female\nbmi              27.9\nchildren            0\nsmoker            yes\nregion      southwest\nName: 0, dtype: object</pre> In\u00a0[86]: Copied! <pre># Normalized and one-hot encoded example\nX_train_normal[0]\n</pre> # Normalized and one-hot encoded example X_train_normal[0] Out[86]: <pre>array([0.60869565, 0.10734463, 0.4       , 1.        , 0.        ,\n       1.        , 0.        , 0.        , 1.        , 0.        ,\n       0.        ])</pre> <p>How about the shapes?</p> In\u00a0[87]: Copied! <pre># Notice the normalized/one-hot encoded shape is larger because of the extra columns\nX_train_normal.shape, X_train.shape\n</pre> # Notice the normalized/one-hot encoded shape is larger because of the extra columns X_train_normal.shape, X_train.shape Out[87]: <pre>((1070, 11), (1070, 6))</pre> <p>Our data is normalized and numerical, let's model it.</p> <p>We'll use the same model as <code>insurance_model_2</code>.</p> In\u00a0[88]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Build the model (3 layers, 100, 10, 1 units)\ninsurance_model_3 = tf.keras.Sequential([\n  tf.keras.layers.Dense(100),\n  tf.keras.layers.Dense(10),\n  tf.keras.layers.Dense(1)\n])\n\n# Compile the model\ninsurance_model_3.compile(loss=tf.keras.losses.mae,\n                          optimizer=tf.keras.optimizers.Adam(),\n                          metrics=['mae'])\n\n# Fit the model for 200 epochs (same as insurance_model_2)\ninsurance_model_3.fit(X_train_normal, y_train, epochs=200, verbose=0) \n</pre> # Set random seed tf.random.set_seed(42)  # Build the model (3 layers, 100, 10, 1 units) insurance_model_3 = tf.keras.Sequential([   tf.keras.layers.Dense(100),   tf.keras.layers.Dense(10),   tf.keras.layers.Dense(1) ])  # Compile the model insurance_model_3.compile(loss=tf.keras.losses.mae,                           optimizer=tf.keras.optimizers.Adam(),                           metrics=['mae'])  # Fit the model for 200 epochs (same as insurance_model_2) insurance_model_3.fit(X_train_normal, y_train, epochs=200, verbose=0)  Out[88]: <pre>&lt;keras.src.callbacks.history.History at 0x1d92d8652b0&gt;</pre> <p>Let's evaluate the model on normalized test set.</p> In\u00a0[89]: Copied! <pre># Evaulate 3rd model\ninsurance_model_3_loss, insurance_model_3_mae = insurance_model_3.evaluate(X_test_normal, y_test)\n</pre> # Evaulate 3rd model insurance_model_3_loss, insurance_model_3_mae = insurance_model_3.evaluate(X_test_normal, y_test) <pre>9/9 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 3221.2153 - mae: 3213.6387 \n</pre> <p>And finally, let's compare the results from <code>insurance_model_2</code> (trained on non-normalized data) and <code>insurance_model_3</code> (trained on normalized data).</p> In\u00a0[90]: Copied! <pre># Compare modelling results from non-normalized data and normalized data\ninsurance_model_2_mae, insurance_model_3_mae\n</pre> # Compare modelling results from non-normalized data and normalized data insurance_model_2_mae, insurance_model_3_mae Out[90]: <pre>(3390.26806640625, 3161.1298828125)</pre> <p>From this we can see normalizing the data results in 10% less error using the same model than not normalizing the data.</p> <p>This is one of the main benefits of normalization: faster convergence time (a fancy way of saying, your model gets to better results faster).</p> <p><code>insurance_model_2</code> may have eventually achieved the same results as <code>insurance_model_3</code> if we left it training for longer.</p> <p>Also, the results may change if we were to alter the architectures of the models, e.g. more hidden units per layer or more layers.</p> <p>But since our main goal as neural network practitioners is to decrease the time between experiments, anything that helps us get better results sooner is a plus.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#01-neural-network-regression-with-tensorflow","title":"01. Neural Network Regression with TensorFlow\u00b6","text":"<p>There are many definitions for a regression problem but in our case, we're going to simplify it to be: predicting a number.</p> <p>For example, you might want to:</p> <ul> <li>Predict the selling price of houses given information about them (such as number of rooms, size, number of bathrooms).</li> <li>Predict the coordinates of a bounding box of an item in an image.</li> <li>Predict the cost of medical insurance for an individual given their demographics (age, sex, gender, race).</li> </ul> <p>In this notebook, we're going to set the foundations for how you can take a sample of inputs (this is your data), build a neural network to discover patterns in those inputs and then make a prediction (in the form of a number) based on those inputs.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Specifically, we're going to go through doing the following with TensorFlow:</p> <ul> <li>Architecture of a regression model</li> <li>Input shapes and output shapes<ul> <li><code>X</code>: features/data (inputs)</li> <li><code>y</code>: labels (outputs)</li> </ul> </li> <li>Creating custom data to view and fit</li> <li>Steps in modelling<ul> <li>Creating a model</li> <li>Compiling a model<ul> <li>Defining a loss function</li> <li>Setting up an optimizer</li> <li>Creating evaluation metrics</li> </ul> </li> <li>Fitting a model (getting it to find patterns in our data)</li> </ul> </li> <li>Evaluating a model<ul> <li>Visualizng the model (\"visualize, visualize, visualize\")</li> <li>Looking at training curves</li> <li>Compare predictions to ground truth (using our evaluation metrics)</li> </ul> </li> <li>Saving a model (so we can use it later)</li> <li>Loading a model</li> </ul> <p>Don't worry if none of these make sense now, we're going to go through each.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#typical-architecture-of-a-regresison-neural-network","title":"Typical architecture of a regresison neural network\u00b6","text":"<p>The word typical is on purpose.</p> <p>Why?</p> <p>Because there are many different ways (actually, there's almost an infinite number of ways) to write neural networks.</p> <p>But the following is a generic setup for ingesting a collection of numbers, finding patterns in them and then outputting some kind of target number.</p> <p>Yes, the previous sentence is vague but we'll see this in action shortly.</p> Hyperparameter Typical value Input layer shape Same shape as number of features (e.g. 3 for # bedrooms, # bathrooms, # car spaces in housing price prediction) Hidden layer(s) Problem specific, minimum = 1, maximum = unlimited Neurons per hidden layer Problem specific, generally 10 to 100 Output layer shape Same shape as desired prediction shape (e.g. 1 for house price) Hidden activation Usually ReLU (rectified linear unit) Output activation None, ReLU, logistic/tanh Loss function MSE (mean square error) or MAE (mean absolute error)/Huber (combination of MAE/MSE) if outliers Optimizer SGD (stochastic gradient descent), Adam <p>Table 1: Typical architecture of a regression network. Source: Adapted from page 293 of Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow Book by Aur\u00e9lien G\u00e9ron</p> <p>Again, if you're new to neural networks and deep learning in general, much of the above table won't make sense. But don't worry, we'll be getting hands-on with all of it soon.</p> <p>\ud83d\udd11 Note: A hyperparameter in machine learning is something a data analyst or developer can set themselves, where as a parameter usually describes something a model learns on its own (a value not explicitly set by an analyst).</p> <p>Okay, enough talk, let's get started writing code.</p> <p>To use TensorFlow, we'll import it as the common alias <code>tf</code> (short for TensorFlow).</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#creating-data-to-view-and-fit","title":"Creating data to view and fit\u00b6","text":"<p>Since we're working on a regression problem (predicting a number) let's create some linear data (a straight line) to model.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#regression-input-shapes-and-output-shapes","title":"Regression input shapes and output shapes\u00b6","text":"<p>One of the most important concepts when working with neural networks are the input and output shapes.</p> <p>The input shape is the shape of your data that goes into the model.</p> <p>The output shape is the shape of your data you want to come out of your model.</p> <p>These will differ depending on the problem you're working on.</p> <p>Neural networks accept numbers and output numbers. These numbers are typically represented as tensors (or arrays).</p> <p>Before, we created data using NumPy arrays, but we could do the same with tensors.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#steps-in-modelling-with-tensorflow","title":"Steps in modelling with TensorFlow\u00b6","text":"<p>Now we know what data we have as well as the input and output shapes, let's see how we'd build a neural network to model it.</p> <p>In TensorFlow, there are typically 3 fundamental steps to creating and training a model.</p> <ol> <li>Creating a model - piece together the layers of a neural network yourself (using the Functional or Sequential API) or import a previously built model (known as transfer learning).</li> <li>Compiling a model - defining how a models performance should be measured (loss/metrics) as well as defining how it should improve (optimizer).</li> <li>Fitting a model - letting the model try to find patterns in the data (how does <code>X</code> get to <code>y</code>).</li> </ol> <p>Let's see these in action using the Keras Sequential API to build a model for our regression data. And then we'll step through each.</p> <p>Note: If you're using TensorFlow 2.7.0+, the <code>fit()</code> function no longer upscales input data to go from <code>(batch_size, )</code> to <code>(batch_size, 1)</code>. To fix this, you'll need to expand the dimension of input data using <code>tf.expand_dims(input_data, axis=-1)</code>.</p> <p>In our case, this means instead of using <code>model.fit(X, y, epochs=5)</code>, use <code>model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)</code>.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#improving-a-model","title":"Improving a model\u00b6","text":"<p>How do you think you'd improve upon our current model?</p> <p>If you guessed by tweaking some of the things we did above, you'd be correct.</p> <p>To improve our model, we alter almost every part of the 3 steps we went through before.</p> <ol> <li>Creating a model - here you might want to add more layers, increase the number of hidden units (also called neurons) within each layer, change the activation functions of each layer.</li> <li>Compiling a model - you might want to choose optimization function or perhaps change the learning rate of the optimization function.</li> <li>Fitting a model - perhaps you could fit a model for more epochs (leave it training for longer) or on more data (give the model more examples to learn from).</li> </ol> <p> There are many different ways to potentially improve a neural network. Some of the most common include: increasing the number of layers (making the network deeper), increasing the number of hidden units (making the network wider) and changing the learning rate. Because these values are all human-changeable, they're referred to as hyperparameters) and the practice of trying to find the best hyperparameters is referred to as hyperparameter tuning.</p> <p>Woah. We just introduced a bunch of possible steps. The important thing to remember is how you alter each of these will depend on the problem you're working on.</p> <p>And the good thing is, over the next few problems, we'll get hands-on with all of them.</p> <p>For now, let's keep it simple, all we'll do is train our model for longer (everything else will stay the same).</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#evaluating-a-model","title":"Evaluating a model\u00b6","text":"<p>A typical workflow you'll go through when building neural networks is:</p> <pre><code>Build a model -&gt; evaluate it -&gt; build (tweak) a model -&gt; evaulate it -&gt; build (tweak) a model -&gt; evaluate it...\n</code></pre> <p>The tweaking comes from maybe not building a model from scratch but adjusting an existing one.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#visualize-visualize-visualize","title":"Visualize, visualize, visualize\u00b6","text":"<p>When it comes to evaluation, you'll want to remember the words: \"visualize, visualize, visualize.\"</p> <p>This is because you're probably better looking at something (doing) than you are thinking about something.</p> <p>It's a good idea to visualize:</p> <ul> <li>The data - what data are you working with? What does it look like?</li> <li>The model itself - what does the architecture look like? What are the different shapes?</li> <li>The training of a model - how does a model perform while it learns?</li> <li>The predictions of a model - how do the predictions of a model line up against the ground truth (the original labels)?</li> </ul> <p>Let's start by visualizing the model.</p> <p>But first, we'll create a little bit of a bigger dataset and a new model we can use (it'll be the same as before, but the more practice the better).</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#split-data-into-trainingtest-set","title":"Split data into training/test set\u00b6","text":"<p>One of the other most common and important steps in a machine learning project is creating a training and test set (and when required, a validation set).</p> <p>Each set serves a specific purpose:</p> <ul> <li>Training set - the model learns from this data, which is typically 70-80% of the total data available (like the course materials you study during the semester).</li> <li>Validation set - the model gets tuned on this data, which is typically 10-15% of the total data available (like the practice exam you take before the final exam).</li> <li>Test set - the model gets evaluated on this data to test what it has learned, it's typically 10-15% of the total data available (like the final exam you take at the end of the semester).</li> </ul> <p>For now, we'll just use a training and test set, this means we'll have a dataset for our model to learn on as well as be evaluated on.</p> <p>We can create them by splitting our <code>X</code> and <code>y</code> arrays.</p> <p>\ud83d\udd11 Note: When dealing with real-world data, this step is typically done right at the start of a project (the test set should always be kept separate from all other data). We want our model to learn on training data and then evaluate it on test data to get an indication of how well it generalizes to unseen examples.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#visualizing-the-data","title":"Visualizing the data\u00b6","text":"<p>Now we've got our training and test data, it's a good idea to visualize it.</p> <p>Let's plot it with some nice colours to differentiate what's what.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#visualizing-the-model","title":"Visualizing the model\u00b6","text":"<p>After you've built a model, you might want to take a look at it (especially if you haven't built many before).</p> <p>You can take a look at the layers and shapes of your model by calling <code>summary()</code> on it.</p> <p>\ud83d\udd11 Note: Visualizing a model is particularly helpful when you run into input and output shape mismatches.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#visualizing-the-predictions","title":"Visualizing the predictions\u00b6","text":"<p>Now we've got a trained model, let's visualize some predictions.</p> <p>To visualize predictions, it's always a good idea to plot them against the ground truth labels.</p> <p>Often you'll see this in the form of <code>y_test</code> vs. <code>y_pred</code> (ground truth vs. predictions).</p> <p>First, we'll make some predictions on the test data (<code>X_test</code>), remember the model has never seen the test data.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#evaluating-predictions","title":"Evaluating predictions\u00b6","text":"<p>Alongisde visualizations, evaulation metrics are your alternative best option for evaluating your model.</p> <p>Depending on the problem you're working on, different models have different evaluation metrics.</p> <p>Two of the main metrics used for regression problems are:</p> <ul> <li>Mean absolute error (MAE) - the mean difference between each of the predictions.</li> <li>Mean squared error (MSE) - the squared mean difference between of the predictions (use if larger errors are more detrimental than smaller errors).</li> </ul> <p>The lower each of these values, the better.</p> <p>You can also use <code>model.evaluate()</code> which will return the loss of the model as well as any metrics setup during the compile step.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#running-experiments-to-improve-a-model","title":"Running experiments to improve a model\u00b6","text":"<p>After seeing the evaluation metrics and the predictions your model makes, it's likely you'll want to improve it.</p> <p>Again, there are many different ways you can do this, but 3 of the main ones are:</p> <ol> <li>Get more data - get more examples for your model to train on (more opportunities to learn patterns).</li> <li>Make your model larger (use a more complex model) - this might come in the form of more layers or more hidden units in each layer.</li> <li>Train for longer - give your model more of a chance to find the patterns in the data.</li> </ol> <p>Since we created our dataset, we could easily make more data but this isn't always the case when you're working with real-world datasets.</p> <p>So let's take a look at how we can improve our model using 2 and 3.</p> <p>To do so, we'll build 3 models and compare their results:</p> <ol> <li><code>model_1</code> - same as original model, 1 layer, trained for 100 epochs.</li> <li><code>model_2</code> - 2 layers, trained for 100 epochs.</li> <li><code>model_3</code> - 2 layers, trained for 500 epochs.</li> </ol> <p>Build <code>model_1</code></p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#comparing-results","title":"Comparing results\u00b6","text":"<p>Now we've got results for 3 similar but slightly different results, let's compare them.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#tracking-your-experiments","title":"Tracking your experiments\u00b6","text":"<p>One really good habit to get into is tracking your modelling experiments to see which perform better than others.</p> <p>We've done a simple version of this above (keeping the results in different variables).</p> <p>\ud83d\udcd6 Resource: But as you build more models, you'll want to look into using tools such as:</p> <ul> <li>TensorBoard - a component of the TensorFlow library to help track modelling experiments (we'll see this later).</li> <li>Weights &amp; Biases - a tool for tracking all kinds of machine learning experiments (the good news for Weights &amp; Biases is it plugs into TensorBoard).</li> </ul>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#saving-a-model","title":"Saving a model\u00b6","text":"<p>Once you've trained a model and found one which performs to your liking, you'll probably want to save it for use elsewhere (like a web application or mobile device).</p> <p>You can save a TensorFlow/Keras model using <code>model.save()</code>.</p> <p>There are two ways to save a model in TensorFlow:</p> <ol> <li>The SavedModel format (default).</li> <li>The HDF5 format.</li> </ol> <p>The main difference between the two is the SavedModel is automatically able to save custom objects (such as special layers) without additional modifications when loading the model back in.</p> <p>Which one should you use?</p> <p>It depends on your situation but the SavedModel format will suffice most of the time.</p> <p>Both methods use the same method call.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#loading-a-model","title":"Loading a model\u00b6","text":"<p>We can load a saved model using the <code>load_model()</code> method.</p> <p>Loading a model for the different formats (SavedModel and HDF5) is the same (as long as the pathnames to the particular formats are correct).</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#downloading-a-model-from-google-colab","title":"Downloading a model (from Google Colab)\u00b6","text":"<p>Say you wanted to get your model from Google Colab to your local machine, you can do one of the following things:</p> <ul> <li>Right click on the file in the files pane and click 'download'.</li> <li>Use the code below.</li> </ul>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#a-larger-example","title":"A larger example\u00b6","text":"<p>Alright, we've seen the fundamentals of building neural network regression models in TensorFlow.</p> <p>Let's step it up a notch and build a model for a more feature rich dataset.</p> <p>More specifically we're going to try predict the cost of medical insurance for individuals based on a number of different parameters such as, <code>age</code>, <code>sex</code>, <code>bmi</code>, <code>children</code>, <code>smoking_status</code> and <code>residential_region</code>.</p> <p>To do, we'll leverage the pubically available Medical Cost dataset available from Kaggle and hosted on GitHub.</p> <p>\ud83d\udd11 Note: When learning machine learning paradigms, you'll often go through a series of foundational techniques and then practice them by working with open-source datasets and examples. Just as we're doing now, learn foundations, put them to work with different problems. Every time you work on something new, it's a good idea to search for something like \"problem X example with Python/TensorFlow\" where you substitute X for your problem.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#preprocessing-data-normalization-and-standardization","title":"Preprocessing data (normalization and standardization)\u00b6","text":"<p>A common practice when working with neural networks is to make sure all of the data you pass to them is in the range 0 to 1.</p> <p>This practice is called normalization (scaling all values from their original range to, e.g. between 0 and 100,000 to be between 0 and 1).</p> <p>There is another process call standardization which converts all of your data to unit variance and 0 mean.</p> <p>These two practices are often part of a preprocessing pipeline (a series of functions to prepare your data for use with neural networks).</p> <p>Knowing this, some of the major steps you'll take to preprocess your data for a neural network include:</p> <ul> <li>Turning all of your data to numbers (a neural network can't handle strings).</li> <li>Making sure your data is in the right shape (verifying input and output shapes).</li> <li>Feature scaling:<ul> <li>Normalizing data (making sure all values are between 0 and 1). This is done by subtracting the minimum value then dividing by the maximum value minus the minimum. This is also referred to as min-max scaling.</li> <li>Standardization (making sure all values have a mean of 0 and a variance of 1). This is done by subtracting the mean value from the target feature and then dividing it by the standard deviation.</li> <li>Which one should you use?<ul> <li>With neural networks you'll tend to favour normalization as they tend to prefer values between 0 and 1 (you'll see this espcially with image processing), however, you'll often find a neural network can perform pretty well with minimal feature scaling.</li> </ul> </li> </ul> </li> </ul> <p>\ud83d\udcd6 Resource: For more on preprocessing data, I'd recommend reading the following resources:</p> <ul> <li>Scikit-Learn's documentation on preprocessing data.</li> <li>Scale, Standardize or Normalize with Scikit-Learn by Jeff Hale.</li> </ul> <p>We've already turned our data into numbers using <code>get_dummies()</code>, let's see how we'd normalize it as well.</p>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<p>We've a covered a whole lot pretty quickly.</p> <p>So now it's time to have a play around with a few things and start to build up your intuition.</p> <p>I emphasise the words play around because that's very important. Try a few things out, run the code and see what happens.</p> <ol> <li>Create your own regression dataset (or make the one we created in \"Create data to view and fit\" bigger) and build fit a model to it.</li> <li>Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform?</li> <li>Try and improve the results we got on the insurance dataset, some things you might want to try include:</li> </ol> <ul> <li>Building a larger model (how does one with 4 dense layers go?).</li> <li>Increasing the number of units in each layer.</li> <li>Lookup the documentation of Adam and find out what the first parameter is, what happens if you increase it by 10x?</li> <li>What happens if you train for longer (say 300 epochs instead of 200)?</li> </ul> <ol> <li>Import the Boston pricing dataset from TensorFlow <code>tf.keras.datasets</code> and model it.</li> </ol>"},{"location":"Learning/Tensorflow/01_neural_network_regression_in_tensorflow/#extra-curriculum","title":"\ud83d\udcd6 Extra curriculum\u00b6","text":"<p>If you're looking for extra materials relating to this notebook, I'd check out the following:</p> <ul> <li>MIT introduction deep learning lecture 1 - gives a great overview of what's happening behind all of the code we're running.</li> <li>Reading: 1-hour of Chapter 1 of Neural Networks and Deep Learning by Michael Nielson - a great in-depth and hands-on example of the intuition behind neural networks.</li> </ul> <p>To practice your regression modelling with TensorFlow, I'd also encourage you to look through Lion Bridge's collection of datasets or Kaggle's datasets, find a regression dataset which sparks your interest and try to model.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/","title":"02. Neural Network Classification with TensorFlow","text":"In\u00a0[1]: Copied! <pre>import tensorflow as tf\nprint(tf.__version__)\n\nimport datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> import tensorflow as tf print(tf.__version__)  import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>2.17.0-dev20240226\nNotebook last run (end-to-end): 2024-04-19 14:13:33.923266\n</pre> In\u00a0[2]: Copied! <pre>from sklearn.datasets import make_circles\n\n# Make 1000 examples\nn_samples = 1000\n\n# Create circles\nX, y = make_circles(n_samples,\n                    noise=0.03,\n                    random_state=42)\n</pre> from sklearn.datasets import make_circles  # Make 1000 examples n_samples = 1000  # Create circles X, y = make_circles(n_samples,                     noise=0.03,                     random_state=42) <p>Wonderful, now we've created some data, let's look at the features (<code>X</code>) and labels (<code>y</code>).</p> In\u00a0[3]: Copied! <pre># Check out the features\nX\n</pre> # Check out the features X Out[3]: <pre>array([[ 0.75424625,  0.23148074],\n       [-0.75615888,  0.15325888],\n       [-0.81539193,  0.17328203],\n       ...,\n       [-0.13690036, -0.81001183],\n       [ 0.67036156, -0.76750154],\n       [ 0.28105665,  0.96382443]])</pre> In\u00a0[4]: Copied! <pre># See the first 10 labels\ny[:10]\n</pre> # See the first 10 labels y[:10] Out[4]: <pre>array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0], dtype=int64)</pre> <p>Okay, we've seen some of our data and labels, how about we move towards visualizing?</p> <p>\ud83d\udd11 Note: One important step of starting any kind of machine learning project is to become one with the data. And one of the best ways to do this is to visualize the data you're working with as much as possible. The data explorer's motto is \"visualize, visualize, visualize\".</p> <p>We'll start with a DataFrame.</p> In\u00a0[5]: Copied! <pre># Make dataframe of features and labels\nimport pandas as pd\ncircles = pd.DataFrame({\"X0\":X[:, 0], \"X1\":X[:, 1], \"label\":y})\ncircles.head()\n</pre> # Make dataframe of features and labels import pandas as pd circles = pd.DataFrame({\"X0\":X[:, 0], \"X1\":X[:, 1], \"label\":y}) circles.head() Out[5]: X0 X1 label 0 0.754246 0.231481 1 1 -0.756159 0.153259 1 2 -0.815392 0.173282 1 3 -0.393731 0.692883 1 4 0.442208 -0.896723 0 <p>What kind of labels are we dealing with?</p> In\u00a0[6]: Copied! <pre># Check out the different labels\ncircles.label.value_counts()\n</pre> # Check out the different labels circles.label.value_counts() Out[6]: <pre>label\n1    500\n0    500\nName: count, dtype: int64</pre> <p>Alright, looks like we're dealing with a binary classification problem. It's binary because there are only two labels (0 or 1).</p> <p>If there were more label options (e.g. 0, 1, 2, 3 or 4), it would be called multiclass classification.</p> <p>Let's take our visualization a step further and plot our data.</p> In\u00a0[7]: Copied! <pre># Visualize with a plot\nimport matplotlib.pyplot as plt\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);\n</pre> # Visualize with a plot import matplotlib.pyplot as plt plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu); <p>Nice! From the plot, can you guess what kind of model we might want to build?</p> <p>How about we try and build one to classify blue or red dots? As in, a model which is able to distinguish blue from red dots.</p> <p>\ud83d\udee0 Practice: Before pushing forward, you might want to spend 10 minutes playing around with the TensorFlow Playground. Try adjusting the different hyperparameters you see and click play to see a neural network train. I think you'll find the data very similar to what we've just created.</p> In\u00a0[8]: Copied! <pre># Check the shapes of our features and labels\nX.shape, y.shape\n</pre> # Check the shapes of our features and labels X.shape, y.shape Out[8]: <pre>((1000, 2), (1000,))</pre> <p>Hmm, where do these numbers come from?</p> In\u00a0[9]: Copied! <pre># Check how many samples we have\nlen(X), len(y)\n</pre> # Check how many samples we have len(X), len(y) Out[9]: <pre>(1000, 1000)</pre> <p>So we've got as many <code>X</code> values as we do <code>y</code> values, that makes sense.</p> <p>Let's check out one example of each.</p> In\u00a0[10]: Copied! <pre># View the first example of features and labels\nX[0], y[0]\n</pre> # View the first example of features and labels X[0], y[0] Out[10]: <pre>(array([0.75424625, 0.23148074]), 1)</pre> <p>Alright, so we've got two <code>X</code> features which lead to one <code>y</code> value.</p> <p>This means our neural network input shape will has to accept a tensor with at least one dimension being two and output a tensor with at least one value.</p> <p>\ud83e\udd14 Note: <code>y</code> having a shape of (1000,) can seem confusing. However, this is because all <code>y</code> values are actually scalars (single values) and therefore don't have a dimension. For now, think of your output shape as being at least the same value as one example of <code>y</code> (in our case, the output from our neural network has to be at least one value).</p> In\u00a0[11]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# 1. Create the model using the Sequential API\nmodel_1 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1)\n])\n\n# 2. Compile the model\nmodel_1.compile(loss=tf.keras.losses.BinaryCrossentropy(), # binary since we are working with 2 clases (0 &amp; 1)\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['accuracy'])\n\n# 3. Fit the model\nmodel_1.fit(X, y, epochs=5)\n</pre> # Set random seed tf.random.set_seed(42)  # 1. Create the model using the Sequential API model_1 = tf.keras.Sequential([   tf.keras.layers.Dense(1) ])  # 2. Compile the model model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(), # binary since we are working with 2 clases (0 &amp; 1)                 optimizer=tf.keras.optimizers.SGD(),                 metrics=['accuracy'])  # 3. Fit the model model_1.fit(X, y, epochs=5) <pre>Epoch 1/5\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step - accuracy: 0.4693 - loss: 4.0181 \nEpoch 2/5\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5154 - loss: 0.7888\nEpoch 3/5\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5144 - loss: 0.7107\nEpoch 4/5\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5164 - loss: 0.6968\nEpoch 5/5\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5137 - loss: 0.6938\n</pre> Out[11]: <pre>&lt;keras.src.callbacks.history.History at 0x2017298b5c0&gt;</pre> <p>Looking at the accuracy metric, our model performs poorly (50% accuracy on a binary classification problem is the equivalent of guessing), but what if we trained it for longer?</p> In\u00a0[12]: Copied! <pre># Train our model for longer (more chances to look at the data)\nmodel_1.fit(X, y, epochs=200, verbose=0) # set verbose=0 to remove training updates\nmodel_1.evaluate(X, y)\n</pre> # Train our model for longer (more chances to look at the data) model_1.fit(X, y, epochs=200, verbose=0) # set verbose=0 to remove training updates model_1.evaluate(X, y) <pre>32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4852 - loss: 0.6927\n</pre> Out[12]: <pre>[0.6933518648147583, 0.4729999899864197]</pre> <p>Even after 200 passes of the data, it's still performing as if it's guessing.</p> <p>What if we added an extra layer and trained for a little longer?</p> In\u00a0[13]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# 1. Create the model (same as model_1 but with an extra layer)\nmodel_2 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1), # add an extra layer\n  tf.keras.layers.Dense(1)\n])\n\n# 2. Compile the model\nmodel_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['accuracy'])\n\n# 3. Fit the model\nmodel_2.fit(X, y, epochs=100, verbose=0) # set verbose=0 to make the output print less\n</pre> # Set random seed tf.random.set_seed(42)  # 1. Create the model (same as model_1 but with an extra layer) model_2 = tf.keras.Sequential([   tf.keras.layers.Dense(1), # add an extra layer   tf.keras.layers.Dense(1) ])  # 2. Compile the model model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),                 optimizer=tf.keras.optimizers.SGD(),                 metrics=['accuracy'])  # 3. Fit the model model_2.fit(X, y, epochs=100, verbose=0) # set verbose=0 to make the output print less Out[13]: <pre>&lt;keras.src.callbacks.history.History at 0x201743d84a0&gt;</pre> In\u00a0[14]: Copied! <pre># Evaluate the model\nmodel_2.evaluate(X, y)\n</pre> # Evaluate the model model_2.evaluate(X, y) <pre>32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4955 - loss: 8.1264 \n</pre> Out[14]: <pre>[7.96460485458374, 0.5]</pre> <p>Still not even as good as guessing (~50% accuracy)... hmm...?</p> <p>Let's remind ourselves of a couple more ways we can use to improve our models.</p> In\u00a0[15]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# 1. Create the model (this time 3 layers)\nmodel_3 = tf.keras.Sequential([\n  # Before TensorFlow 2.7.0\n  # tf.keras.layers.Dense(100), # add 100 dense neurons\n\n  # With TensorFlow 2.7.0\n  # tf.keras.layers.Dense(100, input_shape=(None, 1)), # add 100 dense neurons\n\n  ## After TensorFlow 2.8.0 ##\n  tf.keras.layers.Dense(100), # add 100 dense neurons\n  tf.keras.layers.Dense(10), # add another layer with 10 neurons\n  tf.keras.layers.Dense(1)\n])\n\n# 2. Compile the model\nmodel_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                optimizer=tf.keras.optimizers.Adam(), # use Adam instead of SGD\n                metrics=['accuracy'])\n\n# 3. Fit the model\nmodel_3.fit(X, y, epochs=100, verbose=1) # fit for 100 passes of the data\n</pre> # Set random seed tf.random.set_seed(42)  # 1. Create the model (this time 3 layers) model_3 = tf.keras.Sequential([   # Before TensorFlow 2.7.0   # tf.keras.layers.Dense(100), # add 100 dense neurons    # With TensorFlow 2.7.0   # tf.keras.layers.Dense(100, input_shape=(None, 1)), # add 100 dense neurons    ## After TensorFlow 2.8.0 ##   tf.keras.layers.Dense(100), # add 100 dense neurons   tf.keras.layers.Dense(10), # add another layer with 10 neurons   tf.keras.layers.Dense(1) ])  # 2. Compile the model model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),                 optimizer=tf.keras.optimizers.Adam(), # use Adam instead of SGD                 metrics=['accuracy'])  # 3. Fit the model model_3.fit(X, y, epochs=100, verbose=1) # fit for 100 passes of the data <pre>Epoch 1/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 3ms/step - accuracy: 0.4713 - loss: 3.7724\nEpoch 2/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4204 - loss: 1.5564\nEpoch 3/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4075 - loss: 0.8399\nEpoch 4/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4339 - loss: 0.8157\nEpoch 5/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4728 - loss: 0.7995\nEpoch 6/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7855\nEpoch 7/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7733\nEpoch 8/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7627\nEpoch 9/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7534\nEpoch 10/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.7453\nEpoch 11/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7382\nEpoch 12/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7320\nEpoch 13/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7265\nEpoch 14/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7217\nEpoch 15/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.7175\nEpoch 16/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.7138\nEpoch 17/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7106\nEpoch 18/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7078\nEpoch 19/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7054\nEpoch 20/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7033\nEpoch 21/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.7015\nEpoch 22/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7001\nEpoch 23/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.6988\nEpoch 24/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.6978\nEpoch 25/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.6969\nEpoch 26/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.6962\nEpoch 27/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.6957\nEpoch 28/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - accuracy: 0.4830 - loss: 0.6952\nEpoch 29/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.6949\nEpoch 30/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.6946\nEpoch 31/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.6943\nEpoch 32/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.6942\nEpoch 33/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.6940\nEpoch 34/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.6939\nEpoch 35/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.6939\nEpoch 36/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.6938\nEpoch 37/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4617 - loss: 0.6937\nEpoch 38/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4357 - loss: 0.6937\nEpoch 39/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4335 - loss: 0.6937\nEpoch 40/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4299 - loss: 0.6937\nEpoch 41/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4422 - loss: 0.6937\nEpoch 42/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4504 - loss: 0.6936\nEpoch 43/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4548 - loss: 0.6936\nEpoch 44/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4539 - loss: 0.6936\nEpoch 45/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4492 - loss: 0.6936\nEpoch 46/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4564 - loss: 0.6936\nEpoch 47/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4601 - loss: 0.6936\nEpoch 48/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4633 - loss: 0.6936\nEpoch 49/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4702 - loss: 0.6937\nEpoch 50/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4690 - loss: 0.6937\nEpoch 51/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4699 - loss: 0.6937\nEpoch 52/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4697 - loss: 0.6937\nEpoch 53/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4694 - loss: 0.6937\nEpoch 54/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4691 - loss: 0.6937\nEpoch 55/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4664 - loss: 0.6937\nEpoch 56/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4653 - loss: 0.6937\nEpoch 57/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4647 - loss: 0.6937\nEpoch 58/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4655 - loss: 0.6937\nEpoch 59/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4685 - loss: 0.6937\nEpoch 60/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4677 - loss: 0.6937\nEpoch 61/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4680 - loss: 0.6937\nEpoch 62/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4674 - loss: 0.6937\nEpoch 63/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4661 - loss: 0.6938\nEpoch 64/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4659 - loss: 0.6938\nEpoch 65/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4683 - loss: 0.6938\nEpoch 66/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4655 - loss: 0.6938\nEpoch 67/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4675 - loss: 0.6938\nEpoch 68/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4689 - loss: 0.6938\nEpoch 69/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4667 - loss: 0.6938\nEpoch 70/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4656 - loss: 0.6938\nEpoch 71/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4632 - loss: 0.6938\nEpoch 72/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4622 - loss: 0.6938\nEpoch 73/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4636 - loss: 0.6938\nEpoch 74/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4655 - loss: 0.6939\nEpoch 75/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4660 - loss: 0.6939\nEpoch 76/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4644 - loss: 0.6939\nEpoch 77/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4672 - loss: 0.6939\nEpoch 78/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4649 - loss: 0.6939\nEpoch 79/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4666 - loss: 0.6939\nEpoch 80/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4647 - loss: 0.6939\nEpoch 81/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4609 - loss: 0.6939\nEpoch 82/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4605 - loss: 0.6939\nEpoch 83/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4616 - loss: 0.6940\nEpoch 84/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4608 - loss: 0.6940\nEpoch 85/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4611 - loss: 0.6940\nEpoch 86/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4622 - loss: 0.6940\nEpoch 87/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4611 - loss: 0.6940\nEpoch 88/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4590 - loss: 0.6940\nEpoch 89/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4587 - loss: 0.6940\nEpoch 90/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4579 - loss: 0.6940\nEpoch 91/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4581 - loss: 0.6940\nEpoch 92/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4595 - loss: 0.6941\nEpoch 93/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4552 - loss: 0.6941\nEpoch 94/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4562 - loss: 0.6941\nEpoch 95/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4556 - loss: 0.6941\nEpoch 96/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4543 - loss: 0.6941\nEpoch 97/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4545 - loss: 0.6941\nEpoch 98/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4546 - loss: 0.6941\nEpoch 99/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4552 - loss: 0.6942\nEpoch 100/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4535 - loss: 0.6942\n</pre> Out[15]: <pre>&lt;keras.src.callbacks.history.History at 0x20172dce720&gt;</pre> <p>Still!</p> <p>We've pulled out a few tricks but our model isn't even doing better than guessing.</p> <p>Let's make some visualizations to see what's happening.</p> <p>\ud83d\udd11 Note: Whenever your model is performing strangely or there's something going on with your data you're not quite sure of, remember these three words: visualize, visualize, visualize. Inspect your data, inspect your model, inpsect your model's predictions.</p> <p>To visualize our model's predictions we're going to create a function <code>plot_decision_boundary()</code> which:</p> <ul> <li>Takes in a trained model, features (<code>X</code>) and labels (<code>y</code>).</li> <li>Creates a meshgrid of the different <code>X</code> values.</li> <li>Makes predictions across the meshgrid.</li> <li>Plots the predictions as well as a line between the different zones (where each unique class falls).</li> </ul> <p>If this sounds confusing, let's see it in code and then see the output.</p> <p>\ud83d\udd11 Note: If you're ever unsure of what a function does, try unraveling it and writing it line by line for yourself to see what it does. Break it into small parts and see what each part outputs.</p> In\u00a0[16]: Copied! <pre>import numpy as np\n\ndef plot_decision_boundary(model, X, y):\n  \"\"\"\n  Plots the decision boundary created by a model predicting on X.\n  This function has been adapted from two phenomenal resources:\n   1. CS231n - https://cs231n.github.io/neural-networks-case-study/\n   2. Made with ML basics - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\n  \"\"\"\n  # Define the axis boundaries of the plot and create a meshgrid\n  x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                       np.linspace(y_min, y_max, 100))\n\n  # Create X values (we're going to predict on all of these)\n  x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together: https://numpy.org/devdocs/reference/generated/numpy.c_.html\n\n  # Make predictions using the trained model\n  y_pred = model.predict(x_in)\n\n  # Check for multi-class\n  if model.output_shape[-1] &gt; 1: # checks the final dimension of the model's output shape, if this is &gt; (greater than) 1, it's multi-class\n    print(\"doing multiclass classification...\")\n    # We have to reshape our predictions to get them ready for plotting\n    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n  else:\n    print(\"doing binary classifcation...\")\n    y_pred = np.round(np.max(y_pred, axis=1)).reshape(xx.shape)\n\n  # Plot decision boundary\n  plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n  plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n  plt.xlim(xx.min(), xx.max())\n  plt.ylim(yy.min(), yy.max())\n</pre> import numpy as np  def plot_decision_boundary(model, X, y):   \"\"\"   Plots the decision boundary created by a model predicting on X.   This function has been adapted from two phenomenal resources:    1. CS231n - https://cs231n.github.io/neural-networks-case-study/    2. Made with ML basics - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb   \"\"\"   # Define the axis boundaries of the plot and create a meshgrid   x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1   y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1   xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),                        np.linspace(y_min, y_max, 100))    # Create X values (we're going to predict on all of these)   x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together: https://numpy.org/devdocs/reference/generated/numpy.c_.html    # Make predictions using the trained model   y_pred = model.predict(x_in)    # Check for multi-class   if model.output_shape[-1] &gt; 1: # checks the final dimension of the model's output shape, if this is &gt; (greater than) 1, it's multi-class     print(\"doing multiclass classification...\")     # We have to reshape our predictions to get them ready for plotting     y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)   else:     print(\"doing binary classifcation...\")     y_pred = np.round(np.max(y_pred, axis=1)).reshape(xx.shape)    # Plot decision boundary   plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)   plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)   plt.xlim(xx.min(), xx.max())   plt.ylim(yy.min(), yy.max()) <p>Now we've got a function to plot our model's decision boundary (the cut off point its making between red and blue dots), let's try it out.</p> In\u00a0[17]: Copied! <pre># Check out the predictions our model is making\nplot_decision_boundary(model_3, X, y)\n</pre> # Check out the predictions our model is making plot_decision_boundary(model_3, X, y) <pre>313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step\ndoing binary classifcation...\n</pre> <p>Looks like our model is trying to draw a straight line through the data.</p> <p>What's wrong with doing this?</p> <p>The main issue is our data isn't separable by a straight line.</p> <p>In a regression problem, our model might work. In fact, let's try it.</p> In\u00a0[18]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create some regression data\nX_regression = np.arange(0, 1000, 5)\ny_regression = np.arange(100, 1100, 5)\n\n# Split it into training and test sets\nX_reg_train = X_regression[:150]\nX_reg_test = X_regression[150:]\ny_reg_train = y_regression[:150]\ny_reg_test = y_regression[150:]\n</pre> # Set random seed tf.random.set_seed(42)  # Create some regression data X_regression = np.arange(0, 1000, 5) y_regression = np.arange(100, 1100, 5)  # Split it into training and test sets X_reg_train = X_regression[:150] X_reg_test = X_regression[150:] y_reg_train = y_regression[:150] y_reg_test = y_regression[150:] In\u00a0[19]: Copied! <pre>model_test = tf.keras.Sequential([\n  # Before TensorFlow 2.7.0\n  # tf.keras.layers.Dense(100), # add 100 dense neurons\n\n  # With TensorFlow 2.7.0\n  # tf.keras.layers.Dense(100, input_shape=(None, 1)), # add 100 dense neurons\n\n  ## After TensorFlow 2.8.0 ##\n  tf.keras.layers.Dense(100), # add 100 dense neurons\n  tf.keras.layers.Dense(10), # add another layer with 10 neurons\n  tf.keras.layers.Dense(1)\n])\n\n# 2. Compile the model\nmodel_test.compile(loss=tf.keras.losses.mae,\n                optimizer=tf.keras.optimizers.Adam(), # use Adam instead of SGD\n                metrics=['mae'])\n</pre> model_test = tf.keras.Sequential([   # Before TensorFlow 2.7.0   # tf.keras.layers.Dense(100), # add 100 dense neurons    # With TensorFlow 2.7.0   # tf.keras.layers.Dense(100, input_shape=(None, 1)), # add 100 dense neurons    ## After TensorFlow 2.8.0 ##   tf.keras.layers.Dense(100), # add 100 dense neurons   tf.keras.layers.Dense(10), # add another layer with 10 neurons   tf.keras.layers.Dense(1) ])  # 2. Compile the model model_test.compile(loss=tf.keras.losses.mae,                 optimizer=tf.keras.optimizers.Adam(), # use Adam instead of SGD                 metrics=['mae']) In\u00a0[20]: Copied! <pre># Fit our model to the data\n# Note: Before TensorFlow 2.7.0, this line would work\n# model_3.fit(X_reg_train, y_reg_train, epochs=100)\n\n# After TensorFlow 2.7.0, see here for more: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278\nmodel_test.fit(tf.expand_dims(X_reg_train, axis=-1),\n            y_reg_train,\n            epochs=100)\n</pre> # Fit our model to the data # Note: Before TensorFlow 2.7.0, this line would work # model_3.fit(X_reg_train, y_reg_train, epochs=100)  # After TensorFlow 2.7.0, see here for more: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278 model_test.fit(tf.expand_dims(X_reg_train, axis=-1),             y_reg_train,             epochs=100) <pre>Epoch 1/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 6ms/step - loss: 487.9379 - mae: 488.9716\nEpoch 2/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 346.9504 - mae: 348.0899 \nEpoch 3/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 204.6113 - mae: 205.8709 \nEpoch 4/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 70.2593 - mae: 70.2393 \nEpoch 5/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 80.1273 - mae: 79.7820 \nEpoch 6/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 88.2640 - mae: 88.4543 \nEpoch 7/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 53.6145 - mae: 53.7947 \nEpoch 8/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 49.0636 - mae: 49.0717 \nEpoch 9/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 43.2507 - mae: 43.2126 \nEpoch 10/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 46.1210 - mae: 46.0782 \nEpoch 11/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 43.7796 - mae: 43.8188 \nEpoch 12/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 41.5159 - mae: 41.5407 \nEpoch 13/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 40.4414 - mae: 40.4140 \nEpoch 14/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 41.3143 - mae: 41.2920 \nEpoch 15/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 40.2071 - mae: 40.2172 \nEpoch 16/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 40.7613 - mae: 40.7893 \nEpoch 17/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.8387 - mae: 39.8053 \nEpoch 18/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 40.5822 - mae: 40.5698 \nEpoch 19/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.8768 - mae: 39.8807 \nEpoch 20/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.9630 - mae: 39.9600 \nEpoch 21/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.6956 - mae: 39.6766 \nEpoch 22/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.7170 - mae: 39.7061 \nEpoch 23/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.7147 - mae: 39.7110 \nEpoch 24/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.6130 - mae: 39.6021 \nEpoch 25/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.5977 - mae: 39.5890 \nEpoch 26/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.5747 - mae: 39.5650 \nEpoch 27/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.5323 - mae: 39.5229 \nEpoch 28/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.5035 - mae: 39.4919 \nEpoch 29/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.4764 - mae: 39.4646 \nEpoch 30/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.4674 - mae: 39.4604 \nEpoch 31/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.4113 - mae: 39.3988 \nEpoch 32/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.3846 - mae: 39.3699 \nEpoch 33/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.3622 - mae: 39.3551 \nEpoch 34/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.3695 - mae: 39.3610 \nEpoch 35/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.2743 - mae: 39.2587 \nEpoch 36/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.2665 - mae: 39.2565 \nEpoch 37/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.2265 - mae: 39.2171 \nEpoch 38/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.1773 - mae: 39.1691 \nEpoch 39/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.1630 - mae: 39.1532 \nEpoch 40/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.1117 - mae: 39.1014 \nEpoch 41/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.1120 - mae: 39.1077 \nEpoch 42/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.0677 - mae: 39.0572 \nEpoch 43/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.0115 - mae: 38.9980 \nEpoch 44/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.0076 - mae: 38.9984 \nEpoch 45/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.9650 - mae: 38.9573 \nEpoch 46/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.9257 - mae: 38.9179 \nEpoch 47/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.8884 - mae: 38.8794 \nEpoch 48/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.8522 - mae: 38.8414 \nEpoch 49/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.8019 - mae: 38.7935 \nEpoch 50/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.7851 - mae: 38.7754 \nEpoch 51/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.7265 - mae: 38.7165 \nEpoch 52/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.7523 - mae: 38.7445 \nEpoch 53/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.6480 - mae: 38.6364 \nEpoch 54/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.6548 - mae: 38.6482 \nEpoch 55/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.5819 - mae: 38.5667 \nEpoch 56/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.5475 - mae: 38.5359 \nEpoch 57/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.5345 - mae: 38.5270 \nEpoch 58/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.4910 - mae: 38.4816 \nEpoch 59/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.4528 - mae: 38.4410 \nEpoch 60/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.4010 - mae: 38.3917 \nEpoch 61/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.3729 - mae: 38.3618 \nEpoch 62/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.3225 - mae: 38.3110 \nEpoch 63/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.3463 - mae: 38.3368 \nEpoch 64/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.2403 - mae: 38.2266 \nEpoch 65/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.2440 - mae: 38.2354 \nEpoch 66/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.1954 - mae: 38.1883 \nEpoch 67/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.0944 - mae: 38.0809 \nEpoch 68/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.1159 - mae: 38.1033 \nEpoch 69/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.0538 - mae: 38.0456 \nEpoch 70/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.0569 - mae: 38.0423 \nEpoch 71/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.9683 - mae: 37.9557 \nEpoch 72/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.0189 - mae: 38.0129 \nEpoch 73/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.8708 - mae: 37.8589 \nEpoch 74/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.8845 - mae: 37.8777 \nEpoch 75/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.7954 - mae: 37.7780 \nEpoch 76/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.7736 - mae: 37.7602 \nEpoch 77/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.8286 - mae: 37.8261 \nEpoch 78/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.6436 - mae: 37.6286 \nEpoch 79/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37.6467 - mae: 37.6358 \nEpoch 80/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.6812 - mae: 37.6750 \nEpoch 81/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.5529 - mae: 37.5367 \nEpoch 82/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.5196 - mae: 37.5028 \nEpoch 83/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.5093 - mae: 37.4998 \nEpoch 84/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.4750 - mae: 37.4621 \nEpoch 85/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.3526 - mae: 37.3406  \nEpoch 86/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37.3761 - mae: 37.3615 \nEpoch 87/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37.2650 - mae: 37.2547 \nEpoch 88/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.3105 - mae: 37.2961 \nEpoch 89/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.1641 - mae: 37.1582 \nEpoch 90/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.3469 - mae: 37.3350 \nEpoch 91/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.0692 - mae: 37.0674 \nEpoch 92/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.4917 - mae: 37.4849 \nEpoch 93/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.0107 - mae: 37.0053 \nEpoch 94/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.1842 - mae: 37.1742 \nEpoch 95/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 36.8774 - mae: 36.8692 \nEpoch 96/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.0420 - mae: 37.0377 \nEpoch 97/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 36.7804 - mae: 36.7607 \nEpoch 98/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 36.7776 - mae: 36.7659 \nEpoch 99/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 36.8120 - mae: 36.8078 \nEpoch 100/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 36.6546 - mae: 36.6399 \n</pre> Out[20]: <pre>&lt;keras.src.callbacks.history.History at 0x201770bb380&gt;</pre> In\u00a0[21]: Copied! <pre>model_test.summary()\n</pre> model_test.summary() <pre>Model: \"sequential_3\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 dense_6 (Dense)                      \u2502 (None, 100)                 \u2502             200 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_7 (Dense)                      \u2502 (None, 10)                  \u2502           1,010 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_8 (Dense)                      \u2502 (None, 1)                   \u2502              11 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 3,665 (14.32 KB)\n</pre> <pre> Trainable params: 1,221 (4.77 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <pre> Optimizer params: 2,444 (9.55 KB)\n</pre> <p>Oh wait... we compiled our model for a binary classification problem.</p> <p>No trouble, we can recreate it for a regression problem.</p> In\u00a0[22]: Copied! <pre># Setup random seed\ntf.random.set_seed(42)\n\n# Recreate the model\nmodel_3 = tf.keras.Sequential([\n  tf.keras.layers.Dense(100),\n  tf.keras.layers.Dense(10),\n  tf.keras.layers.Dense(1)\n])\n\n# Change the loss and metrics of our compiled model\nmodel_3.compile(loss=tf.keras.losses.mae, # change the loss function to be regression-specific\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=['mae']) # change the metric to be regression-specific\n\n# Fit the recompiled model\nmodel_3.fit(tf.expand_dims(X_reg_train, axis=-1),\n            y_reg_train,\n            epochs=100)\n</pre> # Setup random seed tf.random.set_seed(42)  # Recreate the model model_3 = tf.keras.Sequential([   tf.keras.layers.Dense(100),   tf.keras.layers.Dense(10),   tf.keras.layers.Dense(1) ])  # Change the loss and metrics of our compiled model model_3.compile(loss=tf.keras.losses.mae, # change the loss function to be regression-specific                 optimizer=tf.keras.optimizers.Adam(),                 metrics=['mae']) # change the metric to be regression-specific  # Fit the recompiled model model_3.fit(tf.expand_dims(X_reg_train, axis=-1),             y_reg_train,             epochs=100) <pre>Epoch 1/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 6ms/step - loss: 488.5253 - mae: 489.1115\nEpoch 2/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 396.2791 - mae: 396.9307 \nEpoch 3/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 303.3047 - mae: 304.0399 \nEpoch 4/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 207.6707 - mae: 208.5094 \nEpoch 5/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 107.3864 - mae: 108.3462 \nEpoch 6/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 46.9067 - mae: 46.4586 \nEpoch 7/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 76.5540 - mae: 76.4521 \nEpoch 8/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 67.3932 - mae: 67.5772 \nEpoch 9/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 44.7325 - mae: 44.7694 \nEpoch 10/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 46.6696 - mae: 46.7539 \nEpoch 11/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 40.8703 - mae: 40.8414 \nEpoch 12/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 42.4601 - mae: 42.4116 \nEpoch 13/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 42.2812 - mae: 42.2912 \nEpoch 14/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 40.2232 - mae: 40.2372 \nEpoch 15/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 40.4902 - mae: 40.4988 \nEpoch 16/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.8062 - mae: 39.7832 \nEpoch 17/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.9811 - mae: 39.9660 \nEpoch 18/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.8529 - mae: 39.8500 \nEpoch 19/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.8665 - mae: 39.8625 \nEpoch 20/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.7435 - mae: 39.7289 \nEpoch 21/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.7560 - mae: 39.7432 \nEpoch 22/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.7272 - mae: 39.7225 \nEpoch 23/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.7281 - mae: 39.7228 \nEpoch 24/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.6495 - mae: 39.6376 \nEpoch 25/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.6456 - mae: 39.6344 \nEpoch 26/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.6273 - mae: 39.6196 \nEpoch 27/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.5767 - mae: 39.5679 \nEpoch 28/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.5691 - mae: 39.5619 \nEpoch 29/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.5216 - mae: 39.5119 \nEpoch 30/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.5056 - mae: 39.4992 \nEpoch 31/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.5064 - mae: 39.5005 \nEpoch 32/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.4377 - mae: 39.4266 \nEpoch 33/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.4279 - mae: 39.4190 \nEpoch 34/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.3995 - mae: 39.3897 \nEpoch 35/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.3672 - mae: 39.3576 \nEpoch 36/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.3580 - mae: 39.3523 \nEpoch 37/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.3224 - mae: 39.3124 \nEpoch 38/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.2826 - mae: 39.2706 \nEpoch 39/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.2623 - mae: 39.2558 \nEpoch 40/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.2603 - mae: 39.2556  \nEpoch 41/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.1853 - mae: 39.1753 \nEpoch 42/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.1664 - mae: 39.1589 \nEpoch 43/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.1551 - mae: 39.1472 \nEpoch 44/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.1001 - mae: 39.0920 \nEpoch 45/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 39.0835 - mae: 39.0733 \nEpoch 46/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.0469 - mae: 39.0359 \nEpoch 47/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 39.0374 - mae: 39.0302 \nEpoch 48/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.9723 - mae: 38.9629 \nEpoch 49/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.9698 - mae: 38.9617 \nEpoch 50/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.9351 - mae: 38.9267 \nEpoch 51/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.9020 - mae: 38.8923 \nEpoch 52/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.8603 - mae: 38.8523 \nEpoch 53/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.8433 - mae: 38.8340 \nEpoch 54/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.7959 - mae: 38.7860 \nEpoch 55/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.7907 - mae: 38.7852 \nEpoch 56/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.7447 - mae: 38.7335 \nEpoch 57/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.7056 - mae: 38.6914 \nEpoch 58/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.6853 - mae: 38.6775 \nEpoch 59/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.6660 - mae: 38.6596 \nEpoch 60/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.5803 - mae: 38.5672 \nEpoch 61/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.5812 - mae: 38.5709 \nEpoch 62/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.5689 - mae: 38.5650 \nEpoch 63/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.5308 - mae: 38.5219 \nEpoch 64/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.4670 - mae: 38.4551 \nEpoch 65/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.4609 - mae: 38.4523 \nEpoch 66/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.4198 - mae: 38.4117 \nEpoch 67/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.3815 - mae: 38.3724 \nEpoch 68/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.3447 - mae: 38.3336 \nEpoch 69/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38.3018 - mae: 38.2922 \nEpoch 70/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.2952 - mae: 38.2915 \nEpoch 71/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.2544 - mae: 38.2448 \nEpoch 72/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.1906 - mae: 38.1775 \nEpoch 73/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.1825 - mae: 38.1725 \nEpoch 74/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.1392 - mae: 38.1295 \nEpoch 75/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.0987 - mae: 38.0877 \nEpoch 76/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.0507 - mae: 38.0416 \nEpoch 77/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38.0162 - mae: 38.0049 \nEpoch 78/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 57ms/step - loss: 37.9768 - mae: 37.9645 \nEpoch 79/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37.9667 - mae: 37.9590 \nEpoch 80/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.8756 - mae: 37.8644 \nEpoch 81/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.8815 - mae: 37.8714 \nEpoch 82/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 11ms/step - loss: 37.8380 - mae: 37.8267 \nEpoch 83/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37.7869 - mae: 37.7776 \nEpoch 84/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 37.7489 - mae: 37.7373 \nEpoch 85/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.7085 - mae: 37.6958 \nEpoch 86/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.6971 - mae: 37.6891 \nEpoch 87/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.6475 - mae: 37.6403 \nEpoch 88/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.6015 - mae: 37.5931 \nEpoch 89/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.5575 - mae: 37.5465 \nEpoch 90/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.5048 - mae: 37.4951 \nEpoch 91/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37.5167 - mae: 37.5089 \nEpoch 92/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37.4110 - mae: 37.3989 \nEpoch 93/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.4065 - mae: 37.3977 \nEpoch 94/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.3554 - mae: 37.3465 \nEpoch 95/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.3073 - mae: 37.2964 \nEpoch 96/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37.2494 - mae: 37.2400 \nEpoch 97/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.2062 - mae: 37.1938 \nEpoch 98/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.1643 - mae: 37.1504 \nEpoch 99/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.1485 - mae: 37.1391 \nEpoch 100/100\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37.0928 - mae: 37.0840 \n</pre> Out[22]: <pre>&lt;keras.src.callbacks.history.History at 0x20177087cb0&gt;</pre> <p>Okay, it seems like our model is learning something (the <code>mae</code> value trends down with each epoch), let's plot its predictions.</p> In\u00a0[23]: Copied! <pre># Make predictions with our trained model\ny_reg_preds = model_3.predict(X_reg_test)\n\n# Plot the model's predictions against our regression data\nplt.figure(figsize=(10, 7))\nplt.scatter(X_reg_train, y_reg_train, c='b', label='Training data')\nplt.scatter(X_reg_test, y_reg_test, c='g', label='Testing data')\nplt.scatter(X_reg_test, y_reg_preds.squeeze(), c='r', label='Predictions')\nplt.legend();\n</pre> # Make predictions with our trained model y_reg_preds = model_3.predict(X_reg_test)  # Plot the model's predictions against our regression data plt.figure(figsize=(10, 7)) plt.scatter(X_reg_train, y_reg_train, c='b', label='Training data') plt.scatter(X_reg_test, y_reg_test, c='g', label='Testing data') plt.scatter(X_reg_test, y_reg_preds.squeeze(), c='r', label='Predictions') plt.legend(); <pre>2/2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 96ms/step\n</pre> <p>Okay, the predictions aren't perfect (if the predictions were perfect, the red would line up with the green), but they look better than complete guessing.</p> <p>So this means our model must be learning something...</p> <p>There must be something we're missing out on for our classification problem.</p> In\u00a0[24]: Copied! <pre># Set the random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_4 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1, activation=tf.keras.activations.linear), # 1 hidden layer with linear activation\n  tf.keras.layers.Dense(1) # output layer\n])\n\n# Compile the model\nmodel_4.compile(loss=tf.keras.losses.binary_crossentropy,\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # note: \"lr\" used to be what was used, now \"learning_rate\" is favoured\n                metrics=[\"accuracy\"])\n\n# Fit the model\nhistory = model_4.fit(X, y, epochs=100)\n</pre> # Set the random seed tf.random.set_seed(42)  # Create the model model_4 = tf.keras.Sequential([   tf.keras.layers.Dense(1, activation=tf.keras.activations.linear), # 1 hidden layer with linear activation   tf.keras.layers.Dense(1) # output layer ])  # Compile the model model_4.compile(loss=tf.keras.losses.binary_crossentropy,                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # note: \"lr\" used to be what was used, now \"learning_rate\" is favoured                 metrics=[\"accuracy\"])  # Fit the model history = model_4.fit(X, y, epochs=100) <pre>Epoch 1/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 3ms/step - accuracy: 0.4830 - loss: 4.6718\nEpoch 2/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 4.3606\nEpoch 3/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 4.1655\nEpoch 4/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 4.0379\nEpoch 5/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 3.8148\nEpoch 6/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 3.4429\nEpoch 7/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 3.0337\nEpoch 8/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 2.3672\nEpoch 9/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 1.2097\nEpoch 10/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 1.1518\nEpoch 11/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 1.1209\nEpoch 12/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 1.0960\nEpoch 13/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 1.0745\nEpoch 14/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 1.0555\nEpoch 15/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 1.0383\nEpoch 16/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 1.0226\nEpoch 17/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 1.0081\nEpoch 18/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.9946\nEpoch 19/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.9820\nEpoch 20/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.9701\nEpoch 21/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.9589\nEpoch 22/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 0.9482\nEpoch 23/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.9382\nEpoch 24/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.9286\nEpoch 25/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.9194\nEpoch 26/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.9107\nEpoch 27/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.9023\nEpoch 28/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8943\nEpoch 29/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8866\nEpoch 30/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8792\nEpoch 31/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8721\nEpoch 32/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8652\nEpoch 33/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8587\nEpoch 34/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8523\nEpoch 35/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8462\nEpoch 36/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8403\nEpoch 37/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8347\nEpoch 38/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8292\nEpoch 39/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8239\nEpoch 40/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8188\nEpoch 41/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8139\nEpoch 42/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8092\nEpoch 43/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8046\nEpoch 44/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.8002\nEpoch 45/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7959\nEpoch 46/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7918\nEpoch 47/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7878\nEpoch 48/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7840\nEpoch 49/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7803\nEpoch 50/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7767\nEpoch 51/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7733\nEpoch 52/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7699\nEpoch 53/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7667\nEpoch 54/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7636\nEpoch 55/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7607\nEpoch 56/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 0.7578\nEpoch 57/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4827 - loss: 0.7551\nEpoch 58/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4807 - loss: 0.7524\nEpoch 59/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4788 - loss: 0.7498\nEpoch 60/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4690 - loss: 0.7474\nEpoch 61/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4534 - loss: 0.7450\nEpoch 62/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4450 - loss: 0.7428\nEpoch 63/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4287 - loss: 0.7406\nEpoch 64/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4251 - loss: 0.7385\nEpoch 65/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4176 - loss: 0.7365\nEpoch 66/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4150 - loss: 0.7345\nEpoch 67/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4112 - loss: 0.7327\nEpoch 68/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4208 - loss: 0.7309\nEpoch 69/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4289 - loss: 0.7292\nEpoch 70/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4405 - loss: 0.7276\nEpoch 71/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4426 - loss: 0.7260\nEpoch 72/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4578 - loss: 0.7245\nEpoch 73/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4595 - loss: 0.7231\nEpoch 74/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4675 - loss: 0.7217\nEpoch 75/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4707 - loss: 0.7204\nEpoch 76/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4737 - loss: 0.7192\nEpoch 77/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4724 - loss: 0.7180\nEpoch 78/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4766 - loss: 0.7168\nEpoch 79/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4767 - loss: 0.7157\nEpoch 80/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4750 - loss: 0.7147\nEpoch 81/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4776 - loss: 0.7137\nEpoch 82/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4811 - loss: 0.7128\nEpoch 83/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4844 - loss: 0.7119\nEpoch 84/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4838 - loss: 0.7110\nEpoch 85/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4834 - loss: 0.7102\nEpoch 86/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4883 - loss: 0.7094\nEpoch 87/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4925 - loss: 0.7086\nEpoch 88/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4904 - loss: 0.7079\nEpoch 89/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4948 - loss: 0.7072\nEpoch 90/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.4951 - loss: 0.7066\nEpoch 91/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4949 - loss: 0.7060\nEpoch 92/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4959 - loss: 0.7054\nEpoch 93/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4965 - loss: 0.7048\nEpoch 94/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4983 - loss: 0.7043\nEpoch 95/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5009 - loss: 0.7037\nEpoch 96/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5023 - loss: 0.7033\nEpoch 97/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5017 - loss: 0.7028\nEpoch 98/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5064 - loss: 0.7023\nEpoch 99/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5079 - loss: 0.7019\nEpoch 100/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5087 - loss: 0.7015\n</pre> <p>Okay, our model performs a little worse than guessing.</p> <p>Let's remind ourselves what our data looks like.</p> In\u00a0[25]: Copied! <pre># Check out our data\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);\n</pre> # Check out our data plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu); <p>And let's see how our model is making predictions on it.</p> In\u00a0[26]: Copied! <pre># Check the deicison boundary (blue is blue class, yellow is the crossover, red is red class)\nplot_decision_boundary(model_4, X, y)\n</pre> # Check the deicison boundary (blue is blue class, yellow is the crossover, red is red class) plot_decision_boundary(model_4, X, y) <pre>WARNING:tensorflow:5 out of the last 316 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x000002017A9149A0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step\ndoing binary classifcation...\n</pre> <p>Well, it looks like we're getting a straight (linear) line prediction again.</p> <p>But our data is non-linear (not a straight line)...</p> <p>What we're going to have to do is add some non-linearity to our model.</p> <p>To do so, we'll use the <code>activation</code> parameter in on of our layers.</p> In\u00a0[27]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model with a non-linear activation\nmodel_5 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1, activation=tf.keras.activations.relu), # can also do activation='relu'\n  tf.keras.layers.Dense(1) # output layer\n])\n\n# Compile the model\nmodel_5.compile(loss=tf.keras.losses.binary_crossentropy,\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory = model_5.fit(X, y, epochs=100)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model with a non-linear activation model_5 = tf.keras.Sequential([   tf.keras.layers.Dense(1, activation=tf.keras.activations.relu), # can also do activation='relu'   tf.keras.layers.Dense(1) # output layer ])  # Compile the model model_5.compile(loss=tf.keras.losses.binary_crossentropy,               optimizer=tf.keras.optimizers.Adam(),               metrics=[\"accuracy\"])  # Fit the model history = model_5.fit(X, y, epochs=100) <pre>Epoch 1/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 2/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 3/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 4/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 5/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 6/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 7/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 8/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 9/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 10/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 11/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 12/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 13/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 14/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 15/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 16/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 17/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 18/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 19/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 20/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 21/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 22/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 23/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 24/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 25/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 26/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 27/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 28/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 29/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 30/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 31/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 32/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 33/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 34/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 35/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 36/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 37/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 38/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 39/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 40/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 41/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 42/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 43/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 44/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 45/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 46/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 47/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 48/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 49/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 50/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 51/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 52/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 53/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 54/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 55/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 56/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 57/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 58/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 59/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 60/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 61/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 62/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 63/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 64/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 65/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 66/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 67/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 68/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 69/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 70/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 71/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 72/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 73/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 74/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 75/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 76/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 77/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 78/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 79/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 80/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 81/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 82/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 83/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 84/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 85/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 86/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 87/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 88/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 89/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 90/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 91/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 92/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 93/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 94/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 95/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 96/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 97/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 98/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 99/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\nEpoch 100/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 8.3336\n</pre> <p>Hmm... still not learning...</p> <p>What we if increased the number of neurons and layers?</p> <p>Say, 2 hidden layers, with ReLU, pronounced \"rel-u\", (short for rectified linear unit), activation on the first one, and 4 neurons each?</p> <p>To see this network in action, check out the TensorFlow Playground demo.</p> <p> The neural network we're going to recreate with TensorFlow code. See it live at TensorFlow Playground.</p> <p>Let's try.</p> <p>Note: in the course, Daniel used <code>lr</code> instead of <code>learning_rate</code>. But for the update, we had changed to <code>learning_rate</code> instead of <code>lr</code>.</p> In\u00a0[28]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model\nmodel_6 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, 4 neurons, ReLU activation\n  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, 4 neurons, ReLU activation\n  tf.keras.layers.Dense(1) # ouput layer\n])\n\n# Compile the model\nmodel_6.compile(loss=tf.keras.losses.binary_crossentropy,\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Adam's default learning rate is 0.001\n                metrics=['accuracy'])\n\n# Fit the model\nhistory = model_6.fit(X, y, epochs=100)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model model_6 = tf.keras.Sequential([   tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, 4 neurons, ReLU activation   tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, 4 neurons, ReLU activation   tf.keras.layers.Dense(1) # ouput layer ])  # Compile the model model_6.compile(loss=tf.keras.losses.binary_crossentropy,                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Adam's default learning rate is 0.001                 metrics=['accuracy'])  # Fit the model history = model_6.fit(X, y, epochs=100) <pre>Epoch 1/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 3ms/step - accuracy: 0.4830 - loss: 6.2658\nEpoch 2/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 5.5252\nEpoch 3/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 5.2501\nEpoch 4/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 4.8496\nEpoch 5/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 4.3150\nEpoch 6/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4830 - loss: 3.6704\nEpoch 7/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 3.3973\nEpoch 8/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4830 - loss: 2.9343\nEpoch 9/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4718 - loss: 2.0000\nEpoch 10/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4598 - loss: 0.9447\nEpoch 11/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4639 - loss: 0.8439\nEpoch 12/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4701 - loss: 0.8198\nEpoch 13/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4667 - loss: 0.8045\nEpoch 14/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4479 - loss: 0.7929\nEpoch 15/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4448 - loss: 0.7837\nEpoch 16/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4414 - loss: 0.7761\nEpoch 17/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4468 - loss: 0.7698\nEpoch 18/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4510 - loss: 0.7644\nEpoch 19/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4549 - loss: 0.7596\nEpoch 20/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4599 - loss: 0.7554\nEpoch 21/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4607 - loss: 0.7516\nEpoch 22/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4612 - loss: 0.7481\nEpoch 23/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4584 - loss: 0.7448\nEpoch 24/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4633 - loss: 0.7417\nEpoch 25/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4619 - loss: 0.7388\nEpoch 26/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4645 - loss: 0.7360\nEpoch 27/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4676 - loss: 0.7334\nEpoch 28/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4676 - loss: 0.7308\nEpoch 29/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4690 - loss: 0.7284\nEpoch 30/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4673 - loss: 0.7260\nEpoch 31/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4697 - loss: 0.7238\nEpoch 32/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4712 - loss: 0.7216\nEpoch 33/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4714 - loss: 0.7195\nEpoch 34/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4714 - loss: 0.7174\nEpoch 35/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4735 - loss: 0.7154\nEpoch 36/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4735 - loss: 0.7135\nEpoch 37/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4735 - loss: 0.7116\nEpoch 38/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4754 - loss: 0.7099\nEpoch 39/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4767 - loss: 0.7076\nEpoch 40/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4923 - loss: 0.7008\nEpoch 41/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4872 - loss: 0.6926\nEpoch 42/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5324 - loss: 0.6873\nEpoch 43/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5235 - loss: 0.6831\nEpoch 44/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5190 - loss: 0.6798\nEpoch 45/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5297 - loss: 0.6767\nEpoch 46/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5052 - loss: 0.6738\nEpoch 47/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5081 - loss: 0.6710\nEpoch 48/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5163 - loss: 0.6683\nEpoch 49/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5159 - loss: 0.6656\nEpoch 50/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5228 - loss: 0.6630\nEpoch 51/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5381 - loss: 0.6605\nEpoch 52/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5544 - loss: 0.6580\nEpoch 53/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5516 - loss: 0.6556\nEpoch 54/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5663 - loss: 0.6533\nEpoch 55/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5756 - loss: 0.6510\nEpoch 56/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6219 - loss: 0.6487\nEpoch 57/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6336 - loss: 0.6464\nEpoch 58/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6372 - loss: 0.6441\nEpoch 59/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6598 - loss: 0.6417\nEpoch 60/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6663 - loss: 0.6393\nEpoch 61/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6657 - loss: 0.6369\nEpoch 62/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.6753 - loss: 0.6346\nEpoch 63/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.6767 - loss: 0.6322\nEpoch 64/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.6832 - loss: 0.6297\nEpoch 65/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6833 - loss: 0.6274\nEpoch 66/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.6783 - loss: 0.6251\nEpoch 67/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.6811 - loss: 0.6226\nEpoch 68/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6861 - loss: 0.6202\nEpoch 69/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.6926 - loss: 0.6177\nEpoch 70/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6965 - loss: 0.6152\nEpoch 71/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6944 - loss: 0.6127\nEpoch 72/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6972 - loss: 0.6101\nEpoch 73/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7003 - loss: 0.6075\nEpoch 74/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - accuracy: 0.7015 - loss: 0.6049\nEpoch 75/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7054 - loss: 0.6022\nEpoch 76/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7125 - loss: 0.5994\nEpoch 77/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7230 - loss: 0.5965\nEpoch 78/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7276 - loss: 0.5935\nEpoch 79/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7334 - loss: 0.5905\nEpoch 80/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7345 - loss: 0.5874\nEpoch 81/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7369 - loss: 0.5844\nEpoch 82/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7392 - loss: 0.5814\nEpoch 83/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7444 - loss: 0.5783\nEpoch 84/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7495 - loss: 0.5752\nEpoch 85/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7571 - loss: 0.5721\nEpoch 86/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7626 - loss: 0.5688\nEpoch 87/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7681 - loss: 0.5656\nEpoch 88/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7670 - loss: 0.5622\nEpoch 89/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7736 - loss: 0.5588\nEpoch 90/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7793 - loss: 0.5552\nEpoch 91/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7798 - loss: 0.5516\nEpoch 92/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7833 - loss: 0.5480\nEpoch 93/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7841 - loss: 0.5442\nEpoch 94/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7903 - loss: 0.5402\nEpoch 95/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7913 - loss: 0.5356\nEpoch 96/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7928 - loss: 0.5310\nEpoch 97/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7949 - loss: 0.5262\nEpoch 98/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8039 - loss: 0.5211\nEpoch 99/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8025 - loss: 0.5157\nEpoch 100/100\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8018 - loss: 0.5104\n</pre> In\u00a0[29]: Copied! <pre># Evaluate the model\nmodel_6.evaluate(X, y)\n</pre> # Evaluate the model model_6.evaluate(X, y) <pre>32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7802 - loss: 0.5153\n</pre> Out[29]: <pre>[0.5108292102813721, 0.7940000295639038]</pre> <p>We're still hitting 50% accuracy, our model is still practically as good as guessing.</p> <p>How do the predictions look?</p> In\u00a0[30]: Copied! <pre># Check out the predictions using 2 hidden layers\nplot_decision_boundary(model_6, X, y)\n</pre> # Check out the predictions using 2 hidden layers plot_decision_boundary(model_6, X, y) <pre>313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step\ndoing binary classifcation...\n</pre> <p>What gives?</p> <p>It seems like our model is the same as the one in the TensorFlow Playground but model it's still drawing straight lines...</p> <p>Ideally, the yellow lines go on the inside of the red circle and the blue circle.</p> <p>Okay, okay, let's model this circle once and for all.</p> <p>One more model (I promise... actually, I'm going to have to break that promise... we'll be building plenty more models).</p> <p>This time we'll change the activation function on our output layer too. Remember the architecture of a classification model? For binary classification, the output layer activation is usually the Sigmoid activation function.</p> In\u00a0[31]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model\nmodel_7 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, ReLU activation\n  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, ReLU activation\n  tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) # ouput layer, sigmoid activation\n])\n\n# Compile the model\nmodel_7.compile(loss=tf.keras.losses.binary_crossentropy,\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=['accuracy'])\n\n# Fit the model\nhistory = model_7.fit(X, y, epochs=100, verbose=0)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model model_7 = tf.keras.Sequential([   tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, ReLU activation   tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, ReLU activation   tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) # ouput layer, sigmoid activation ])  # Compile the model model_7.compile(loss=tf.keras.losses.binary_crossentropy,                 optimizer=tf.keras.optimizers.Adam(),                 metrics=['accuracy'])  # Fit the model history = model_7.fit(X, y, epochs=100, verbose=0) In\u00a0[32]: Copied! <pre># Evaluate our model\nmodel_7.evaluate(X, y)\n</pre> # Evaluate our model model_7.evaluate(X, y) <pre>32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8287 - loss: 0.4659\n</pre> Out[32]: <pre>[0.4603603780269623, 0.8299999833106995]</pre> <p>Woah! It looks like our model is getting some incredible results, let's check them out.</p> In\u00a0[33]: Copied! <pre># View the predictions of the model with relu and sigmoid activations\nplot_decision_boundary(model_7, X, y)\n</pre> # View the predictions of the model with relu and sigmoid activations plot_decision_boundary(model_7, X, y) <pre>313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step\ndoing binary classifcation...\n</pre> <p>Nice! It looks like our model is almost perfectly (apart from a few examples) separating the two circles.</p> <p>\ud83e\udd14 Question: What's wrong with the predictions we've made? Are we really evaluating our model correctly here? Hint: what data did the model learn on and what did we predict on?</p> <p>Before we answer that, it's important to recognize what we've just covered.</p> <p>\ud83d\udd11 Note: The combination of linear (straight lines) and non-linear (non-straight lines) functions is one of the key fundamentals of neural networks.</p> <p>Think of it like this:</p> <p>If I gave you an unlimited amount of straight lines and non-straight lines, what kind of patterns could you draw?</p> <p>That's essentially what neural networks do to find patterns in data.</p> <p>Now you might be thinking, \"but I haven't seen a linear function or a non-linear function before...\"</p> <p>Oh but you have.</p> <p>We've been using them the whole time.</p> <p>They're what power the layers in the models we just built.</p> <p>To get some intuition about the activation functions we've just used, let's create them and then try them on some toy data.</p> In\u00a0[34]: Copied! <pre># Create a toy tensor (similar to the data we pass into our model)\nA = tf.cast(tf.range(-10, 10), tf.float32)\nA\n</pre> # Create a toy tensor (similar to the data we pass into our model) A = tf.cast(tf.range(-10, 10), tf.float32) A Out[34]: <pre>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,\n         1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n      dtype=float32)&gt;</pre> <p>How does this look?</p> In\u00a0[35]: Copied! <pre># Visualize our toy tensor\nplt.plot(A);\n</pre> # Visualize our toy tensor plt.plot(A); <p>A straight (linear) line!</p> <p>Nice, now let's recreate the sigmoid function and see what it does to our data. You can also find a pre-built sigmoid function at <code>tf.keras.activations.sigmoid</code>.</p> In\u00a0[36]: Copied! <pre># Sigmoid - https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid\ndef sigmoid(x):\n  return 1 / (1 + tf.exp(-x))\n\n# Use the sigmoid function on our tensor\nsigmoid(A)\n</pre> # Sigmoid - https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid def sigmoid(x):   return 1 / (1 + tf.exp(-x))  # Use the sigmoid function on our tensor sigmoid(A) Out[36]: <pre>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([4.5397872e-05, 1.2339458e-04, 3.3535014e-04, 9.1105117e-04,\n       2.4726233e-03, 6.6928510e-03, 1.7986210e-02, 4.7425874e-02,\n       1.1920292e-01, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,\n       8.8079703e-01, 9.5257413e-01, 9.8201376e-01, 9.9330717e-01,\n       9.9752742e-01, 9.9908900e-01, 9.9966466e-01, 9.9987662e-01],\n      dtype=float32)&gt;</pre> <p>And how does it look?</p> In\u00a0[37]: Copied! <pre># Plot sigmoid modified tensor\nplt.plot(sigmoid(A));\n</pre> # Plot sigmoid modified tensor plt.plot(sigmoid(A)); <p>A non-straight (non-linear) line!</p> <p>Okay, how about the ReLU function (ReLU turns all negatives to 0 and positive numbers stay the same)?</p> In\u00a0[38]: Copied! <pre># ReLU - https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu\ndef relu(x):\n  return tf.maximum(0, x)\n\n# Pass toy tensor through ReLU function\nrelu(A)\n</pre> # ReLU - https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu def relu(x):   return tf.maximum(0, x)  # Pass toy tensor through ReLU function relu(A) Out[38]: <pre>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6.,\n       7., 8., 9.], dtype=float32)&gt;</pre> <p>How does the ReLU-modified tensor look?</p> In\u00a0[39]: Copied! <pre># Plot ReLU-modified tensor\nplt.plot(relu(A));\n</pre> # Plot ReLU-modified tensor plt.plot(relu(A)); <p>Another non-straight line!</p> <p>Well, how about TensorFlow's linear activation function?</p> In\u00a0[40]: Copied! <pre># Linear - https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear (returns input non-modified...)\ntf.keras.activations.linear(A)\n</pre> # Linear - https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear (returns input non-modified...) tf.keras.activations.linear(A) Out[40]: <pre>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,\n         1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n      dtype=float32)&gt;</pre> <p>Hmm, it looks like our inputs are unmodified...</p> In\u00a0[41]: Copied! <pre># Does the linear activation change anything?\nA == tf.keras.activations.linear(A)\n</pre> # Does the linear activation change anything? A == tf.keras.activations.linear(A) Out[41]: <pre>&lt;tf.Tensor: shape=(20,), dtype=bool, numpy=\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True])&gt;</pre> <p>Okay, so it makes sense now the model doesn't really learn anything when using only linear activation functions, because the linear activation function doesn't change our input data in anyway.</p> <p>Where as, with our non-linear functions, our data gets manipulated. A neural network uses these kind of transformations at a large scale to figure draw patterns between its inputs and outputs.</p> <p>Now rather than dive into the guts of neural networks, we're going to keep coding applying what we've learned to different problems but if you want a more in-depth look at what's going on behind the scenes, check out the Extra Curriculum section below.</p> <p>\ud83d\udcd6  Resource: For more on activation functions, check out the machine learning cheatsheet page on them.</p> In\u00a0[42]: Copied! <pre># How many examples are in the whole dataset?\nlen(X)\n</pre> # How many examples are in the whole dataset? len(X) Out[42]: <pre>1000</pre> In\u00a0[43]: Copied! <pre># Split data into train and test sets\nX_train, y_train = X[:800], y[:800] # 80% of the data for the training set\nX_test, y_test = X[800:], y[800:] # 20% of the data for the test set\n\n# Check the shapes of the data\nX_train.shape, X_test.shape # 800 examples in the training set, 200 examples in the test set\n</pre> # Split data into train and test sets X_train, y_train = X[:800], y[:800] # 80% of the data for the training set X_test, y_test = X[800:], y[800:] # 20% of the data for the test set  # Check the shapes of the data X_train.shape, X_test.shape # 800 examples in the training set, 200 examples in the test set Out[43]: <pre>((800, 2), (200, 2))</pre> <p>Great, now we've got training and test sets, let's model the training data and evaluate what our model has learned on the test set.</p> In\u00a0[44]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create the model (same as model_7)\nmodel_8 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output\n])\n\n# Compile the model\nmodel_8.compile(loss=tf.keras.losses.binary_crossentropy,\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # increase learning rate from 0.001 to 0.01 for faster learning\n                metrics=['accuracy'])\n\n# Fit the model\nhistory = model_8.fit(X_train, y_train, epochs=25)\n</pre> # Set random seed tf.random.set_seed(42)  # Create the model (same as model_7) model_8 = tf.keras.Sequential([   tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output ])  # Compile the model model_8.compile(loss=tf.keras.losses.binary_crossentropy,                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # increase learning rate from 0.001 to 0.01 for faster learning                 metrics=['accuracy'])  # Fit the model history = model_8.fit(X_train, y_train, epochs=25) <pre>Epoch 1/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 3ms/step - accuracy: 0.4535 - loss: 0.7185\nEpoch 2/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5016 - loss: 0.6954\nEpoch 3/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4979 - loss: 0.6930\nEpoch 4/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4930 - loss: 0.6922\nEpoch 5/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4996 - loss: 0.6921\nEpoch 6/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4967 - loss: 0.6917\nEpoch 7/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5061 - loss: 0.6906\nEpoch 8/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5146 - loss: 0.6878\nEpoch 9/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5199 - loss: 0.6785\nEpoch 10/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5592 - loss: 0.6611 \nEpoch 11/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6009 - loss: 0.6495 \nEpoch 12/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6251 - loss: 0.6335 \nEpoch 13/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6678 - loss: 0.6180 \nEpoch 14/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6856 - loss: 0.6006 \nEpoch 15/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6998 - loss: 0.5826 \nEpoch 16/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7258 - loss: 0.5563 \nEpoch 17/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7777 - loss: 0.5033 \nEpoch 18/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8015 - loss: 0.4648 \nEpoch 19/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8355 - loss: 0.4368 \nEpoch 20/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8548 - loss: 0.4124\nEpoch 21/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8757 - loss: 0.3927 \nEpoch 22/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8730 - loss: 0.3781 \nEpoch 23/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8741 - loss: 0.3656 \nEpoch 24/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8624 - loss: 0.3576 \nEpoch 25/25\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8707 - loss: 0.3478 \n</pre> In\u00a0[45]: Copied! <pre># Evaluate our model on the test set\nloss, accuracy = model_8.evaluate(X_test, y_test)\nprint(f\"Model loss on the test set: {loss}\")\nprint(f\"Model accuracy on the test set: {100*accuracy:.2f}%\")\n</pre> # Evaluate our model on the test set loss, accuracy = model_8.evaluate(X_test, y_test) print(f\"Model loss on the test set: {loss}\") print(f\"Model accuracy on the test set: {100*accuracy:.2f}%\") <pre>7/7 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8902 - loss: 0.3174  \nModel loss on the test set: 0.3311477601528168\nModel accuracy on the test set: 88.00%\n</pre> <p>100% accuracy? Nice!</p> <p>Now, when we started to create <code>model_8</code> we said it was going to be the same as <code>model_7</code> but you might've found that to be a little lie.</p> <p>That's because we changed a few things:</p> <ul> <li>The <code>activation</code> parameter - We used strings (<code>\"relu\"</code> &amp; <code>\"sigmoid\"</code>) instead of using library paths (<code>tf.keras.activations.relu</code>), in TensorFlow, they both offer the same functionality.</li> <li>The <code>learning_rate</code> (also <code>lr</code>) parameter - We increased the learning rate parameter in the Adam optimizer to <code>0.01</code> instead of <code>0.001</code> (an increase of 10x).<ul> <li>You can think of the learning rate as how quickly a model learns. The higher the learning rate, the faster the model's capacity to learn, however, there's such a thing as a too high learning rate, where a model tries to learn too fast and doesn't learn anything. We'll see a trick to find the ideal learning rate soon.</li> </ul> </li> <li>The number of epochs - We lowered the number of epochs (using the <code>epochs</code> parameter) from 100 to 25 but our model still got an incredible result on both the training and test sets.<ul> <li>One of the reasons our model performed well in even less epochs (remember a single epoch is the model trying to learn patterns in the data by looking at it once, so 25 epochs means the model gets 25 chances) than before is because we increased the learning rate.</li> </ul> </li> </ul> <p>We know our model is performing well based on the evaluation metrics but let's see how it performs visually.</p> In\u00a0[46]: Copied! <pre># Plot the decision boundaries for the training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_8, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_8, X=X_test, y=y_test)\nplt.show()\n</pre> # Plot the decision boundaries for the training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_8, X=X_train, y=y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_8, X=X_test, y=y_test) plt.show() <pre>313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step\ndoing binary classifcation...\n313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step\ndoing binary classifcation...\n</pre> <p>Check that out! How cool. With a few tweaks, our model is now predicting the blue and red circles almost perfectly.</p> In\u00a0[47]: Copied! <pre># You can access the information in the history variable using the .history attribute\npd.DataFrame(history.history)\n</pre> # You can access the information in the history variable using the .history attribute pd.DataFrame(history.history) Out[47]: accuracy loss 0 0.43375 0.707851 1 0.49875 0.694228 2 0.48875 0.692185 3 0.51625 0.690773 4 0.51750 0.689282 5 0.53000 0.687196 6 0.53625 0.684352 7 0.54625 0.679538 8 0.56375 0.668187 9 0.59875 0.652840 10 0.62625 0.638076 11 0.64875 0.622055 12 0.67750 0.605658 13 0.69375 0.587679 14 0.71375 0.570109 15 0.76500 0.531554 16 0.80625 0.483048 17 0.82125 0.449459 18 0.84875 0.419913 19 0.86625 0.395091 20 0.87875 0.375061 21 0.88250 0.360068 22 0.88500 0.347148 23 0.88250 0.337774 24 0.89000 0.328164 <p>Inspecting the outputs, we can see the loss values going down and the accuracy going up.</p> <p>How's it look (visualize, visualize, visualize)?</p> In\u00a0[48]: Copied! <pre># Plot the loss curves\npd.DataFrame(history.history).plot()\nplt.title(\"Model_8 training curves\")\n</pre> # Plot the loss curves pd.DataFrame(history.history).plot() plt.title(\"Model_8 training curves\") Out[48]: <pre>Text(0.5, 1.0, 'Model_8 training curves')</pre> <p>Beautiful. This is the ideal plot we'd be looking for when dealing with a classification problem, loss going down, accuracy going up.</p> <p>\ud83d\udd11 Note: For many problems, the loss function going down means the model is improving (the predictions it's making are getting closer to the ground truth labels).</p> In\u00a0[49]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model (same as model_8)\nmodel_9 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\n# Compile the model\nmodel_9.compile(loss=\"binary_crossentropy\", # we can use strings here too\n              optimizer=\"Adam\", # same as tf.keras.optimizers.Adam() with default settings\n              metrics=[\"accuracy\"])\n\n# Create a learning rate scheduler callback\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20)) # traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch\n\n# Fit the model (passing the lr_scheduler callback)\nhistory = model_9.fit(X_train,\n                      y_train,\n                      epochs=100,\n                      callbacks=[lr_scheduler])\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model (same as model_8) model_9 = tf.keras.Sequential([   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(1, activation=\"sigmoid\") ])  # Compile the model model_9.compile(loss=\"binary_crossentropy\", # we can use strings here too               optimizer=\"Adam\", # same as tf.keras.optimizers.Adam() with default settings               metrics=[\"accuracy\"])  # Create a learning rate scheduler callback lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20)) # traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch  # Fit the model (passing the lr_scheduler callback) history = model_9.fit(X_train,                       y_train,                       epochs=100,                       callbacks=[lr_scheduler]) <pre>Epoch 1/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 3ms/step - accuracy: 0.4833 - loss: 0.6941 - learning_rate: 1.0000e-04\nEpoch 2/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4852 - loss: 0.6940 - learning_rate: 1.1220e-04\nEpoch 3/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4910 - loss: 0.6938 - learning_rate: 1.2589e-04\nEpoch 4/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4915 - loss: 0.6936 - learning_rate: 1.4125e-04\nEpoch 5/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4861 - loss: 0.6935 - learning_rate: 1.5849e-04\nEpoch 6/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4882 - loss: 0.6933 - learning_rate: 1.7783e-04\nEpoch 7/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4971 - loss: 0.6931 - learning_rate: 1.9953e-04\nEpoch 8/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4999 - loss: 0.6928 - learning_rate: 2.2387e-04\nEpoch 9/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5031 - loss: 0.6926 - learning_rate: 2.5119e-04\nEpoch 10/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4975 - loss: 0.6923 - learning_rate: 2.8184e-04\nEpoch 11/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4963 - loss: 0.6920 - learning_rate: 3.1623e-04\nEpoch 12/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4991 - loss: 0.6917 - learning_rate: 3.5481e-04\nEpoch 13/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5010 - loss: 0.6913 - learning_rate: 3.9811e-04\nEpoch 14/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5177 - loss: 0.6910 - learning_rate: 4.4668e-04\nEpoch 15/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5253 - loss: 0.6906 - learning_rate: 5.0119e-04\nEpoch 16/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5330 - loss: 0.6901 - learning_rate: 5.6234e-04\nEpoch 17/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5289 - loss: 0.6896 - learning_rate: 6.3096e-04\nEpoch 18/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5392 - loss: 0.6891 - learning_rate: 7.0795e-04\nEpoch 19/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5372 - loss: 0.6886 - learning_rate: 7.9433e-04\nEpoch 20/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5464 - loss: 0.6879 - learning_rate: 8.9125e-04\nEpoch 21/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5464 - loss: 0.6873 - learning_rate: 0.0010\nEpoch 22/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5428 - loss: 0.6865 - learning_rate: 0.0011\nEpoch 23/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5475 - loss: 0.6857 - learning_rate: 0.0013\nEpoch 24/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5520 - loss: 0.6846 - learning_rate: 0.0014\nEpoch 25/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5502 - loss: 0.6836 - learning_rate: 0.0016\nEpoch 26/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5615 - loss: 0.6824 - learning_rate: 0.0018\nEpoch 27/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5743 - loss: 0.6809 - learning_rate: 0.0020\nEpoch 28/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5765 - loss: 0.6795 - learning_rate: 0.0022\nEpoch 29/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5904 - loss: 0.6777 - learning_rate: 0.0025\nEpoch 30/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5962 - loss: 0.6757 - learning_rate: 0.0028\nEpoch 31/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5942 - loss: 0.6733 - learning_rate: 0.0032\nEpoch 32/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5907 - loss: 0.6704 - learning_rate: 0.0035\nEpoch 33/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5904 - loss: 0.6669 - learning_rate: 0.0040\nEpoch 34/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5972 - loss: 0.6629 - learning_rate: 0.0045\nEpoch 35/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6077 - loss: 0.6575 - learning_rate: 0.0050\nEpoch 36/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6330 - loss: 0.6469 - learning_rate: 0.0056\nEpoch 37/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6652 - loss: 0.6245 - learning_rate: 0.0063\nEpoch 38/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7217 - loss: 0.5752 - learning_rate: 0.0071\nEpoch 39/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7821 - loss: 0.5039 - learning_rate: 0.0079\nEpoch 40/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8655 - loss: 0.4148 - learning_rate: 0.0089\nEpoch 41/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8984 - loss: 0.3350 - learning_rate: 0.0100\nEpoch 42/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9069 - loss: 0.2828 - learning_rate: 0.0112\nEpoch 43/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9246 - loss: 0.2388 - learning_rate: 0.0126\nEpoch 44/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9394 - loss: 0.2067 - learning_rate: 0.0141\nEpoch 45/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9589 - loss: 0.1811 - learning_rate: 0.0158\nEpoch 46/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9509 - loss: 0.1679 - learning_rate: 0.0178\nEpoch 47/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9583 - loss: 0.1471 - learning_rate: 0.0200\nEpoch 48/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9602 - loss: 0.1298 - learning_rate: 0.0224\nEpoch 49/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9649 - loss: 0.1207 - learning_rate: 0.0251\nEpoch 50/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9642 - loss: 0.1140 - learning_rate: 0.0282\nEpoch 51/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9499 - loss: 0.1238 - learning_rate: 0.0316\nEpoch 52/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9611 - loss: 0.1096 - learning_rate: 0.0355\nEpoch 53/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9554 - loss: 0.1185 - learning_rate: 0.0398\nEpoch 54/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9656 - loss: 0.0894 - learning_rate: 0.0447\nEpoch 55/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9559 - loss: 0.1324 - learning_rate: 0.0501\nEpoch 56/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9162 - loss: 0.2374 - learning_rate: 0.0562\nEpoch 57/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9075 - loss: 0.3001 - learning_rate: 0.0631\nEpoch 58/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9578 - loss: 0.1188 - learning_rate: 0.0708\nEpoch 59/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9605 - loss: 0.1295 - learning_rate: 0.0794\nEpoch 60/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - accuracy: 0.9717 - loss: 0.1014 - learning_rate: 0.0891\nEpoch 61/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9619 - loss: 0.1088 - learning_rate: 0.1000\nEpoch 62/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9557 - loss: 0.1247 - learning_rate: 0.1122\nEpoch 63/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8971 - loss: 0.3277 - learning_rate: 0.1259\nEpoch 64/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9014 - loss: 0.3194 - learning_rate: 0.1413\nEpoch 65/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9405 - loss: 0.1477 - learning_rate: 0.1585\nEpoch 66/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9614 - loss: 0.1023 - learning_rate: 0.1778\nEpoch 67/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7683 - loss: 0.8885 - learning_rate: 0.1995\nEpoch 68/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8840 - loss: 0.3150 - learning_rate: 0.2239\nEpoch 69/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9152 - loss: 0.2059 - learning_rate: 0.2512\nEpoch 70/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8798 - loss: 0.3155 - learning_rate: 0.2818\nEpoch 71/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9302 - loss: 0.1901 - learning_rate: 0.3162\nEpoch 72/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8791 - loss: 0.3337 - learning_rate: 0.3548\nEpoch 73/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7443 - loss: 0.4584 - learning_rate: 0.3981\nEpoch 74/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7971 - loss: 0.4335 - learning_rate: 0.4467\nEpoch 75/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8514 - loss: 0.2845 - learning_rate: 0.5012\nEpoch 76/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8404 - loss: 0.3574 - learning_rate: 0.5623\nEpoch 77/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8778 - loss: 0.3084 - learning_rate: 0.6310\nEpoch 78/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8180 - loss: 0.3658 - learning_rate: 0.7079\nEpoch 79/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7148 - loss: 0.6216 - learning_rate: 0.7943\nEpoch 80/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5139 - loss: 0.7005 - learning_rate: 0.8913\nEpoch 81/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5143 - loss: 0.7190 - learning_rate: 1.0000\nEpoch 82/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5281 - loss: 0.7205 - learning_rate: 1.1220\nEpoch 83/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5281 - loss: 0.7096 - learning_rate: 1.2589\nEpoch 84/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5096 - loss: 0.7046 - learning_rate: 1.4125\nEpoch 85/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5056 - loss: 0.7081 - learning_rate: 1.5849\nEpoch 86/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4885 - loss: 0.7155 - learning_rate: 1.7783\nEpoch 87/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5006 - loss: 0.7280 - learning_rate: 1.9953\nEpoch 88/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5006 - loss: 0.7439 - learning_rate: 2.2387\nEpoch 89/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4899 - loss: 0.7570 - learning_rate: 2.5119\nEpoch 90/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5003 - loss: 0.7507 - learning_rate: 2.8184\nEpoch 91/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4978 - loss: 0.7172 - learning_rate: 3.1623\nEpoch 92/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4781 - loss: 0.7097 - learning_rate: 3.5481\nEpoch 93/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4798 - loss: 0.7143 - learning_rate: 3.9811\nEpoch 94/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.4732 - loss: 0.7217 - learning_rate: 4.4668\nEpoch 95/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4732 - loss: 0.7360 - learning_rate: 5.0119\nEpoch 96/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4849 - loss: 0.7729 - learning_rate: 5.6234\nEpoch 97/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4844 - loss: 0.8143 - learning_rate: 6.3096\nEpoch 98/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4892 - loss: 0.8602 - learning_rate: 7.0795\nEpoch 99/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4892 - loss: 0.9049 - learning_rate: 7.9433\nEpoch 100/100\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.4892 - loss: 0.9503 - learning_rate: 8.9125\n</pre> <p>Now our model has finished training, let's have a look at the training history.</p> In\u00a0[50]: Copied! <pre># Checkout the history\npd.DataFrame(history.history).plot(figsize=(10,7), xlabel=\"epochs\");\n</pre> # Checkout the history pd.DataFrame(history.history).plot(figsize=(10,7), xlabel=\"epochs\"); <p>As you you see the learning rate exponentially increases as the number of epochs increases.</p> <p>And you can see the model's accuracy goes up (and loss goes down) at a specific point when the learning rate slowly increases.</p> <p>To figure out where this infliction point is, we can plot the loss versus the log-scale learning rate.</p> In\u00a0[51]: Copied! <pre># Plot the learning rate versus the loss\nlrs = 1e-4 * (10 ** (np.arange(100)/20))\nplt.figure(figsize=(10, 7))\nplt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\nplt.xlabel(\"Learning Rate\")\nplt.ylabel(\"Loss\")\nplt.title(\"Learning rate vs. loss\");\n</pre> # Plot the learning rate versus the loss lrs = 1e-4 * (10 ** (np.arange(100)/20)) plt.figure(figsize=(10, 7)) plt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale plt.xlabel(\"Learning Rate\") plt.ylabel(\"Loss\") plt.title(\"Learning rate vs. loss\"); <p>To figure out the ideal value of the learning rate (at least the ideal value to begin training our model), the rule of thumb is to take the learning rate value where the loss is still decreasing but not quite flattened out (usually about 10x smaller than the bottom of the curve).</p> <p>In this case, our ideal learning rate ends up between <code>0.01</code> ($10^{-2}$) and <code>0.02</code>.</p> <p></p> <p>The ideal learning rate at the start of model training is somewhere just before the loss curve bottoms out (a value where the loss is still decreasing).</p> In\u00a0[52]: Copied! <pre># Example of other typical learning rate values\n10**0, 10**-1, 10**-2, 10**-3, 1e-4\n</pre> # Example of other typical learning rate values 10**0, 10**-1, 10**-2, 10**-3, 1e-4 Out[52]: <pre>(1, 0.1, 0.01, 0.001, 0.0001)</pre> <p>Now we've estimated the ideal learning rate (we'll use <code>0.02</code>) for our model, let's refit it.</p> In\u00a0[53]: Copied! <pre># Set the random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_10 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\n# Compile the model with the ideal learning rate\nmodel_10.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.02), # to adjust the learning rate, you need to use tf.keras.optimizers.Adam (not \"adam\")\n                metrics=[\"accuracy\"])\n\n# Fit the model for 20 epochs (5 less than before)\nhistory = model_10.fit(X_train, y_train, epochs=20)\n</pre> # Set the random seed tf.random.set_seed(42)  # Create the model model_10 = tf.keras.Sequential([   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(1, activation=\"sigmoid\") ])  # Compile the model with the ideal learning rate model_10.compile(loss=\"binary_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.02), # to adjust the learning rate, you need to use tf.keras.optimizers.Adam (not \"adam\")                 metrics=[\"accuracy\"])  # Fit the model for 20 epochs (5 less than before) history = model_10.fit(X_train, y_train, epochs=20) <pre>Epoch 1/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 3ms/step - accuracy: 0.5438 - loss: 0.6876\nEpoch 2/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5670 - loss: 0.6777\nEpoch 3/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5740 - loss: 0.6697\nEpoch 4/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.5743 - loss: 0.6605\nEpoch 5/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.5904 - loss: 0.6495\nEpoch 6/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.6109 - loss: 0.6349\nEpoch 7/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.6123 - loss: 0.6221\nEpoch 8/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.6770 - loss: 0.5789\nEpoch 9/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7165 - loss: 0.5212 \nEpoch 10/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.7473 - loss: 0.4530 \nEpoch 11/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8028 - loss: 0.3890\nEpoch 12/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.8546 - loss: 0.3222\nEpoch 13/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9093 - loss: 0.2580 \nEpoch 14/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9508 - loss: 0.2018\nEpoch 15/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9624 - loss: 0.1617 \nEpoch 16/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9666 - loss: 0.1407 \nEpoch 17/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9713 - loss: 0.1266\nEpoch 18/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9710 - loss: 0.1212\nEpoch 19/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9714 - loss: 0.1165\nEpoch 20/20\n25/25 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9784 - loss: 0.1051\n</pre> <p>Nice! With a little higher learning rate (<code>0.02</code> instead of <code>0.01</code>) we reach a higher accuracy than <code>model_8</code> in less epochs (<code>20</code> instead of <code>25</code>).</p> <p>\ud83d\udee0 Practice: Now you've seen an example of what can happen when you change the learning rate, try changing the learning rate value in the TensorFlow Playground and see what happens. What happens if you increase it? What happens if you decrease it?</p> In\u00a0[54]: Copied! <pre># Evaluate model on the test dataset\nmodel_10.evaluate(X_test, y_test)\n</pre> # Evaluate model on the test dataset model_10.evaluate(X_test, y_test) <pre>7/7 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9745 - loss: 0.0957  \n</pre> Out[54]: <pre>[0.0894651785492897, 0.9750000238418579]</pre> <p>Let's see how the predictions look.</p> In\u00a0[55]: Copied! <pre># Plot the decision boundaries for the training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_10, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_10, X=X_test, y=y_test)\nplt.show()\n</pre> # Plot the decision boundaries for the training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_10, X=X_train, y=y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_10, X=X_test, y=y_test) plt.show() <pre>313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step\ndoing binary classifcation...\n313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step\ndoing binary classifcation...\n</pre> <p>And as we can see, almost perfect again.</p> <p>These are the kind of experiments you'll be running often when building your own models.</p> <p>Start with default settings and see how they perform on your data.</p> <p>And if they don't perform as well as you'd like, improve them.</p> <p>Let's look at a few more ways to evaluate our classification models.</p> In\u00a0[56]: Copied! <pre># Check the accuracy of our model\nloss, accuracy = model_10.evaluate(X_test, y_test)\nprint(f\"Model loss on test set: {loss}\")\nprint(f\"Model accuracy on test set: {(accuracy*100):.2f}%\")\n</pre> # Check the accuracy of our model loss, accuracy = model_10.evaluate(X_test, y_test) print(f\"Model loss on test set: {loss}\") print(f\"Model accuracy on test set: {(accuracy*100):.2f}%\") <pre>7/7 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9745 - loss: 0.0957 \nModel loss on test set: 0.0894651785492897\nModel accuracy on test set: 97.50%\n</pre> <p>How about a confusion matrix?</p> <p> Anatomy of a confusion matrix (what we're going to be creating). Correct predictions appear down the diagonal (from top left to bottom right).</p> <p>We can make a confusion matrix using Scikit-Learn's <code>confusion_matrix</code> method.</p> In\u00a0[67]: Copied! <pre># Create a confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Make predictions\ny_preds = model_10.predict(X_test)\n\n# Create confusion matrix\nconfusion_matrix(y_test, y_preds)\n</pre> # Create a confusion matrix from sklearn.metrics import confusion_matrix  # Make predictions y_preds = model_10.predict(X_test)  # Create confusion matrix confusion_matrix(y_test, y_preds) <pre>7/7 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step \n</pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[67], line 8\n      5 y_preds = model_10.predict(X_test)\n      7 # Create confusion matrix\n----&gt; 8 confusion_matrix(y_test, y_preds)\n\nFile D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213, in validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n    207 try:\n    208     with config_context(\n    209         skip_parameter_validation=(\n    210             prefer_skip_nested_validation or global_skip_validation\n    211         )\n    212     ):\n--&gt; 213         return func(*args, **kwargs)\n    214 except InvalidParameterError as e:\n    215     # When the function is just a wrapper around an estimator, we allow\n    216     # the function to delegate validation to the estimator, but we replace\n    217     # the name of the estimator by the name of the function in the error\n    218     # message to avoid confusion.\n    219     msg = re.sub(\n    220         r\"parameter of \\w+ must be\",\n    221         f\"parameter of {func.__qualname__} must be\",\n    222         str(e),\n    223     )\n\nFile D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:319, in confusion_matrix(y_true, y_pred, labels, sample_weight, normalize)\n    224 @validate_params(\n    225     {\n    226         \"y_true\": [\"array-like\"],\n   (...)\n    235     y_true, y_pred, *, labels=None, sample_weight=None, normalize=None\n    236 ):\n    237     \"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\n    238 \n    239     By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n   (...)\n    317     (0, 2, 1, 1)\n    318     \"\"\"\n--&gt; 319     y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    320     if y_type not in (\"binary\", \"multiclass\"):\n    321         raise ValueError(\"%s is not supported\" % y_type)\n\nFile D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:94, in _check_targets(y_true, y_pred)\n     91     y_type = {\"multiclass\"}\n     93 if len(y_type) &gt; 1:\n---&gt; 94     raise ValueError(\n     95         \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n     96             type_true, type_pred\n     97         )\n     98     )\n    100 # We can't have more than one value on y_type =&gt; The set is no more needed\n    101 y_type = y_type.pop()\n\nValueError: Classification metrics can't handle a mix of binary and continuous targets</pre> <p>Ahh, it seems our predictions aren't in the format they need to be.</p> <p>Let's check them out.</p> In\u00a0[68]: Copied! <pre># View the first 10 predictions\ny_preds[:10]\n</pre> # View the first 10 predictions y_preds[:10] Out[68]: <pre>array([[9.5806485e-01],\n       [9.9525082e-01],\n       [9.9885440e-01],\n       [9.7500271e-01],\n       [1.0511072e-01],\n       [1.7831451e-04],\n       [9.9182343e-01],\n       [2.9515686e-05],\n       [9.6916127e-01],\n       [1.7080535e-04]], dtype=float32)</pre> <p>What about our test labels?</p> In\u00a0[69]: Copied! <pre># View the first 10 test labels\ny_test[:10]\n</pre> # View the first 10 test labels y_test[:10] Out[69]: <pre>array([1, 1, 1, 1, 0, 0, 1, 0, 1, 0], dtype=int64)</pre> <p>It looks like we need to get our predictions into the binary format (0 or 1).</p> <p>But you might be wondering, what format are they currently in?</p> <p>In their current format (<code>9.8526537e-01</code>), they're in a form called prediction probabilities.</p> <p>You'll see this often with the outputs of neural networks. Often they won't be exact values but more a probability of how likely they are to be one value or another.</p> <p>So one of the steps you'll often see after making predicitons with a neural network is converting the prediction probabilities into labels.</p> <p>In our case, since our ground truth labels (<code>y_test</code>) are binary (0 or 1), we can convert the prediction probabilities using to their binary form using <code>tf.round()</code>.</p> In\u00a0[70]: Copied! <pre># Convert prediction probabilities to binary format and view the first 10\ntf.round(y_preds)[:10]\n</pre> # Convert prediction probabilities to binary format and view the first 10 tf.round(y_preds)[:10] Out[70]: <pre>&lt;tf.Tensor: shape=(10, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [1.],\n       [0.]], dtype=float32)&gt;</pre> <p>Wonderful! Now we can use the <code>confusion_matrix</code> function.</p> In\u00a0[71]: Copied! <pre># Create a confusion matrix\nconfusion_matrix(y_test, tf.round(y_preds))\n</pre> # Create a confusion matrix confusion_matrix(y_test, tf.round(y_preds)) Out[71]: <pre>array([[98,  3],\n       [ 2, 97]], dtype=int64)</pre> <p>Alright, we can see the highest numbers are down the diagonal (from top left to bottom right) so this a good sign, but the rest of the matrix doesn't really tell us much.</p> <p>How about we make a function to make our confusion matrix a little more visual?</p> In\u00a0[72]: Copied! <pre># Note: The following confusion matrix code is a remix of Scikit-Learn's\n# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n# and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\nimport itertools\n\nfigsize = (10, 10)\n\n# Create the confusion matrix\ncm = confusion_matrix(y_test, tf.round(y_preds))\ncm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\nn_classes = cm.shape[0]\n\n# Let's prettify it\nfig, ax = plt.subplots(figsize=figsize)\n# Create a matrix plot\ncax = ax.matshow(cm, cmap=plt.cm.Blues) # https://matplotlib.org/3.2.0/api/_as_gen/matplotlib.axes.Axes.matshow.html\nfig.colorbar(cax)\n\n# Create classes\nclasses = False\n\nif classes:\n  labels = classes\nelse:\n  labels = np.arange(cm.shape[0])\n\n# Label the axes\nax.set(title=\"Confusion Matrix\",\n       xlabel=\"Predicted label\",\n       ylabel=\"True label\",\n       xticks=np.arange(n_classes),\n       yticks=np.arange(n_classes),\n       xticklabels=labels,\n       yticklabels=labels)\n\n# Set x-axis labels to bottom\nax.xaxis.set_label_position(\"bottom\")\nax.xaxis.tick_bottom()\n\n# Adjust label size\nax.xaxis.label.set_size(20)\nax.yaxis.label.set_size(20)\nax.title.set_size(20)\n\n# Set threshold for different colors\nthreshold = (cm.max() + cm.min()) / 2.\n\n# Plot the text on each cell\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n  plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n           horizontalalignment=\"center\",\n           color=\"white\" if cm[i, j] &gt; threshold else \"black\",\n           size=15)\n</pre> # Note: The following confusion matrix code is a remix of Scikit-Learn's # plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html # and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb import itertools  figsize = (10, 10)  # Create the confusion matrix cm = confusion_matrix(y_test, tf.round(y_preds)) cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it n_classes = cm.shape[0]  # Let's prettify it fig, ax = plt.subplots(figsize=figsize) # Create a matrix plot cax = ax.matshow(cm, cmap=plt.cm.Blues) # https://matplotlib.org/3.2.0/api/_as_gen/matplotlib.axes.Axes.matshow.html fig.colorbar(cax)  # Create classes classes = False  if classes:   labels = classes else:   labels = np.arange(cm.shape[0])  # Label the axes ax.set(title=\"Confusion Matrix\",        xlabel=\"Predicted label\",        ylabel=\"True label\",        xticks=np.arange(n_classes),        yticks=np.arange(n_classes),        xticklabels=labels,        yticklabels=labels)  # Set x-axis labels to bottom ax.xaxis.set_label_position(\"bottom\") ax.xaxis.tick_bottom()  # Adjust label size ax.xaxis.label.set_size(20) ax.yaxis.label.set_size(20) ax.title.set_size(20)  # Set threshold for different colors threshold = (cm.max() + cm.min()) / 2.  # Plot the text on each cell for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):   plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",            horizontalalignment=\"center\",            color=\"white\" if cm[i, j] &gt; threshold else \"black\",            size=15) <p>That looks much better. It seems our model has made almost perfect predictions on the test set except for two false positives (top right corner).</p> In\u00a0[73]: Copied! <pre># What does itertools.product do? Combines two things into each combination\nimport itertools\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n  print(i, j)\n</pre> # What does itertools.product do? Combines two things into each combination import itertools for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):   print(i, j) <pre>0 0\n0 1\n1 0\n1 1\n</pre> In\u00a0[74]: Copied! <pre>import tensorflow as tf\nfrom tensorflow.keras.datasets import fashion_mnist\n\n# The data has already been sorted into training and test sets for us\n(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n</pre> import tensorflow as tf from tensorflow.keras.datasets import fashion_mnist  # The data has already been sorted into training and test sets for us (train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data() <pre>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 40s 2us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 1us/step\n</pre> <p>Now let's check out an example.</p> In\u00a0[75]: Copied! <pre># Show the first training example\nprint(f\"Training sample:\\n{train_data[0]}\\n\")\nprint(f\"Training label: {train_labels[0]}\")\n</pre> # Show the first training example print(f\"Training sample:\\n{train_data[0]}\\n\") print(f\"Training label: {train_labels[0]}\") <pre>Training sample:\n[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n    0   1   4   0   0   0   0   1   1   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n   54   0   0   0   1   3   4   0   0   3]\n [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n  144 123  23   0   0   0   0  12  10   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n  107 156 161 109  64  23  77 130  72  15]\n [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n  216 163 127 121 122 146 141  88 172  66]\n [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n  223 223 215 213 164 127 123 196 229   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n  235 227 224 222 224 221 223 245 173   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n  180 212 210 211 213 223 220 243 202   0]\n [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n  169 227 208 218 224 212 226 197 209  52]\n [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n  198 221 215 213 222 220 245 119 167  56]\n [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n  232 213 218 223 234 217 217 209  92   0]\n [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n  222 221 216 223 229 215 218 255  77   0]\n [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n  211 218 224 223 219 215 224 244 159   0]\n [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n  224 234 176 188 250 248 233 238 215   0]\n [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n  255 255 221 234 221 211 220 232 246   0]\n [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n  188 154 191 210 204 209 222 228 225   0]\n [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n  168 219 221 215 217 223 223 224 229  29]\n [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n  239 223 218 212 209 222 220 221 230  67]\n [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n  199 206 186 181 177 172 181 205 206 115]\n [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n  195 191 198 192 176 156 167 177 210  92]\n [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n  210 210 211 188 188 194 192 216 170   0]\n [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n  182 182 181 176 166 168  99  58   0   0]\n [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]]\n\nTraining label: 9\n</pre> <p>Woah, we get a large list of numbers, followed (the data) by a single number (the class label).</p> <p>What about the shapes?</p> In\u00a0[76]: Copied! <pre># Check the shape of our data\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n</pre> # Check the shape of our data train_data.shape, train_labels.shape, test_data.shape, test_labels.shape Out[76]: <pre>((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))</pre> In\u00a0[77]: Copied! <pre># Check shape of a single example\ntrain_data[0].shape, train_labels[0].shape\n</pre> # Check shape of a single example train_data[0].shape, train_labels[0].shape Out[77]: <pre>((28, 28), ())</pre> <p>Okay, 60,000 training examples each with shape (28, 28) and a label each as well as 10,000 test examples of shape (28, 28).</p> <p>But these are just numbers, let's visualize.</p> In\u00a0[78]: Copied! <pre># Plot a single example\nimport matplotlib.pyplot as plt\nplt.imshow(train_data[7]);\n</pre> # Plot a single example import matplotlib.pyplot as plt plt.imshow(train_data[7]); <p>Hmm, but what about its label?</p> In\u00a0[79]: Copied! <pre># Check our samples label\ntrain_labels[7]\n</pre> # Check our samples label train_labels[7] Out[79]: <pre>2</pre> <p>It looks like our labels are in numerical form. And while this is fine for a neural network, you might want to have them in human readable form.</p> <p>Let's create a small list of the class names (we can find them on the dataset's GitHub page).</p> <p>\ud83d\udd11 Note: Whilst this dataset has been prepared for us and ready to go, it's important to remember many datasets won't be ready to go like this one. Often you'll have to do a few preprocessing steps to have it ready to use with a neural network (we'll see more of this when we work with our own data later).</p> In\u00a0[80]: Copied! <pre>class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n# How many classes are there (this'll be our output shape)?\nlen(class_names)\n</pre> class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']  # How many classes are there (this'll be our output shape)? len(class_names) Out[80]: <pre>10</pre> <p>Now we have these, let's plot another example.</p> <p>\ud83e\udd14 Question: Pay particular attention to what the data we're working with looks like. Is it only straight lines? Or does it have non-straight lines as well? Do you think if we wanted to find patterns in the photos of clothes (which are actually collections of pixels), will our model need non-linearities (non-straight lines) or not?</p> In\u00a0[81]: Copied! <pre># Plot an example image and its label\nplt.imshow(train_data[17], cmap=plt.cm.binary) # change the colours to black &amp; white\nplt.title(class_names[train_labels[17]]);\n</pre> # Plot an example image and its label plt.imshow(train_data[17], cmap=plt.cm.binary) # change the colours to black &amp; white plt.title(class_names[train_labels[17]]); In\u00a0[82]: Copied! <pre># Plot multiple random images of fashion MNIST\nimport random\nplt.figure(figsize=(7, 7))\nfor i in range(4):\n  ax = plt.subplot(2, 2, i + 1)\n  rand_index = random.choice(range(len(train_data)))\n  plt.imshow(train_data[rand_index], cmap=plt.cm.binary)\n  plt.title(class_names[train_labels[rand_index]])\n  plt.axis(False)\n</pre> # Plot multiple random images of fashion MNIST import random plt.figure(figsize=(7, 7)) for i in range(4):   ax = plt.subplot(2, 2, i + 1)   rand_index = random.choice(range(len(train_data)))   plt.imshow(train_data[rand_index], cmap=plt.cm.binary)   plt.title(class_names[train_labels[rand_index]])   plt.axis(False) <p>Alright, let's build a model to figure out the relationship between the pixel values and their labels.</p> <p>Since this is a multiclass classification problem, we'll need to make a few changes to our architecture (inline with Table 1 above):</p> <ul> <li>The input shape will have to deal with 28x28 tensors (the height and width of our images).<ul> <li>We're actually going to squash the input into a tensor (vector) of shape <code>(784)</code>.</li> </ul> </li> <li>The output shape will have to be 10 because we need our model to predict for 10 different classes.<ul> <li>We'll also change the <code>activation</code> parameter of our output layer to be <code>\"softmax\"</code> instead of <code>'sigmoid'</code>. As we'll see the <code>\"softmax\"</code> activation function outputs a series of values between 0 &amp; 1 (the same shape as output shape, which together add up to ~1. The index with the highest value is predicted by the model to be the most likely class.</li> </ul> </li> <li>We'll need to change our loss function from a binary loss function to a multiclass loss function.<ul> <li>More specifically, since our labels are in integer form, we'll use <code>tf.keras.losses.SparseCategoricalCrossentropy()</code>, if our labels were one-hot encoded (e.g. they looked something like <code>[0, 0, 1, 0, 0...]</code>), we'd use <code>tf.keras.losses.CategoricalCrossentropy()</code>.</li> </ul> </li> <li>We'll also use the <code>validation_data</code> parameter when calling the <code>fit()</code> function. This will give us an idea of how the model performs on the test set during training.</li> </ul> <p>You ready? Let's go.</p> In\u00a0[83]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_11 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784, the Flatten layer does this for us)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel_11.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # different loss function for multiclass classifcation\n                 optimizer=tf.keras.optimizers.Adam(),\n                 metrics=[\"accuracy\"])\n\n# Fit the model\nnon_norm_history = model_11.fit(train_data,\n                                train_labels,\n                                epochs=10,\n                                validation_data=(test_data, test_labels)) # see how the model performs on the test set during training\n</pre> # Set random seed tf.random.set_seed(42)  # Create the model model_11 = tf.keras.Sequential([   tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784, the Flatten layer does this for us)   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax ])  # Compile the model model_11.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # different loss function for multiclass classifcation                  optimizer=tf.keras.optimizers.Adam(),                  metrics=[\"accuracy\"])  # Fit the model non_norm_history = model_11.fit(train_data,                                 train_labels,                                 epochs=10,                                 validation_data=(test_data, test_labels)) # see how the model performs on the test set during training <pre>D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n</pre> <pre>Epoch 1/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 3ms/step - accuracy: 0.0983 - loss: 2.4172 - val_accuracy: 0.1001 - val_loss: 2.3025\nEpoch 2/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.0986 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025\nEpoch 3/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 3ms/step - accuracy: 0.0986 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025\nEpoch 4/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025\nEpoch 5/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025\nEpoch 6/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025\nEpoch 7/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025\nEpoch 8/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025\nEpoch 9/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025\nEpoch 10/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025\n</pre> In\u00a0[84]: Copied! <pre># Check the shapes of our model\n# Note: the \"None\" in (None, 784) is for batch_size, we'll cover this in a later module\nmodel_11.summary()\n</pre> # Check the shapes of our model # Note: the \"None\" in (None, 784) is for batch_size, we'll cover this in a later module model_11.summary() <pre>Model: \"sequential_12\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 flatten (Flatten)                    \u2502 (None, 784)                 \u2502               0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_31 (Dense)                     \u2502 (None, 4)                   \u2502           3,140 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_32 (Dense)                     \u2502 (None, 4)                   \u2502              20 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_33 (Dense)                     \u2502 (None, 10)                  \u2502              50 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 9,632 (37.63 KB)\n</pre> <pre> Trainable params: 3,210 (12.54 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <pre> Optimizer params: 6,422 (25.09 KB)\n</pre> <p>Alright, our model gets to about ~35% accuracy after 10 epochs using a similar style model to what we used on our binary classification problem.</p> <p>Which is better than guessing (guessing with 10 classes would result in about 10% accuracy) but we can do better.</p> <p>Do you remember when we talked about neural networks preferring numbers between 0 and 1? (if not, treat this as a reminder)</p> <p>Well, right now, the data we have isn't between 0 and 1, in other words, it's not normalized (hence why we used the <code>non_norm_history</code> variable when calling <code>fit()</code>). It's pixel values are between 0 and 255.</p> <p>Let's see.</p> In\u00a0[85]: Copied! <pre># Check the min and max values of the training data\ntrain_data.min(), train_data.max()\n</pre> # Check the min and max values of the training data train_data.min(), train_data.max() Out[85]: <pre>(0, 255)</pre> <p>We can get these values between 0 and 1 by dividing the entire array by the maximum: <code>255.0</code> (dividing by a float also converts to a float).</p> <p>Doing so will result in all of our data being between 0 and 1 (known as scaling or normalization).</p> In\u00a0[86]: Copied! <pre># Divide train and test images by the maximum value (normalize it)\ntrain_data = train_data / 255.0\ntest_data = test_data / 255.0\n\n# Check the min and max values of the training data\ntrain_data.min(), train_data.max()\n</pre> # Divide train and test images by the maximum value (normalize it) train_data = train_data / 255.0 test_data = test_data / 255.0  # Check the min and max values of the training data train_data.min(), train_data.max() Out[86]: <pre>(0.0, 1.0)</pre> <p>Beautiful! Now our data is between 0 and 1. Let's see what happens when we model it.</p> <p>We'll use the same model as before (<code>model_11</code>) except this time the data will be normalized.</p> In\u00a0[87]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_12 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel_12.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(),\n                 metrics=[\"accuracy\"])\n\n# Fit the model (to the normalized data)\nnorm_history = model_12.fit(train_data,\n                            train_labels,\n                            epochs=10,\n                            validation_data=(test_data, test_labels))\n</pre> # Set random seed tf.random.set_seed(42)  # Create the model model_12 = tf.keras.Sequential([   tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax ])  # Compile the model model_12.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),                  optimizer=tf.keras.optimizers.Adam(),                  metrics=[\"accuracy\"])  # Fit the model (to the normalized data) norm_history = model_12.fit(train_data,                             train_labels,                             epochs=10,                             validation_data=(test_data, test_labels)) <pre>Epoch 1/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 3ms/step - accuracy: 0.2158 - loss: 1.9761 - val_accuracy: 0.4401 - val_loss: 1.3918\nEpoch 2/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.4754 - loss: 1.3161 - val_accuracy: 0.5242 - val_loss: 1.1941\nEpoch 3/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.5355 - loss: 1.1648 - val_accuracy: 0.5452 - val_loss: 1.1227\nEpoch 4/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.5472 - loss: 1.1011 - val_accuracy: 0.5576 - val_loss: 1.0870\nEpoch 5/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.5635 - loss: 1.0678 - val_accuracy: 0.5718 - val_loss: 1.0619\nEpoch 6/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 2ms/step - accuracy: 0.5744 - loss: 1.0477 - val_accuracy: 0.5874 - val_loss: 1.0474\nEpoch 7/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 2ms/step - accuracy: 0.5877 - loss: 1.0305 - val_accuracy: 0.6094 - val_loss: 1.0314\nEpoch 8/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 2ms/step - accuracy: 0.6136 - loss: 1.0109 - val_accuracy: 0.6562 - val_loss: 1.0011\nEpoch 9/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 2ms/step - accuracy: 0.6583 - loss: 0.9772 - val_accuracy: 0.6928 - val_loss: 0.9538\nEpoch 10/10\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 3ms/step - accuracy: 0.6958 - loss: 0.9224 - val_accuracy: 0.7093 - val_loss: 0.8947\n</pre> <p>Woah, we used the exact same model as before but we with normalized data we're now seeing a much higher accuracy value!</p> <p>Let's plot each model's history (their loss curves).</p> In\u00a0[88]: Copied! <pre>import pandas as pd\n# Plot non-normalized data loss curves\npd.DataFrame(non_norm_history.history).plot(title=\"Non-normalized Data\")\n# Plot normalized data loss curves\npd.DataFrame(norm_history.history).plot(title=\"Normalized data\");\n</pre> import pandas as pd # Plot non-normalized data loss curves pd.DataFrame(non_norm_history.history).plot(title=\"Non-normalized Data\") # Plot normalized data loss curves pd.DataFrame(norm_history.history).plot(title=\"Normalized data\"); <p>Wow. From these two plots, we can see how much quicker our model with the normalized data (<code>model_12</code>) improved than the model with the non-normalized data (<code>model_11</code>).</p> <p>\ud83d\udd11 Note: The same model with even slightly different data can produce dramatically different results. So when you're comparing models, it's important to make sure you're comparing them on the same criteria (e.g. same architecture but different data or same data but different architecture).</p> <p>How about we find the ideal learning rate and see what happens?</p> <p>We'll use the same architecture we've been using.</p> In\u00a0[89]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_13 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel_13.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(),\n                 metrics=[\"accuracy\"])\n\n# Create the learning rate callback\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))\n\n# Fit the model\nfind_lr_history = model_13.fit(train_data,\n                               train_labels,\n                               epochs=40, # model already doing pretty good with current LR, probably don't need 100 epochs\n                               validation_data=(test_data, test_labels),\n                               callbacks=[lr_scheduler])\n</pre> # Set random seed tf.random.set_seed(42)  # Create the model model_13 = tf.keras.Sequential([   tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax ])  # Compile the model model_13.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),                  optimizer=tf.keras.optimizers.Adam(),                  metrics=[\"accuracy\"])  # Create the learning rate callback lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))  # Fit the model find_lr_history = model_13.fit(train_data,                                train_labels,                                epochs=40, # model already doing pretty good with current LR, probably don't need 100 epochs                                validation_data=(test_data, test_labels),                                callbacks=[lr_scheduler]) <pre>D:\\anaconda\\envs\\py3-TF2.0\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n</pre> <pre>Epoch 1/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 3ms/step - accuracy: 0.3501 - loss: 1.6252 - val_accuracy: 0.5883 - val_loss: 1.0367 - learning_rate: 0.0010\nEpoch 2/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.6031 - loss: 0.9875 - val_accuracy: 0.6861 - val_loss: 0.8441 - learning_rate: 0.0011\nEpoch 3/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7117 - loss: 0.7836 - val_accuracy: 0.7452 - val_loss: 0.7132 - learning_rate: 0.0013\nEpoch 4/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 2ms/step - accuracy: 0.7515 - loss: 0.6917 - val_accuracy: 0.7586 - val_loss: 0.6820 - learning_rate: 0.0014\nEpoch 5/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7632 - loss: 0.6543 - val_accuracy: 0.7611 - val_loss: 0.6566 - learning_rate: 0.0016\nEpoch 6/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 2ms/step - accuracy: 0.7689 - loss: 0.6329 - val_accuracy: 0.7647 - val_loss: 0.6471 - learning_rate: 0.0018\nEpoch 7/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 2ms/step - accuracy: 0.7719 - loss: 0.6226 - val_accuracy: 0.7652 - val_loss: 0.6440 - learning_rate: 0.0020\nEpoch 8/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7735 - loss: 0.6182 - val_accuracy: 0.7667 - val_loss: 0.6424 - learning_rate: 0.0022\nEpoch 9/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7736 - loss: 0.6158 - val_accuracy: 0.7641 - val_loss: 0.6441 - learning_rate: 0.0025\nEpoch 10/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7735 - loss: 0.6150 - val_accuracy: 0.7620 - val_loss: 0.6458 - learning_rate: 0.0028\nEpoch 11/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7742 - loss: 0.6144 - val_accuracy: 0.7584 - val_loss: 0.6536 - learning_rate: 0.0032\nEpoch 12/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7731 - loss: 0.6157 - val_accuracy: 0.7560 - val_loss: 0.6611 - learning_rate: 0.0035\nEpoch 13/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7739 - loss: 0.6160 - val_accuracy: 0.7545 - val_loss: 0.6638 - learning_rate: 0.0040\nEpoch 14/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7731 - loss: 0.6164 - val_accuracy: 0.7525 - val_loss: 0.6669 - learning_rate: 0.0045\nEpoch 15/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7725 - loss: 0.6174 - val_accuracy: 0.7508 - val_loss: 0.6720 - learning_rate: 0.0050\nEpoch 16/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7721 - loss: 0.6178 - val_accuracy: 0.7505 - val_loss: 0.6710 - learning_rate: 0.0056\nEpoch 17/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7742 - loss: 0.6178 - val_accuracy: 0.7558 - val_loss: 0.6642 - learning_rate: 0.0063\nEpoch 18/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7756 - loss: 0.6190 - val_accuracy: 0.7582 - val_loss: 0.6623 - learning_rate: 0.0071\nEpoch 19/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7759 - loss: 0.6211 - val_accuracy: 0.7580 - val_loss: 0.6633 - learning_rate: 0.0079\nEpoch 20/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7744 - loss: 0.6238 - val_accuracy: 0.7560 - val_loss: 0.6689 - learning_rate: 0.0089\nEpoch 21/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7726 - loss: 0.6282 - val_accuracy: 0.7554 - val_loss: 0.6709 - learning_rate: 0.0100\nEpoch 22/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7708 - loss: 0.6316 - val_accuracy: 0.7494 - val_loss: 0.6829 - learning_rate: 0.0112\nEpoch 23/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7701 - loss: 0.6387 - val_accuracy: 0.7481 - val_loss: 0.6884 - learning_rate: 0.0126\nEpoch 24/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7688 - loss: 0.6435 - val_accuracy: 0.7453 - val_loss: 0.6955 - learning_rate: 0.0141\nEpoch 25/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 2ms/step - accuracy: 0.7645 - loss: 0.6528 - val_accuracy: 0.7411 - val_loss: 0.7083 - learning_rate: 0.0158\nEpoch 26/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7639 - loss: 0.6578 - val_accuracy: 0.7323 - val_loss: 0.7232 - learning_rate: 0.0178\nEpoch 27/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7601 - loss: 0.6704 - val_accuracy: 0.7353 - val_loss: 0.7220 - learning_rate: 0.0200\nEpoch 28/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7560 - loss: 0.6822 - val_accuracy: 0.7359 - val_loss: 0.7109 - learning_rate: 0.0224\nEpoch 29/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7494 - loss: 0.6977 - val_accuracy: 0.7361 - val_loss: 0.7232 - learning_rate: 0.0251\nEpoch 30/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7448 - loss: 0.7072 - val_accuracy: 0.7295 - val_loss: 0.7468 - learning_rate: 0.0282\nEpoch 31/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7308 - loss: 0.7486 - val_accuracy: 0.7294 - val_loss: 0.7358 - learning_rate: 0.0316\nEpoch 32/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7338 - loss: 0.7515 - val_accuracy: 0.7248 - val_loss: 0.7697 - learning_rate: 0.0355\nEpoch 33/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7263 - loss: 0.7760 - val_accuracy: 0.7163 - val_loss: 0.7975 - learning_rate: 0.0398\nEpoch 34/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7080 - loss: 0.8292 - val_accuracy: 0.6639 - val_loss: 0.8910 - learning_rate: 0.0447\nEpoch 35/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.6835 - loss: 0.8798 - val_accuracy: 0.6585 - val_loss: 0.9027 - learning_rate: 0.0501\nEpoch 36/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.6496 - loss: 0.9609 - val_accuracy: 0.6823 - val_loss: 0.9020 - learning_rate: 0.0562\nEpoch 37/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.4564 - loss: 1.3500 - val_accuracy: 0.3779 - val_loss: 1.3239 - learning_rate: 0.0631\nEpoch 38/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.3945 - loss: 1.3394 - val_accuracy: 0.4003 - val_loss: 1.4029 - learning_rate: 0.0708\nEpoch 39/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.3927 - loss: 1.3779 - val_accuracy: 0.3692 - val_loss: 1.3400 - learning_rate: 0.0794\nEpoch 40/40\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.3832 - loss: 1.4105 - val_accuracy: 0.3095 - val_loss: 1.5997 - learning_rate: 0.0891\n</pre> In\u00a0[90]: Copied! <pre># Plot the learning rate decay curve\nimport numpy as np\nimport matplotlib.pyplot as plt\nlrs = 1e-3 * (10**(np.arange(40)/20))\nplt.semilogx(lrs, find_lr_history.history[\"loss\"]) # want the x-axis to be log-scale\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"Loss\")\nplt.title(\"Finding the ideal learning rate\");\n</pre> # Plot the learning rate decay curve import numpy as np import matplotlib.pyplot as plt lrs = 1e-3 * (10**(np.arange(40)/20)) plt.semilogx(lrs, find_lr_history.history[\"loss\"]) # want the x-axis to be log-scale plt.xlabel(\"Learning rate\") plt.ylabel(\"Loss\") plt.title(\"Finding the ideal learning rate\"); <p>In this case, it looks like somewhere close to the default learning rate of the Adam optimizer (<code>0.001</code>) is the ideal learning rate.</p> <p>Let's refit a model using the ideal learning rate.</p> In\u00a0[91]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_14 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel_14.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # ideal learning rate (same as default)\n                 metrics=[\"accuracy\"])\n\n# Fit the model\nhistory = model_14.fit(train_data,\n                       train_labels,\n                       epochs=20,\n                       validation_data=(test_data, test_labels))\n</pre> # Set random seed tf.random.set_seed(42)  # Create the model model_14 = tf.keras.Sequential([   tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax ])  # Compile the model model_14.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # ideal learning rate (same as default)                  metrics=[\"accuracy\"])  # Fit the model history = model_14.fit(train_data,                        train_labels,                        epochs=20,                        validation_data=(test_data, test_labels)) <pre>Epoch 1/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 3ms/step - accuracy: 0.4631 - loss: 1.4866 - val_accuracy: 0.6686 - val_loss: 0.9178\nEpoch 2/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.6946 - loss: 0.8475 - val_accuracy: 0.7302 - val_loss: 0.7608\nEpoch 3/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7487 - loss: 0.7210 - val_accuracy: 0.7604 - val_loss: 0.6907\nEpoch 4/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7755 - loss: 0.6521 - val_accuracy: 0.7787 - val_loss: 0.6429\nEpoch 5/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7917 - loss: 0.6083 - val_accuracy: 0.7880 - val_loss: 0.6178\nEpoch 6/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.7995 - loss: 0.5839 - val_accuracy: 0.7958 - val_loss: 0.6021\nEpoch 7/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8034 - loss: 0.5680 - val_accuracy: 0.8004 - val_loss: 0.5919\nEpoch 8/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8070 - loss: 0.5576 - val_accuracy: 0.8037 - val_loss: 0.5856\nEpoch 9/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8102 - loss: 0.5496 - val_accuracy: 0.8069 - val_loss: 0.5805\nEpoch 10/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8127 - loss: 0.5430 - val_accuracy: 0.8076 - val_loss: 0.5766\nEpoch 11/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8144 - loss: 0.5377 - val_accuracy: 0.8083 - val_loss: 0.5737\nEpoch 12/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8154 - loss: 0.5335 - val_accuracy: 0.8091 - val_loss: 0.5708\nEpoch 13/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8170 - loss: 0.5298 - val_accuracy: 0.8097 - val_loss: 0.5691\nEpoch 14/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8175 - loss: 0.5269 - val_accuracy: 0.8111 - val_loss: 0.5674\nEpoch 15/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8188 - loss: 0.5242 - val_accuracy: 0.8110 - val_loss: 0.5662\nEpoch 16/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8197 - loss: 0.5219 - val_accuracy: 0.8110 - val_loss: 0.5650\nEpoch 17/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8209 - loss: 0.5198 - val_accuracy: 0.8116 - val_loss: 0.5645\nEpoch 18/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8222 - loss: 0.5179 - val_accuracy: 0.8114 - val_loss: 0.5640\nEpoch 19/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8221 - loss: 0.5163 - val_accuracy: 0.8121 - val_loss: 0.5632\nEpoch 20/20\n1875/1875 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 3ms/step - accuracy: 0.8225 - loss: 0.5147 - val_accuracy: 0.8127 - val_loss: 0.5627\n</pre> <p>Now we've got a model trained with a close-to-ideal learning rate and performing pretty well, we've got a couple of options.</p> <p>We could:</p> <ul> <li>Evaluate its performance using other classification metrics (such as a confusion matrix or classification report).</li> <li>Assess some of its predictions (through visualizations).</li> <li>Improve its accuracy (by training it for longer or changing the architecture).</li> <li>Save and export it for use in an application.</li> </ul> <p>Let's go through the first two options.</p> <p>First we'll create a classification matrix to visualize its predictions across the different classes.</p> In\u00a0[92]: Copied! <pre># Note: The following confusion matrix code is a remix of Scikit-Learn's\n# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n# and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\n# Our function needs a different name to sklearn's plot_confusion_matrix\ndef make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15):\n  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n\n  If classes is passed, confusion matrix will be labelled, if not, integer class values\n  will be used.\n\n  Args:\n    y_true: Array of truth labels (must be same shape as y_pred).\n    y_pred: Array of predicted labels (must be same shape as y_true).\n    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n    figsize: Size of output figure (default=(10, 10)).\n    text_size: Size of output figure text (default=15).\n\n  Returns:\n    A labelled confusion matrix plot comparing y_true and y_pred.\n\n  Example usage:\n    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n                          y_pred=y_preds, # predicted labels\n                          classes=class_names, # array of class label names\n                          figsize=(15, 15),\n                          text_size=10)\n  \"\"\"\n  # Create the confustion matrix\n  cm = confusion_matrix(y_true, y_pred)\n  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n  n_classes = cm.shape[0] # find the number of classes we're dealing with\n\n  # Plot the figure and make it pretty\n  fig, ax = plt.subplots(figsize=figsize)\n  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n  fig.colorbar(cax)\n\n  # Are there a list of classes?\n  if classes:\n    labels = classes\n  else:\n    labels = np.arange(cm.shape[0])\n\n  # Label the axes\n  ax.set(title=\"Confusion Matrix\",\n         xlabel=\"Predicted label\",\n         ylabel=\"True label\",\n         xticks=np.arange(n_classes), # create enough axis slots for each class\n         yticks=np.arange(n_classes),\n         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n         yticklabels=labels)\n\n  # Make x-axis labels appear on bottom\n  ax.xaxis.set_label_position(\"bottom\")\n  ax.xaxis.tick_bottom()\n\n  # Set the threshold for different colors\n  threshold = (cm.max() + cm.min()) / 2.\n\n  # Plot the text on each cell\n  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] &gt; threshold else \"black\",\n             size=text_size)\n</pre> # Note: The following confusion matrix code is a remix of Scikit-Learn's # plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html # and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb import itertools from sklearn.metrics import confusion_matrix  # Our function needs a different name to sklearn's plot_confusion_matrix def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15):   \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.    If classes is passed, confusion matrix will be labelled, if not, integer class values   will be used.    Args:     y_true: Array of truth labels (must be same shape as y_pred).     y_pred: Array of predicted labels (must be same shape as y_true).     classes: Array of class labels (e.g. string form). If `None`, integer labels are used.     figsize: Size of output figure (default=(10, 10)).     text_size: Size of output figure text (default=15).    Returns:     A labelled confusion matrix plot comparing y_true and y_pred.    Example usage:     make_confusion_matrix(y_true=test_labels, # ground truth test labels                           y_pred=y_preds, # predicted labels                           classes=class_names, # array of class label names                           figsize=(15, 15),                           text_size=10)   \"\"\"   # Create the confustion matrix   cm = confusion_matrix(y_true, y_pred)   cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it   n_classes = cm.shape[0] # find the number of classes we're dealing with    # Plot the figure and make it pretty   fig, ax = plt.subplots(figsize=figsize)   cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better   fig.colorbar(cax)    # Are there a list of classes?   if classes:     labels = classes   else:     labels = np.arange(cm.shape[0])    # Label the axes   ax.set(title=\"Confusion Matrix\",          xlabel=\"Predicted label\",          ylabel=\"True label\",          xticks=np.arange(n_classes), # create enough axis slots for each class          yticks=np.arange(n_classes),          xticklabels=labels, # axes will labeled with class names (if they exist) or ints          yticklabels=labels)    # Make x-axis labels appear on bottom   ax.xaxis.set_label_position(\"bottom\")   ax.xaxis.tick_bottom()    # Set the threshold for different colors   threshold = (cm.max() + cm.min()) / 2.    # Plot the text on each cell   for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):     plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",              horizontalalignment=\"center\",              color=\"white\" if cm[i, j] &gt; threshold else \"black\",              size=text_size) <p>Since a confusion matrix compares the truth labels (<code>test_labels</code>) to the predicted labels, we have to make some predictions with our model.</p> In\u00a0[93]: Copied! <pre># Make predictions with the most recent model\ny_probs = model_14.predict(test_data) # \"probs\" is short for probabilities\n\n# View the first 5 predictions\ny_probs[:5]\n</pre> # Make predictions with the most recent model y_probs = model_14.predict(test_data) # \"probs\" is short for probabilities  # View the first 5 predictions y_probs[:5] <pre>313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 3ms/step\n</pre> Out[93]: <pre>array([[3.32330121e-03, 2.81442535e-05, 1.14267032e-05, 2.78759544e-05,\n        1.65281119e-06, 1.43556625e-01, 8.79438361e-04, 1.46590874e-01,\n        2.21113864e-04, 7.05359578e-01],\n       [1.41393888e-04, 5.80133497e-10, 7.16942549e-01, 3.28307215e-05,\n        2.21403509e-01, 0.00000000e+00, 6.14793114e-02, 0.00000000e+00,\n        3.05352700e-07, 0.00000000e+00],\n       [1.92334678e-03, 9.97694075e-01, 4.38925463e-06, 3.76133510e-04,\n        1.18040466e-10, 1.25610046e-29, 1.94088898e-06, 1.92841159e-13,\n        4.23684901e-16, 6.21246943e-11],\n       [3.18397913e-04, 9.98650610e-01, 2.81396501e-06, 1.02779898e-03,\n        1.53851765e-10, 6.76019936e-31, 4.12143322e-07, 1.32739143e-13,\n        8.32940635e-17, 1.84258511e-12],\n       [1.31434500e-01, 1.67336722e-04, 1.86377615e-01, 1.60328615e-02,\n        4.82553691e-02, 6.68696530e-18, 6.15544081e-01, 2.38717523e-20,\n        2.18813238e-03, 2.09246382e-15]], dtype=float32)</pre> <p>Our model outputs a list of prediction probabilities, meaning, it outputs a number for how likely it thinks a particular class is to be the label.</p> <p>The higher the number in the prediction probabilities list, the more likely the model believes that is the right class.</p> <p>To find the highest value we can use the <code>argmax()</code> method.</p> In\u00a0[94]: Copied! <pre># See the predicted class number and label for the first example\ny_probs[0].argmax(), class_names[y_probs[0].argmax()]\n</pre> # See the predicted class number and label for the first example y_probs[0].argmax(), class_names[y_probs[0].argmax()] Out[94]: <pre>(9, 'Ankle boot')</pre> <p>Now let's do the same for all of the predictions.</p> In\u00a0[95]: Copied! <pre># Convert all of the predictions from probabilities to labels\ny_preds = y_probs.argmax(axis=1)\n\n# View the first 10 prediction labels\ny_preds[:10]\n</pre> # Convert all of the predictions from probabilities to labels y_preds = y_probs.argmax(axis=1)  # View the first 10 prediction labels y_preds[:10] Out[95]: <pre>array([9, 2, 1, 1, 6, 1, 6, 6, 5, 7], dtype=int64)</pre> <p>Wonderful, now we've got our model's predictions in label form, let's create a confusion matrix to view them against the truth labels.</p> In\u00a0[96]: Copied! <pre># Check out the non-prettified confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_true=test_labels,\n                 y_pred=y_preds)\n</pre> # Check out the non-prettified confusion matrix from sklearn.metrics import confusion_matrix confusion_matrix(y_true=test_labels,                  y_pred=y_preds) Out[96]: <pre>array([[756,   2,  32,  59,   5,   2, 129,   3,  12,   0],\n       [ 17, 931,   2,  43,   3,   0,   2,   0,   2,   0],\n       [ 13,   0, 608,  47, 177,   0, 149,   0,   6,   0],\n       [ 17,   8,  19, 852,  50,   1,  39,   3,  11,   0],\n       [  2,   0, 102,  50, 764,   0,  70,   0,  12,   0],\n       [  2,   0,   0,   0,   0, 899,   0,  61,   6,  32],\n       [126,   0, 122,  56, 130,   1, 537,   0,  28,   0],\n       [  0,   0,   0,   0,   0,  35,   0, 935,   0,  30],\n       [  5,   0,   1,   9,  17,  14,  45,   4, 905,   0],\n       [  0,   0,   0,   0,   0,  10,   1,  48,   1, 940]], dtype=int64)</pre> <p>That confusion matrix is hard to comprehend, let's make it prettier using the function we created before.</p> In\u00a0[97]: Copied! <pre># Make a prettier confusion matrix\nmake_confusion_matrix(y_true=test_labels,\n                      y_pred=y_preds,\n                      classes=class_names,\n                      figsize=(15, 15),\n                      text_size=10)\n</pre> # Make a prettier confusion matrix make_confusion_matrix(y_true=test_labels,                       y_pred=y_preds,                       classes=class_names,                       figsize=(15, 15),                       text_size=10) <p>That looks much better! (one of my favourites sights in the world is a confusion matrix with dark squares down the diagonal)</p> <p>Except the results aren't as good as they could be...</p> <p>It looks like our model is getting confused between the <code>Shirt</code> and <code>T-shirt/top</code> classes (e.g. predicting <code>Shirt</code> when it's actually a <code>T-shirt/top</code>).</p> <p>\ud83e\udd14 Question: Does it make sense that our model is getting confused between the <code>Shirt</code> and <code>T-shirt/top</code> classes? Why do you think this might be? What's one way you could investigate?</p> <p>We've seen how our models predictions line up to the truth labels using a confusion matrix, but how about we visualize some?</p> <p>Let's create a function to plot a random image along with its prediction.</p> <p>\ud83d\udd11 Note: Often when working with images and other forms of visual data, it's a good idea to visualize as much as possible to develop a further understanding of the data and the outputs of your model.</p> In\u00a0[98]: Copied! <pre>import random\n\n# Create a function for plotting a random image along with its prediction\ndef plot_random_image(model, images, true_labels, classes):\n  \"\"\"Picks a random image, plots it and labels it with a predicted and truth label.\n\n  Args:\n    model: a trained model (trained on data similar to what's in images).\n    images: a set of random images (in tensor form).\n    true_labels: array of ground truth labels for images.\n    classes: array of class names for images.\n\n  Returns:\n    A plot of a random image from `images` with a predicted class label from `model`\n    as well as the truth class label from `true_labels`.\n  \"\"\"\n  # Setup random integer\n  i = random.randint(0, len(images))\n\n  # Create predictions and targets\n  target_image = images[i]\n  pred_probs = model.predict(target_image.reshape(1, 28, 28)) # have to reshape to get into right size for model\n  pred_label = classes[pred_probs.argmax()]\n  true_label = classes[true_labels[i]]\n\n  # Plot the target image\n  plt.imshow(target_image, cmap=plt.cm.binary)\n\n  # Change the color of the titles depending on if the prediction is right or wrong\n  if pred_label == true_label:\n    color = \"green\"\n  else:\n    color = \"red\"\n\n  # Add xlabel information (prediction/true label)\n  plt.xlabel(\"Pred: {} {:2.0f}% (True: {})\".format(pred_label,\n                                                   100*tf.reduce_max(pred_probs),\n                                                   true_label),\n             color=color) # set the color to green or red\n</pre> import random  # Create a function for plotting a random image along with its prediction def plot_random_image(model, images, true_labels, classes):   \"\"\"Picks a random image, plots it and labels it with a predicted and truth label.    Args:     model: a trained model (trained on data similar to what's in images).     images: a set of random images (in tensor form).     true_labels: array of ground truth labels for images.     classes: array of class names for images.    Returns:     A plot of a random image from `images` with a predicted class label from `model`     as well as the truth class label from `true_labels`.   \"\"\"   # Setup random integer   i = random.randint(0, len(images))    # Create predictions and targets   target_image = images[i]   pred_probs = model.predict(target_image.reshape(1, 28, 28)) # have to reshape to get into right size for model   pred_label = classes[pred_probs.argmax()]   true_label = classes[true_labels[i]]    # Plot the target image   plt.imshow(target_image, cmap=plt.cm.binary)    # Change the color of the titles depending on if the prediction is right or wrong   if pred_label == true_label:     color = \"green\"   else:     color = \"red\"    # Add xlabel information (prediction/true label)   plt.xlabel(\"Pred: {} {:2.0f}% (True: {})\".format(pred_label,                                                    100*tf.reduce_max(pred_probs),                                                    true_label),              color=color) # set the color to green or red In\u00a0[99]: Copied! <pre># Check out a random image as well as its prediction\nplot_random_image(model=model_14,\n                  images=test_data,\n                  true_labels=test_labels,\n                  classes=class_names)\n</pre> # Check out a random image as well as its prediction plot_random_image(model=model_14,                   images=test_data,                   true_labels=test_labels,                   classes=class_names) <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 51ms/step\n</pre> <p>After running the cell above a few times you'll start to get a visual understanding of the relationship between the model's predictions and the true labels.</p> <p>Did you figure out which predictions the model gets confused on?</p> <p>It seems to mix up classes which are similar, for example, <code>Sneaker</code> with <code>Ankle boot</code>.</p> <p>Looking at the images, you can see how this might be the case.</p> <p>The overall shape of a <code>Sneaker</code> and an <code>Ankle Boot</code> are similar.</p> <p>The overall shape might be one of the patterns the model has learned and so therefore when two images have a similar shape, their predictions get mixed up.</p> In\u00a0[100]: Copied! <pre># Find the layers of our most recent model\nmodel_14.layers\n</pre> # Find the layers of our most recent model model_14.layers Out[100]: <pre>[&lt;Flatten name=flatten_3, built=True&gt;,\n &lt;Dense name=dense_40, built=True&gt;,\n &lt;Dense name=dense_41, built=True&gt;,\n &lt;Dense name=dense_42, built=True&gt;]</pre> <p>We can access a target layer using indexing.</p> In\u00a0[101]: Copied! <pre># Extract a particular layer\nmodel_14.layers[1]\n</pre> # Extract a particular layer model_14.layers[1] Out[101]: <pre>&lt;Dense name=dense_40, built=True&gt;</pre> <p>And we can find the patterns learned by a particular layer using the <code>get_weights()</code> method.</p> <p>The <code>get_weights()</code> method returns the weights (also known as a weights matrix) and biases (also known as a bias vector) of a particular layer.</p> In\u00a0[102]: Copied! <pre># Get the patterns of a layer in our network\nweights, biases = model_14.layers[1].get_weights()\n\n# Shape = 1 weight matrix the size of our input data (28x28) per neuron (4)\nweights, weights.shape\n</pre> # Get the patterns of a layer in our network weights, biases = model_14.layers[1].get_weights()  # Shape = 1 weight matrix the size of our input data (28x28) per neuron (4) weights, weights.shape Out[102]: <pre>(array([[ 0.45907176, -0.46185845,  0.569868  ,  0.39837158],\n        [ 0.34134197, -1.0099722 ,  0.4683142 ,  0.6066443 ],\n        [ 0.7053194 , -0.930662  ,  0.39949802,  0.49607873],\n        ...,\n        [ 0.28680536,  0.03652136,  0.0915413 , -0.4339213 ],\n        [-0.49932057,  0.13051574, -0.31600925,  0.16438061],\n        [ 0.48601484, -0.43576252,  0.22481441,  0.23154569]],\n       dtype=float32),\n (784, 4))</pre> <p>The weights matrix is the same shape as the input data, which in our case is 784 (28x28 pixels). And there's a copy of the weights matrix for each neuron the in the selected layer (our selected layer has 4 neurons).</p> <p>Each value in the weights matrix corresponds to how a particular value in the input data influences the network's decisions.</p> <p>These values start out as random numbers (they're set by the <code>kernel_initializer</code> parameter when creating a layer, the default is <code>\"glorot_uniform\"</code>) and are then updated to better representative values of the data (non-random) by the neural network during training.</p> <p> Example workflow of how a supervised neural network starts with random weights and updates them to better represent the data by looking at examples of ideal outputs.</p> <p>Now let's check out the bias vector.</p> In\u00a0[103]: Copied! <pre># Shape = 1 bias per neuron (we use 4 neurons in the first layer)\nbiases, biases.shape\n</pre> # Shape = 1 bias per neuron (we use 4 neurons in the first layer) biases, biases.shape Out[103]: <pre>(array([ 1.0610301 ,  1.841785  ,  0.5734505 , -0.34848416], dtype=float32),\n (4,))</pre> <p>Every neuron has a bias vector. Each of these is paired with a weight matrix.</p> <p>The bias values get initialized as zeroes by default (using the <code>bias_initializer</code> parameter).</p> <p>The bias vector dictates how much the patterns within the corresponding weights matrix should influence the next layer.</p> In\u00a0[104]: Copied! <pre># Can now calculate the number of paramters in our model\nmodel_14.summary()\n</pre> # Can now calculate the number of paramters in our model model_14.summary() <pre>Model: \"sequential_15\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 flatten_3 (Flatten)                  \u2502 (None, 784)                 \u2502               0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_40 (Dense)                     \u2502 (None, 4)                   \u2502           3,140 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_41 (Dense)                     \u2502 (None, 4)                   \u2502              20 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_42 (Dense)                     \u2502 (None, 10)                  \u2502              50 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 9,632 (37.63 KB)\n</pre> <pre> Trainable params: 3,210 (12.54 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <pre> Optimizer params: 6,422 (25.09 KB)\n</pre> <p>Now we've built a few deep learning models, it's a good time to point out the whole concept of inputs and outputs not only relates to a model as a whole but to every layer within a model.</p> <p>You might've already guessed this, but starting from the input layer, each subsequent layer's input is the output of the previous layer.</p> <p>We can see this clearly using the utility <code>plot_model()</code>.</p> In\u00a0[105]: Copied! <pre>from tensorflow.keras.utils import plot_model\n\n# See the inputs and outputs of each layer\nplot_model(model_14, show_shapes=True)\n</pre> from tensorflow.keras.utils import plot_model  # See the inputs and outputs of each layer plot_model(model_14, show_shapes=True) <pre>You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n</pre>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#02-neural-network-classification-with-tensorflow","title":"02. Neural Network Classification with TensorFlow\u00b6","text":"<p>Okay, we've seen how to deal with a regression problem in TensorFlow, let's look at how we can approach a classification problem.</p> <p>A classification problem involves predicting whether something is one thing or another.</p> <p>For example, you might want to:</p> <ul> <li>Predict whether or not someone has heart disease based on their health parameters. This is called binary classification since there are only two options.</li> <li>Decide whether a photo of is of food, a person or a dog. This is called multi-class classification since there are more than two options.</li> <li>Predict what categories should be assigned to a Wikipedia article. This is called multi-label classification since a single article could have more than one category assigned.</li> </ul> <p>In this notebook, we're going to work through a number of different classification problems with TensorFlow. In other words, taking a set of inputs and predicting what class those set of inputs belong to.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Specifically, we're going to go through doing the following with TensorFlow:</p> <ul> <li>Architecture of a classification model</li> <li>Input shapes and output shapes<ul> <li><code>X</code>: features/data (inputs)</li> <li><code>y</code>: labels (outputs)<ul> <li>\"What class do the inputs belong to?\"</li> </ul> </li> </ul> </li> <li>Creating custom data to view and fit</li> <li>Steps in modelling for binary and mutliclass classification<ul> <li>Creating a model</li> <li>Compiling a model<ul> <li>Defining a loss function</li> <li>Setting up an optimizer<ul> <li>Finding the best learning rate</li> </ul> </li> <li>Creating evaluation metrics</li> </ul> </li> <li>Fitting a model (getting it to find patterns in our data)</li> <li>Improving a model</li> </ul> </li> <li>The power of non-linearity</li> <li>Evaluating classification models<ul> <li>Visualizng the model (\"visualize, visualize, visualize\")</li> <li>Looking at training curves</li> <li>Compare predictions to ground truth (using our evaluation metrics)</li> </ul> </li> </ul>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#typical-architecture-of-a-classification-neural-network","title":"Typical architecture of a classification neural network\u00b6","text":"<p>The word typical is on purpose.</p> <p>Because the architecture of a classification neural network can widely vary depending on the problem you're working on.</p> <p>However, there are some fundamentals all deep neural networks contain:</p> <ul> <li>An input layer.</li> <li>Some hidden layers.</li> <li>An output layer.</li> </ul> <p>Much of the rest is up to the data analyst creating the model.</p> <p>The following are some standard values you'll often use in your classification neural networks.</p> Hyperparameter Binary Classification Multiclass classification Input layer shape Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) Same as binary classification Hidden layer(s) Problem specific, minimum = 1, maximum = unlimited Same as binary classification Neurons per hidden layer Problem specific, generally 10 to 100 Same as binary classification Output layer shape 1 (one class or the other) 1 per class (e.g. 3 for food, person or dog photo) Hidden activation Usually ReLU (rectified linear unit) Same as binary classification Output activation Sigmoid Softmax Loss function Cross entropy (<code>tf.keras.losses.BinaryCrossentropy</code> in TensorFlow) Cross entropy (<code>tf.keras.losses.CategoricalCrossentropy</code> in TensorFlow) Optimizer SGD (stochastic gradient descent), Adam Same as binary classification <p>Table 1: Typical architecture of a classification network. Source: Adapted from page 295 of Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow Book by Aur\u00e9lien G\u00e9ron</p> <p>Don't worry if not much of the above makes sense right now, we'll get plenty of experience as we go through this notebook.</p> <p>Let's start by importing TensorFlow as the common alias <code>tf</code>. For this notebook, make sure you're using version 2.x+.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#creating-data-to-view-and-fit","title":"Creating data to view and fit\u00b6","text":"<p>We could start by importing a classification dataset but let's practice making some of our own classification data.</p> <p>\ud83d\udd11 Note: It's a common practice to get you and model you build working on a toy (or simple) dataset before moving to your actual problem. Treat it as a rehersal experiment before the actual experiment(s).</p> <p>Since classification is predicting whether something is one thing or another, let's make some data to reflect that.</p> <p>To do so, we'll use Scikit-Learn's <code>make_circles()</code> function.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#input-and-output-shapes","title":"Input and output shapes\u00b6","text":"<p>One of the most common issues you'll run into when building neural networks is shape mismatches.</p> <p>More specifically, the shape of the input data and the shape of the output data.</p> <p>In our case, we want to input <code>X</code> and get our model to predict <code>y</code>.</p> <p>So let's check out the shapes of <code>X</code> and <code>y</code>.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#steps-in-modelling","title":"Steps in modelling\u00b6","text":"<p>Now we know what data we have as well as the input and output shapes, let's see how we'd build a neural network to model it.</p> <p>In TensorFlow, there are typically 3 fundamental steps to creating and training a model.</p> <ol> <li>Creating a model - piece together the layers of a neural network yourself (using the functional or sequential API) or import a previously built model (known as transfer learning).</li> <li>Compiling a model - defining how a model's performance should be measured (loss/metrics) as well as defining how it should improve (optimizer).</li> <li>Fitting a model - letting the model try to find patterns in the data (how does <code>X</code> get to <code>y</code>).</li> </ol> <p>Let's see these in action using the Sequential API to build a model for our regression data. And then we'll step through each.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#improving-a-model","title":"Improving a model\u00b6","text":"<p>To improve our model, we can alter almost every part of the 3 steps we went through before.</p> <ol> <li>Creating a model - here you might want to add more layers, increase the number of hidden units (also called neurons) within each layer, change the activation functions of each layer.</li> <li>Compiling a model - you might want to choose a different optimization function (such as the Adam optimizer, which is usually pretty good for many problems) or perhaps change the learning rate of the optimization function.</li> <li>Fitting a model - perhaps you could fit a model for more epochs (leave it training for longer).</li> </ol> <p> There are many different ways to potentially improve a neural network. Some of the most common include: increasing the number of layers (making the network deeper), increasing the number of hidden units (making the network wider) and changing the learning rate. Because these values are all human-changeable, they're referred to as hyperparameters) and the practice of trying to find the best hyperparameters is referred to as hyperparameter tuning.</p> <p>How about we try adding more neurons, an extra layer and our friend the Adam optimizer?</p> <p>Surely doing this will result in predictions better than guessing...</p> <p>Note: The following message (below this one) can be ignored if you're running TensorFlow 2.8.0+, the error seems to have been fixed.</p> <p>Note: If you're using TensorFlow 2.7.0+ (but not 2.8.0+) the original code from the following cells may have caused some errors. They've since been updated to fix those errors. You can see explanations on what happened at the following resources:</p> <ul> <li>Example Colab Notebook</li> <li>TensorFlow for Deep Learning GitHub Discussion on TensorFlow 2.7.0 breaking changes</li> </ul>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#the-missing-piece-non-linearity","title":"The missing piece: Non-linearity\u00b6","text":"<p>Okay, so we saw our neural network can model straight lines (with ability a little bit better than guessing).</p> <p>What about non-straight (non-linear) lines?</p> <p>If we're going to model our classification data (the red and blue circles), we're going to need some non-linear lines.</p> <p>\ud83d\udd28 Practice: Before we get to the next steps, I'd encourage you to play around with the TensorFlow Playground (check out what the data has in common with our own classification data) for 10-minutes. In particular the tab which says \"activation\". Once you're done, come back.</p> <p>Did you try out the activation options? If so, what did you find?</p> <p>If you didn't, don't worry, let's see it in code.</p> <p>We're going to replicate the neural network you can see at this link: TensorFlow Playground.</p> <p> The neural network we're going to recreate with TensorFlow code. See it live at TensorFlow Playground.</p> <p>The main change we'll add to models we've built before is the use of the <code>activation</code> keyword.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#evaluating-and-improving-our-classification-model","title":"Evaluating and improving our classification model\u00b6","text":"<p>If you answered the question above, you might've picked up what we've been doing wrong.</p> <p>We've been evaluating our model on the same data it was trained on.</p> <p>A better approach would be to split our data into training, validation (optional) and test sets.</p> <p>Once we've done that, we'll train our model on the training set (let it find patterns in the data) and then see how well it learned the patterns by using it to predict values on the test set.</p> <p>Let's do it.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#plot-the-loss-curves","title":"Plot the loss curves\u00b6","text":"<p>Looking at the plots above, we can see the outputs of our model are very good.</p> <p>But how did our model go whilst it was learning?</p> <p>As in, how did the performance change everytime the model had a chance to look at the data (once every epoch)?</p> <p>To figure this out, we can check the loss curves (also referred to as the learning curves).</p> <p>You might've seen we've been using the variable <code>history</code> when calling the <code>fit()</code> function on a model (<code>fit()</code> returns a <code>History</code> object).</p> <p>This is where we'll get the information for how our model is performing as it learns.</p> <p>Let's see how we might use it.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#finding-the-best-learning-rate","title":"Finding the best learning rate\u00b6","text":"<p>Aside from the architecture itself (the layers, number of neurons, activations, etc), the most important hyperparameter you can tune for your neural network models is the learning rate.</p> <p>In <code>model_8</code> you saw we lowered the Adam optimizer's learning rate from the default of <code>0.001</code> (default) to <code>0.01</code>.</p> <p>And you might be wondering why we did this.</p> <p>Put it this way, it was a lucky guess.</p> <p>I just decided to try a lower learning rate and see how the model went.</p> <p>Now you might be thinking, \"Seriously? You can do that?\"</p> <p>And the answer is yes. You can change any of the hyperparamaters of your neural networks.</p> <p>With practice, you'll start to see what kind of hyperparameters work and what don't.</p> <p>That's an important thing to understand about machine learning and deep learning in general. It's very experimental. You build a model and evaluate it, build a model and evaluate it.</p> <p>That being said, I want to introduce you a trick which will help you find the optimal learning rate (at least to begin training with) for your models going forward.</p> <p>To do so, we're going to use the following:</p> <ul> <li>A learning rate callback.<ul> <li>You can think of a callback as an extra piece of functionality you can add to your model while its training.</li> </ul> </li> <li>Another model (we could use the same ones as above, we we're practicing building models here).</li> <li>A modified loss curves plot.</li> </ul> <p>We'll go through each with code, then explain what's going on.</p> <p>\ud83d\udd11 Note: The default hyperparameters of many neural network building blocks in TensorFlow are setup in a way which usually work right out of the box (e.g. the Adam optimizer's default settings can usually get good results on many datasets). So it's a good idea to try the defaults first, then adjust as needed.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#more-classification-evaluation-methods","title":"More classification evaluation methods\u00b6","text":"<p>Alongside the visualizations we've been making, there are a number of different evaluation metrics we can use to evaluate our classification models.</p> Metric name/Evaluation method Defintion Code Accuracy Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. <code>sklearn.metrics.accuracy_score()</code> or <code>tf.keras.metrics.Accuracy()</code> Precision Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). <code>sklearn.metrics.precision_score()</code> or <code>tf.keras.metrics.Precision()</code> Recall Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. <code>sklearn.metrics.recall_score()</code> or <code>tf.keras.metrics.Recall()</code> F1-score Combines precision and recall into one metric. 1 is best, 0 is worst. <code>sklearn.metrics.f1_score()</code> Confusion matrix Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). Custom function or <code>sklearn.metrics.plot_confusion_matrix()</code> Classification report Collection of some of the main classification metrics such as precision, recall and f1-score. <code>sklearn.metrics.classification_report()</code> <p>\ud83d\udd11 Note: Every classification problem will require different kinds of evaluation methods. But you should be familiar with at least the ones above.</p> <p>Let's start with accuracy.</p> <p>Because we passed <code>[\"accuracy\"]</code> to the <code>metrics</code> parameter when we compiled our model, calling <code>evaluate()</code> on it will return the loss as well as accuracy.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#working-with-a-larger-example-multiclass-classification","title":"Working with a larger example (multiclass classification)\u00b6","text":"<p>We've seen a binary classification example (predicting if a data point is part of a red circle or blue circle) but what if you had multiple different classes of things?</p> <p>For example, say you were a fashion company and you wanted to build a neural network to predict whether a piece of clothing was a shoe, a shirt or a jacket (3 different options).</p> <p>When you have more than two classes as an option, this is known as multiclass classification.</p> <p>The good news is, the things we've learned so far (with a few tweaks) can be applied to multiclass classification problems as well.</p> <p>Let's see it in action.</p> <p>To start, we'll need some data. The good thing for us is TensorFlow has a multiclass classication dataset known as Fashion MNIST built-in. Meaning we can get started straight away.</p> <p>We can import it using the <code>tf.keras.datasets</code> module.</p> <p>\ud83d\udcd6 Resource: The following multiclass classification problem has been adapted from the TensorFlow classification guide. A good exercise would be to once you've gone through the following example, replicate the TensorFlow guide.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#what-patterns-is-our-model-learning","title":"What patterns is our model learning?\u00b6","text":"<p>We've been talking a lot about how a neural network finds patterns in numbers, but what exactly do these patterns look like?</p> <p>Let's crack open one of our models and find out.</p> <p>First, we'll get a list of layers in our most recent model (<code>model_14</code>) using the <code>layers</code> attribute.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#how-a-model-learns-in-brief","title":"How a model learns (in brief)\u00b6","text":"<p>Alright, we've trained a bunch of models, but we've never really discussed what's going on under the hood. So how exactly does a model learn?</p> <p>A model learns by updating and improving its weight matrices and biases values every epoch (in our case, when we call the <code>fit()</code> fucntion).</p> <p>It does so by comparing the patterns its learned between the data and labels to the actual labels.</p> <p>If the current patterns (weight matrices and bias values) don't result in a desirable decrease in the loss function (higher loss means worse predictions), the optimizer tries to steer the model to update its patterns in the right way (using the real labels as a reference).</p> <p>This process of using the real labels as a reference to improve the model's predictions is called backpropagation.</p> <p>In other words, data and labels pass through a model (forward pass) and it attempts to learn the relationship between the data and labels.</p> <p>And if this learned relationship isn't close to the actual relationship or it could be improved, the model does so by going back through itself (backward pass) and tweaking its weights matrices and bias values to better represent the data.</p> <p>If all of this sounds confusing (and it's fine if it does, the above is a very succinct description), check out the resources in the extra-curriculum section for more.</p>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#exercises","title":"Exercises \ud83d\udee0\u00b6","text":"<ol> <li>Play with neural networks in the TensorFlow Playground for 10-minutes. Especially try different values of the learning, what happens when you decrease it? What happens when you increase it?</li> <li>Replicate the model pictured in the TensorFlow Playground diagram below using TensorFlow code. Compile it using the Adam optimizer, binary crossentropy loss and accuracy metric. Once it's compiled check a summary of the model.  Try this network out for yourself on the TensorFlow Playground website. Hint: there are 5 hidden layers but the output layer isn't pictured, you'll have to decide what the output layer should be based on the input data.</li> <li>Create a classification dataset using Scikit-Learn's <code>make_moons()</code> function, visualize it and then build a model to fit it at over 85% accuracy.</li> <li>Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediciton labels at the same time. Hint: see the classifcation tutorial in the TensorFlow documentation for ideas.</li> <li>Recreate TensorFlow's softmax activation function in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it.</li> <li>Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after.</li> <li>Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example, plot 3 images of the <code>T-shirt</code> class with their predictions.</li> </ol>"},{"location":"Learning/Tensorflow/02_neural_network_classification_in_tensorflow/#extra-curriculum","title":"Extra curriculum \ud83d\udcd6\u00b6","text":"<ul> <li>Watch 3Blue1Brown's neural networks video 2: Gradient descent, how neural networks learn. After you're done, write 100 words about what you've learned.<ul> <li>If you haven't already, watch video 1: But what is a Neural Network?. Note the activation function they talk about at the end.</li> </ul> </li> <li>Watch MIT's introduction to deep learning lecture 1 (if you haven't already) to get an idea of the concepts behind using linear and non-linear functions.</li> <li>Spend 1-hour reading Michael Nielsen's Neural Networks and Deep Learning book.</li> <li>Read the ML-Glossary documentation on activation functions. Which one is your favourite?<ul> <li>After you've read the ML-Glossary, see which activation functions are available in TensorFlow by searching \"tensorflow activation functions\".</li> </ul> </li> </ul>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/","title":"04. Transfer Learning with TensorFlow Part 1: Feature Extraction","text":"In\u00a0[1]: Copied! <pre># Add timestamp\nimport datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> # Add timestamp import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>Notebook last run (end-to-end): 2024-04-20 13:58:54.678802\n</pre> In\u00a0[2]: Copied! <pre># Are we using a GPU?\n!nvidia-smi\n</pre> # Are we using a GPU? !nvidia-smi <pre>Sat Apr 20 13:58:58 2024       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 517.48       Driver Version: 517.48       CUDA Version: 11.7     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n| N/A   46C    P3    12W /  N/A |      0MiB /  4096MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</pre> In\u00a0[4]: Copied! <pre>import tensorflow as tf\n</pre> import tensorflow as tf Out[4]: <pre>&lt;function tensorflow.python.framework.config.list_physical_devices(device_type=None)&gt;</pre> In\u00a0[7]: Copied! <pre>print(tf.config.list_physical_devices(\"GPU\"))\n</pre> print(tf.config.list_physical_devices(\"GPU\")) <pre>[]\n</pre> <p>If the cell above doesn't output something which looks like:</p> <pre><code>Fri Sep  4 03:35:21 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                 ERR! |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>Go to Runtime -&gt; Change Runtime Type -&gt; Hardware Accelerator and select \"GPU\", then rerun the cell above.</p> In\u00a0[2]: Copied! <pre># Get data (10% of labels)\nimport zipfile\n\n# Download data\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n\n# Unzip the downloaded file\nzip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\", \"r\")\nzip_ref.extractall()\nzip_ref.close()\n</pre> # Get data (10% of labels) import zipfile  # Download data !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip  # Unzip the downloaded file zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\", \"r\") zip_ref.extractall() zip_ref.close() <pre>--2023-05-11 04:22:37--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 74.125.68.128, 74.125.24.128, 142.250.4.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|74.125.68.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 168546183 (161M) [application/zip]\nSaving to: \u201810_food_classes_10_percent.zip\u2019\n\n10_food_classes_10_ 100%[===================&gt;] 160.74M  19.2MB/s    in 9.7s    \n\n2023-05-11 04:22:48 (16.5 MB/s) - \u201810_food_classes_10_percent.zip\u2019 saved [168546183/168546183]\n\n</pre> In\u00a0[3]: Copied! <pre># How many images in each folder?\nimport os\n\n# Walk through 10 percent data directory and list number of files\nfor dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n</pre> # How many images in each folder? import os  # Walk through 10 percent data directory and list number of files for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):   print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\") <pre>There are 2 directories and 0 images in '10_food_classes_10_percent'.\nThere are 10 directories and 0 images in '10_food_classes_10_percent/test'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\nThere are 10 directories and 0 images in '10_food_classes_10_percent/train'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n</pre> <p>Notice how each of the training directories now has 75 images rather than 750 images. This is key to demonstrating how well transfer learning can perform with less labelled images.</p> <p>The test directories still have the same amount of images. This means we'll be training on less data but evaluating our models on the same amount of test data.</p> In\u00a0[4]: Copied! <pre># Setup data inputs\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nIMAGE_SHAPE = (224, 224)\nBATCH_SIZE = 32\n\ntrain_dir = \"10_food_classes_10_percent/train/\"\ntest_dir = \"10_food_classes_10_percent/test/\"\n\ntrain_datagen = ImageDataGenerator(rescale=1/255.)\ntest_datagen = ImageDataGenerator(rescale=1/255.)\n\nprint(\"Training images:\")\ntrain_data_10_percent = train_datagen.flow_from_directory(train_dir,\n                                               target_size=IMAGE_SHAPE,\n                                               batch_size=BATCH_SIZE,\n                                               class_mode=\"categorical\")\n\nprint(\"Testing images:\")\ntest_data = train_datagen.flow_from_directory(test_dir,\n                                              target_size=IMAGE_SHAPE,\n                                              batch_size=BATCH_SIZE,\n                                              class_mode=\"categorical\")\n</pre> # Setup data inputs from tensorflow.keras.preprocessing.image import ImageDataGenerator  IMAGE_SHAPE = (224, 224) BATCH_SIZE = 32  train_dir = \"10_food_classes_10_percent/train/\" test_dir = \"10_food_classes_10_percent/test/\"  train_datagen = ImageDataGenerator(rescale=1/255.) test_datagen = ImageDataGenerator(rescale=1/255.)  print(\"Training images:\") train_data_10_percent = train_datagen.flow_from_directory(train_dir,                                                target_size=IMAGE_SHAPE,                                                batch_size=BATCH_SIZE,                                                class_mode=\"categorical\")  print(\"Testing images:\") test_data = train_datagen.flow_from_directory(test_dir,                                               target_size=IMAGE_SHAPE,                                               batch_size=BATCH_SIZE,                                               class_mode=\"categorical\") <pre>Training images:\nFound 750 images belonging to 10 classes.\nTesting images:\nFound 2500 images belonging to 10 classes.\n</pre> <p>Excellent! Loading in the data we can see we've got 750 images in the training dataset belonging to 10 classes (75 per class) and 2500 images in the test set belonging to 10 classes (250 per class).</p> In\u00a0[5]: Copied! <pre># Create tensorboard callback (functionized because need to create a new one for each model)\nimport datetime\ndef create_tensorboard_callback(dir_name, experiment_name):\n  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=log_dir\n  )\n  print(f\"Saving TensorBoard log files to: {log_dir}\")\n  return tensorboard_callback\n</pre> # Create tensorboard callback (functionized because need to create a new one for each model) import datetime def create_tensorboard_callback(dir_name, experiment_name):   log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")   tensorboard_callback = tf.keras.callbacks.TensorBoard(       log_dir=log_dir   )   print(f\"Saving TensorBoard log files to: {log_dir}\")   return tensorboard_callback <p>Because you're likely to run multiple experiments, it's a good idea to be able to track them in some way.</p> <p>In our case, our function saves a model's performance logs to a directory named <code>[dir_name]/[experiment_name]/[current_timestamp]</code>, where:</p> <ul> <li><code>dir_name</code> is the overall logs directory</li> <li><code>experiment_name</code> is the particular experiment</li> <li><code>current_timestamp</code> is the time the experiment started based on Python's <code>datetime.datetime().now()</code></li> </ul> <p>\ud83d\udd11 Note: Depending on your use case, the above experimenting tracking naming method may work or you might require something more specific. The good news is, the TensorBoard callback makes it easy to track modelling logs as long as you specify where to track them. So you can get as creative as you like with how you name your experiments, just make sure you or your team can understand them.</p> In\u00a0[6]: Copied! <pre>import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\n</pre> import tensorflow as tf import tensorflow_hub as hub from tensorflow.keras import layers <p>Now we'll get the feature vector URLs of two common computer vision architectures, EfficientNetB0 (2019) and ResNetV250 (2016) from TensorFlow Hub using the steps above.</p> <p>We're getting both of these because we're going to compare them to see which performs better on our data.</p> <p>\ud83d\udd11 Note: Comparing different model architecture performance on the same data is a very common practice. The simple reason is because you want to know which model performs best for your problem.</p> <p>Update: As of 14 August 2021, EfficientNet V2 pretrained models are available on TensorFlow Hub. The original code in this notebook uses EfficientNet V1, it has been left unchanged. In my experiments with this dataset, V1 outperforms V2. Best to experiment with your own data and see what suits you.</p> In\u00a0[7]: Copied! <pre># Resnet 50 V2 feature vector\nresnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n\n# Original: EfficientNetB0 feature vector (version 1)\nefficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n\n# # New: EfficientNetB0 feature vector (version 2)\n# efficientnet_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\"\n</pre> # Resnet 50 V2 feature vector resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"  # Original: EfficientNetB0 feature vector (version 1) efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"  # # New: EfficientNetB0 feature vector (version 2) # efficientnet_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\" <p>These URLs link to a saved pretrained model on TensorFlow Hub.</p> <p>When we use them in our model, the model will automatically be downloaded for us to use.</p> <p>To do this, we can use the <code>KerasLayer()</code> model inside the TensorFlow hub library.</p> <p>Since we're going to be comparing two models, to save ourselves code, we'll create a function <code>create_model()</code>. This function will take a model's TensorFlow Hub URL, instatiate a Keras Sequential model with the appropriate number of output layers and return the model.</p> In\u00a0[8]: Copied! <pre>def create_model(model_url, num_classes=10):\n  \"\"\"Takes a TensorFlow Hub URL and creates a Keras Sequential model with it.\n  \n  Args:\n    model_url (str): A TensorFlow Hub feature extraction URL.\n    num_classes (int): Number of output neurons in output layer,\n      should be equal to number of target classes, default 10.\n\n  Returns:\n    An uncompiled Keras Sequential model with model_url as feature\n    extractor layer and Dense output layer with num_classes outputs.\n  \"\"\"\n  # Download the pretrained model and save it as a Keras layer\n  feature_extractor_layer = hub.KerasLayer(model_url,\n                                           trainable=False, # freeze the underlying patterns\n                                           name='feature_extraction_layer',\n                                           input_shape=IMAGE_SHAPE+(3,)) # define the input image shape\n  \n  # Create our own model\n  model = tf.keras.Sequential([\n    feature_extractor_layer, # use the feature extraction layer as the base\n    layers.Dense(num_classes, activation='softmax', name='output_layer') # create our own output layer      \n  ])\n\n  return model\n</pre> def create_model(model_url, num_classes=10):   \"\"\"Takes a TensorFlow Hub URL and creates a Keras Sequential model with it.      Args:     model_url (str): A TensorFlow Hub feature extraction URL.     num_classes (int): Number of output neurons in output layer,       should be equal to number of target classes, default 10.    Returns:     An uncompiled Keras Sequential model with model_url as feature     extractor layer and Dense output layer with num_classes outputs.   \"\"\"   # Download the pretrained model and save it as a Keras layer   feature_extractor_layer = hub.KerasLayer(model_url,                                            trainable=False, # freeze the underlying patterns                                            name='feature_extraction_layer',                                            input_shape=IMAGE_SHAPE+(3,)) # define the input image shape      # Create our own model   model = tf.keras.Sequential([     feature_extractor_layer, # use the feature extraction layer as the base     layers.Dense(num_classes, activation='softmax', name='output_layer') # create our own output layer         ])    return model <p>Great! Now we've got a function for creating a model, we'll use it to first create a model using the ResNetV250 architecture as our feature extraction layer.</p> <p>Once the model is instantiated, we'll compile it using <code>categorical_crossentropy</code> as our loss function, the Adam optimizer and accuracy as our metric.</p> In\u00a0[9]: Copied! <pre># Create model\nresnet_model = create_model(resnet_url, num_classes=train_data_10_percent.num_classes)\n\n# Compile\nresnet_model.compile(loss='categorical_crossentropy',\n                     optimizer=tf.keras.optimizers.Adam(),\n                     metrics=['accuracy'])\n</pre> # Create model resnet_model = create_model(resnet_url, num_classes=train_data_10_percent.num_classes)  # Compile resnet_model.compile(loss='categorical_crossentropy',                      optimizer=tf.keras.optimizers.Adam(),                      metrics=['accuracy']) <p> What our current model looks like. A ResNet50V2 backbone with a custom dense layer on top (10 classes instead of 1000 ImageNet classes). Note: The Image shows ResNet34 instead of ResNet50. Image source: https://arxiv.org/abs/1512.03385.</p> <p>Beautiful. Time to fit the model.</p> <p>We've got the training data ready in <code>train_data_10_percent</code> as well as the test data saved as <code>test_data</code>.</p> <p>But before we call the fit function, there's one more thing we're going to add, a callback. More specifically, a TensorBoard callback so we can track the performance of our model on TensorBoard.</p> <p>We can add a callback to our model by using the <code>callbacks</code> parameter in the fit function.</p> <p>In our case, we'll pass the <code>callbacks</code> parameter the <code>create_tensorboard_callback()</code> we created earlier with some specific inputs so we know what experiments we're running.</p> <p>Let's keep this experiment short and train for 5 epochs.</p> In\u00a0[10]: Copied! <pre># Fit the model\nresnet_history = resnet_model.fit(train_data_10_percent,\n                                  epochs=5,\n                                  steps_per_epoch=len(train_data_10_percent),\n                                  validation_data=test_data,\n                                  validation_steps=len(test_data),\n                                  # Add TensorBoard callback to model (callbacks parameter takes a list)\n                                  callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\", # save experiment logs here\n                                                                         experiment_name=\"resnet50V2\")]) # name of log files\n</pre> # Fit the model resnet_history = resnet_model.fit(train_data_10_percent,                                   epochs=5,                                   steps_per_epoch=len(train_data_10_percent),                                   validation_data=test_data,                                   validation_steps=len(test_data),                                   # Add TensorBoard callback to model (callbacks parameter takes a list)                                   callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\", # save experiment logs here                                                                          experiment_name=\"resnet50V2\")]) # name of log files <pre>Saving TensorBoard log files to: tensorflow_hub/resnet50V2/20230511-042304\nEpoch 1/5\n24/24 [==============================] - 28s 680ms/step - loss: 1.7915 - accuracy: 0.3880 - val_loss: 1.1463 - val_accuracy: 0.6368\nEpoch 2/5\n24/24 [==============================] - 14s 620ms/step - loss: 0.8710 - accuracy: 0.7560 - val_loss: 0.8359 - val_accuracy: 0.7352\nEpoch 3/5\n24/24 [==============================] - 14s 618ms/step - loss: 0.6160 - accuracy: 0.8373 - val_loss: 0.7385 - val_accuracy: 0.7676\nEpoch 4/5\n24/24 [==============================] - 15s 624ms/step - loss: 0.4705 - accuracy: 0.8907 - val_loss: 0.7016 - val_accuracy: 0.7728\nEpoch 5/5\n24/24 [==============================] - 15s 640ms/step - loss: 0.3744 - accuracy: 0.9200 - val_loss: 0.6764 - val_accuracy: 0.7772\n</pre> <p>Wow!</p> <p>It seems that after only 5 epochs, the ResNetV250 feature extraction model was able to blow any of the architectures we made out of the water, achieving around 90% accuracy on the training set and nearly 80% accuracy on the test set...with only 10 percent of the training images!</p> <p>That goes to show the power of transfer learning. And it's one of the main reasons whenever you're trying to model your own datasets, you should look into what pretrained models already exist.</p> <p>Let's check out our model's training curves using our <code>plot_loss_curves</code> function.</p> In\u00a0[11]: Copied! <pre># If you wanted to, you could really turn this into a helper function to load in with a helper.py script...\nimport matplotlib.pyplot as plt\n\n# Plot the validation and training data separately\ndef plot_loss_curves(history):\n  \"\"\"\n  Returns separate loss curves for training and validation metrics.\n  \"\"\" \n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  accuracy = history.history['accuracy']\n  val_accuracy = history.history['val_accuracy']\n\n  epochs = range(len(history.history['loss']))\n\n  # Plot loss\n  plt.plot(epochs, loss, label='training_loss')\n  plt.plot(epochs, val_loss, label='val_loss')\n  plt.title('Loss')\n  plt.xlabel('Epochs')\n  plt.legend()\n\n  # Plot accuracy\n  plt.figure()\n  plt.plot(epochs, accuracy, label='training_accuracy')\n  plt.plot(epochs, val_accuracy, label='val_accuracy')\n  plt.title('Accuracy')\n  plt.xlabel('Epochs')\n  plt.legend();\n</pre> # If you wanted to, you could really turn this into a helper function to load in with a helper.py script... import matplotlib.pyplot as plt  # Plot the validation and training data separately def plot_loss_curves(history):   \"\"\"   Returns separate loss curves for training and validation metrics.   \"\"\"    loss = history.history['loss']   val_loss = history.history['val_loss']    accuracy = history.history['accuracy']   val_accuracy = history.history['val_accuracy']    epochs = range(len(history.history['loss']))    # Plot loss   plt.plot(epochs, loss, label='training_loss')   plt.plot(epochs, val_loss, label='val_loss')   plt.title('Loss')   plt.xlabel('Epochs')   plt.legend()    # Plot accuracy   plt.figure()   plt.plot(epochs, accuracy, label='training_accuracy')   plt.plot(epochs, val_accuracy, label='val_accuracy')   plt.title('Accuracy')   plt.xlabel('Epochs')   plt.legend(); In\u00a0[12]: Copied! <pre>plot_loss_curves(resnet_history)\n</pre> plot_loss_curves(resnet_history) <p>And what about a summary of our model?</p> In\u00a0[13]: Copied! <pre># Resnet summary \nresnet_model.summary()\n</pre> # Resnet summary  resnet_model.summary() <pre>Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n feature_extraction_layer (K  (None, 2048)             23564800  \n erasLayer)                                                      \n                                                                 \n output_layer (Dense)        (None, 10)                20490     \n                                                                 \n=================================================================\nTotal params: 23,585,290\nTrainable params: 20,490\nNon-trainable params: 23,564,800\n_________________________________________________________________\n</pre> <p>You can see the power of TensorFlow Hub here. The feature extraction layer has 23,564,800 parameters which are prelearned patterns the model has already learned on the ImageNet dataset. Since we set <code>trainable=False</code>, these patterns remain frozen (non-trainable) during training.</p> <p>This means during training the model updates the 20,490 parameters in the output layer to suit our dataset.</p> <p>Okay, we've trained a ResNetV250 model, time to do the same with EfficientNetB0 model.</p> <p>The setup will be the exact same as before, except for the <code>model_url</code> parameter in the <code>create_model()</code> function and the <code>experiment_name</code> parameter in the <code>create_tensorboard_callback()</code> function.</p> In\u00a0[14]: Copied! <pre># Create model\nefficientnet_model = create_model(model_url=efficientnet_url, # use EfficientNetB0 TensorFlow Hub URL\n                                  num_classes=train_data_10_percent.num_classes)\n\n# Compile EfficientNet model\nefficientnet_model.compile(loss='categorical_crossentropy',\n                           optimizer=tf.keras.optimizers.Adam(),\n                           metrics=['accuracy'])\n\n# Fit EfficientNet model \nefficientnet_history = efficientnet_model.fit(train_data_10_percent, # only use 10% of training data\n                                              epochs=5, # train for 5 epochs\n                                              steps_per_epoch=len(train_data_10_percent),\n                                              validation_data=test_data,\n                                              validation_steps=len(test_data),\n                                              callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\", \n                                                                                     # Track logs under different experiment name\n                                                                                     experiment_name=\"efficientnetB0\")])\n</pre> # Create model efficientnet_model = create_model(model_url=efficientnet_url, # use EfficientNetB0 TensorFlow Hub URL                                   num_classes=train_data_10_percent.num_classes)  # Compile EfficientNet model efficientnet_model.compile(loss='categorical_crossentropy',                            optimizer=tf.keras.optimizers.Adam(),                            metrics=['accuracy'])  # Fit EfficientNet model  efficientnet_history = efficientnet_model.fit(train_data_10_percent, # only use 10% of training data                                               epochs=5, # train for 5 epochs                                               steps_per_epoch=len(train_data_10_percent),                                               validation_data=test_data,                                               validation_steps=len(test_data),                                               callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",                                                                                       # Track logs under different experiment name                                                                                      experiment_name=\"efficientnetB0\")]) <pre>Saving TensorBoard log files to: tensorflow_hub/efficientnetB0/20230511-042443\nEpoch 1/5\n24/24 [==============================] - 26s 693ms/step - loss: 1.7622 - accuracy: 0.5053 - val_loss: 1.2434 - val_accuracy: 0.7268\nEpoch 2/5\n24/24 [==============================] - 15s 634ms/step - loss: 1.0120 - accuracy: 0.7693 - val_loss: 0.8459 - val_accuracy: 0.8240\nEpoch 3/5\n24/24 [==============================] - 15s 642ms/step - loss: 0.7289 - accuracy: 0.8347 - val_loss: 0.6823 - val_accuracy: 0.8460\nEpoch 4/5\n24/24 [==============================] - 15s 637ms/step - loss: 0.5871 - accuracy: 0.8667 - val_loss: 0.6004 - val_accuracy: 0.8592\nEpoch 5/5\n24/24 [==============================] - 14s 620ms/step - loss: 0.5013 - accuracy: 0.8947 - val_loss: 0.5504 - val_accuracy: 0.8636\n</pre> <p>Holy smokes! The EfficientNetB0 model does even better than the ResNetV250 model! Achieving over 85% accuracy on the test set...again with only 10% of the training data.</p> <p>How cool is that?</p> <p>With a couple of lines of code we're able to leverage state of the art models and adjust them to our own use case.</p> <p>Let's check out the loss curves.</p> In\u00a0[15]: Copied! <pre>plot_loss_curves(efficientnet_history)\n</pre> plot_loss_curves(efficientnet_history) <p>From the look of the EfficientNetB0 model's loss curves, it looks like if we kept training our model for longer, it might improve even further. Perhaps that's something you might want to try?</p> <p>Let's check out the model summary.</p> In\u00a0[16]: Copied! <pre>efficientnet_model.summary()\n</pre> efficientnet_model.summary() <pre>Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n feature_extraction_layer (K  (None, 1280)             4049564   \n erasLayer)                                                      \n                                                                 \n output_layer (Dense)        (None, 10)                12810     \n                                                                 \n=================================================================\nTotal params: 4,062,374\nTrainable params: 12,810\nNon-trainable params: 4,049,564\n_________________________________________________________________\n</pre> <p>It seems despite having over four times less parameters (4,049,564 vs. 23,564,800) than the ResNet50V2 extraction layer, the  EfficientNetB0 feature extraction layer yields better performance. Now it's clear where the \"efficient\" name came from.</p> In\u00a0[17]: Copied! <pre># Upload TensorBoard dev records\n!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n  --name \"EfficientNetB0 vs. ResNet50V2\" \\\n  --description \"Comparing two different TF Hub feature extraction models architectures using 10% of training images\" \\\n  --one_shot\n</pre> # Upload TensorBoard dev records !tensorboard dev upload --logdir ./tensorflow_hub/ \\   --name \"EfficientNetB0 vs. ResNet50V2\" \\   --description \"Comparing two different TF Hub feature extraction models architectures using 10% of training images\" \\   --one_shot <pre>2023-05-11 04:26:10.912881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n***** TensorBoard Uploader *****\n\nThis will upload your TensorBoard logs to https://tensorboard.dev/ from\nthe following directory:\n\n./tensorflow_hub/\n\nThis TensorBoard will be visible to everyone. Do not upload sensitive\ndata.\n\nYour use of this service is subject to Google's Terms of Service\n&lt;https://policies.google.com/terms&gt; and Privacy Policy\n&lt;https://policies.google.com/privacy&gt;, and TensorBoard.dev's Terms of Service\n&lt;https://tensorboard.dev/policy/terms/&gt;.\n\nThis notice will not be shown again while you are logged into the uploader.\nTo log out, run `tensorboard dev auth revoke`.\n\nContinue? (yes/NO) yes\n\nTo sign in with the TensorBoard uploader:\n\n1. On your computer or phone, visit:\n\n   https://www.google.com/device\n\n2. Sign in with your Google account, then enter:\n\n   PCB-DVW-YTS\n\n\n\nNew experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/dIzMI7IkT7OHD1PmA4mMRQ/\n\n[2023-05-11T04:41:27] Started scanning logdir.\n[2023-05-11T04:41:32] Total uploaded: 60 scalars, 0 tensors, 2 binary objects (5.7 MB)\n[2023-05-11T04:41:32] Done scanning logdir.\n\n\nDone. View your TensorBoard at https://tensorboard.dev/experiment/dIzMI7IkT7OHD1PmA4mMRQ/\n</pre> <p>Every time you upload something to TensorBoad.dev you'll get a new experiment ID. The experiment ID will look something like this: https://tensorboard.dev/experiment/73taSKxXQeGPQsNBcVvY3g/ (this is the actual experiment from this notebook).</p> <p>If you upload the same directory again, you'll get a new experiment ID to go along with it.</p> <p>This means to track your experiments, you may want to look into how you name your uploads. That way when you find them on TensorBoard.dev you can tell what happened during each experiment (e.g. \"efficientnet0_10_percent_data\").</p> In\u00a0[21]: Copied! <pre># Check out experiments\n# !tensorboard dev list # uncomment to see\n</pre> # Check out experiments # !tensorboard dev list # uncomment to see In\u00a0[19]: Copied! <pre># Delete an experiment\n!tensorboard dev delete --experiment_id n6kd8XZ3Rdy1jSgSLH5WjA\n</pre> # Delete an experiment !tensorboard dev delete --experiment_id n6kd8XZ3Rdy1jSgSLH5WjA <pre>2023-05-11 04:41:41.121171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nNo such experiment n6kd8XZ3Rdy1jSgSLH5WjA. Either it never existed or it has already been deleted.\n\n</pre> In\u00a0[22]: Copied! <pre># Check to see if experiments still exist\n# !tensorboard dev list # uncomment to see\n</pre> # Check to see if experiments still exist # !tensorboard dev list # uncomment to see"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#04-transfer-learning-with-tensorflow-part-1-feature-extraction","title":"04. Transfer Learning with TensorFlow Part 1: Feature Extraction\u00b6","text":"<p>We've built a bunch of convolutional neural networks from scratch and they all seem to be learning, however, there is still plenty of room for improvement.</p> <p>To improve our model(s), we could spend a while trying different configurations, adding more layers, changing the learning rate, adjusting the number of neurons per layer and more.</p> <p>However, doing this is very time consuming.</p> <p>Luckily, there's a technique we can use to save time.</p> <p>It's called transfer learning, in other words, taking the patterns (also called weights) another model has learned from another problem and using them for our own problem.</p> <p>There are two main benefits to using transfer learning:</p> <ol> <li>Can leverage an existing neural network architecture proven to work on problems similar to our own.</li> <li>Can leverage a working neural network architecture which has already learned patterns on similar data to our own. This often results in achieving great results with less custom data.</li> </ol> <p>What this means is, instead of hand-crafting our own neural network architectures or building them from scratch, we can utilise models which have worked for others.</p> <p>And instead of training our own models from scratch on our own datasets, we can take the patterns a model has learned from datasets such as ImageNet (millions of images of different objects) and use them as the foundation of our own. Doing this often leads to getting great results with less data.</p> <p>Over the next few notebooks, we'll see the power of transfer learning in action.</p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to go through the following with TensorFlow:</p> <ul> <li>Introduce transfer learning (a way to beat all of our old self-built models)</li> <li>Using a smaller dataset to experiment faster (10% of training samples of 10 classes of food)</li> <li>Build a transfer learning feature extraction model using TensorFlow Hub</li> <li>Introduce the TensorBoard callback to track model training results</li> <li>Compare model results using TensorBoard</li> </ul>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#using-a-gpu","title":"Using a GPU\u00b6","text":"<p>To begin, let's check to see if we're using a GPU. Using a GPU will make sure our model trains faster than using just a CPU.</p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#transfer-leanring-with-tensorflow-hub-getting-great-results-with-10-of-the-data","title":"Transfer leanring with TensorFlow Hub: Getting great results with 10% of the data\u00b6","text":"<p>If you've been thinking, \"surely someone else has spent the time crafting the right model for the job...\" then you're in luck.</p> <p>For many of the problems you'll want to use deep learning for, chances are, a working model already exists.</p> <p>And the good news is, you can access many of them on TensorFlow Hub.</p> <p>TensorFlow Hub is a repository for existing model components. It makes it so you can import and use a fully trained model with as little as a URL.</p> <p>Now, I really want to demonstrate the power of transfer learning to you.</p> <p>To do so, what if I told you we could get much of the same results (or better) than our best model has gotten so far with only 10% of the original data, in other words, 10x less data.</p> <p>This seems counterintuitive right?</p> <p>Wouldn't you think more examples of what a picture of food looked like led to better results?</p> <p>And you'd be right if you thought so, generally, more data leads to better results.</p> <p>However, what if you didn't have more data? What if instead of 750 images per class, you had 75 images per class?</p> <p>Collecting 675 more images of a certain class could take a long time.</p> <p>So this is where another major benefit of transfer learning comes in.</p> <p>Transfer learning often allows you to get great results with less data.</p> <p>But don't just take my word for it. Let's download a subset of the data we've been using, namely 10% of the training data from the <code>10_food_classes</code> dataset and use it to train a food image classifier on.</p> <p> What we're working towards building. Taking a pre-trained model and adding our own custom layers on top, extracting all of the underlying patterns learned on another dataset our own images.</p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#downloading-and-becoming-one-with-the-data","title":"Downloading and becoming one with the data\u00b6","text":""},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#creating-data-loaders-preparing-the-data","title":"Creating data loaders (preparing the data)\u00b6","text":"<p>Now we've downloaded the data, let's use the <code>ImageDataGenerator</code> class along with the <code>flow_from_directory</code> method to load in our images.</p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#setting-up-callbacks-things-to-run-whilst-our-model-trains","title":"Setting up callbacks (things to run whilst our model trains)\u00b6","text":"<p>Before we build a model, there's an important concept we're going to get familiar with because it's going to play a key role in our future model building experiments.</p> <p>And that concept is callbacks.</p> <p>Callbacks are extra functionality you can add to your models to be performed during or after training. Some of the most popular callbacks include:</p> <ul> <li>Experiment tracking with TensorBoard - log the performance of multiple models and then view and compare these models in a visual way on TensorBoard (a dashboard for inspecting neural network parameters). Helpful to compare the results of different models on your data.</li> <li>Model checkpointing - save your model as it trains so you can stop training if needed and come back to continue off where you left. Helpful if training takes a long time and can't be done in one sitting.</li> <li>Early stopping - leave your model training for an arbitrary amount of time and have it stop training automatically when it ceases to improve. Helpful when you've got a large dataset and don't know how long training will take.</li> </ul> <p>We'll explore each of these overtime but for this notebook, we'll see how the TensorBoard callback can be used.</p> <p>The TensorBoard callback can be accessed using <code>tf.keras.callbacks.TensorBoard()</code>.</p> <p>Its main functionality is saving a model's training performance metrics to a specified <code>log_dir</code>.</p> <p>By default, logs are recorded every epoch using the <code>update_freq='epoch'</code> parameter. This is a good default since tracking model performance too often can slow down model training.</p> <p>To track our modelling experiments using TensorBoard, let's create a function which creates a TensorBoard callback for us.</p> <p>\ud83d\udd11 Note: We create a function for creating a TensorBoard callback because as we'll see later on, each model needs its own TensorBoard callback instance (so the function will create a new one each time it's run).</p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#creating-models-using-tensorflow-hub","title":"Creating models using TensorFlow Hub\u00b6","text":"<p>In the past we've used TensorFlow to create our own models layer by layer from scratch.</p> <p>Now we're going to do a similar process, except the majority of our model's layers are going to come from TensorFlow Hub.</p> <p>In fact, we're going to use two models from TensorFlow Hub:</p> <ol> <li>ResNetV2 -  a state of the art computer vision model architecture from 2016.</li> <li>EfficientNet - a state of the art computer vision architecture from 2019.</li> </ol> <p>State of the art means that at some point, both of these models have achieved the lowest error rate on ImageNet (ILSVRC-2012-CLS), the gold standard of computer vision benchmarks.</p> <p>You might be wondering, how do you find these models on TensorFlow Hub?</p> <p>Here are the steps I took:</p> <ol> <li>Go to tfhub.dev.</li> <li>Choose your problem domain, e.g. \"Image\" (we're using food images).</li> <li>Select your TF version, which in our case is TF2.</li> <li>Remove all \"Problem domanin\" filters except for the problem you're working on.</li> </ol> <ul> <li>Note: \"Image feature vector\" can be used alongside almost any problem, we'll get to this soon.</li> </ul> <ol> <li>The models listed are all models which could potentially be used for your problem.</li> </ol> <p>\ud83e\udd14 Question: I see many options for image classification models, how do I know which is best?</p> <p>You can see a list of state of the art models on paperswithcode.com, a resource for collecting the latest in deep learning paper results which have code implementations for the findings they report.</p> <p>Since we're working with images, our target are the models which perform best on ImageNet.</p> <p>You'll probably find not all of the model architectures listed on paperswithcode appear on TensorFlow Hub. And this is okay, we can still use what's available.</p> <p>To find our models, let's narrow down our search using the Architecture tab.</p> <ol> <li>Select the Architecture tab on TensorFlow Hub and you'll see a dropdown menu of architecture names appear.</li> </ol> <ul> <li>The rule of thumb here is generally, names with larger numbers means better performing models. For example, EfficientNetB4 performs better than EfficientNetB0.<ul> <li>However, the tradeoff with larger numbers can mean they take longer to compute.</li> </ul> </li> </ul> <ol> <li>Select EfficientNetB0 and you should see something like the following: </li> <li>Clicking the one titled \"efficientnet/b0/feature-vector\" brings us to a page with a button that says \"Copy URL\". That URL is what we can use to harness the power of EfficientNetB0.</li> </ol> <ul> <li>Copying the URL should give you something like this: https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1</li> </ul> <p>\ud83e\udd14 Question: I thought we were doing image classification, why do we choose feature vector and not classification?</p> <p>Great observation. This is where the differnet types of transfer learning come into play, as is, feature extraction and fine-tuning.</p> <ol> <li>\"As is\" transfer learning is when you take a pretrained model as it is and apply it to your task without any changes.</li> </ol> <ul> <li><p>For example, many computer vision models are pretrained on the ImageNet dataset which contains 1000 different classes of images. This means passing a single image to this model will produce 1000 different prediction probability values (1 for each class).</p> <ul> <li>This is helpful if you have 1000 classes of image you'd like to classify and they're all the same as the ImageNet classes, however, it's not helpful if you want to classify only a small subset of classes (such as 10 different kinds of food). Model's with <code>\"/classification\"</code> in their name on TensorFlow Hub provide this kind of functionality.</li> </ul> </li> </ul> <ol> <li>Feature extraction transfer learning is when you take the underlying patterns (also called weights) a pretrained model has learned and adjust its outputs to be more suited to your problem.</li> </ol> <ul> <li><p>For example, say the pretrained model you were using had 236 different layers (EfficientNetB0 has 236 layers), but the top layer outputs 1000 classes because it was pretrained on ImageNet. To adjust this to your own problem, you might remove the original activation layer and replace it with your own but with the right number of output classes. The important part here is that only the top few layers become trainable, the rest remain frozen.</p> <ul> <li>This way all the underlying patterns remain in the rest of the layers and you can utilise them for your own problem. This kind of transfer learning is very helpful when your data is similar to the data a model has been pretrained on.</li> </ul> </li> </ul> <ol> <li><p>Fine-tuning transfer learning is when you take the underlying patterns (also called weights) of a pretrained model and adjust (fine-tune) them to your own problem.</p> <ul> <li>This usually means training some, many or all of the layers in the pretrained model. This is useful when you've got a large dataset (e.g. 100+ images per class) where your data is slightly different to the data the original model was trained on.</li> </ul> </li> </ol> <p>A common workflow is to \"freeze\" all of the learned patterns in the bottom layers of a pretrained model so they're untrainable. And then train the top 2-3 layers of so the pretrained model can adjust its outputs to your custom data (feature extraction).</p> <p>After you've trained the top 2-3 layers, you can then gradually \"unfreeze\" more and more layers and run the training process on your own data to further fine-tune the pretrained model.</p> <p>\ud83e\udd14 Question: Why train only the top 2-3 layers in feature extraction?</p> <p>The lower a layer is in a computer vision model as in, the closer it is to the input layer, the larger the features it learn. For example, a bottom layer in a computer vision model to identify images of cats or dogs might learn the outline of legs, where as, layers closer to the output might learn the shape of teeth. Often, you'll want the larger features (learned patterns are also called features) to remain, since these are similar for both animals, where as, the differences remain in the more fine-grained features.</p> <p> The different kinds of transfer learning. An original model, a feature extraction model (only top 2-3 layers change) and a fine-tuning model (many or all of original model get changed).</p> <p>Okay, enough talk, let's see this in action. Once we do, we'll explain what's happening.</p> <p>First we'll import TensorFlow and TensorFlow Hub.</p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#comparing-models-using-tensorboard","title":"Comparing models using TensorBoard\u00b6","text":"<p>Alright, even though we've already compared the performance of our two models by looking at the accuracy scores. But what if you had more than two models?</p> <p>That's where an experiment tracking tool like TensorBoard (preinstalled in Google Colab) comes in.</p> <p>The good thing is, since we set up a TensorBoard callback, all of our model's training logs have been saved automatically. To visualize them, we can upload the results to TensorBoard.dev.</p> <p>Uploading your results to TensorBoard.dev enables you to track and share multiple different modelling experiments. So if you needed to show someone your results, you could send them a link to your TensorBoard.dev as well as the accompanying Colab notebook.</p> <p>\ud83d\udd11 Note: These experiments are public, do not upload sensitive data. You can delete experiments if needed.</p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#uploading-experiments-to-tensorboard","title":"Uploading experiments to TensorBoard\u00b6","text":"<p>To upload a series of TensorFlow logs to TensorBoard, we can use the following command:</p> <pre><code>Upload TensorBoard dev records\n\n!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n  --name \"EfficientNetB0 vs. ResNet50V2\" \\ \n  --description \"Comparing two different TF Hub feature extraction models architectures using 10% of training images\" \\ \n  --one_shot\n</code></pre> <p>Where:</p> <ul> <li><code>--logdir</code> is the target upload directory</li> <li><code>--name</code> is the name of the experiment</li> <li><code>--description</code> is a brief description of the experiment</li> <li><code>--one_shot</code> exits the TensorBoard uploader once uploading is finished</li> </ul> <p>Running the <code>tensorboard dev upload</code> command will first ask you to authorize the upload to TensorBoard.dev. After you've authorized the upload, your log files will be uploaded.</p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#listing-experiments-youve-saved-to-tensorboard","title":"Listing experiments you've saved to TensorBoard\u00b6","text":"<p>To see all of the experiments you've uploaded you can use the command:</p> <p><code>tensorboard dev list</code></p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#deleting-experiments-from-tensorboard","title":"Deleting experiments from TensorBoard\u00b6","text":"<p>Remember, all uploads to TensorBoard.dev are public, so to delete an experiment you can use the command:</p> <p><code>tensorboard dev delete --experiment_id [INSERT_EXPERIMENT_ID]</code></p>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Build and fit a model using the same data we have here but with the MobileNetV2 architecture feature extraction (<code>mobilenet_v2_100_224/feature_vector</code>) from TensorFlow Hub, how does it perform compared to our other models?</li> <li>Name 3 different image classification models on TensorFlow Hub that we haven't used.</li> <li>Build a model to classify images of two different things you've taken photos of.</li> </ol> <ul> <li>You can use any feature extraction layer from TensorFlow Hub you like for this.</li> <li>You should aim to have at least 10 images of each class, for example to build a fridge versus oven classifier, you'll want 10 images of fridges and 10 images of ovens.</li> </ul> <ol> <li>What is the current best performing model on ImageNet?</li> </ol> <ul> <li>Hint: you might want to check sotabench.com for this.</li> </ul>"},{"location":"Learning/Tensorflow/04_transfer_learning_in_tensorflow_part_1_feature_extraction/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<ul> <li>Read through the TensorFlow Transfer Learning Guide and define the main two types of transfer learning in your own words.</li> <li>Go through the Transfer Learning with TensorFlow Hub tutorial on the TensorFlow website and rewrite all of the code yourself into a new Google Colab notebook making comments about what each step does along the way.</li> <li>We haven't covered fine-tuning with TensorFlow Hub in this notebook, but if you'd like to know more, go through the fine-tuning a TensorFlow Hub model tutorial on the TensorFlow homepage.How to fine-tune a tensorflow hub model:</li> <li>Look into experiment tracking with Weights &amp; Biases, how could you integrate it with our existing TensorBoard logs?</li> </ul>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/","title":"10. Milestone Project 3: Time series forecasting in TensorFlow (BitPredict \ud83d\udcb0\ud83d\udcc8)","text":"In\u00a0[\u00a0]: Copied! <pre># Check for GPU\n!nvidia-smi -L\n</pre> # Check for GPU !nvidia-smi -L <pre>GPU 0: Tesla K80 (UUID: GPU-c7456639-4229-1150-8316-e4197bf2c93e)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Download Bitcoin historical data from GitHub \n# Note: you'll need to select \"Raw\" to download the data in the correct format\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv \n</pre> # Download Bitcoin historical data from GitHub  # Note: you'll need to select \"Raw\" to download the data in the correct format !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv  <pre>--2021-09-27 03:40:22--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 178509 (174K) [text/plain]\nSaving to: \u2018BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\u2019\n\nBTC_USD_2013-10-01_ 100%[===================&gt;] 174.33K  --.-KB/s    in 0.01s   \n\n2021-09-27 03:40:22 (16.5 MB/s) - \u2018BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\u2019 saved [178509/178509]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Import with pandas \nimport pandas as pd\n# Parse dates and set date column to index\ndf = pd.read_csv(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", \n                 parse_dates=[\"Date\"], \n                 index_col=[\"Date\"]) # parse the date column (tell pandas column 1 is a datetime)\ndf.head()\n</pre> # Import with pandas  import pandas as pd # Parse dates and set date column to index df = pd.read_csv(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\",                   parse_dates=[\"Date\"],                   index_col=[\"Date\"]) # parse the date column (tell pandas column 1 is a datetime) df.head() Out[\u00a0]: Currency Closing Price (USD) 24h Open (USD) 24h High (USD) 24h Low (USD) Date 2013-10-01 BTC 123.65499 124.30466 124.75166 122.56349 2013-10-02 BTC 125.45500 123.65499 125.75850 123.63383 2013-10-03 BTC 108.58483 125.45500 125.66566 83.32833 2013-10-04 BTC 118.67466 108.58483 118.67500 107.05816 2013-10-05 BTC 121.33866 118.67466 121.93633 118.00566 <p>Looking good! Let's get some more info.</p> In\u00a0[\u00a0]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 2787 entries, 2013-10-01 to 2021-05-18\nData columns (total 5 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   Currency             2787 non-null   object \n 1   Closing Price (USD)  2787 non-null   float64\n 2   24h Open (USD)       2787 non-null   float64\n 3   24h High (USD)       2787 non-null   float64\n 4   24h Low (USD)        2787 non-null   float64\ndtypes: float64(4), object(1)\nmemory usage: 130.6+ KB\n</pre> <p>Because we told pandas to parse the date column and set it as the index, its not in the list of columns.</p> <p>You can also see there isn't many samples.</p> In\u00a0[\u00a0]: Copied! <pre># How many samples do we have?\nlen(df)\n</pre> # How many samples do we have? len(df) Out[\u00a0]: <pre>2787</pre> <p>We've collected the historical price of Bitcoin for the past ~8 years but there's only 2787 total samples.</p> <p>This is something you'll run into with time series data problems. Often, the number of samples isn't as large as other kinds of data.</p> <p>For example, collecting one sample at different time frames results in:</p> 1 sample per timeframe Number of samples per year Second 31,536,000 Hour 8,760 Day 365 Week 52 Month 12 <p>\ud83d\udd11 Note: The frequency at which a time series value is collected is often referred to as seasonality. This is usually mesaured in number of samples per year. For example, collecting the price of Bitcoin once per day would result in a time series with a seasonality of 365. Time series data collected with different seasonality values often exhibit seasonal patterns (e.g. electricity demand behing higher in Summer months for air conditioning than Winter months). For more on different time series patterns, see Forecasting: Principles and Practice Chapter 2.3.</p> <p> Example of different kinds of patterns you'll see in time series data. Notice the bottom right time series (Google stock price changes) has little to no patterns, making it difficult to predict. See Forecasting: Principles and Practice Chapter 2.3 for full graphic.</p> <p>Deep learning algorithms usually flourish with lots of data, in the range of thousands to millions of samples.</p> <p>In our case, we've got the daily prices of Bitcoin, a max of 365 samples per year.</p> <p>But that doesn't we can't try them with our data.</p> <p>To simplify, let's remove some of the columns from our data so we're only left with a date index and the closing price.</p> In\u00a0[\u00a0]: Copied! <pre># Only want closing price for each day \nbitcoin_prices = pd.DataFrame(df[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\": \"Price\"})\nbitcoin_prices.head()\n</pre> # Only want closing price for each day  bitcoin_prices = pd.DataFrame(df[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\": \"Price\"}) bitcoin_prices.head() Out[\u00a0]: Price Date 2013-10-01 123.65499 2013-10-02 125.45500 2013-10-03 108.58483 2013-10-04 118.67466 2013-10-05 121.33866 <p>Much better!</p> <p>But that's only five days worth of Bitcoin prices, let's plot everything we've got.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nbitcoin_prices.plot(figsize=(10, 7))\nplt.ylabel(\"BTC Price\")\nplt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16)\nplt.legend(fontsize=14);\n</pre> import matplotlib.pyplot as plt bitcoin_prices.plot(figsize=(10, 7)) plt.ylabel(\"BTC Price\") plt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16) plt.legend(fontsize=14); <p>Woah, looks like it would've been a good idea to buy Bitcoin back in 2014.</p> In\u00a0[\u00a0]: Copied! <pre># Importing and formatting historical Bitcoin data with Python\nimport csv\nfrom datetime import datetime\n\ntimesteps = []\nbtc_price = []\nwith open(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", \"r\") as f:\n  csv_reader = csv.reader(f, delimiter=\",\") # read in the target CSV\n  next(csv_reader) # skip first line (this gets rid of the column titles)\n  for line in csv_reader:\n    timesteps.append(datetime.strptime(line[1], \"%Y-%m-%d\")) # get the dates as dates (not strings), strptime = string parse time\n    btc_price.append(float(line[2])) # get the closing price as float\n\n# View first 10 of each\ntimesteps[:10], btc_price[:10]\n</pre> # Importing and formatting historical Bitcoin data with Python import csv from datetime import datetime  timesteps = [] btc_price = [] with open(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", \"r\") as f:   csv_reader = csv.reader(f, delimiter=\",\") # read in the target CSV   next(csv_reader) # skip first line (this gets rid of the column titles)   for line in csv_reader:     timesteps.append(datetime.strptime(line[1], \"%Y-%m-%d\")) # get the dates as dates (not strings), strptime = string parse time     btc_price.append(float(line[2])) # get the closing price as float  # View first 10 of each timesteps[:10], btc_price[:10] Out[\u00a0]: <pre>([datetime.datetime(2013, 10, 1, 0, 0),\n  datetime.datetime(2013, 10, 2, 0, 0),\n  datetime.datetime(2013, 10, 3, 0, 0),\n  datetime.datetime(2013, 10, 4, 0, 0),\n  datetime.datetime(2013, 10, 5, 0, 0),\n  datetime.datetime(2013, 10, 6, 0, 0),\n  datetime.datetime(2013, 10, 7, 0, 0),\n  datetime.datetime(2013, 10, 8, 0, 0),\n  datetime.datetime(2013, 10, 9, 0, 0),\n  datetime.datetime(2013, 10, 10, 0, 0)],\n [123.65499,\n  125.455,\n  108.58483,\n  118.67466,\n  121.33866,\n  120.65533,\n  121.795,\n  123.033,\n  124.049,\n  125.96116])</pre> <p>Beautiful! Now, let's see how things look.</p> In\u00a0[\u00a0]: Copied! <pre># Plot from CSV\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.figure(figsize=(10, 7))\nplt.plot(timesteps, btc_price)\nplt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16)\nplt.xlabel(\"Date\")\nplt.ylabel(\"BTC Price\");\n</pre> # Plot from CSV import matplotlib.pyplot as plt import numpy as np plt.figure(figsize=(10, 7)) plt.plot(timesteps, btc_price) plt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16) plt.xlabel(\"Date\") plt.ylabel(\"BTC Price\"); <p>Ho ho! Would you look at that! Just like the pandas plot. And because we formatted the <code>timesteps</code> to be <code>datetime</code> objects, <code>matplotlib</code> displays a fantastic looking date axis.</p> In\u00a0[\u00a0]: Copied! <pre># Get bitcoin date array\ntimesteps = bitcoin_prices.index.to_numpy()\nprices = bitcoin_prices[\"Price\"].to_numpy()\n\ntimesteps[:10], prices[:10]\n</pre> # Get bitcoin date array timesteps = bitcoin_prices.index.to_numpy() prices = bitcoin_prices[\"Price\"].to_numpy()  timesteps[:10], prices[:10] Out[\u00a0]: <pre>(array(['2013-10-01T00:00:00.000000000', '2013-10-02T00:00:00.000000000',\n        '2013-10-03T00:00:00.000000000', '2013-10-04T00:00:00.000000000',\n        '2013-10-05T00:00:00.000000000', '2013-10-06T00:00:00.000000000',\n        '2013-10-07T00:00:00.000000000', '2013-10-08T00:00:00.000000000',\n        '2013-10-09T00:00:00.000000000', '2013-10-10T00:00:00.000000000'],\n       dtype='datetime64[ns]'),\n array([123.65499, 125.455  , 108.58483, 118.67466, 121.33866, 120.65533,\n        121.795  , 123.033  , 124.049  , 125.96116]))</pre> <p>And now we'll use the ever faithful <code>train_test_split</code> from Scikit-Learn to create our train and test sets.</p> In\u00a0[\u00a0]: Copied! <pre># Wrong way to make train/test sets for time series\nfrom sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(timesteps, # dates\n                                                    prices, # prices\n                                                    test_size=0.2,\n                                                    random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape \n</pre> # Wrong way to make train/test sets for time series from sklearn.model_selection import train_test_split   X_train, X_test, y_train, y_test = train_test_split(timesteps, # dates                                                     prices, # prices                                                     test_size=0.2,                                                     random_state=42) X_train.shape, X_test.shape, y_train.shape, y_test.shape  Out[\u00a0]: <pre>((2229,), (558,), (2229,), (558,))</pre> <p>Looks like the splits worked well, but let's not trust numbers on a page, let's visualize, visualize, visualize!</p> In\u00a0[\u00a0]: Copied! <pre># Let's plot wrong train and test splits\nplt.figure(figsize=(10, 7))\nplt.scatter(X_train, y_train, s=5, label=\"Train data\")\nplt.scatter(X_test, y_test, s=5, label=\"Test data\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"BTC Price\")\nplt.legend(fontsize=14)\nplt.show();\n</pre> # Let's plot wrong train and test splits plt.figure(figsize=(10, 7)) plt.scatter(X_train, y_train, s=5, label=\"Train data\") plt.scatter(X_test, y_test, s=5, label=\"Test data\") plt.xlabel(\"Date\") plt.ylabel(\"BTC Price\") plt.legend(fontsize=14) plt.show(); <p>Hmmm... what's wrong with this plot?</p> <p>Well, let's remind ourselves of what we're trying to do.</p> <p>We're trying to use the historical price of Bitcoin to predict future prices of Bitcoin.</p> <p>With this in mind, our seen data (training set) is what?</p> <p>Prices of Bitcoin in the past.</p> <p>And our unseen data (test set) is?</p> <p>Prices of Bitcoin in the future.</p> <p>Does the plot above reflect this?</p> <p>No.</p> <p>Our test data is scattered all throughout the training data.</p> <p>This kind of random split is okay for datasets without a time component (such as images or passages of text for classification problems) but for time series, we've got to take the time factor into account.</p> <p>To fix this, we've got to split our data in a way that reflects what we're actually trying to do.</p> <p>We need to split our historical Bitcoin data to have a dataset that reflects the past (train set) and a dataset that reflects the future (test set).</p> In\u00a0[\u00a0]: Copied! <pre># Create train and test splits the right way for time series data\nsplit_size = int(0.8 * len(prices)) # 80% train, 20% test\n\n# Create train data splits (everything before the split)\nX_train, y_train = timesteps[:split_size], prices[:split_size]\n\n# Create test data splits (everything after the split)\nX_test, y_test = timesteps[split_size:], prices[split_size:]\n\nlen(X_train), len(X_test), len(y_train), len(y_test)\n</pre> # Create train and test splits the right way for time series data split_size = int(0.8 * len(prices)) # 80% train, 20% test  # Create train data splits (everything before the split) X_train, y_train = timesteps[:split_size], prices[:split_size]  # Create test data splits (everything after the split) X_test, y_test = timesteps[split_size:], prices[split_size:]  len(X_train), len(X_test), len(y_train), len(y_test) Out[\u00a0]: <pre>(2229, 558, 2229, 558)</pre> <p>Okay, looks like our custom made splits are the same lengths as the splits we made with <code>train_test_split</code>.</p> <p>But again, these are numbers on a page.</p> <p>And you know how the saying goes, trust one eye more than two ears.</p> <p>Let's visualize.</p> In\u00a0[\u00a0]: Copied! <pre># Plot correctly made splits\nplt.figure(figsize=(10, 7))\nplt.scatter(X_train, y_train, s=5, label=\"Train data\")\nplt.scatter(X_test, y_test, s=5, label=\"Test data\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"BTC Price\")\nplt.legend(fontsize=14)\nplt.show();\n</pre> # Plot correctly made splits plt.figure(figsize=(10, 7)) plt.scatter(X_train, y_train, s=5, label=\"Train data\") plt.scatter(X_test, y_test, s=5, label=\"Test data\") plt.xlabel(\"Date\") plt.ylabel(\"BTC Price\") plt.legend(fontsize=14) plt.show(); <p>That looks much better!</p> <p>Do you see what's happened here?</p> <p>We're going to be using the training set (past) to train a model to try and predict values on the test set (future).</p> <p>Because the test set is an artificial future, we can guage how our model might perform on actual future data.</p> <p>\ud83d\udd11 Note: The amount of data you reserve for your test set not set in stone. You could have 80/20, 90/10, 95/5 splits or in some cases, you might not even have enough data to split into train and test sets (see the resource below). The point is to remember the test set is a pseudofuture and not the actual future, it is only meant to give you an indication of how the models you're building are performing.</p> <p>\ud83d\udcd6 Resource: Working with time series data can be tricky compared to other kinds of data. And there are a few pitfalls to watch out for, such as how much data to use for a test set. The article 3 facts about time series forecasting that surprise experienced machine learning practitioners talks about different things to watch out for when working with time series data, I'd recommend reading it.</p> In\u00a0[\u00a0]: Copied! <pre># Create a function to plot time series data\ndef plot_time_series(timesteps, values, format='.', start=0, end=None, label=None):\n  \"\"\"\n  Plots a timesteps (a series of points in time) against values (a series of values across timesteps).\n  \n  Parameters\n  ---------\n  timesteps : array of timesteps\n  values : array of values across time\n  format : style of plot, default \".\"\n  start : where to start the plot (setting a value will index from start of timesteps &amp; values)\n  end : where to end the plot (setting a value will index from end of timesteps &amp; values)\n  label : label to show on plot of values\n  \"\"\"\n  # Plot the series\n  plt.plot(timesteps[start:end], values[start:end], format, label=label)\n  plt.xlabel(\"Time\")\n  plt.ylabel(\"BTC Price\")\n  if label:\n    plt.legend(fontsize=14) # make label bigger\n  plt.grid(True)\n</pre> # Create a function to plot time series data def plot_time_series(timesteps, values, format='.', start=0, end=None, label=None):   \"\"\"   Plots a timesteps (a series of points in time) against values (a series of values across timesteps).      Parameters   ---------   timesteps : array of timesteps   values : array of values across time   format : style of plot, default \".\"   start : where to start the plot (setting a value will index from start of timesteps &amp; values)   end : where to end the plot (setting a value will index from end of timesteps &amp; values)   label : label to show on plot of values   \"\"\"   # Plot the series   plt.plot(timesteps[start:end], values[start:end], format, label=label)   plt.xlabel(\"Time\")   plt.ylabel(\"BTC Price\")   if label:     plt.legend(fontsize=14) # make label bigger   plt.grid(True) In\u00a0[\u00a0]: Copied! <pre># Try out our plotting function\nplt.figure(figsize=(10, 7))\nplot_time_series(timesteps=X_train, values=y_train, label=\"Train data\")\nplot_time_series(timesteps=X_test, values=y_test, label=\"Test data\")\n</pre> # Try out our plotting function plt.figure(figsize=(10, 7)) plot_time_series(timesteps=X_train, values=y_train, label=\"Train data\") plot_time_series(timesteps=X_test, values=y_test, label=\"Test data\") <p>Looking good!</p> <p>Time for some modelling experiments.</p> In\u00a0[\u00a0]: Copied! <pre># Create a na\u00efve forecast\nnaive_forecast = y_test[:-1] # Na\u00efve forecast equals every value excluding the last value\nnaive_forecast[:10], naive_forecast[-10:] # View frist 10 and last 10 \n</pre> # Create a na\u00efve forecast naive_forecast = y_test[:-1] # Na\u00efve forecast equals every value excluding the last value naive_forecast[:10], naive_forecast[-10:] # View frist 10 and last 10  Out[\u00a0]: <pre>(array([9226.48582088, 8794.35864452, 8798.04205463, 9081.18687849,\n        8711.53433917, 8760.89271814, 8749.52059102, 8656.97092235,\n        8500.64355816, 8469.2608989 ]),\n array([57107.12067189, 58788.20967893, 58102.19142623, 55715.54665129,\n        56573.5554719 , 52147.82118698, 49764.1320816 , 50032.69313676,\n        47885.62525472, 45604.61575361]))</pre> In\u00a0[\u00a0]: Copied! <pre># Plot naive forecast\nplt.figure(figsize=(10, 7))\nplot_time_series(timesteps=X_train, values=y_train, label=\"Train data\")\nplot_time_series(timesteps=X_test, values=y_test, label=\"Test data\")\nplot_time_series(timesteps=X_test[1:], values=naive_forecast, format=\"-\", label=\"Naive forecast\");\n</pre> # Plot naive forecast plt.figure(figsize=(10, 7)) plot_time_series(timesteps=X_train, values=y_train, label=\"Train data\") plot_time_series(timesteps=X_test, values=y_test, label=\"Test data\") plot_time_series(timesteps=X_test[1:], values=naive_forecast, format=\"-\", label=\"Naive forecast\"); <p>The naive forecast looks like it's following the data well.</p> <p>Let's zoom in to take a better look.</p> <p>We can do so by creating an offset value and passing it to the <code>start</code> parameter of our <code>plot_time_series()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 7))\noffset = 300 # offset the values by 300 timesteps \nplot_time_series(timesteps=X_test, values=y_test, start=offset, label=\"Test data\")\nplot_time_series(timesteps=X_test[1:], values=naive_forecast, format=\"-\", start=offset, label=\"Naive forecast\");\n</pre> plt.figure(figsize=(10, 7)) offset = 300 # offset the values by 300 timesteps  plot_time_series(timesteps=X_test, values=y_test, start=offset, label=\"Test data\") plot_time_series(timesteps=X_test[1:], values=naive_forecast, format=\"-\", start=offset, label=\"Naive forecast\"); <p>When we zoom in we see the na\u00efve forecast comes slightly after the test data. This makes sense because the naive forecast uses the previous timestep value to predict the next timestep value.</p> <p>Forecast made. Time to evaluate it.</p> In\u00a0[\u00a0]: Copied! <pre># Let's get TensorFlow! \nimport tensorflow as tf\n</pre> # Let's get TensorFlow!  import tensorflow as tf <p>And since TensorFlow doesn't have a ready made version of MASE (mean aboslute scaled error), how about we create our own?</p> <p>We'll take inspiration from sktime's (Scikit-Learn for time series) <code>MeanAbsoluteScaledError</code> class which calculates the MASE.</p> In\u00a0[\u00a0]: Copied! <pre># MASE implemented courtesy of sktime - https://github.com/alan-turing-institute/sktime/blob/ee7a06843a44f4aaec7582d847e36073a9ab0566/sktime/performance_metrics/forecasting/_functions.py#L16\ndef mean_absolute_scaled_error(y_true, y_pred):\n  \"\"\"\n  Implement MASE (assuming no seasonality of data).\n  \"\"\"\n  mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n\n  # Find MAE of naive forecast (no seasonality)\n  mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) # our seasonality is 1 day (hence the shifting of 1 day)\n\n  return mae / mae_naive_no_season\n</pre> # MASE implemented courtesy of sktime - https://github.com/alan-turing-institute/sktime/blob/ee7a06843a44f4aaec7582d847e36073a9ab0566/sktime/performance_metrics/forecasting/_functions.py#L16 def mean_absolute_scaled_error(y_true, y_pred):   \"\"\"   Implement MASE (assuming no seasonality of data).   \"\"\"   mae = tf.reduce_mean(tf.abs(y_true - y_pred))    # Find MAE of naive forecast (no seasonality)   mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) # our seasonality is 1 day (hence the shifting of 1 day)    return mae / mae_naive_no_season <p>You'll notice the version of MASE above doesn't take in the training values like sktime's <code>mae_loss()</code>. In our case, we're comparing the MAE of our predictions on the test to the MAE of the na\u00efve forecast on the test set.</p> <p>In practice, if we've created the function correctly, the na\u00efve model should achieve an MASE of 1 (or very close to 1). Any model worse than the na\u00efve forecast will achieve an MASE of &gt;1 and any model better than the na\u00efve forecast will achieve an MASE of &lt;1.</p> <p>Let's put each of our different evaluation metrics together into a function.</p> In\u00a0[\u00a0]: Copied! <pre>def evaluate_preds(y_true, y_pred):\n  # Make sure float32 (for metric calculations)\n  y_true = tf.cast(y_true, dtype=tf.float32)\n  y_pred = tf.cast(y_pred, dtype=tf.float32)\n\n  # Calculate various metrics\n  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n  rmse = tf.sqrt(mse)\n  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n  mase = mean_absolute_scaled_error(y_true, y_pred)\n  \n  return {\"mae\": mae.numpy(),\n          \"mse\": mse.numpy(),\n          \"rmse\": rmse.numpy(),\n          \"mape\": mape.numpy(),\n          \"mase\": mase.numpy()}\n</pre> def evaluate_preds(y_true, y_pred):   # Make sure float32 (for metric calculations)   y_true = tf.cast(y_true, dtype=tf.float32)   y_pred = tf.cast(y_pred, dtype=tf.float32)    # Calculate various metrics   mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)   mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)   rmse = tf.sqrt(mse)   mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)   mase = mean_absolute_scaled_error(y_true, y_pred)      return {\"mae\": mae.numpy(),           \"mse\": mse.numpy(),           \"rmse\": rmse.numpy(),           \"mape\": mape.numpy(),           \"mase\": mase.numpy()} <p>Looking good! How about we test our function on the naive forecast?</p> In\u00a0[\u00a0]: Copied! <pre>naive_results = evaluate_preds(y_true=y_test[1:],\n                               y_pred=naive_forecast)\nnaive_results\n</pre> naive_results = evaluate_preds(y_true=y_test[1:],                                y_pred=naive_forecast) naive_results Out[\u00a0]: <pre>{'mae': 567.9802,\n 'mape': 2.516525,\n 'mase': 0.99957,\n 'mse': 1147547.0,\n 'rmse': 1071.2362}</pre> <p>Alright, looks like we've got some baselines to beat.</p> <p>Taking a look at the na\u00efve forecast's MAE, it seems on average each forecast is ~$567 different than the actual Bitcoin price.</p> <p>How does this compare to the average price of Bitcoin in the test dataset?</p> In\u00a0[\u00a0]: Copied! <pre># Find average price of Bitcoin in test dataset\ntf.reduce_mean(y_test).numpy()\n</pre> # Find average price of Bitcoin in test dataset tf.reduce_mean(y_test).numpy() Out[\u00a0]: <pre>20056.632963737226</pre> <p>Okay, looking at these two values is starting to give us an idea of how our model is performing:</p> <ul> <li><p>The average price of Bitcoin in the test dataset is: $20,056 (note: average may not be the best measure here, since the highest price is over 3x this value and the lowest price is over 4x lower)</p> </li> <li><p>Each prediction in naive forecast is on average off by: $567</p> </li> </ul> <p>Is this enough to say it's a good model?</p> <p>That's up your own interpretation. Personally, I'd prefer a model which was closer to the mark.</p> <p>How about we try and build one?</p> In\u00a0[\u00a0]: Copied! <pre>HORIZON = 1 # predict 1 step at a time\nWINDOW_SIZE = 7 # use a week worth of timesteps to predict the horizon\n</pre> HORIZON = 1 # predict 1 step at a time WINDOW_SIZE = 7 # use a week worth of timesteps to predict the horizon <p>Now we'll write a function to take in an array and turn it into a window and horizon.</p> In\u00a0[\u00a0]: Copied! <pre># Create function to label windowed data\ndef get_labelled_windows(x, horizon=1):\n  \"\"\"\n  Creates labels for windowed dataset.\n\n  E.g. if horizon=1 (default)\n  Input: [1, 2, 3, 4, 5, 6] -&gt; Output: ([1, 2, 3, 4, 5], [6])\n  \"\"\"\n  return x[:, :-horizon], x[:, -horizon:]\n</pre> # Create function to label windowed data def get_labelled_windows(x, horizon=1):   \"\"\"   Creates labels for windowed dataset.    E.g. if horizon=1 (default)   Input: [1, 2, 3, 4, 5, 6] -&gt; Output: ([1, 2, 3, 4, 5], [6])   \"\"\"   return x[:, :-horizon], x[:, -horizon:] In\u00a0[\u00a0]: Copied! <pre># Test out the window labelling function\ntest_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0), horizon=HORIZON)\nprint(f\"Window: {tf.squeeze(test_window).numpy()} -&gt; Label: {tf.squeeze(test_label).numpy()}\")\n</pre> # Test out the window labelling function test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0), horizon=HORIZON) print(f\"Window: {tf.squeeze(test_window).numpy()} -&gt; Label: {tf.squeeze(test_label).numpy()}\") <pre>Window: [1 2 3 4 5 6 7] -&gt; Label: 8\n</pre> <p>Oh yeah, that's what I'm talking about!</p> <p>Now we need a way to make windows for an entire time series.</p> <p>We could do this with Python for loops, however, for large time series, that'd be quite slow.</p> <p>To speed things up, we'll leverage NumPy's array indexing.</p> <p>Let's write a function which:</p> <ol> <li>Creates a window step of specific window size, for example: <code>[[0, 1, 2, 3, 4, 5, 6, 7]]</code></li> <li>Uses NumPy indexing to create a 2D of multiple window steps, for example:</li> </ol> <pre><code>[[0, 1, 2, 3, 4, 5, 6, 7],\n [1, 2, 3, 4, 5, 6, 7, 8],\n [2, 3, 4, 5, 6, 7, 8, 9]]\n</code></pre> <ol> <li>Uses the 2D array of multuple window steps to index on a target series</li> <li>Uses the <code>get_labelled_windows()</code> function we created above to turn the window steps into windows with a specified horizon</li> </ol> <p>\ud83d\udcd6 Resource: The function created below has been adapted from Syafiq Kamarul Azman's article Fast and Robust Sliding Window Vectorization with NumPy.</p> In\u00a0[\u00a0]: Copied! <pre># Create function to view NumPy arrays as windows \ndef make_windows(x, window_size=7, horizon=1):\n  \"\"\"\n  Turns a 1D array into a 2D array of sequential windows of window_size.\n  \"\"\"\n  # 1. Create a window of specific window_size (add the horizon on the end for later labelling)\n  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n  # print(f\"Window step:\\n {window_step}\")\n\n  # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n  # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")\n\n  # 3. Index on the target array (time series) with 2D array of multiple window steps\n  windowed_array = x[window_indexes]\n\n  # 4. Get the labelled windows\n  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n\n  return windows, labels\n</pre> # Create function to view NumPy arrays as windows  def make_windows(x, window_size=7, horizon=1):   \"\"\"   Turns a 1D array into a 2D array of sequential windows of window_size.   \"\"\"   # 1. Create a window of specific window_size (add the horizon on the end for later labelling)   window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)   # print(f\"Window step:\\n {window_step}\")    # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)   window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size   # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")    # 3. Index on the target array (time series) with 2D array of multiple window steps   windowed_array = x[window_indexes]    # 4. Get the labelled windows   windows, labels = get_labelled_windows(windowed_array, horizon=horizon)    return windows, labels <p>Phew! A few steps there... let's see how it goes.</p> In\u00a0[\u00a0]: Copied! <pre>full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n</pre> full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON) len(full_windows), len(full_labels) Out[\u00a0]: <pre>(2780, 2780)</pre> <p>Of course we have to visualize, visualize, visualize!</p> In\u00a0[\u00a0]: Copied! <pre># View the first 3 windows/labels\nfor i in range(3):\n  print(f\"Window: {full_windows[i]} -&gt; Label: {full_labels[i]}\")\n</pre> # View the first 3 windows/labels for i in range(3):   print(f\"Window: {full_windows[i]} -&gt; Label: {full_labels[i]}\") <pre>Window: [123.65499 125.455   108.58483 118.67466 121.33866 120.65533 121.795  ] -&gt; Label: [123.033]\nWindow: [125.455   108.58483 118.67466 121.33866 120.65533 121.795   123.033  ] -&gt; Label: [124.049]\nWindow: [108.58483 118.67466 121.33866 120.65533 121.795   123.033   124.049  ] -&gt; Label: [125.96116]\n</pre> In\u00a0[\u00a0]: Copied! <pre># View the last 3 windows/labels\nfor i in range(3):\n  print(f\"Window: {full_windows[i-3]} -&gt; Label: {full_labels[i-3]}\")\n</pre> # View the last 3 windows/labels for i in range(3):   print(f\"Window: {full_windows[i-3]} -&gt; Label: {full_labels[i-3]}\") <pre>Window: [58788.20967893 58102.19142623 55715.54665129 56573.5554719\n 52147.82118698 49764.1320816  50032.69313676] -&gt; Label: [47885.62525472]\nWindow: [58102.19142623 55715.54665129 56573.5554719  52147.82118698\n 49764.1320816  50032.69313676 47885.62525472] -&gt; Label: [45604.61575361]\nWindow: [55715.54665129 56573.5554719  52147.82118698 49764.1320816\n 50032.69313676 47885.62525472 45604.61575361] -&gt; Label: [43144.47129086]\n</pre> <p>\ud83d\udd11 Note: You can find a function which achieves similar results to the ones we implemented above at <code>tf.keras.preprocessing.timeseries_dataset_from_array()</code>. Just like ours, it takes in an array and returns a windowed dataset. It has the benefit of returning data in the form of a tf.data.Dataset instance (we'll see how to do this with our own data later).</p> In\u00a0[\u00a0]: Copied! <pre># Make the train/test splits\ndef make_train_test_splits(windows, labels, test_split=0.2):\n  \"\"\"\n  Splits matching pairs of windows and labels into train and test splits.\n  \"\"\"\n  split_size = int(len(windows) * (1-test_split)) # this will default to 80% train/20% test\n  train_windows = windows[:split_size]\n  train_labels = labels[:split_size]\n  test_windows = windows[split_size:]\n  test_labels = labels[split_size:]\n  return train_windows, test_windows, train_labels, test_labels\n</pre> # Make the train/test splits def make_train_test_splits(windows, labels, test_split=0.2):   \"\"\"   Splits matching pairs of windows and labels into train and test splits.   \"\"\"   split_size = int(len(windows) * (1-test_split)) # this will default to 80% train/20% test   train_windows = windows[:split_size]   train_labels = labels[:split_size]   test_windows = windows[split_size:]   test_labels = labels[split_size:]   return train_windows, test_windows, train_labels, test_labels <p>Look at that amazing function, lets test it.</p> In\u00a0[\u00a0]: Copied! <pre>train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)\n</pre> train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels) len(train_windows), len(test_windows), len(train_labels), len(test_labels) Out[\u00a0]: <pre>(2224, 556, 2224, 556)</pre> <p>Notice the default split of 80% training data and 20% testing data (this split can be adjusted if needed).</p> <p>How do the first 5 samples of the training windows and labels looks?</p> In\u00a0[\u00a0]: Copied! <pre>train_windows[:5], train_labels[:5]\n</pre> train_windows[:5], train_labels[:5] Out[\u00a0]: <pre>(array([[123.65499, 125.455  , 108.58483, 118.67466, 121.33866, 120.65533,\n         121.795  ],\n        [125.455  , 108.58483, 118.67466, 121.33866, 120.65533, 121.795  ,\n         123.033  ],\n        [108.58483, 118.67466, 121.33866, 120.65533, 121.795  , 123.033  ,\n         124.049  ],\n        [118.67466, 121.33866, 120.65533, 121.795  , 123.033  , 124.049  ,\n         125.96116],\n        [121.33866, 120.65533, 121.795  , 123.033  , 124.049  , 125.96116,\n         125.27966]]), array([[123.033  ],\n        [124.049  ],\n        [125.96116],\n        [125.27966],\n        [125.9275 ]]))</pre> In\u00a0[\u00a0]: Copied! <pre># Check to see if same (accounting for horizon and window size)\nnp.array_equal(np.squeeze(train_labels[:-HORIZON-1]), y_train[WINDOW_SIZE:])\n</pre> # Check to see if same (accounting for horizon and window size) np.array_equal(np.squeeze(train_labels[:-HORIZON-1]), y_train[WINDOW_SIZE:]) Out[\u00a0]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>import os\n\n# Create a function to implement a ModelCheckpoint callback with a specific filename \ndef create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model\n                                            verbose=0, # only output a limited amount of text\n                                            save_best_only=True) # save only the best model to file\n</pre> import os  # Create a function to implement a ModelCheckpoint callback with a specific filename  def create_model_checkpoint(model_name, save_path=\"model_experiments\"):   return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model                                             verbose=0, # only output a limited amount of text                                             save_best_only=True) # save only the best model to file In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Set random seed for as reproducible results as possible\ntf.random.set_seed(42)\n\n# Construct model\nmodel_1 = tf.keras.Sequential([\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(HORIZON, activation=\"linear\") # linear activation is the same as having no activation                        \n], name=\"model_1_dense\") # give the model a name so we can save it\n\n# Compile model\nmodel_1.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"mae\"]) # we don't necessarily need this when the loss function is already MAE\n\n# Fit model\nmodel_1.fit(x=train_windows, # train windows of 7 timesteps of Bitcoin prices\n            y=train_labels, # horizon value of 1 (using the previous 7 timesteps to predict next day)\n            epochs=100,\n            verbose=1,\n            batch_size=128,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_1.name)]) # create ModelCheckpoint callback to save best model\n</pre> import tensorflow as tf from tensorflow.keras import layers  # Set random seed for as reproducible results as possible tf.random.set_seed(42)  # Construct model model_1 = tf.keras.Sequential([   layers.Dense(128, activation=\"relu\"),   layers.Dense(HORIZON, activation=\"linear\") # linear activation is the same as having no activation                         ], name=\"model_1_dense\") # give the model a name so we can save it  # Compile model model_1.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"mae\"]) # we don't necessarily need this when the loss function is already MAE  # Fit model model_1.fit(x=train_windows, # train windows of 7 timesteps of Bitcoin prices             y=train_labels, # horizon value of 1 (using the previous 7 timesteps to predict next day)             epochs=100,             verbose=1,             batch_size=128,             validation_data=(test_windows, test_labels),             callbacks=[create_model_checkpoint(model_name=model_1.name)]) # create ModelCheckpoint callback to save best model <pre>Epoch 1/100\n18/18 [==============================] - 3s 12ms/step - loss: 780.3455 - mae: 780.3455 - val_loss: 2279.6526 - val_mae: 2279.6526\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 2/100\n18/18 [==============================] - 0s 4ms/step - loss: 247.6756 - mae: 247.6756 - val_loss: 1005.9991 - val_mae: 1005.9991\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 3/100\n18/18 [==============================] - 0s 4ms/step - loss: 188.4116 - mae: 188.4116 - val_loss: 923.2862 - val_mae: 923.2862\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 4/100\n18/18 [==============================] - 0s 4ms/step - loss: 169.4340 - mae: 169.4340 - val_loss: 900.5872 - val_mae: 900.5872\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 5/100\n18/18 [==============================] - 0s 4ms/step - loss: 165.0895 - mae: 165.0895 - val_loss: 895.2238 - val_mae: 895.2238\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 6/100\n18/18 [==============================] - 0s 4ms/step - loss: 158.5210 - mae: 158.5210 - val_loss: 855.1982 - val_mae: 855.1982\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 7/100\n18/18 [==============================] - 0s 5ms/step - loss: 151.3566 - mae: 151.3566 - val_loss: 840.9168 - val_mae: 840.9168\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 8/100\n18/18 [==============================] - 0s 5ms/step - loss: 145.2560 - mae: 145.2560 - val_loss: 803.5956 - val_mae: 803.5956\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 9/100\n18/18 [==============================] - 0s 4ms/step - loss: 144.3546 - mae: 144.3546 - val_loss: 799.5455 - val_mae: 799.5455\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 10/100\n18/18 [==============================] - 0s 4ms/step - loss: 141.2943 - mae: 141.2943 - val_loss: 763.5010 - val_mae: 763.5010\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 11/100\n18/18 [==============================] - 0s 4ms/step - loss: 135.6595 - mae: 135.6595 - val_loss: 771.3357 - val_mae: 771.3357\nEpoch 12/100\n18/18 [==============================] - 0s 5ms/step - loss: 134.1700 - mae: 134.1700 - val_loss: 782.8079 - val_mae: 782.8079\nEpoch 13/100\n18/18 [==============================] - 0s 4ms/step - loss: 134.6015 - mae: 134.6015 - val_loss: 784.4449 - val_mae: 784.4449\nEpoch 14/100\n18/18 [==============================] - 0s 4ms/step - loss: 130.6127 - mae: 130.6127 - val_loss: 751.3234 - val_mae: 751.3234\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 15/100\n18/18 [==============================] - 0s 5ms/step - loss: 128.8347 - mae: 128.8347 - val_loss: 696.5757 - val_mae: 696.5757\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 16/100\n18/18 [==============================] - 0s 5ms/step - loss: 124.7739 - mae: 124.7739 - val_loss: 702.4698 - val_mae: 702.4698\nEpoch 17/100\n18/18 [==============================] - 0s 4ms/step - loss: 123.4474 - mae: 123.4474 - val_loss: 704.9241 - val_mae: 704.9241\nEpoch 18/100\n18/18 [==============================] - 0s 5ms/step - loss: 122.2105 - mae: 122.2105 - val_loss: 667.9725 - val_mae: 667.9725\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 19/100\n18/18 [==============================] - 0s 4ms/step - loss: 121.7263 - mae: 121.7263 - val_loss: 718.8797 - val_mae: 718.8797\nEpoch 20/100\n18/18 [==============================] - 0s 4ms/step - loss: 119.2420 - mae: 119.2420 - val_loss: 657.0667 - val_mae: 657.0667\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 21/100\n18/18 [==============================] - 0s 4ms/step - loss: 121.2275 - mae: 121.2275 - val_loss: 637.0330 - val_mae: 637.0330\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 22/100\n18/18 [==============================] - 0s 4ms/step - loss: 119.9544 - mae: 119.9544 - val_loss: 671.2490 - val_mae: 671.2490\nEpoch 23/100\n18/18 [==============================] - 0s 5ms/step - loss: 121.9248 - mae: 121.9248 - val_loss: 633.3593 - val_mae: 633.3593\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 24/100\n18/18 [==============================] - 0s 5ms/step - loss: 116.3666 - mae: 116.3666 - val_loss: 624.4852 - val_mae: 624.4852\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 25/100\n18/18 [==============================] - 0s 4ms/step - loss: 114.6816 - mae: 114.6816 - val_loss: 619.7571 - val_mae: 619.7571\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 26/100\n18/18 [==============================] - 0s 4ms/step - loss: 116.4455 - mae: 116.4455 - val_loss: 615.6364 - val_mae: 615.6364\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 27/100\n18/18 [==============================] - 0s 5ms/step - loss: 116.5868 - mae: 116.5868 - val_loss: 615.9631 - val_mae: 615.9631\nEpoch 28/100\n18/18 [==============================] - 0s 4ms/step - loss: 113.4691 - mae: 113.4691 - val_loss: 608.0920 - val_mae: 608.0920\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 29/100\n18/18 [==============================] - 0s 5ms/step - loss: 113.7598 - mae: 113.7598 - val_loss: 621.9306 - val_mae: 621.9306\nEpoch 30/100\n18/18 [==============================] - 0s 4ms/step - loss: 116.8613 - mae: 116.8613 - val_loss: 604.4056 - val_mae: 604.4056\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 31/100\n18/18 [==============================] - 0s 4ms/step - loss: 111.9375 - mae: 111.9375 - val_loss: 609.3882 - val_mae: 609.3882\nEpoch 32/100\n18/18 [==============================] - 0s 4ms/step - loss: 112.4175 - mae: 112.4175 - val_loss: 603.0588 - val_mae: 603.0588\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 33/100\n18/18 [==============================] - 0s 4ms/step - loss: 112.6697 - mae: 112.6697 - val_loss: 645.6975 - val_mae: 645.6975\nEpoch 34/100\n18/18 [==============================] - 0s 4ms/step - loss: 111.9867 - mae: 111.9867 - val_loss: 604.7632 - val_mae: 604.7632\nEpoch 35/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.9451 - mae: 110.9451 - val_loss: 593.4648 - val_mae: 593.4648\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 36/100\n18/18 [==============================] - 0s 5ms/step - loss: 114.4816 - mae: 114.4816 - val_loss: 608.0073 - val_mae: 608.0073\nEpoch 37/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.2017 - mae: 110.2017 - val_loss: 597.2309 - val_mae: 597.2309\nEpoch 38/100\n18/18 [==============================] - 0s 5ms/step - loss: 112.2372 - mae: 112.2372 - val_loss: 637.9797 - val_mae: 637.9797\nEpoch 39/100\n18/18 [==============================] - 0s 4ms/step - loss: 115.1289 - mae: 115.1289 - val_loss: 587.4679 - val_mae: 587.4679\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 40/100\n18/18 [==============================] - 0s 5ms/step - loss: 110.0854 - mae: 110.0854 - val_loss: 592.7117 - val_mae: 592.7117\nEpoch 41/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.6343 - mae: 110.6343 - val_loss: 593.8997 - val_mae: 593.8997\nEpoch 42/100\n18/18 [==============================] - 0s 4ms/step - loss: 113.5762 - mae: 113.5762 - val_loss: 636.3674 - val_mae: 636.3674\nEpoch 43/100\n18/18 [==============================] - 0s 5ms/step - loss: 116.2286 - mae: 116.2286 - val_loss: 662.9264 - val_mae: 662.9264\nEpoch 44/100\n18/18 [==============================] - 0s 4ms/step - loss: 120.0192 - mae: 120.0192 - val_loss: 635.6360 - val_mae: 635.6360\nEpoch 45/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.9675 - mae: 110.9675 - val_loss: 601.9926 - val_mae: 601.9926\nEpoch 46/100\n18/18 [==============================] - 0s 4ms/step - loss: 111.6012 - mae: 111.6012 - val_loss: 593.3531 - val_mae: 593.3531\nEpoch 47/100\n18/18 [==============================] - 0s 6ms/step - loss: 109.6161 - mae: 109.6161 - val_loss: 637.0014 - val_mae: 637.0014\nEpoch 48/100\n18/18 [==============================] - 0s 5ms/step - loss: 109.1368 - mae: 109.1368 - val_loss: 598.4199 - val_mae: 598.4199\nEpoch 49/100\n18/18 [==============================] - 0s 5ms/step - loss: 112.4355 - mae: 112.4355 - val_loss: 579.7040 - val_mae: 579.7040\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 50/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.2108 - mae: 110.2108 - val_loss: 639.2326 - val_mae: 639.2326\nEpoch 51/100\n18/18 [==============================] - 0s 5ms/step - loss: 111.0958 - mae: 111.0958 - val_loss: 597.3575 - val_mae: 597.3575\nEpoch 52/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.7351 - mae: 110.7351 - val_loss: 580.7227 - val_mae: 580.7227\nEpoch 53/100\n18/18 [==============================] - 0s 5ms/step - loss: 111.1785 - mae: 111.1785 - val_loss: 648.3588 - val_mae: 648.3588\nEpoch 54/100\n18/18 [==============================] - 0s 4ms/step - loss: 114.0832 - mae: 114.0832 - val_loss: 593.2007 - val_mae: 593.2007\nEpoch 55/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.4910 - mae: 110.4910 - val_loss: 579.5065 - val_mae: 579.5065\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 56/100\n18/18 [==============================] - 0s 5ms/step - loss: 108.0489 - mae: 108.0489 - val_loss: 807.3851 - val_mae: 807.3851\nEpoch 57/100\n18/18 [==============================] - 0s 4ms/step - loss: 125.0614 - mae: 125.0614 - val_loss: 674.1654 - val_mae: 674.1654\nEpoch 58/100\n18/18 [==============================] - 0s 4ms/step - loss: 115.4340 - mae: 115.4340 - val_loss: 582.2698 - val_mae: 582.2698\nEpoch 59/100\n18/18 [==============================] - 0s 5ms/step - loss: 110.0881 - mae: 110.0881 - val_loss: 606.7637 - val_mae: 606.7637\nEpoch 60/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.7156 - mae: 108.7156 - val_loss: 602.3102 - val_mae: 602.3102\nEpoch 61/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.1525 - mae: 108.1525 - val_loss: 573.9990 - val_mae: 573.9990\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 62/100\n18/18 [==============================] - 0s 4ms/step - loss: 107.3727 - mae: 107.3727 - val_loss: 581.7012 - val_mae: 581.7012\nEpoch 63/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.7667 - mae: 110.7667 - val_loss: 637.5252 - val_mae: 637.5252\nEpoch 64/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.1539 - mae: 110.1539 - val_loss: 586.6601 - val_mae: 586.6601\nEpoch 65/100\n18/18 [==============================] - 0s 5ms/step - loss: 108.2325 - mae: 108.2325 - val_loss: 573.5620 - val_mae: 573.5620\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 66/100\n18/18 [==============================] - 0s 5ms/step - loss: 108.6825 - mae: 108.6825 - val_loss: 572.2206 - val_mae: 572.2206\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 67/100\n18/18 [==============================] - 0s 4ms/step - loss: 106.6371 - mae: 106.6371 - val_loss: 646.6349 - val_mae: 646.6349\nEpoch 68/100\n18/18 [==============================] - 0s 4ms/step - loss: 114.1603 - mae: 114.1603 - val_loss: 681.8561 - val_mae: 681.8561\nEpoch 69/100\n18/18 [==============================] - 0s 5ms/step - loss: 124.5514 - mae: 124.5514 - val_loss: 655.9885 - val_mae: 655.9885\nEpoch 70/100\n18/18 [==============================] - 0s 4ms/step - loss: 125.0235 - mae: 125.0235 - val_loss: 601.0032 - val_mae: 601.0032\nEpoch 71/100\n18/18 [==============================] - 0s 6ms/step - loss: 110.3652 - mae: 110.3652 - val_loss: 595.3962 - val_mae: 595.3962\nEpoch 72/100\n18/18 [==============================] - 0s 5ms/step - loss: 107.9285 - mae: 107.9285 - val_loss: 573.7085 - val_mae: 573.7085\nEpoch 73/100\n18/18 [==============================] - 0s 4ms/step - loss: 109.5085 - mae: 109.5085 - val_loss: 580.4180 - val_mae: 580.4180\nEpoch 74/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.7380 - mae: 108.7380 - val_loss: 576.1211 - val_mae: 576.1211\nEpoch 75/100\n18/18 [==============================] - 0s 5ms/step - loss: 107.9404 - mae: 107.9404 - val_loss: 591.1477 - val_mae: 591.1477\nEpoch 76/100\n18/18 [==============================] - 0s 5ms/step - loss: 109.4232 - mae: 109.4232 - val_loss: 597.8605 - val_mae: 597.8605\nEpoch 77/100\n18/18 [==============================] - 0s 4ms/step - loss: 107.5879 - mae: 107.5879 - val_loss: 571.9299 - val_mae: 571.9299\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 78/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.1598 - mae: 108.1598 - val_loss: 575.2383 - val_mae: 575.2383\nEpoch 79/100\n18/18 [==============================] - 0s 4ms/step - loss: 107.9175 - mae: 107.9175 - val_loss: 617.3071 - val_mae: 617.3071\nEpoch 80/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.9510 - mae: 108.9510 - val_loss: 583.4847 - val_mae: 583.4847\nEpoch 81/100\n18/18 [==============================] - 0s 5ms/step - loss: 106.0505 - mae: 106.0505 - val_loss: 570.0802 - val_mae: 570.0802\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 82/100\n18/18 [==============================] - 0s 4ms/step - loss: 115.6827 - mae: 115.6827 - val_loss: 575.7382 - val_mae: 575.7382\nEpoch 83/100\n18/18 [==============================] - 0s 5ms/step - loss: 110.9379 - mae: 110.9379 - val_loss: 659.6570 - val_mae: 659.6570\nEpoch 84/100\n18/18 [==============================] - 0s 5ms/step - loss: 111.4836 - mae: 111.4836 - val_loss: 570.1959 - val_mae: 570.1959\nEpoch 85/100\n18/18 [==============================] - 0s 4ms/step - loss: 107.5948 - mae: 107.5948 - val_loss: 601.5945 - val_mae: 601.5945\nEpoch 86/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.9426 - mae: 108.9426 - val_loss: 592.8107 - val_mae: 592.8107\nEpoch 87/100\n18/18 [==============================] - 0s 5ms/step - loss: 105.7717 - mae: 105.7717 - val_loss: 603.6169 - val_mae: 603.6169\nEpoch 88/100\n18/18 [==============================] - 0s 5ms/step - loss: 107.9217 - mae: 107.9217 - val_loss: 569.0500 - val_mae: 569.0500\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 89/100\n18/18 [==============================] - 0s 5ms/step - loss: 106.0344 - mae: 106.0344 - val_loss: 568.9512 - val_mae: 568.9512\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 90/100\n18/18 [==============================] - 0s 5ms/step - loss: 105.4977 - mae: 105.4977 - val_loss: 581.7681 - val_mae: 581.7681\nEpoch 91/100\n18/18 [==============================] - 0s 5ms/step - loss: 108.8468 - mae: 108.8468 - val_loss: 573.6023 - val_mae: 573.6023\nEpoch 92/100\n18/18 [==============================] - 0s 5ms/step - loss: 110.8884 - mae: 110.8884 - val_loss: 576.8247 - val_mae: 576.8247\nEpoch 93/100\n18/18 [==============================] - 0s 5ms/step - loss: 113.8781 - mae: 113.8781 - val_loss: 608.3018 - val_mae: 608.3018\nEpoch 94/100\n18/18 [==============================] - 0s 5ms/step - loss: 110.5763 - mae: 110.5763 - val_loss: 601.6047 - val_mae: 601.6047\nEpoch 95/100\n18/18 [==============================] - 0s 4ms/step - loss: 106.5906 - mae: 106.5906 - val_loss: 570.3652 - val_mae: 570.3652\nEpoch 96/100\n18/18 [==============================] - 0s 4ms/step - loss: 116.9515 - mae: 116.9515 - val_loss: 615.2581 - val_mae: 615.2581\nEpoch 97/100\n18/18 [==============================] - 0s 5ms/step - loss: 108.0739 - mae: 108.0739 - val_loss: 580.3073 - val_mae: 580.3073\nEpoch 98/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.7102 - mae: 108.7102 - val_loss: 586.6512 - val_mae: 586.6512\nEpoch 99/100\n18/18 [==============================] - 0s 5ms/step - loss: 109.0488 - mae: 109.0488 - val_loss: 570.0629 - val_mae: 570.0629\nEpoch 100/100\n18/18 [==============================] - 0s 4ms/step - loss: 106.1845 - mae: 106.1845 - val_loss: 585.9763 - val_mae: 585.9763\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdcf48bd110&gt;</pre> <p>Because of the small size of our data (less than 3000 total samples), the model trains very fast.</p> <p>Let's evaluate it.</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate model on test data\nmodel_1.evaluate(test_windows, test_labels)\n</pre> # Evaluate model on test data model_1.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 2ms/step - loss: 585.9762 - mae: 585.9762\n</pre> Out[\u00a0]: <pre>[585.9761962890625, 585.9761962890625]</pre> <p>You'll notice the model achieves the same <code>val_loss</code> (in this case, this is MAE) as the last epoch.</p> <p>But if we load in the version of <code>model_1</code> which was saved to file using the <code>ModelCheckpoint</code> callback, we should see an improvement in results.</p> In\u00a0[\u00a0]: Copied! <pre># Load in saved best performing model_1 and evaluate on test data\nmodel_1 = tf.keras.models.load_model(\"model_experiments/model_1_dense\")\nmodel_1.evaluate(test_windows, test_labels)\n</pre> # Load in saved best performing model_1 and evaluate on test data model_1 = tf.keras.models.load_model(\"model_experiments/model_1_dense\") model_1.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 3ms/step - loss: 568.9512 - mae: 568.9512\n</pre> Out[\u00a0]: <pre>[568.951171875, 568.951171875]</pre> <p>Much better! Due to the fluctuating performance of the model during training, loading back in the best performing model see's a sizeable improvement in MAE.</p> In\u00a0[\u00a0]: Copied! <pre>def make_preds(model, input_data):\n  \"\"\"\n  Uses model to make predictions on input_data.\n\n  Parameters\n  ----------\n  model: trained model \n  input_data: windowed input data (same kind of data model was trained on)\n\n  Returns model predictions on input_data.\n  \"\"\"\n  forecast = model.predict(input_data)\n  return tf.squeeze(forecast) # return 1D array of predictions\n</pre> def make_preds(model, input_data):   \"\"\"   Uses model to make predictions on input_data.    Parameters   ----------   model: trained model    input_data: windowed input data (same kind of data model was trained on)    Returns model predictions on input_data.   \"\"\"   forecast = model.predict(input_data)   return tf.squeeze(forecast) # return 1D array of predictions <p>Nice!</p> <p>Now let's use our <code>make_preds()</code> and see how it goes.</p> In\u00a0[\u00a0]: Copied! <pre># Make predictions using model_1 on the test dataset and view the results\nmodel_1_preds = make_preds(model_1, test_windows)\nlen(model_1_preds), model_1_preds[:10]\n</pre> # Make predictions using model_1 on the test dataset and view the results model_1_preds = make_preds(model_1, test_windows) len(model_1_preds), model_1_preds[:10] Out[\u00a0]: <pre>(556, &lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\n array([8861.711, 8769.886, 9015.71 , 8795.517, 8723.809, 8730.11 ,\n        8691.95 , 8502.054, 8460.961, 8516.547], dtype=float32)&gt;)</pre> <p>\ud83d\udd11 Note: With these outputs, our model isn't forecasting yet. It's only making predictions on the test dataset. Forecasting would involve a model making predictions into the future, however, the test dataset is only a pseudofuture.</p> <p>Excellent! Now we've got some prediction values, let's use the <code>evaluate_preds()</code> we created before to compare them to the ground truth.</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate preds\nmodel_1_results = evaluate_preds(y_true=tf.squeeze(test_labels), # reduce to right shape\n                                 y_pred=model_1_preds)\nmodel_1_results\n</pre> # Evaluate preds model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels), # reduce to right shape                                  y_pred=model_1_preds) model_1_results Out[\u00a0]: <pre>{'mae': 568.95123,\n 'mape': 2.5448983,\n 'mase': 0.9994897,\n 'mse': 1171744.0,\n 'rmse': 1082.4713}</pre> <p>How did our model go? Did it beat the na\u00efve forecast?</p> In\u00a0[\u00a0]: Copied! <pre>naive_results\n</pre> naive_results Out[\u00a0]: <pre>{'mae': 567.9802,\n 'mape': 2.516525,\n 'mase': 0.99957,\n 'mse': 1147547.0,\n 'rmse': 1071.2362}</pre> <p>It looks like our na\u00efve model beats our first deep model on nearly every metric.</p> <p>That goes to show the power of the na\u00efve model and the reason for having a baseline for any machine learning project.</p> <p>And of course, no evaluation would be finished without visualizing the results.</p> <p>Let's use the <code>plot_time_series()</code> function to plot <code>model_1_preds</code> against the test data.</p> In\u00a0[\u00a0]: Copied! <pre>offset = 300\nplt.figure(figsize=(10, 7))\n# Account for the test_window offset and index into test_labels to ensure correct plotting\nplot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\")\nplot_time_series(timesteps=X_test[-len(test_windows):], values=model_1_preds, start=offset, format=\"-\", label=\"model_1_preds\")\n</pre> offset = 300 plt.figure(figsize=(10, 7)) # Account for the test_window offset and index into test_labels to ensure correct plotting plot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\") plot_time_series(timesteps=X_test[-len(test_windows):], values=model_1_preds, start=offset, format=\"-\", label=\"model_1_preds\") <p>What's wrong with these predictions?</p> <p>As mentioned before, they're on the test dataset. So they're not actual forecasts.</p> <p>With our current model setup, how do you think we'd make forecasts for the future?</p> <p>Have a think about it for now, we'll cover this later on.</p> In\u00a0[\u00a0]: Copied! <pre>HORIZON = 1 # predict one step at a time\nWINDOW_SIZE = 30 # use 30 timesteps in the past\n</pre> HORIZON = 1 # predict one step at a time WINDOW_SIZE = 30 # use 30 timesteps in the past In\u00a0[\u00a0]: Copied! <pre># Make windowed data with appropriate horizon and window sizes\nfull_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n</pre> # Make windowed data with appropriate horizon and window sizes full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON) len(full_windows), len(full_labels) Out[\u00a0]: <pre>(2757, 2757)</pre> In\u00a0[\u00a0]: Copied! <pre># Make train and testing windows\ntrain_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)\n</pre> # Make train and testing windows train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels) len(train_windows), len(test_windows), len(train_labels), len(test_labels) Out[\u00a0]: <pre>(2205, 552, 2205, 552)</pre> <p>Data prepared!</p> <p>Now let's construct <code>model_2</code>, a model with the same architecture as <code>model_1</code> as well as the same training routine.</p> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Create model (same model as model 1 but data input will be different)\nmodel_2 = tf.keras.Sequential([\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(HORIZON) # need to predict horizon number of steps into the future\n], name=\"model_2_dense\")\n\nmodel_2.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\nmodel_2.fit(train_windows,\n            train_labels,\n            epochs=100,\n            batch_size=128,\n            verbose=0,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_2.name)])\n</pre> tf.random.set_seed(42)  # Create model (same model as model 1 but data input will be different) model_2 = tf.keras.Sequential([   layers.Dense(128, activation=\"relu\"),   layers.Dense(HORIZON) # need to predict horizon number of steps into the future ], name=\"model_2_dense\")  model_2.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam())  model_2.fit(train_windows,             train_labels,             epochs=100,             batch_size=128,             verbose=0,             validation_data=(test_windows, test_labels),             callbacks=[create_model_checkpoint(model_name=model_2.name)]) <pre>INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdcf1094290&gt;</pre> <p>Once again, training goes nice and fast.</p> <p>Let's evaluate our model's performance.</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate model 2 preds\nmodel_2.evaluate(test_windows, test_labels)\n</pre> # Evaluate model 2 preds model_2.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 2ms/step - loss: 608.9615\n</pre> Out[\u00a0]: <pre>608.9614868164062</pre> <p>Hmmm... is that the best it did?</p> <p>How about we try loading in the best performing <code>model_2</code> which was saved to file thanks to our <code>ModelCheckpoint</code> callback.</p> In\u00a0[\u00a0]: Copied! <pre># Load in best performing model\nmodel_2 = tf.keras.models.load_model(\"model_experiments/model_2_dense/\")\nmodel_2.evaluate(test_windows, test_labels)\n</pre> # Load in best performing model model_2 = tf.keras.models.load_model(\"model_experiments/model_2_dense/\") model_2.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 2ms/step - loss: 608.9615\n</pre> Out[\u00a0]: <pre>608.9614868164062</pre> <p>Excellent! Loading back in the best performing model see's a performance boost.</p> <p>But let's not stop there, let's make some predictions with <code>model_2</code> and then evaluate them just as we did before.</p> In\u00a0[\u00a0]: Copied! <pre># Get forecast predictions\nmodel_2_preds = make_preds(model_2,\n                           input_data=test_windows)\n</pre> # Get forecast predictions model_2_preds = make_preds(model_2,                            input_data=test_windows) In\u00a0[\u00a0]: Copied! <pre># Evaluate results for model 2 predictions\nmodel_2_results = evaluate_preds(y_true=tf.squeeze(test_labels), # remove 1 dimension of test labels\n                                 y_pred=model_2_preds)\nmodel_2_results\n</pre> # Evaluate results for model 2 predictions model_2_results = evaluate_preds(y_true=tf.squeeze(test_labels), # remove 1 dimension of test labels                                  y_pred=model_2_preds) model_2_results Out[\u00a0]: <pre>{'mae': 608.9615,\n 'mape': 2.7693386,\n 'mase': 1.0644706,\n 'mse': 1281438.8,\n 'rmse': 1132.0065}</pre> <p>It looks like <code>model_2</code> performs worse than the na\u00efve model as well as <code>model_1</code>!</p> <p>Does this mean a smaller window size is better? (I'll leave this as a challenge you can experiment with)</p> <p>How do the predictions look?</p> In\u00a0[\u00a0]: Copied! <pre>offset = 300\nplt.figure(figsize=(10, 7))\n# Account for the test_window offset\nplot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"test_data\")\nplot_time_series(timesteps=X_test[-len(test_windows):], values=model_2_preds, start=offset, format=\"-\", label=\"model_2_preds\") \n</pre> offset = 300 plt.figure(figsize=(10, 7)) # Account for the test_window offset plot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"test_data\") plot_time_series(timesteps=X_test[-len(test_windows):], values=model_2_preds, start=offset, format=\"-\", label=\"model_2_preds\")  In\u00a0[\u00a0]: Copied! <pre>HORIZON = 7\nWINDOW_SIZE = 30\n\nfull_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n</pre> HORIZON = 7 WINDOW_SIZE = 30  full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON) len(full_windows), len(full_labels) Out[\u00a0]: <pre>(2751, 2751)</pre> <p>And we'll split the full dataset windows into training and test sets.</p> In\u00a0[\u00a0]: Copied! <pre>train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels, test_split=0.2)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)\n</pre> train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels, test_split=0.2) len(train_windows), len(test_windows), len(train_labels), len(test_labels) Out[\u00a0]: <pre>(2200, 551, 2200, 551)</pre> <p>Now let's build, compile, fit and evaluate a model.</p> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Create model (same as model_1 except with different data input size)\nmodel_3 = tf.keras.Sequential([\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(HORIZON)\n], name=\"model_3_dense\")\n\nmodel_3.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\nmodel_3.fit(train_windows,\n            train_labels,\n            batch_size=128,\n            epochs=100,\n            verbose=0,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_3.name)])\n</pre> tf.random.set_seed(42)  # Create model (same as model_1 except with different data input size) model_3 = tf.keras.Sequential([   layers.Dense(128, activation=\"relu\"),   layers.Dense(HORIZON) ], name=\"model_3_dense\")  model_3.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam())  model_3.fit(train_windows,             train_labels,             batch_size=128,             epochs=100,             verbose=0,             validation_data=(test_windows, test_labels),             callbacks=[create_model_checkpoint(model_name=model_3.name)]) <pre>INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdcf1d36350&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># How did our model with a larger window size and horizon go?\nmodel_3.evaluate(test_windows, test_labels)\n</pre> # How did our model with a larger window size and horizon go? model_3.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 2ms/step - loss: 1321.5201\n</pre> Out[\u00a0]: <pre>1321.5201416015625</pre> <p>To compare apples to apples (best performing model to best performing model), we've got to load in the best version of <code>model_3</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Load in best version of model_3 and evaluate\nmodel_3 = tf.keras.models.load_model(\"model_experiments/model_3_dense/\")\nmodel_3.evaluate(test_windows, test_labels)\n</pre> # Load in best version of model_3 and evaluate model_3 = tf.keras.models.load_model(\"model_experiments/model_3_dense/\") model_3.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 2ms/step - loss: 1237.5063\n</pre> Out[\u00a0]: <pre>1237.50634765625</pre> <p>In this case, the error will be higher because we're predicting 7 steps at a time.</p> <p>This makes sense though because the further you try and predict, the larger your error will be (think of trying to predict the weather 7 days in advance).</p> <p>Let's make predictions with our model using the <code>make_preds()</code> function and evaluate them using the <code>evaluate_preds()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># The predictions are going to be 7 steps at a time (this is the HORIZON size)\nmodel_3_preds = make_preds(model_3,\n                           input_data=test_windows)\nmodel_3_preds[:5]\n</pre> # The predictions are going to be 7 steps at a time (this is the HORIZON size) model_3_preds = make_preds(model_3,                            input_data=test_windows) model_3_preds[:5] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(5, 7), dtype=float32, numpy=\narray([[9004.693 , 9048.1   , 9425.088 , 9258.258 , 9495.798 , 9558.451 ,\n        9357.354 ],\n       [8735.507 , 8840.304 , 9247.793 , 8885.6   , 9097.188 , 9174.328 ,\n        9156.819 ],\n       [8672.508 , 8782.388 , 9123.8545, 8770.37  , 9007.13  , 9003.87  ,\n        9042.724 ],\n       [8874.398 , 8784.737 , 9043.901 , 8943.051 , 9033.479 , 9176.488 ,\n        9039.676 ],\n       [8825.891 , 8777.4375, 8926.779 , 8870.178 , 9213.232 , 9268.156 ,\n        8942.485 ]], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Calculate model_3 results - these are going to be multi-dimensional because\n# we're trying to predict more than one step at a time.\nmodel_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n                                 y_pred=model_3_preds)\nmodel_3_results\n</pre> # Calculate model_3 results - these are going to be multi-dimensional because # we're trying to predict more than one step at a time. model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),                                  y_pred=model_3_preds) model_3_results Out[\u00a0]: <pre>{'mae': array([ 513.60516 ,  355.08356 ,  327.17035 ,  358.50977 ,  420.53207 ,\n         537.8539  ,  545.6606  ,  485.92307 ,  584.4969  ,  687.3814  ,\n         836.22675 ,  755.1571  ,  731.4959  ,  775.3398  ,  567.9548  ,\n         266.80865 ,  188.80217 ,  188.1077  ,  253.09521 ,  301.4336  ,\n         151.10742 ,  196.81424 ,  191.46184 ,  231.65067 ,  143.6114  ,\n         122.59033 ,  132.78844 ,  190.8116  ,  179.1598  ,  228.25949 ,\n         314.44016 ,  379.093   ,  278.3254  ,  295.34604 ,  299.38525 ,\n         248.64963 ,  299.75635 ,  259.6937  ,  180.30559 ,  206.72887 ,\n         374.62906 ,  144.85129 ,  142.33607 ,  131.1158  ,   93.94057 ,\n          54.825542,   73.7943  ,  103.59989 ,  121.3337  ,  168.67209 ,\n         183.90945 ,  152.25314 ,  186.57137 ,  146.91309 ,  240.42955 ,\n         351.00668 ,  540.9516  ,  549.15674 ,  521.2422  ,  526.8553  ,\n         453.36313 ,  257.98166 ,  277.29492 ,  301.82465 ,  455.71756 ,\n         458.96002 ,  503.44427 ,  522.3119  ,  223.07687 ,  250.09473 ,\n         297.14468 ,  400.56976 ,  495.79785 ,  364.33664 ,  283.3654  ,\n         325.59457 ,  238.21178 ,  318.9777  ,  460.77246 ,  651.0755  ,\n         835.88074 ,  669.9654  ,  319.5452  ,  261.99496 ,  142.39217 ,\n         136.62834 ,  154.75252 ,  221.32631 ,  290.50446 ,  503.8846  ,\n         414.2602  ,  434.35727 ,  377.98926 ,  251.7899  ,  204.28418 ,\n         388.22684 ,  360.65945 ,  493.80902 ,  614.86035 ,  754.7017  ,\n         533.7708  ,  378.98898 ,  280.49484 ,  339.48062 ,  413.12875 ,\n         452.87933 ,  550.53906 ,  634.57214 ,  935.57227 ,  931.6003  ,\n         881.2804  ,  426.40094 ,  179.45885 ,  121.66225 ,  160.43806 ,\n         372.07037 ,  341.85776 ,  476.52475 ,  618.3239  , 1038.8976  ,\n        1569.5022  , 2157.1196  , 1987.6074  , 2158.6108  , 2303.5603  ,\n        2662.9421  , 1405.5017  ,  728.30145 ,  351.70047 ,  322.03168 ,\n         493.94742 ,  435.48492 ,  565.55615 ,  350.1165  ,  289.0203  ,\n         251.21645 ,  409.05566 ,  342.2103  ,  320.83594 ,  330.88596 ,\n         357.8457  ,  335.9481  ,  206.1031  ,  544.3837  ,  700.0093  ,\n         468.91965 ,  404.4963  ,  172.80176 ,  308.33664 ,  210.47566 ,\n         318.95444 ,  486.1319  ,  428.87982 ,  533.91095 ,  433.7813  ,\n         396.89447 ,  138.40346 ,  189.96617 ,  170.39133 ,  181.54387 ,\n         282.8902  ,  264.3889  ,  250.62172 ,  240.33713 ,  276.9541  ,\n         326.0306  ,  489.61572 ,  686.2451  ,  526.9798  ,  603.0119  ,\n         825.38275 ,  871.04694 ,  990.06903 , 1090.0593  ,  560.2842  ,\n         310.5219  ,  371.39035 ,  348.45996 ,  355.73465 ,  429.56473 ,\n         581.2839  ,  550.5218  ,  635.4312  ,  913.40375 ,  840.71844 ,\n         305.03488 ,  493.93414 ,  751.3177  ,  410.63434 ,  220.62459 ,\n         282.25473 ,  291.85352 ,  422.50293 ,  458.65375 ,  637.2345  ,\n         647.82367 ,  417.24442 ,  220.23717 ,  246.72168 ,  200.22935 ,\n         455.14926 ,  719.6058  ,  696.3037  ,  485.09836 ,  294.9534  ,\n         170.52371 ,  211.82463 ,  270.56488 ,  189.89355 ,  171.21721 ,\n         366.4471  ,  231.96303 ,  318.78726 ,  273.79352 ,  358.55582 ,\n         412.22797 ,  512.31573 ,  185.43848 ,  196.38113 ,  200.00586 ,\n         224.58678 ,  213.41965 ,  186.23926 ,  113.37096 ,  172.23117 ,\n         168.89885 ,  236.09584 ,  307.59683 ,  328.93903 ,  566.5961  ,\n         285.04282 ,  300.4495  ,  125.45201 ,  168.94322 ,  137.25754 ,\n         143.50404 ,  145.27776 ,  107.340126,   77.16044 ,  131.87096 ,\n         134.01953 ,  167.45871 ,  137.92188 ,  148.91281 ,  204.95467 ,\n         157.89732 ,  196.95201 ,  167.93861 ,  156.45578 ,  188.6808  ,\n         161.44113 ,   90.997765,  136.84096 ,  198.51799 ,  230.13881 ,\n         294.7839  ,  594.24286 ,  699.64856 ,  815.137   ,  905.33887 ,\n        1127.2999  , 1342.6952  , 1317.0686  ,  590.1699  ,  296.34543 ,\n         243.4368  ,  256.3987  ,  222.28488 ,  323.8387  ,   82.51339 ,\n         120.14453 ,  249.14857 ,  205.73674 ,  243.45451 ,  250.64857 ,\n         287.27567 ,  224.3803  ,  266.55972 ,  221.50935 ,  218.49539 ,\n         272.42242 ,  279.5964  ,  252.58775 ,  381.56473 ,  417.97028 ,\n         624.5562  ,  368.0364  ,  327.25473 ,  263.4396  ,  349.95242 ,\n         398.62485 ,  297.07465 ,  147.04924 ,  164.54367 ,  313.36246 ,\n         477.7486  ,  675.042   ,  897.269   , 1094.7098  , 1460.177   ,\n        1398.5858  ,  952.72375 ,  645.9594  ,  166.17912 ,  144.66867 ,\n         189.1822  ,  304.81375 ,  435.10742 ,  449.64426 ,  425.244   ,\n         441.93637 ,  407.29605 ,  252.4036  ,  248.36928 ,  336.17062 ,\n         482.8111  ,  437.53043 ,  533.0855  ,  346.98047 ,  127.68722 ,\n         110.208565,  301.75223 ,  195.50697 ,  174.65248 ,  238.4707  ,\n         302.3711  ,  313.51703 ,  310.1289  ,  200.29701 ,  172.47209 ,\n         140.2302  ,  252.48479 ,  289.32407 ,  343.62222 ,  504.11816 ,\n         635.1339  ,  602.131   ,  519.0612  ,  214.90291 ,  195.91197 ,\n         265.03683 ,  198.9008  ,  345.51227 ,  517.22235 ,  631.02997 ,\n         988.4249  , 1174.2136  , 1196.2849  , 1253.93    ,  526.06573 ,\n         210.31306 ,  215.27205 ,  169.86008 ,  283.78543 ,  269.76117 ,\n         228.63477 ,  186.64719 ,  410.5434  ,  601.2659  ,  618.12164 ,\n         768.7538  , 1158.5449  , 1232.8798  , 1254.429   ,  423.02524 ,\n         390.03613 ,  367.23926 ,  209.55803 ,  530.2231  ,  821.38293 ,\n         812.37195 ,  741.19464 ,  953.48285 , 1258.9928  , 1844.9332  ,\n        1605.2852  , 1112.7673  ,  594.18945 ,  549.7676  ,  632.5438  ,\n         829.2656  , 1103.2213  , 1130.1024  , 1033.3527  ,  878.5851  ,\n         595.4096  , 1115.6515  , 1371.1725  , 1385.25    ,  387.52484 ,\n         303.74945 ,  495.12305 ,  719.5804  ,  648.9032  ,  766.1624  ,\n         683.23157 ,  596.4121  ,  523.09235 ,  577.34015 , 1337.6241  ,\n        2454.597   , 2759.2363  , 3135.8757  , 3407.3806  , 3602.6362  ,\n        2527.1584  , 1158.1016  ,  679.84375 ,  748.5586  , 1073.4896  ,\n        1217.7305  , 1459.5664  , 2502.7336  , 3075.8264  , 3090.2869  ,\n        2564.758   , 2660.5317  , 2928.9348  , 3690.5566  , 3525.1433  ,\n        4183.6704  , 5144.0693  , 4384.6533  , 4164.9614  , 4431.0967  ,\n        3335.121   , 3017.6858  , 2589.5403  , 4103.6313  , 5582.089   ,\n        5108.456   , 1382.9648  ,  953.5435  , 1081.6842  , 2483.1992  ,\n        2992.056   , 3127.0942  , 2972.2717  , 3054.6477  , 3283.7107  ,\n        3617.3652  , 1170.2556  , 1074.1886  , 1059.3181  , 1044.5385  ,\n        1060.6202  , 2115.7234  , 3569.8901  , 3474.3647  , 2448.0173  ,\n        2641.4202  , 3188.6016  , 4899.913   , 5516.293   , 4635.4883  ,\n        4677.036   , 5732.8804  , 5866.7344  , 8083.341   , 5049.8237  ,\n        1476.2316  , 1873.3314  , 1219.1033  , 2324.0698  , 2467.0044  ,\n        3005.0864  , 2805.0486  , 3688.2556  , 2962.7131  , 3664.236   ,\n        5618.6836  , 6925.0913  , 9751.091   , 8790.822   , 5057.624   ,\n        2971.6863  , 1715.495   , 1125.7689  , 2201.432   , 3969.5005  ,\n        2988.5112  , 2911.9336  , 2796.869   , 3937.8755  , 5449.5913  ,\n        6395.02    , 6148.2524  , 5437.706   , 4291.8267  , 2362.3962  ,\n        1657.3444  , 1426.63    , 1647.7142  , 2730.3962  , 1813.716   ,\n        2061.149   , 3095.72    , 4312.8096  , 5500.7227  , 5258.029   ,\n        5152.029   , 1331.4045  , 1634.4017  , 2493.4336  , 3957.9548  ,\n        4499.2153  , 2215.8533  , 2587.4136  , 1622.4392  ,  591.8778  ,\n        1297.4688  , 1284.5931  ,  915.22656 ,  940.4565  , 1084.6562  ,\n         875.89954 , 1041.3527  , 2299.2512  , 3152.6038  , 3518.3923  ,\n        2531.5212  , 2682.5737  , 3094.2947  , 3829.3286  , 5550.609   ,\n        7159.343   , 8820.526   , 8587.338   , 6777.814   , 5193.2866  ,\n        4793.332   , 2605.2517  , 1668.3544  , 2510.7244  , 4105.6865  ,\n        6278.3906  , 4109.759   , 1711.1211  , 2261.9878  , 2384.754   ,\n        1246.6602  , 1672.8494  , 1103.1322  , 1743.3594  , 1608.452   ,\n        1933.8995  , 2742.3455  , 3945.3633  , 5765.9414  , 6955.8384  ,\n        8107.707   ], dtype=float32),\n 'mape': array([ 5.8601456 ,  4.087344  ,  3.7758112 ,  4.187648  ,  4.9781313 ,\n         6.4483633 ,  6.614449  ,  6.0616755 ,  7.547588  ,  9.095916  ,\n        11.300213  , 10.396466  , 10.109091  , 10.697323  ,  7.8628383 ,\n         3.6872823 ,  2.585636  ,  2.505     ,  3.3504546 ,  4.013595  ,\n         2.0036254 ,  2.6397336 ,  2.6087892 ,  3.160931  ,  1.9612147 ,\n         1.6679796 ,  1.7952247 ,  2.5901198 ,  2.436256  ,  3.1327312 ,\n         4.3595414 ,  5.2856445 ,  3.9403565 ,  4.3142104 ,  4.3733163 ,\n         3.6376827 ,  4.362974  ,  3.7757344 ,  2.6192985 ,  2.9007592 ,\n         5.1501136 ,  2.014742  ,  1.9706616 ,  1.8104095 ,  1.3007166 ,\n         0.75351447,  1.0192169 ,  1.4282547 ,  1.6741827 ,  2.3589606 ,\n         2.563099  ,  2.1327207 ,  2.5974777 ,  2.017332  ,  3.1387668 ,\n         4.502132  ,  6.959625  ,  6.9602156 ,  6.5331526 ,  6.5757833 ,\n         5.610295  ,  3.0642097 ,  3.2474015 ,  3.4847279 ,  5.222716  ,\n         5.229475  ,  5.727943  ,  5.9273834 ,  2.5284538 ,  2.8663971 ,\n         3.4309018 ,  4.6874056 ,  5.841506  ,  4.30981   ,  3.3552744 ,\n         3.8162014 ,  2.7363582 ,  3.4623704 ,  4.971694  ,  7.0260944 ,\n         8.974986  ,  7.167823  ,  3.401335  ,  2.7761257 ,  1.522323  ,\n         1.4535459 ,  1.6354691 ,  2.252781  ,  2.9475648 ,  5.073901  ,\n         4.1032834 ,  4.295298  ,  3.7052956 ,  2.4620814 ,  2.013232  ,\n         3.8428285 ,  3.6306674 ,  5.0162754 ,  6.2794676 ,  7.757465  ,\n         5.495671  ,  3.8907118 ,  2.8546908 ,  3.531113  ,  4.464891  ,\n         5.0162473 ,  6.1257253 ,  7.1294317 , 10.711446  , 10.66408   ,\n        10.087247  ,  4.893642  ,  2.071729  ,  1.362841  ,  1.8011061 ,\n         4.219873  ,  4.055696  ,  5.744903  ,  7.492872  , 15.032968  ,\n        24.38846   , 35.42812   , 34.702423  , 39.86109   , 42.83657   ,\n        49.6046    , 26.081123  , 13.669959  ,  6.360867  ,  5.4986367 ,\n         7.9925113 ,  6.8575478 ,  8.82061   ,  5.311197  ,  4.5279207 ,\n         3.870413  ,  6.2283797 ,  5.2470355 ,  5.1045485 ,  5.329424  ,\n         5.7789664 ,  5.358487  ,  3.2211263 ,  8.118441  , 10.231978  ,\n         6.726861  ,  5.757309  ,  2.4041245 ,  4.2864537 ,  2.977692  ,\n         4.454852  ,  6.9261975 ,  6.1590724 ,  7.7638254 ,  6.308201  ,\n         5.7371497 ,  1.9719449 ,  2.7191157 ,  2.3998728 ,  2.568591  ,\n         3.953004  ,  3.6584775 ,  3.453558  ,  3.301869  ,  3.7942226 ,\n         4.310324  ,  6.4319835 ,  8.567106  ,  6.293662  ,  7.0238814 ,\n         9.469167  ,  9.915066  , 11.217036  , 12.28066   ,  6.209284  ,\n         3.2802734 ,  3.8508573 ,  3.6124747 ,  3.7172396 ,  4.6127787 ,\n         6.2338514 ,  6.037696  ,  7.1359625 , 10.132258  ,  9.337389  ,\n         3.379447  ,  5.190343  ,  7.822919  ,  4.2644997 ,  2.328503  ,\n         3.0320923 ,  3.122556  ,  4.5721846 ,  5.013454  ,  7.0593185 ,\n         7.180876  ,  4.6288185 ,  2.4294462 ,  2.6893413 ,  2.1346936 ,\n         4.6703887 ,  7.429678  ,  7.177754  ,  4.9274387 ,  2.969763  ,\n         1.7036035 ,  2.1020818 ,  2.7900844 ,  1.9541025 ,  1.7562655 ,\n         3.8170404 ,  2.4557195 ,  3.335657  ,  2.8912652 ,  3.8006802 ,\n         4.3685193 ,  5.4371476 ,  1.96905   ,  2.086854  ,  2.1329875 ,\n         2.4014482 ,  2.2829742 ,  1.9878384 ,  1.2127163 ,  1.8306142 ,\n         1.8156711 ,  2.5207567 ,  3.2923858 ,  3.5837193 ,  6.1877694 ,\n         3.1183949 ,  3.2884538 ,  1.3740205 ,  1.8472785 ,  1.5017135 ,\n         1.5671413 ,  1.5889224 ,  1.1630745 ,  0.83710796,  1.4223018 ,\n         1.4407477 ,  1.8030001 ,  1.4797007 ,  1.599079  ,  2.2175813 ,\n         1.7130904 ,  2.1422603 ,  1.8232663 ,  1.7015574 ,  2.0563016 ,\n         1.7596645 ,  0.98229814,  1.4560648 ,  2.1070251 ,  2.4114208 ,\n         3.0499995 ,  5.7738442 ,  6.618569  ,  7.495159  ,  8.227665  ,\n        10.186895  , 11.928129  , 11.676125  ,  5.199727  ,  2.5929737 ,\n         2.1249924 ,  2.2335236 ,  1.9233094 ,  2.846623  ,  0.7076666 ,\n         1.0350616 ,  2.132228  ,  1.778991  ,  2.1161108 ,  2.1635604 ,\n         2.4792838 ,  1.9270526 ,  2.2551262 ,  1.8457135 ,  1.8112589 ,\n         2.262672  ,  2.3515143 ,  2.1173332 ,  3.2111585 ,  3.5736024 ,\n         5.3715696 ,  3.18694   ,  2.858538  ,  2.295805  ,  3.0580947 ,\n         3.4806054 ,  2.5900245 ,  1.2625037 ,  1.4050376 ,  2.7622259 ,\n         4.3049536 ,  6.2830725 ,  8.513568  , 10.517086  , 14.161078  ,\n        13.640608  ,  9.320237  ,  6.3080745 ,  1.6300542 ,  1.3983166 ,\n         1.8191801 ,  2.8875985 ,  4.068832  ,  4.1736484 ,  3.92286   ,\n         4.0472727 ,  3.7387483 ,  2.3069727 ,  2.3063238 ,  3.1941137 ,\n         4.5895944 ,  4.157942  ,  5.0558047 ,  3.3005743 ,  1.2102847 ,\n         1.037109  ,  2.8088598 ,  1.8151615 ,  1.6353209 ,  2.2353542 ,\n         2.8412042 ,  2.952003  ,  2.9187465 ,  1.8913074 ,  1.6178632 ,\n         1.2982845 ,  2.3039308 ,  2.6034784 ,  3.017652  ,  4.444261  ,\n         5.5989385 ,  5.26695   ,  4.535601  ,  1.8671715 ,  1.7055075 ,\n         2.3196962 ,  1.7318225 ,  2.7897227 ,  4.062603  ,  4.8944497 ,\n         7.6816416 ,  9.107798  ,  9.207181  ,  9.502263  ,  3.9754786 ,\n         1.5744586 ,  1.5997065 ,  1.242373  ,  2.0770247 ,  1.974714  ,\n         1.6764196 ,  1.3502706 ,  2.7858412 ,  3.9915662 ,  4.0835853 ,\n         5.016299  ,  7.6388373 ,  8.078356  ,  8.1091385 ,  2.6612654 ,\n         2.4994845 ,  2.2963924 ,  1.304031  ,  3.2585588 ,  4.9145164 ,\n         4.7377896 ,  4.253874  ,  5.353144  ,  6.9800787 , 10.170477  ,\n         8.772201  ,  6.0037975 ,  3.184015  ,  3.0090022 ,  3.637018  ,\n         4.7740936 ,  6.289234  ,  6.4191875 ,  5.904088  ,  4.9963098 ,\n         3.1906037 ,  5.8562336 ,  7.18103   ,  7.211652  ,  2.0219958 ,\n         1.5912417 ,  2.624319  ,  3.8534112 ,  3.5140653 ,  4.1405296 ,\n         3.6843975 ,  3.215591  ,  2.8228219 ,  2.8898468 ,  6.2849083 ,\n        11.458009  , 12.342688  , 13.764669  , 14.824157  , 15.566204  ,\n        10.829556  ,  4.9278235 ,  2.8345613 ,  3.0226748 ,  4.311801  ,\n         4.7200484 ,  5.516797  ,  9.303045  , 11.212711  , 11.037065  ,\n         8.631595  ,  8.57773   ,  9.389048  , 11.497772  , 10.463535  ,\n        11.683612  , 13.823776  , 11.273857  , 10.805961  , 11.400717  ,\n         8.4966135 ,  7.8893795 ,  7.169477  , 11.423848  , 15.540573  ,\n        14.24638   ,  3.8430922 ,  2.545029  ,  2.926236  ,  7.288314  ,\n         8.999365  ,  9.586316  ,  9.271836  ,  9.423251  , 10.242945  ,\n        11.396603  ,  3.6557918 ,  3.3454742 ,  3.2290876 ,  3.159098  ,\n         3.1852577 ,  6.1412444 , 10.163153  ,  9.673946  ,  6.657339  ,\n         6.975828  ,  8.377453  , 12.244584  , 13.00238   , 10.547294  ,\n        10.269914  , 12.4054    , 12.598526  , 17.187769  , 10.61679   ,\n         3.0717697 ,  3.7662785 ,  2.3915377 ,  4.4263554 ,  4.6217527 ,\n         5.4785805 ,  5.1152287 ,  6.826434  ,  5.61515   ,  7.0909567 ,\n        11.599258  , 14.51015   , 20.678543  , 18.670784  , 10.813841  ,\n         6.395925  ,  3.68131   ,  2.323965  ,  4.4593787 ,  8.030951  ,\n         5.9532943 ,  5.6428704 ,  5.1379447 ,  7.1625338 ,  9.831415  ,\n        11.209798  , 10.556711  ,  9.350725  ,  7.3507624 ,  3.9944    ,\n         2.8233469 ,  2.3929296 ,  2.8540497 ,  4.729086  ,  3.236382  ,\n         3.6810744 ,  5.6685753 ,  8.005048  , 10.247115  ,  9.747856  ,\n         9.5460415 ,  2.4771707 ,  2.9854116 ,  4.3493814 ,  6.8806467 ,\n         7.758685  ,  3.8011405 ,  4.4251776 ,  2.7610898 ,  1.0093751 ,\n         2.2422    ,  2.2289147 ,  1.58688   ,  1.6239213 ,  1.8814766 ,\n         1.5162641 ,  1.7166423 ,  3.7594385 ,  5.0978    ,  5.664717  ,\n         4.0754285 ,  4.3486743 ,  5.1419683 ,  6.618967  ,  9.823991  ,\n        12.98137   , 16.388018  , 16.260675  , 13.249782  , 10.180592  ,\n         9.383685  ,  5.1218452 ,  3.2893803 ,  4.634893  ,  7.4068375 ,\n        11.284324  ,  7.287351  ,  3.0196326 ,  4.006139  ,  4.172643  ,\n         2.233759  ,  3.0054173 ,  1.9702865 ,  3.1210938 ,  2.7774746 ,\n         3.4924886 ,  5.1419373 ,  7.6572022 , 11.470654  , 14.311543  ,\n        17.28017   ], dtype=float32),\n 'mase': 2.2020736,\n 'mse': array([3.15562156e+05, 1.69165547e+05, 1.44131812e+05, 1.76002922e+05,\n        2.63519750e+05, 3.91517188e+05, 3.99524688e+05, 3.44422312e+05,\n        4.93892156e+05, 6.88785625e+05, 9.18703562e+05, 8.00988125e+05,\n        6.35508625e+05, 6.45535125e+05, 3.57740000e+05, 1.03007117e+05,\n        5.33132578e+04, 4.65829805e+04, 1.10349188e+05, 1.11647695e+05,\n        4.35359297e+04, 5.45387773e+04, 4.55849375e+04, 7.64886406e+04,\n        4.03074453e+04, 2.71072188e+04, 1.90268887e+04, 4.55625000e+04,\n        4.25345820e+04, 6.63246172e+04, 1.16741711e+05, 1.69440984e+05,\n        1.10486977e+05, 1.59707406e+05, 1.61631141e+05, 1.23861828e+05,\n        1.50757328e+05, 1.31196297e+05, 5.50281289e+04, 6.28155391e+04,\n        1.77474391e+05, 2.68843926e+04, 2.53033750e+04, 2.58044258e+04,\n        1.34592480e+04, 3.88203076e+03, 9.22772559e+03, 1.22399551e+04,\n        2.20868379e+04, 3.60825352e+04, 4.87962109e+04, 3.74652812e+04,\n        4.10560898e+04, 4.17418750e+04, 1.10490836e+05, 2.00762031e+05,\n        3.48149000e+05, 3.58060812e+05, 3.28375281e+05, 2.96505844e+05,\n        2.33354141e+05, 1.54047203e+05, 1.43171328e+05, 1.62586469e+05,\n        2.96452719e+05, 2.66880719e+05, 2.94189062e+05, 3.10067656e+05,\n        6.27178750e+04, 8.63868672e+04, 9.69545391e+04, 2.01814359e+05,\n        2.97053594e+05, 2.00674891e+05, 1.27305148e+05, 1.32509578e+05,\n        8.46020391e+04, 1.74817453e+05, 2.95115844e+05, 4.75888719e+05,\n        7.21894500e+05, 4.62711281e+05, 1.33154750e+05, 8.13640156e+04,\n        3.43108672e+04, 3.45204180e+04, 5.36851641e+04, 7.42413281e+04,\n        1.09339375e+05, 2.79682844e+05, 2.19119422e+05, 1.99934359e+05,\n        1.65903391e+05, 8.16738047e+04, 4.82395195e+04, 1.65301000e+05,\n        1.81699391e+05, 3.30645000e+05, 4.63625750e+05, 6.75044000e+05,\n        3.34911500e+05, 1.69343750e+05, 1.03219234e+05, 1.48347609e+05,\n        2.98684844e+05, 3.71952656e+05, 4.43306688e+05, 5.27904688e+05,\n        1.12945275e+06, 1.00652938e+06, 8.03246875e+05, 2.21277969e+05,\n        6.64886406e+04, 2.66182090e+04, 3.29688867e+04, 1.68663250e+05,\n        1.76257219e+05, 2.99365562e+05, 4.31793438e+05, 1.91346712e+06,\n        3.90353825e+06, 6.29502500e+06, 5.49042400e+06, 6.25676850e+06,\n        6.28618000e+06, 7.23876400e+06, 2.08143525e+06, 6.66786500e+05,\n        1.44639844e+05, 1.83413594e+05, 3.41026562e+05, 3.02069719e+05,\n        3.71339750e+05, 2.02418219e+05, 1.36151141e+05, 8.24758203e+04,\n        1.96006719e+05, 1.61595656e+05, 1.71401391e+05, 2.00606359e+05,\n        2.37198891e+05, 1.58720172e+05, 6.36030352e+04, 3.38904031e+05,\n        5.23524156e+05, 2.86212188e+05, 1.92326125e+05, 3.73765547e+04,\n        1.31962000e+05, 9.33163047e+04, 1.18443672e+05, 3.00598719e+05,\n        2.28996984e+05, 3.57954281e+05, 2.36219078e+05, 1.71530359e+05,\n        2.68482539e+04, 4.79163125e+04, 4.00419297e+04, 5.41031875e+04,\n        1.07641391e+05, 9.80020000e+04, 7.08350078e+04, 6.55985312e+04,\n        8.53740000e+04, 1.33837578e+05, 2.72516312e+05, 6.54133500e+05,\n        4.97212750e+05, 6.11190062e+05, 9.92907688e+05, 9.56692312e+05,\n        1.09541412e+06, 1.19283125e+06, 3.75002219e+05, 2.22053797e+05,\n        2.60620031e+05, 1.88109625e+05, 1.87927500e+05, 2.49058609e+05,\n        3.79192656e+05, 4.07706781e+05, 6.24783062e+05, 9.62079375e+05,\n        7.95344562e+05, 1.40557531e+05, 2.65227406e+05, 6.26269500e+05,\n        2.05804234e+05, 6.53089570e+04, 1.33562969e+05, 1.14692070e+05,\n        2.26913016e+05, 2.61226500e+05, 5.08516250e+05, 4.90744656e+05,\n        2.38721828e+05, 6.85555703e+04, 7.30576641e+04, 5.36113750e+04,\n        3.28223562e+05, 6.26447375e+05, 5.17735438e+05, 3.32536062e+05,\n        1.72657266e+05, 8.28138125e+04, 1.43392656e+05, 8.83169844e+04,\n        5.15846680e+04, 4.20798125e+04, 1.78049141e+05, 1.17351570e+05,\n        1.16870422e+05, 1.02416836e+05, 1.67488781e+05, 2.05509953e+05,\n        2.73221469e+05, 4.73881641e+04, 4.60577500e+04, 5.33725703e+04,\n        7.87071094e+04, 7.06486719e+04, 5.23131758e+04, 2.50280723e+04,\n        4.52024609e+04, 4.14732461e+04, 7.00107344e+04, 1.24132148e+05,\n        1.34168141e+05, 3.38401562e+05, 9.75599844e+04, 1.10023000e+05,\n        1.99194824e+04, 3.32738164e+04, 2.43711914e+04, 2.76617012e+04,\n        2.70386660e+04, 1.58115801e+04, 1.16166348e+04, 2.74852148e+04,\n        2.95266426e+04, 3.95874922e+04, 2.81846035e+04, 3.19155312e+04,\n        5.60368477e+04, 3.98338398e+04, 5.02201602e+04, 3.19759512e+04,\n        2.96142285e+04, 4.48333711e+04, 4.32265664e+04, 1.52425576e+04,\n        2.86077324e+04, 4.23488125e+04, 7.25032578e+04, 1.21924688e+05,\n        6.41490688e+05, 8.50461562e+05, 1.11135362e+06, 1.22003100e+06,\n        1.54110162e+06, 2.03452138e+06, 1.76440575e+06, 4.22100781e+05,\n        1.30513797e+05, 7.30581250e+04, 8.56110391e+04, 7.42401094e+04,\n        1.49466766e+05, 1.25623496e+04, 2.13100293e+04, 9.81795703e+04,\n        7.28018203e+04, 1.26064742e+05, 1.01337961e+05, 1.24553164e+05,\n        7.13400000e+04, 8.41950469e+04, 6.25289375e+04, 8.18789453e+04,\n        1.27303195e+05, 8.94661250e+04, 1.04100523e+05, 1.66147031e+05,\n        2.32076422e+05, 4.41965062e+05, 1.75838281e+05, 1.76445328e+05,\n        9.12169375e+04, 1.67975469e+05, 1.87876422e+05, 1.07636930e+05,\n        3.26578105e+04, 5.35412461e+04, 1.38143531e+05, 3.28638219e+05,\n        6.38893688e+05, 1.08279325e+06, 1.55802525e+06, 2.42136925e+06,\n        2.08980088e+06, 9.83019312e+05, 4.27435500e+05, 5.84014961e+04,\n        4.02855312e+04, 5.27582578e+04, 1.33694297e+05, 2.41687594e+05,\n        2.54570500e+05, 2.18475578e+05, 2.39844719e+05, 1.72920359e+05,\n        9.46134531e+04, 8.14810703e+04, 1.83791891e+05, 3.41631531e+05,\n        2.67090906e+05, 3.54819906e+05, 1.69607641e+05, 2.63053145e+04,\n        2.11339043e+04, 1.11686047e+05, 6.08852578e+04, 4.64014297e+04,\n        6.97661953e+04, 1.16936102e+05, 1.44631031e+05, 1.14626898e+05,\n        7.92795234e+04, 5.69158320e+04, 3.03999824e+04, 9.47752891e+04,\n        1.19674977e+05, 2.15400000e+05, 3.15528031e+05, 4.17611781e+05,\n        3.91754000e+05, 2.83129906e+05, 8.15816328e+04, 5.00839805e+04,\n        1.01595930e+05, 6.17696953e+04, 3.16726438e+05, 5.76748000e+05,\n        7.41103312e+05, 1.33218000e+06, 1.56516188e+06, 1.55553338e+06,\n        1.68517900e+06, 3.21850031e+05, 6.30693242e+04, 7.78374766e+04,\n        5.12543047e+04, 1.14444250e+05, 1.13520102e+05, 7.03014531e+04,\n        4.71108398e+04, 4.52435781e+05, 7.79108188e+05, 7.32536312e+05,\n        1.03909088e+06, 1.56208738e+06, 1.63956762e+06, 1.72789975e+06,\n        4.00525438e+05, 2.34608641e+05, 1.76724859e+05, 6.78014609e+04,\n        3.90382062e+05, 9.80141562e+05, 1.03537131e+06, 8.44669812e+05,\n        1.38893975e+06, 1.98341138e+06, 3.62835425e+06, 2.66920750e+06,\n        1.42117000e+06, 4.54602219e+05, 3.65967062e+05, 9.46589188e+05,\n        1.45184738e+06, 1.85900112e+06, 1.89930475e+06, 1.82373112e+06,\n        1.21286088e+06, 4.94641594e+05, 1.49650312e+06, 2.03091800e+06,\n        2.25657350e+06, 2.11182750e+05, 1.21396250e+05, 3.04308844e+05,\n        7.11903438e+05, 6.20312000e+05, 8.05386562e+05, 6.26902875e+05,\n        5.66389875e+05, 4.27517438e+05, 7.83491562e+05, 3.63025775e+06,\n        8.18037650e+06, 1.07676250e+07, 1.26563580e+07, 1.33500370e+07,\n        1.33957390e+07, 6.52511100e+06, 1.60696775e+06, 8.56758062e+05,\n        1.15616712e+06, 1.69269000e+06, 2.17122950e+06, 3.34218200e+06,\n        7.84994850e+06, 1.08378930e+07, 1.05726990e+07, 1.01602540e+07,\n        1.14205900e+07, 1.16179750e+07, 1.73510000e+07, 1.79301800e+07,\n        2.70626460e+07, 3.72665640e+07, 2.97687340e+07, 2.14580120e+07,\n        2.44066140e+07, 1.56006580e+07, 1.03771360e+07, 9.93532800e+06,\n        2.18889460e+07, 3.45001800e+07, 2.72495080e+07, 2.51545225e+06,\n        2.11410550e+06, 1.65101288e+06, 8.78065000e+06, 1.42263970e+07,\n        1.50277600e+07, 1.42628750e+07, 1.11520980e+07, 1.31385740e+07,\n        1.44270080e+07, 2.10807200e+06, 2.37278650e+06, 2.06521262e+06,\n        1.51271400e+06, 1.63591925e+06, 6.01038950e+06, 1.48181850e+07,\n        1.51462730e+07, 8.88826900e+06, 1.03656340e+07, 1.24236510e+07,\n        3.12300180e+07, 4.44122480e+07, 3.35616920e+07, 3.37026000e+07,\n        4.33024720e+07, 4.04647440e+07, 6.68120760e+07, 2.68142400e+07,\n        3.37228800e+06, 6.18199650e+06, 3.33604150e+06, 9.76003100e+06,\n        9.50538200e+06, 1.48300340e+07, 1.08088990e+07, 1.52812380e+07,\n        1.21298770e+07, 1.47110990e+07, 4.46196920e+07, 6.19225800e+07,\n        1.05739048e+08, 8.06071760e+07, 3.00356620e+07, 1.30779130e+07,\n        4.55988750e+06, 1.67929975e+06, 6.99432300e+06, 1.63487150e+07,\n        1.21498980e+07, 1.21798540e+07, 1.68979740e+07, 2.39387020e+07,\n        3.73065680e+07, 5.09610480e+07, 4.59523240e+07, 3.20003840e+07,\n        2.06371540e+07, 8.14507050e+06, 4.14795750e+06, 4.23499100e+06,\n        3.09784650e+06, 8.37917500e+06, 7.36108300e+06, 7.06094350e+06,\n        1.57310800e+07, 2.78767160e+07, 4.07339880e+07, 3.28521600e+07,\n        2.89016500e+07, 3.40228600e+06, 3.84223250e+06, 8.10806000e+06,\n        1.74778820e+07, 2.13824600e+07, 5.77506950e+06, 7.30700800e+06,\n        3.70581275e+06, 6.14524500e+05, 1.92765088e+06, 2.29573075e+06,\n        1.09031975e+06, 1.19423488e+06, 2.01535425e+06, 1.38583700e+06,\n        2.60915325e+06, 7.48127550e+06, 1.25728690e+07, 1.49188240e+07,\n        7.64712100e+06, 9.50861100e+06, 1.16537410e+07, 2.15388360e+07,\n        4.32744960e+07, 6.52254160e+07, 9.30125040e+07, 8.49970400e+07,\n        5.87007080e+07, 3.43532560e+07, 2.82685780e+07, 9.30442400e+06,\n        4.66932300e+06, 8.00189950e+06, 2.05975580e+07, 4.07234120e+07,\n        1.93118840e+07, 4.04353375e+06, 5.92508700e+06, 7.37346950e+06,\n        2.68488000e+06, 5.27761500e+06, 1.84823362e+06, 4.87232050e+06,\n        4.87888400e+06, 5.77534400e+06, 1.21702240e+07, 2.70199400e+07,\n        4.94704840e+07, 6.90300960e+07, 8.50554880e+07], dtype=float32),\n 'rmse': array([  561.7492  ,   411.2974  ,   379.64694 ,   419.52707 ,\n          513.34174 ,   625.7134  ,   632.07965 ,   586.87506 ,\n          702.7746  ,   829.9311  ,   958.4903  ,   894.97943 ,\n          797.1879  ,   803.452   ,   598.1137  ,   320.94724 ,\n          230.89664 ,   215.8309  ,   332.18848 ,   334.13724 ,\n          208.65266 ,   233.53539 ,   213.50632 ,   276.5658  ,\n          200.76715 ,   164.6427  ,   137.93799 ,   213.45375 ,\n          206.23912 ,   257.53568 ,   341.67487 ,   411.6321  ,\n          332.3958  ,   399.6341  ,   402.03375 ,   351.9401  ,\n          388.27478 ,   362.2103  ,   234.58073 ,   250.63026 ,\n          421.2771  ,   163.96461 ,   159.07036 ,   160.63757 ,\n          116.014   ,    62.305946,    96.06105 ,   110.63433 ,\n          148.61641 ,   189.95403 ,   220.89862 ,   193.55951 ,\n          202.62302 ,   204.30829 ,   332.4016  ,   448.06476 ,\n          590.04156 ,   598.38184 ,   573.0404  ,   544.5235  ,\n          483.0674  ,   392.4885  ,   378.37988 ,   403.22012 ,\n          544.47473 ,   516.605   ,   542.39197 ,   556.8372  ,\n          250.43535 ,   293.91644 ,   311.3752  ,   449.23752 ,\n          545.02625 ,   447.9675  ,   356.79846 ,   364.01865 ,\n          290.8643  ,   418.11176 ,   543.24567 ,   689.84686 ,\n          849.64374 ,   680.2288  ,   364.9038  ,   285.2438  ,\n          185.23193 ,   185.7967  ,   231.7006  ,   272.47263 ,\n          330.66504 ,   528.85046 ,   468.10193 ,   447.1402  ,\n          407.3124  ,   285.7863  ,   219.63496 ,   406.57227 ,\n          426.26212 ,   575.0174  ,   680.9007  ,   821.6106  ,\n          578.7154  ,   411.51398 ,   321.27753 ,   385.1592  ,\n          546.5207  ,   609.8793  ,   665.8128  ,   726.57056 ,\n         1062.7572  ,  1003.25934 ,   896.2404  ,   470.40195 ,\n          257.8539  ,   163.15088 ,   181.57336 ,   410.6863  ,\n          419.83    ,   547.14307 ,   657.1099  ,  1383.2812  ,\n         1975.7374  ,  2508.9888  ,  2343.1653  ,  2501.3535  ,\n         2507.2256  ,  2690.495   ,  1442.718   ,   816.56995 ,\n          380.3155  ,   428.26813 ,   583.9748  ,   549.6087  ,\n          609.3766  ,   449.90912 ,   368.98663 ,   287.18607 ,\n          442.72644 ,   401.98962 ,   414.00653 ,   447.891   ,\n          487.03067 ,   398.39697 ,   252.1964  ,   582.15466 ,\n          723.54974 ,   534.98804 ,   438.55002 ,   193.33018 ,\n          363.26575 ,   305.47717 ,   344.15646 ,   548.26886 ,\n          478.53625 ,   598.29285 ,   486.0237  ,   414.16226 ,\n          163.85437 ,   218.89795 ,   200.1048  ,   232.60092 ,\n          328.0875  ,   313.0527  ,   266.14847 ,   256.1221  ,\n          292.1883  ,   365.8382  ,   522.03094 ,   808.7852  ,\n          705.1331  ,   781.7865  ,   996.4475  ,   978.1065  ,\n         1046.6204  ,  1092.1682  ,   612.37427 ,   471.22586 ,\n          510.50952 ,   433.7161  ,   433.50607 ,   499.05774 ,\n          615.78625 ,   638.5192  ,   790.4321  ,   980.85645 ,\n          891.8209  ,   374.91006 ,   515.0024  ,   791.3719  ,\n          453.65652 ,   255.55615 ,   365.4627  ,   338.66217 ,\n          476.35388 ,   511.10318 ,   713.1032  ,   700.5317  ,\n          488.5917  ,   261.8312  ,   270.2918  ,   231.54132 ,\n          572.908   ,   791.48425 ,   719.5384  ,   576.6594  ,\n          415.52045 ,   287.7739  ,   378.6722  ,   297.18173 ,\n          227.1226  ,   205.13364 ,   421.95868 ,   342.56616 ,\n          341.86316 ,   320.02628 ,   409.25394 ,   453.33206 ,\n          522.70593 ,   217.68823 ,   214.61069 ,   231.02502 ,\n          280.54785 ,   265.7982  ,   228.72073 ,   158.20264 ,\n          212.60869 ,   203.64981 ,   264.5954  ,   352.3239  ,\n          366.2897  ,   581.72296 ,   312.34595 ,   331.69714 ,\n          141.1364  ,   182.4111  ,   156.11276 ,   166.31807 ,\n          164.43439 ,   125.7441  ,   107.780495,   165.78665 ,\n          171.83319 ,   198.96605 ,   167.8827  ,   178.64919 ,\n          236.72102 ,   199.58418 ,   224.09854 ,   178.8182  ,\n          172.08786 ,   211.73892 ,   207.91    ,   123.46075 ,\n          169.1382  ,   205.78828 ,   269.2643  ,   349.1772  ,\n          800.93115 ,   922.2048  ,  1054.2076  ,  1104.5502  ,\n         1241.4113  ,  1426.3665  ,  1328.3093  ,   649.6928  ,\n          361.26694 ,   270.29266 ,   292.59366 ,   272.4704  ,\n          386.6093  ,   112.081894,   145.97955 ,   313.3362  ,\n          269.8181  ,   355.05597 ,   318.33624 ,   352.9209  ,\n          267.0955  ,   290.16382 ,   250.05786 ,   286.145   ,\n          356.79572 ,   299.1089  ,   322.64612 ,   407.6114  ,\n          481.7431  ,   664.8045  ,   419.33078 ,   420.05396 ,\n          302.0214  ,   409.84808 ,   433.44717 ,   328.0807  ,\n          180.71472 ,   231.3898  ,   371.67667 ,   573.2698  ,\n          799.3082  ,  1040.5736  ,  1248.2089  ,  1556.075   ,\n         1445.6144  ,   991.4733  ,   653.7855  ,   241.664   ,\n          200.71255 ,   229.69164 ,   365.6423  ,   491.61737 ,\n          504.54974 ,   467.4137  ,   489.7394  ,   415.83694 ,\n          307.593   ,   285.44888 ,   428.70956 ,   584.49255 ,\n          516.80835 ,   595.6676  ,   411.8345  ,   162.18913 ,\n          145.37505 ,   334.1946  ,   246.74936 ,   215.40991 ,\n          264.1329  ,   341.95923 ,   380.3039  ,   338.56595 ,\n          281.5662  ,   238.5704  ,   174.35591 ,   307.85596 ,\n          345.94073 ,   464.11203 ,   561.7188  ,   646.22894 ,\n          625.9026  ,   532.09955 ,   285.625   ,   223.7945  ,\n          318.74115 ,   248.5351  ,   562.78455 ,   759.4393  ,\n          860.8736  ,  1154.201   ,  1251.0643  ,  1247.2102  ,\n         1298.1444  ,   567.3183  ,   251.13608 ,   278.9937  ,\n          226.39412 ,   338.29608 ,   336.92746 ,   265.14423 ,\n          217.05034 ,   672.6335  ,   882.671   ,   855.88336 ,\n         1019.35803 ,  1249.835   ,  1280.4559  ,  1314.4961  ,\n          632.8708  ,   484.36414 ,   420.38657 ,   260.38715 ,\n          624.8056  ,   990.02106 ,  1017.5319  ,   919.0592  ,\n         1178.533   ,  1408.3364  ,  1904.8239  ,  1633.771   ,\n         1192.1284  ,   674.242   ,   604.9521  ,   972.9281  ,\n         1204.9263  ,  1363.4519  ,  1378.1527  ,  1350.4559  ,\n         1101.2997  ,   703.3076  ,  1223.3164  ,  1425.1029  ,\n         1502.1896  ,   459.54623 ,   348.41965 ,   551.64197 ,\n          843.7437  ,   787.5989  ,   897.4333  ,   791.77203 ,\n          752.5888  ,   653.84814 ,   885.1505  ,  1905.3235  ,\n         2860.1357  ,  3281.406   ,  3557.5776  ,  3653.7703  ,\n         3660.019   ,  2554.4297  ,  1267.6624  ,   925.6123  ,\n         1075.2522  ,  1301.0342  ,  1473.5092  ,  1828.1636  ,\n         2801.7761  ,  3292.0957  ,  3251.5688  ,  3187.5154  ,\n         3379.4363  ,  3408.5151  ,  4165.453   ,  4234.4043  ,\n         5202.1772  ,  6104.635   ,  5456.073   ,  4632.28    ,\n         4940.305   ,  3949.767   ,  3221.3562  ,  3152.0356  ,\n         4678.5625  ,  5873.6855  ,  5220.106   ,  1586.0177  ,\n         1453.9965  ,  1284.9175  ,  2963.2163  ,  3771.7898  ,\n         3876.5652  ,  3776.622   ,  3339.4758  ,  3624.7173  ,\n         3798.29    ,  1451.92    ,  1540.3851  ,  1437.0847  ,\n         1229.9243  ,  1279.0306  ,  2451.6096  ,  3849.44    ,\n         3891.8213  ,  2981.3203  ,  3219.5703  ,  3524.72    ,\n         5588.383   ,  6664.252   ,  5793.246   ,  5805.394   ,\n         6580.4614  ,  6361.1904  ,  8173.865   ,  5178.2466  ,\n         1836.379   ,  2486.362   ,  1826.4835  ,  3124.1047  ,\n         3083.0798  ,  3850.9783  ,  3287.689   ,  3909.1228  ,\n         3482.797   ,  3835.5054  ,  6679.7974  ,  7869.091   ,\n        10282.95    ,  8978.149   ,  5480.48    ,  3616.3398  ,\n         2135.3894  ,  1295.878   ,  2644.6782  ,  4043.3542  ,\n         3485.6704  ,  3489.9646  ,  4110.7144  ,  4892.719   ,\n         6107.91    ,  7138.7007  ,  6778.814   ,  5656.8877  ,\n         4542.8135  ,  2853.957   ,  2036.6534  ,  2057.9094  ,\n         1760.0701  ,  2894.6804  ,  2713.1316  ,  2657.2437  ,\n         3966.2427  ,  5279.841   ,  6382.3184  ,  5731.68    ,\n         5376.0254  ,  1844.5288  ,  1960.1613  ,  2847.4656  ,\n         4180.656   ,  4624.117   ,  2403.1375  ,  2703.1477  ,\n         1925.0488  ,   783.9162  ,  1388.3987  ,  1515.1669  ,\n         1044.1838  ,  1092.8105  ,  1419.6317  ,  1177.2158  ,\n         1615.2874  ,  2735.192   ,  3545.8242  ,  3862.4895  ,\n         2765.3428  ,  3083.6038  ,  3413.7578  ,  4640.995   ,\n         6578.3354  ,  8076.225   ,  9644.3     ,  9219.384   ,\n         7661.638   ,  5861.165   ,  5316.8203  ,  3050.3152  ,\n         2160.8616  ,  2828.763   ,  4538.453   ,  6381.4897  ,\n         4394.529   ,  2010.854   ,  2434.1501  ,  2715.4133  ,\n         1638.5604  ,  2297.3062  ,  1359.4976  ,  2207.3335  ,\n         2208.8198  ,  2403.1946  ,  3488.5847  ,  5198.071   ,\n         7033.5254  ,  8308.436   ,  9222.554   ], dtype=float32)}</pre> In\u00a0[\u00a0]: Copied! <pre>def evaluate_preds(y_true, y_pred):\n  # Make sure float32 (for metric calculations)\n  y_true = tf.cast(y_true, dtype=tf.float32)\n  y_pred = tf.cast(y_pred, dtype=tf.float32)\n\n  # Calculate various metrics\n  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n  rmse = tf.sqrt(mse)\n  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n  mase = mean_absolute_scaled_error(y_true, y_pred)\n\n  # Account for different sized metrics (for longer horizons, reduce to single number)\n  if mae.ndim &gt; 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean\n    mae = tf.reduce_mean(mae)\n    mse = tf.reduce_mean(mse)\n    rmse = tf.reduce_mean(rmse)\n    mape = tf.reduce_mean(mape)\n    mase = tf.reduce_mean(mase)\n\n  return {\"mae\": mae.numpy(),\n          \"mse\": mse.numpy(),\n          \"rmse\": rmse.numpy(),\n          \"mape\": mape.numpy(),\n          \"mase\": mase.numpy()}\n</pre> def evaluate_preds(y_true, y_pred):   # Make sure float32 (for metric calculations)   y_true = tf.cast(y_true, dtype=tf.float32)   y_pred = tf.cast(y_pred, dtype=tf.float32)    # Calculate various metrics   mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)   mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)   rmse = tf.sqrt(mse)   mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)   mase = mean_absolute_scaled_error(y_true, y_pred)    # Account for different sized metrics (for longer horizons, reduce to single number)   if mae.ndim &gt; 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean     mae = tf.reduce_mean(mae)     mse = tf.reduce_mean(mse)     rmse = tf.reduce_mean(rmse)     mape = tf.reduce_mean(mape)     mase = tf.reduce_mean(mase)    return {\"mae\": mae.numpy(),           \"mse\": mse.numpy(),           \"rmse\": rmse.numpy(),           \"mape\": mape.numpy(),           \"mase\": mase.numpy()} <p>Now we've updated <code>evaluate_preds()</code> to work with multiple shapes, how does it look?</p> In\u00a0[\u00a0]: Copied! <pre># Get model_3 results aggregated to single values\nmodel_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n                                 y_pred=model_3_preds)\nmodel_3_results\n</pre> # Get model_3 results aggregated to single values model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),                                  y_pred=model_3_preds) model_3_results Out[\u00a0]: <pre>{'mae': 1237.5063,\n 'mape': 5.5588784,\n 'mase': 2.2020736,\n 'mse': 5405198.5,\n 'rmse': 1425.7477}</pre> <p>Time to visualize.</p> <p>If our prediction evaluation metrics were mutli-dimensional, how do you think the predictions will look like if we plot them?</p> In\u00a0[\u00a0]: Copied! <pre>offset = 300\nplt.figure(figsize=(10, 7))\nplot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\")\n# Checking the shape of model_3_preds results in [n_test_samples, HORIZON] (this will screw up the plot)\nplot_time_series(timesteps=X_test[-len(test_windows):], values=model_3_preds, start=offset, label=\"model_3_preds\")\n</pre> offset = 300 plt.figure(figsize=(10, 7)) plot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\") # Checking the shape of model_3_preds results in [n_test_samples, HORIZON] (this will screw up the plot) plot_time_series(timesteps=X_test[-len(test_windows):], values=model_3_preds, start=offset, label=\"model_3_preds\") <p>When we try to plot our multi-horizon predicts, we get a funky looking plot.</p> <p>Again, we can fix this by aggregating our model's predictions.</p> <p>\ud83d\udd11 Note: Aggregating the predictions (e.g. reducing a 7-day horizon to one value such as the mean) loses information from the original prediction. As in, the model predictions were trained to be made for 7-days but by reducing them to one, we gain the ability to plot them visually but we lose the extra information contained across multiple days.</p> In\u00a0[\u00a0]: Copied! <pre>offset = 300\nplt.figure(figsize=(10, 7))\n# Plot model_3_preds by aggregating them (note: this condenses information so the preds will look fruther ahead than the test data)\nplot_time_series(timesteps=X_test[-len(test_windows):], \n                 values=test_labels[:, 0], \n                 start=offset, \n                 label=\"Test_data\")\nplot_time_series(timesteps=X_test[-len(test_windows):], \n                 values=tf.reduce_mean(model_3_preds, axis=1), \n                 format=\"-\",\n                 start=offset, \n                 label=\"model_3_preds\")\n</pre> offset = 300 plt.figure(figsize=(10, 7)) # Plot model_3_preds by aggregating them (note: this condenses information so the preds will look fruther ahead than the test data) plot_time_series(timesteps=X_test[-len(test_windows):],                   values=test_labels[:, 0],                   start=offset,                   label=\"Test_data\") plot_time_series(timesteps=X_test[-len(test_windows):],                   values=tf.reduce_mean(model_3_preds, axis=1),                   format=\"-\",                  start=offset,                   label=\"model_3_preds\") In\u00a0[\u00a0]: Copied! <pre>pd.DataFrame({\"naive\": naive_results[\"mae\"], \n              \"horizon_1_window_7\": model_1_results[\"mae\"], \n              \"horizon_1_window_30\": model_2_results[\"mae\"], \n              \"horizon_7_window_30\": model_3_results[\"mae\"]}, index=[\"mae\"]).plot(figsize=(10, 7), kind=\"bar\");\n</pre> pd.DataFrame({\"naive\": naive_results[\"mae\"],                \"horizon_1_window_7\": model_1_results[\"mae\"],                \"horizon_1_window_30\": model_2_results[\"mae\"],                \"horizon_7_window_30\": model_3_results[\"mae\"]}, index=[\"mae\"]).plot(figsize=(10, 7), kind=\"bar\"); <p>Woah, our na\u00efve model is performing best (it's very hard to beat a na\u00efve model in open systems) but the dense model with a horizon of 1 and a window size of 7 looks to be performing cloest.</p> <p>Because of this, let's use <code>HORIZON=1</code> and <code>WINDOW_SIZE=7</code> for our next series of modelling experiments (in other words, we'll use the previous week of Bitcoin prices to try and predict the next day).</p> <p>\ud83d\udd11 Note: You might be wondering, why are the na\u00efve results so good? One of the reasons could be due the presence of autocorrelation in the data. If a time series has autocorrelation it means the value at <code>t+1</code> (the next timestep) is typically close to the value at <code>t</code> (the current timestep). In other words, today's value is probably pretty close to yesterday's value. Of course, this isn't always the case but when it is, a na\u00efve model will often get fairly good results.</p> <p>\ud83d\udcd6 Resource: For more on how autocorrelation influences a model's predictions, see the article How (not) to use Machine Learning for time series forecasting: Avoiding the pitfalls by Vegard Flovik</p> In\u00a0[\u00a0]: Copied! <pre>HORIZON = 1 # predict next day\nWINDOW_SIZE = 7 # use previous week worth of data\n</pre> HORIZON = 1 # predict next day WINDOW_SIZE = 7 # use previous week worth of data In\u00a0[\u00a0]: Copied! <pre># Create windowed dataset\nfull_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n</pre> # Create windowed dataset full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON) len(full_windows), len(full_labels) Out[\u00a0]: <pre>(2780, 2780)</pre> In\u00a0[\u00a0]: Copied! <pre># Create train/test splits\ntrain_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)\n</pre> # Create train/test splits train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels) len(train_windows), len(test_windows), len(train_labels), len(test_labels) Out[\u00a0]: <pre>(2224, 556, 2224, 556)</pre> <p>Data windowed!</p> <p>Now, since we're going to be using Conv1D layers, we need to make sure our input shapes are correct.</p> <p>The Conv1D layer in TensorFlow takes an input of: <code>(batch_size, timesteps, input_dim)</code>.</p> <p>In our case, the <code>batch_size</code> (by default this is 32 but we can change it) is handled for us but the other values will be:</p> <ul> <li><code>timesteps = WINDOW_SIZE</code> - the <code>timesteps</code> is also often referred to as <code>features</code>, our features are the previous <code>WINDOW_SIZE</code> values of Bitcoin</li> <li><code>input_dim = HORIZON</code> - our model views <code>WINDOW_SIZE</code> (one week) worth of data at a time to predict <code>HORIZON</code> (one day)</li> </ul> <p>Right now, our data has the <code>timesteps</code> dimension ready but we'll have to adjust it to have the <code>input_dim</code> dimension.</p> In\u00a0[\u00a0]: Copied! <pre># Check data sample shapes\ntrain_windows[0].shape # returns (WINDOW_SIZE, )\n</pre> # Check data sample shapes train_windows[0].shape # returns (WINDOW_SIZE, ) Out[\u00a0]: <pre>(7,)</pre> <p>To fix this, we could adjust the shape of all of our <code>train_windows</code> or we could use a <code>tf.keras.layers.Lamdba</code> (called a Lambda layer) to do this for us in our model.</p> <p>The Lambda layer wraps a function into a layer which can be used with a model.</p> <p>Let's try it out.</p> In\u00a0[\u00a0]: Copied! <pre># Before we pass our data to the Conv1D layer, we have to reshape it in order to make sure it works\nx = tf.constant(train_windows[0])\nexpand_dims_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1)) # add an extra dimension for timesteps\nprint(f\"Original shape: {x.shape}\") # (WINDOW_SIZE)\nprint(f\"Expanded shape: {expand_dims_layer(x).shape}\") # (WINDOW_SIZE, input_dim) \nprint(f\"Original values with expanded shape:\\n {expand_dims_layer(x)}\")\n</pre> # Before we pass our data to the Conv1D layer, we have to reshape it in order to make sure it works x = tf.constant(train_windows[0]) expand_dims_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1)) # add an extra dimension for timesteps print(f\"Original shape: {x.shape}\") # (WINDOW_SIZE) print(f\"Expanded shape: {expand_dims_layer(x).shape}\") # (WINDOW_SIZE, input_dim)  print(f\"Original values with expanded shape:\\n {expand_dims_layer(x)}\") <pre>Original shape: (7,)\nExpanded shape: (7, 1)\nOriginal values with expanded shape:\n [[123.65499]\n [125.455  ]\n [108.58483]\n [118.67466]\n [121.33866]\n [120.65533]\n [121.795  ]]\n</pre> <p>Looking good!</p> <p>Now we've got a Lambda layer, let's build, compile, fit and evaluate a Conv1D model on our data.</p> <p>\ud83d\udd11 Note: If you run the model below without the Lambda layer, you'll get an input shape error (one of the most common errors when building neural networks).</p> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Create model\nmodel_4 = tf.keras.Sequential([\n  # Create Lambda layer to reshape inputs, without this layer, the model will error\n  layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # resize the inputs to adjust for window size / Conv1D 3D input requirements\n  layers.Conv1D(filters=128, kernel_size=5, padding=\"causal\", activation=\"relu\"),\n  layers.Dense(HORIZON)\n], name=\"model_4_conv1D\")\n\n# Compile model\nmodel_4.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\n# Fit model\nmodel_4.fit(train_windows,\n            train_labels,\n            batch_size=128, \n            epochs=100,\n            verbose=0,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_4.name)])\n</pre> tf.random.set_seed(42)  # Create model model_4 = tf.keras.Sequential([   # Create Lambda layer to reshape inputs, without this layer, the model will error   layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # resize the inputs to adjust for window size / Conv1D 3D input requirements   layers.Conv1D(filters=128, kernel_size=5, padding=\"causal\", activation=\"relu\"),   layers.Dense(HORIZON) ], name=\"model_4_conv1D\")  # Compile model model_4.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam())  # Fit model model_4.fit(train_windows,             train_labels,             batch_size=128,              epochs=100,             verbose=0,             validation_data=(test_windows, test_labels),             callbacks=[create_model_checkpoint(model_name=model_4.name)]) <pre>INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdcf1dfba50&gt;</pre> <p>What does the Lambda layer look like in a summary?</p> In\u00a0[\u00a0]: Copied! <pre>model_4.summary()\n</pre> model_4.summary() <pre>Model: \"model_4_conv1D\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlambda_1 (Lambda)            (None, 1, 7)              0         \n_________________________________________________________________\nconv1d (Conv1D)              (None, 1, 128)            4608      \n_________________________________________________________________\ndense_6 (Dense)              (None, 1, 1)              129       \n=================================================================\nTotal params: 4,737\nTrainable params: 4,737\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>The Lambda layer appears the same as any other regular layer.</p> <p>Time to evaluate the Conv1D model.</p> In\u00a0[\u00a0]: Copied! <pre># Load in best performing Conv1D model and evaluate it on the test data\nmodel_4 = tf.keras.models.load_model(\"model_experiments/model_4_conv1D\")\nmodel_4.evaluate(test_windows, test_labels)\n</pre> # Load in best performing Conv1D model and evaluate it on the test data model_4 = tf.keras.models.load_model(\"model_experiments/model_4_conv1D\") model_4.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 3ms/step - loss: 570.8283\n</pre> Out[\u00a0]: <pre>570.8283081054688</pre> In\u00a0[\u00a0]: Copied! <pre># Make predictions\nmodel_4_preds = make_preds(model_4, test_windows)\nmodel_4_preds[:10]\n</pre> # Make predictions model_4_preds = make_preds(model_4, test_windows) model_4_preds[:10] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([8851.464, 8754.471, 8983.928, 8759.672, 8703.627, 8708.295,\n       8661.667, 8494.839, 8435.317, 8492.115], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate predictions\nmodel_4_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n                                 y_pred=model_4_preds)\nmodel_4_results\n</pre> # Evaluate predictions model_4_results = evaluate_preds(y_true=tf.squeeze(test_labels),                                  y_pred=model_4_preds) model_4_results Out[\u00a0]: <pre>{'mae': 570.8283,\n 'mape': 2.5593357,\n 'mase': 1.0027872,\n 'mse': 1176671.1,\n 'rmse': 1084.7448}</pre> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Let's build an LSTM model with the Functional API\ninputs = layers.Input(shape=(WINDOW_SIZE))\nx = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs) # expand input dimension to be compatible with LSTM\n# print(x.shape)\n# x = layers.LSTM(128, activation=\"relu\", return_sequences=True)(x) # this layer will error if the inputs are not the right shape\nx = layers.LSTM(128, activation=\"relu\")(x) # using the tanh loss function results in a massive error\n# print(x.shape)\n# Add another optional dense layer (you could add more of these to see if they improve model performance)\n# x = layers.Dense(32, activation=\"relu\")(x)\noutput = layers.Dense(HORIZON)(x)\nmodel_5 = tf.keras.Model(inputs=inputs, outputs=output, name=\"model_5_lstm\")\n\n# Compile model\nmodel_5.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\n# Seems when saving the model several warnings are appearing: https://github.com/tensorflow/tensorflow/issues/47554 \nmodel_5.fit(train_windows,\n            train_labels,\n            epochs=100,\n            verbose=0,\n            batch_size=128,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_5.name)])\n</pre> tf.random.set_seed(42)  # Let's build an LSTM model with the Functional API inputs = layers.Input(shape=(WINDOW_SIZE)) x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs) # expand input dimension to be compatible with LSTM # print(x.shape) # x = layers.LSTM(128, activation=\"relu\", return_sequences=True)(x) # this layer will error if the inputs are not the right shape x = layers.LSTM(128, activation=\"relu\")(x) # using the tanh loss function results in a massive error # print(x.shape) # Add another optional dense layer (you could add more of these to see if they improve model performance) # x = layers.Dense(32, activation=\"relu\")(x) output = layers.Dense(HORIZON)(x) model_5 = tf.keras.Model(inputs=inputs, outputs=output, name=\"model_5_lstm\")  # Compile model model_5.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam())  # Seems when saving the model several warnings are appearing: https://github.com/tensorflow/tensorflow/issues/47554  model_5.fit(train_windows,             train_labels,             epochs=100,             verbose=0,             batch_size=128,             validation_data=(test_windows, test_labels),             callbacks=[create_model_checkpoint(model_name=model_5.name)]) <pre>WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdcf17b5190&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Load in best version of model 5 and evaluate on the test data\nmodel_5 = tf.keras.models.load_model(\"model_experiments/model_5_lstm/\")\nmodel_5.evaluate(test_windows, test_labels)\n</pre> # Load in best version of model 5 and evaluate on the test data model_5 = tf.keras.models.load_model(\"model_experiments/model_5_lstm/\") model_5.evaluate(test_windows, test_labels) <pre>WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n18/18 [==============================] - 0s 2ms/step - loss: 596.6447\n</pre> Out[\u00a0]: <pre>596.6446533203125</pre> <p>Now we've got the best performing LSTM model loaded in, let's make predictions with it and evaluate them.</p> In\u00a0[\u00a0]: Copied! <pre># Make predictions with our LSTM model\nmodel_5_preds = make_preds(model_5, test_windows)\nmodel_5_preds[:10]\n</pre> # Make predictions with our LSTM model model_5_preds = make_preds(model_5, test_windows) model_5_preds[:10] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([8991.225, 8823.2  , 9009.359, 8847.859, 8742.254, 8788.655,\n       8744.746, 8552.568, 8514.823, 8542.873], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate model 5 preds\nmodel_5_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n                                 y_pred=model_5_preds)\nmodel_5_results\n</pre> # Evaluate model 5 preds model_5_results = evaluate_preds(y_true=tf.squeeze(test_labels),                                  y_pred=model_5_preds) model_5_results Out[\u00a0]: <pre>{'mae': 596.64465,\n 'mape': 2.6838453,\n 'mase': 1.0481395,\n 'mse': 1273486.9,\n 'rmse': 1128.4888}</pre> <p>Hmmm... it seems even with an LSTM-powered RNN we weren't able to beat our na\u00efve models results.</p> <p>Perhaps adding another variable will help?</p> <p>\ud83d\udd11 Note: I'm putting this here again as a reminder that because neural networks are such powerful algorithms, they can be used for almost any problem, however, that doesn't mean they'll achieve performant or usable results. You're probably starting to clue onto this now.</p> In\u00a0[\u00a0]: Copied! <pre># Let's make a multivariate time series\nbitcoin_prices.head()\n</pre> # Let's make a multivariate time series bitcoin_prices.head() Out[\u00a0]: Price Date 2013-10-01 123.65499 2013-10-02 125.45500 2013-10-03 108.58483 2013-10-04 118.67466 2013-10-05 121.33866 <p>Alright, time to add another feature column, the block reward size.</p> <p>First, we'll need to create variables for the different block reward sizes as well as the dates they came into play.</p> <p>The following block rewards and dates were sourced from cmcmarkets.com.</p> Block Reward Start Date 50 3 January 2009 (2009-01-03) 25 28 November 2012 12.5 9 July 2016 6.25 11 May 2020 3.125 TBA (expected 2024) 1.5625 TBA (expected 2028) <p>\ud83d\udd11 Note: Since our Bitcoin historical data starts from 01 October 2013, none of the timesteps in our multivariate time series will have a block reward of 50.</p> In\u00a0[\u00a0]: Copied! <pre># Block reward values\nblock_reward_1 = 50 # 3 January 2009 (2009-01-03) - this block reward isn't in our dataset (it starts from 01 October 2013)\nblock_reward_2 = 25 # 28 November 2012 \nblock_reward_3 = 12.5 # 9 July 2016\nblock_reward_4 = 6.25 # 11 May 2020\n\n# Block reward dates (datetime form of the above date stamps)\nblock_reward_2_datetime = np.datetime64(\"2012-11-28\")\nblock_reward_3_datetime = np.datetime64(\"2016-07-09\")\nblock_reward_4_datetime = np.datetime64(\"2020-05-11\")\n</pre> # Block reward values block_reward_1 = 50 # 3 January 2009 (2009-01-03) - this block reward isn't in our dataset (it starts from 01 October 2013) block_reward_2 = 25 # 28 November 2012  block_reward_3 = 12.5 # 9 July 2016 block_reward_4 = 6.25 # 11 May 2020  # Block reward dates (datetime form of the above date stamps) block_reward_2_datetime = np.datetime64(\"2012-11-28\") block_reward_3_datetime = np.datetime64(\"2016-07-09\") block_reward_4_datetime = np.datetime64(\"2020-05-11\") <p>We're going to get the days (indexes) for different block reward values.</p> <p>This is important because if we're going to use multiple variables for our time series, they have to the same frequency as our original variable. For example, if our Bitcoin prices are daily, we need the block reward values to be daily as well.</p> <p>\ud83d\udd11 Note: For using multiple variables, make sure they're the same frequency as each other. If your variables aren't at the same frequency (e.g. Bitcoin prices are daily but block rewards are weekly), you may need to transform them in a way that they can be used with your model.</p> In\u00a0[\u00a0]: Copied! <pre># Get date indexes for when to add in different block dates\nblock_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days\nblock_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days\nblock_reward_2_days, block_reward_3_days\n</pre> # Get date indexes for when to add in different block dates block_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days block_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days block_reward_2_days, block_reward_3_days Out[\u00a0]: <pre>(1012, 2414)</pre> <p>Now we can add another feature to our dataset <code>block_reward</code> (this gets lower over time so it may lead to increasing prices of Bitcoin).</p> In\u00a0[\u00a0]: Copied! <pre># Add block_reward column\nbitcoin_prices_block = bitcoin_prices.copy()\nbitcoin_prices_block[\"block_reward\"] = None\n\n# Set values of block_reward column (it's the last column hence -1 indexing on iloc)\nbitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2\nbitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days, -1] = block_reward_3\nbitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4\nbitcoin_prices_block.head()\n</pre> # Add block_reward column bitcoin_prices_block = bitcoin_prices.copy() bitcoin_prices_block[\"block_reward\"] = None  # Set values of block_reward column (it's the last column hence -1 indexing on iloc) bitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2 bitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days, -1] = block_reward_3 bitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4 bitcoin_prices_block.head() Out[\u00a0]: Price block_reward Date 2013-10-01 123.65499 25 2013-10-02 125.45500 25 2013-10-03 108.58483 25 2013-10-04 118.67466 25 2013-10-05 121.33866 25 <p>Woohoo! We've officially added another variable to our time series data.</p> <p>Let's see what it looks like.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the block reward/price over time\n# Note: Because of the different scales of our values we'll scale them to be between 0 and 1.\nfrom sklearn.preprocessing import minmax_scale\nscaled_price_block_df = pd.DataFrame(minmax_scale(bitcoin_prices_block[[\"Price\", \"block_reward\"]]), # we need to scale the data first\n                                     columns=bitcoin_prices_block.columns,\n                                     index=bitcoin_prices_block.index)\nscaled_price_block_df.plot(figsize=(10, 7));\n</pre> # Plot the block reward/price over time # Note: Because of the different scales of our values we'll scale them to be between 0 and 1. from sklearn.preprocessing import minmax_scale scaled_price_block_df = pd.DataFrame(minmax_scale(bitcoin_prices_block[[\"Price\", \"block_reward\"]]), # we need to scale the data first                                      columns=bitcoin_prices_block.columns,                                      index=bitcoin_prices_block.index) scaled_price_block_df.plot(figsize=(10, 7)); <p>When we scale the block reward and the Bitcoin price, we can see the price goes up as the block reward goes down, perhaps this information will be helpful to our model's performance.</p> In\u00a0[\u00a0]: Copied! <pre># Setup dataset hyperparameters\nHORIZON = 1\nWINDOW_SIZE = 7\n</pre> # Setup dataset hyperparameters HORIZON = 1 WINDOW_SIZE = 7 In\u00a0[\u00a0]: Copied! <pre># Make a copy of the Bitcoin historical data with block reward feature\nbitcoin_prices_windowed = bitcoin_prices_block.copy()\n\n# Add windowed columns\nfor i in range(WINDOW_SIZE): # Shift values for each step in WINDOW_SIZE\n  bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1)\nbitcoin_prices_windowed.head(10)\n</pre> # Make a copy of the Bitcoin historical data with block reward feature bitcoin_prices_windowed = bitcoin_prices_block.copy()  # Add windowed columns for i in range(WINDOW_SIZE): # Shift values for each step in WINDOW_SIZE   bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1) bitcoin_prices_windowed.head(10) Out[\u00a0]: Price block_reward Price+1 Price+2 Price+3 Price+4 Price+5 Price+6 Price+7 Date 2013-10-01 123.65499 25 NaN NaN NaN NaN NaN NaN NaN 2013-10-02 125.45500 25 123.65499 NaN NaN NaN NaN NaN NaN 2013-10-03 108.58483 25 125.45500 123.65499 NaN NaN NaN NaN NaN 2013-10-04 118.67466 25 108.58483 125.45500 123.65499 NaN NaN NaN NaN 2013-10-05 121.33866 25 118.67466 108.58483 125.45500 123.65499 NaN NaN NaN 2013-10-06 120.65533 25 121.33866 118.67466 108.58483 125.45500 123.65499 NaN NaN 2013-10-07 121.79500 25 120.65533 121.33866 118.67466 108.58483 125.45500 123.65499 NaN 2013-10-08 123.03300 25 121.79500 120.65533 121.33866 118.67466 108.58483 125.45500 123.65499 2013-10-09 124.04900 25 123.03300 121.79500 120.65533 121.33866 118.67466 108.58483 125.45500 2013-10-10 125.96116 25 124.04900 123.03300 121.79500 120.65533 121.33866 118.67466 108.58483 <p>Now that we've got a windowed dataset, let's separate features (<code>X</code>) from labels (<code>y</code>).</p> <p>Remember in our windowed dataset, we're trying to use the previous <code>WINDOW_SIZE</code> steps to predict <code>HORIZON</code> steps.</p> <pre><code>Window for a week (7) to predict a horizon of 1 (multivariate time series)\nWINDOW_SIZE &amp; block_reward -&gt; HORIZON\n\n[0, 1, 2, 3, 4, 5, 6, block_reward] -&gt; [7]\n[1, 2, 3, 4, 5, 6, 7, block_reward] -&gt; [8]\n[2, 3, 4, 5, 6, 7, 8, block_reward] -&gt; [9]\n</code></pre> <p>We'll also remove the <code>NaN</code> values using pandas <code>dropna()</code> method, this equivalent to starting our windowing function at <code>sample 0 (the first sample) + WINDOW_SIZE</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Let's create X &amp; y, remove the NaN's and convert to float32 to prevent TensorFlow errors \nX = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32) \ny = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\nX.head()\n</pre> # Let's create X &amp; y, remove the NaN's and convert to float32 to prevent TensorFlow errors  X = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32)  y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32) X.head() Out[\u00a0]: block_reward Price+1 Price+2 Price+3 Price+4 Price+5 Price+6 Price+7 Date 2013-10-08 25.0 121.794998 120.655327 121.338661 118.674660 108.584831 125.455002 123.654991 2013-10-09 25.0 123.032997 121.794998 120.655327 121.338661 118.674660 108.584831 125.455002 2013-10-10 25.0 124.049004 123.032997 121.794998 120.655327 121.338661 118.674660 108.584831 2013-10-11 25.0 125.961159 124.049004 123.032997 121.794998 120.655327 121.338661 118.674660 2013-10-12 25.0 125.279663 125.961159 124.049004 123.032997 121.794998 120.655327 121.338661 In\u00a0[\u00a0]: Copied! <pre># View labels\ny.head()\n</pre> # View labels y.head() Out[\u00a0]: <pre>Date\n2013-10-08    123.032997\n2013-10-09    124.049004\n2013-10-10    125.961159\n2013-10-11    125.279663\n2013-10-12    125.927498\nName: Price, dtype: float32</pre> <p>What a good looking dataset, let's split it into train and test sets using an 80/20 split just as we've done before.</p> In\u00a0[\u00a0]: Copied! <pre># Make train and test sets\nsplit_size = int(len(X) * 0.8)\nX_train, y_train = X[:split_size], y[:split_size]\nX_test, y_test = X[split_size:], y[split_size:]\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Make train and test sets split_size = int(len(X) * 0.8) X_train, y_train = X[:split_size], y[:split_size] X_test, y_test = X[split_size:], y[split_size:] len(X_train), len(y_train), len(X_test), len(y_test) Out[\u00a0]: <pre>(2224, 2224, 556, 556)</pre> <p>Training and test multivariate time series datasets made! Time to build a model.</p> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Make multivariate time series model\nmodel_6 = tf.keras.Sequential([\n  layers.Dense(128, activation=\"relu\"),\n  # layers.Dense(128, activation=\"relu\"), # adding an extra layer here should lead to beating the naive model\n  layers.Dense(HORIZON)\n], name=\"model_6_dense_multivariate\")\n\n# Compile\nmodel_6.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\n# Fit\nmodel_6.fit(X_train, y_train,\n            epochs=100,\n            batch_size=128,\n            verbose=0, # only print 1 line per epoch\n            validation_data=(X_test, y_test),\n            callbacks=[create_model_checkpoint(model_name=model_6.name)])\n</pre> tf.random.set_seed(42)  # Make multivariate time series model model_6 = tf.keras.Sequential([   layers.Dense(128, activation=\"relu\"),   # layers.Dense(128, activation=\"relu\"), # adding an extra layer here should lead to beating the naive model   layers.Dense(HORIZON) ], name=\"model_6_dense_multivariate\")  # Compile model_6.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam())  # Fit model_6.fit(X_train, y_train,             epochs=100,             batch_size=128,             verbose=0, # only print 1 line per epoch             validation_data=(X_test, y_test),             callbacks=[create_model_checkpoint(model_name=model_6.name)]) <pre>INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdceed05590&gt;</pre> <p>Multivariate model fit!</p> <p>You might've noticed that the model inferred the input shape of our data automatically (the data now has an extra feature). Often this will be the case, however, if you're running into shape issues, you can always explicitly define the input shape using <code>input_shape</code> parameter of the first layer in a model.</p> <p>Time to evaluate our multivariate model.</p> In\u00a0[\u00a0]: Copied! <pre># Make sure best model is loaded and evaluate\nmodel_6 = tf.keras.models.load_model(\"model_experiments/model_6_dense_multivariate\")\nmodel_6.evaluate(X_test, y_test)\n</pre> # Make sure best model is loaded and evaluate model_6 = tf.keras.models.load_model(\"model_experiments/model_6_dense_multivariate\") model_6.evaluate(X_test, y_test) <pre>18/18 [==============================] - 0s 2ms/step - loss: 567.5873\n</pre> Out[\u00a0]: <pre>567.5873413085938</pre> In\u00a0[\u00a0]: Copied! <pre># Make predictions on multivariate data\nmodel_6_preds = tf.squeeze(model_6.predict(X_test))\nmodel_6_preds[:10]\n</pre> # Make predictions on multivariate data model_6_preds = tf.squeeze(model_6.predict(X_test)) model_6_preds[:10] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([8836.276, 8763.8  , 9040.486, 8741.225, 8719.326, 8765.071,\n       8661.102, 8496.891, 8463.231, 8521.585], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate preds\nmodel_6_results = evaluate_preds(y_true=y_test,\n                                 y_pred=model_6_preds)\nmodel_6_results\n</pre> # Evaluate preds model_6_results = evaluate_preds(y_true=y_test,                                  y_pred=model_6_preds) model_6_results Out[\u00a0]: <pre>{'mae': 567.5874,\n 'mape': 2.541387,\n 'mase': 0.99709386,\n 'mse': 1161688.4,\n 'rmse': 1077.8165}</pre> <p>Hmmm... how do these results compare to <code>model_1</code> (same window size and horizon but without the block reward feature)?</p> In\u00a0[\u00a0]: Copied! <pre>model_1_results\n</pre> model_1_results Out[\u00a0]: <pre>{'mae': 568.95123,\n 'mape': 2.5448983,\n 'mase': 0.9994897,\n 'mse': 1171744.0,\n 'rmse': 1082.4713}</pre> <p>It looks like the adding in the block reward may have helped our model slightly.</p> <p>But there a few more things we could try.</p> <p>\ud83d\udcd6 Resource: For different ideas on how to improve a neural network model (from a model perspective), refer to the Improving a model section in notebook 02.</p> <p>\ud83d\udee0 Exercise(s):</p> <ol> <li>Try adding an extra <code>tf.keras.layers.Dense()</code> layer with 128 hidden units to <code>model_6</code>, how does this effect model performance?</li> <li>Is there a better way to create this model? As in, should the <code>block_reward</code> feature be bundled in with the Bitcoin historical price feature? Perhaps you could test whether building a multi-input model (e.g. one model input for Bitcoin price history and one model input for <code>block_reward</code>)  works better? See Model 4: Hybrid embedding section of notebook 09 for an idea on how to create a multi-input model.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># Create NBeatsBlock custom layer \nclass NBeatsBlock(tf.keras.layers.Layer):\n  def __init__(self, # the constructor takes all the hyperparameters for the layer\n               input_size: int,\n               theta_size: int,\n               horizon: int,\n               n_neurons: int,\n               n_layers: int,\n               **kwargs): # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)\n    super().__init__(**kwargs)\n    self.input_size = input_size\n    self.theta_size = theta_size\n    self.horizon = horizon\n    self.n_neurons = n_neurons\n    self.n_layers = n_layers\n\n    # Block contains stack of 4 fully connected layers each has ReLU activation\n    self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for _ in range(n_layers)]\n    # Output of block is a theta layer with linear activation\n    self.theta_layer = tf.keras.layers.Dense(theta_size, activation=\"linear\", name=\"theta\")\n\n  def call(self, inputs): # the call method is what runs when the layer is called \n    x = inputs \n    for layer in self.hidden: # pass inputs through each hidden layer \n      x = layer(x)\n    theta = self.theta_layer(x) \n    # Output the backcast and forecast from theta\n    backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]\n    return backcast, forecast\n</pre> # Create NBeatsBlock custom layer  class NBeatsBlock(tf.keras.layers.Layer):   def __init__(self, # the constructor takes all the hyperparameters for the layer                input_size: int,                theta_size: int,                horizon: int,                n_neurons: int,                n_layers: int,                **kwargs): # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)     super().__init__(**kwargs)     self.input_size = input_size     self.theta_size = theta_size     self.horizon = horizon     self.n_neurons = n_neurons     self.n_layers = n_layers      # Block contains stack of 4 fully connected layers each has ReLU activation     self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for _ in range(n_layers)]     # Output of block is a theta layer with linear activation     self.theta_layer = tf.keras.layers.Dense(theta_size, activation=\"linear\", name=\"theta\")    def call(self, inputs): # the call method is what runs when the layer is called      x = inputs      for layer in self.hidden: # pass inputs through each hidden layer        x = layer(x)     theta = self.theta_layer(x)      # Output the backcast and forecast from theta     backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]     return backcast, forecast <p>Setting up the <code>NBeatsBlock</code> custom layer we see:</p> <ul> <li>The class inherits from <code>tf.keras.layers.Layer</code> (this gives it all of the methods assosciated with <code>tf.keras.layers.Layer</code>)</li> <li>The constructor (<code>def __init__(...)</code>) takes all of the layer hyperparameters as well as the <code>**kwargs</code> argument<ul> <li>The <code>**kwargs</code> argument takes care of all of the hyperparameters which aren't mentioned in the constructor such as, <code>input_shape</code>, <code>trainable</code> and <code>name</code></li> </ul> </li> <li>In the constructor, the block architecture layers are created:<ul> <li>The hidden layers are created as a stack of fully connected with <code>n_nuerons</code> hidden units layers with ReLU activation</li> <li>The theta layer uses <code>theta_size</code> hidden units as well as linear activation</li> </ul> </li> <li>The <code>call()</code> method is what is run when the layer is called:<ul> <li>It first passes the inputs (the historical Bitcoin data) through each of the hidden layers (a stack of fully connected layers with ReLU activation)</li> <li>After the inputs have been through each of the fully connected layers, they get passed through the theta layer where the backcast (backwards predictions, shape: <code>input_size</code>) and forecast (forward predictions, shape: <code>horizon</code>) are returned</li> </ul> </li> </ul> <p> Using TensorFlow layer subclassing to replicate the N-BEATS basic block. See section 3.1 of N-BEATS paper for details.</p> <p>Let's see our block replica in action by together by creating a toy version of <code>NBeatsBlock</code>.</p> <p>\ud83d\udcd6  Resource: Much of the creation of the time series materials (the ones you're going through now), including replicating the N-BEATS algorithm were streamed live on Twitch. If you'd like to see replays of how the algorithm was replicated, check out the Time series research and TensorFlow course material creation playlist on the Daniel Bourke arXiv YouTube channel.</p> In\u00a0[\u00a0]: Copied! <pre># Set up dummy NBeatsBlock layer to represent inputs and outputs\ndummy_nbeats_block_layer = NBeatsBlock(input_size=WINDOW_SIZE, \n                                       theta_size=WINDOW_SIZE+HORIZON, # backcast + forecast \n                                       horizon=HORIZON,\n                                       n_neurons=128,\n                                       n_layers=4)\n</pre> # Set up dummy NBeatsBlock layer to represent inputs and outputs dummy_nbeats_block_layer = NBeatsBlock(input_size=WINDOW_SIZE,                                         theta_size=WINDOW_SIZE+HORIZON, # backcast + forecast                                         horizon=HORIZON,                                        n_neurons=128,                                        n_layers=4) In\u00a0[\u00a0]: Copied! <pre># Create dummy inputs (have to be same size as input_size)\ndummy_inputs = tf.expand_dims(tf.range(WINDOW_SIZE) + 1, axis=0) # input shape to the model has to reflect Dense layer input requirements (ndim=2)\ndummy_inputs\n</pre> # Create dummy inputs (have to be same size as input_size) dummy_inputs = tf.expand_dims(tf.range(WINDOW_SIZE) + 1, axis=0) # input shape to the model has to reflect Dense layer input requirements (ndim=2) dummy_inputs Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7]], dtype=int32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Pass dummy inputs to dummy NBeatsBlock layer\nbackcast, forecast = dummy_nbeats_block_layer(dummy_inputs)\n# These are the activation outputs of the theta layer (they'll be random due to no training of the model)\nprint(f\"Backcast: {tf.squeeze(backcast.numpy())}\")\nprint(f\"Forecast: {tf.squeeze(forecast.numpy())}\")\n</pre> # Pass dummy inputs to dummy NBeatsBlock layer backcast, forecast = dummy_nbeats_block_layer(dummy_inputs) # These are the activation outputs of the theta layer (they'll be random due to no training of the model) print(f\"Backcast: {tf.squeeze(backcast.numpy())}\") print(f\"Forecast: {tf.squeeze(forecast.numpy())}\") <pre>Backcast: [ 0.19014978  0.83798355 -0.32870018  0.25159916 -0.47540277 -0.77836645\n -0.5299447 ]\nForecast: -0.7554212808609009\n</pre> In\u00a0[\u00a0]: Copied! <pre>HORIZON = 1 # how far to predict forward\nWINDOW_SIZE = 7 # how far to lookback\n</pre> HORIZON = 1 # how far to predict forward WINDOW_SIZE = 7 # how far to lookback In\u00a0[\u00a0]: Copied! <pre># Create NBEATS data inputs (NBEATS works with univariate time series)\nbitcoin_prices.head()\n</pre> # Create NBEATS data inputs (NBEATS works with univariate time series) bitcoin_prices.head() Out[\u00a0]: Price Date 2013-10-01 123.65499 2013-10-02 125.45500 2013-10-03 108.58483 2013-10-04 118.67466 2013-10-05 121.33866 In\u00a0[\u00a0]: Copied! <pre># Add windowed columns\nbitcoin_prices_nbeats = bitcoin_prices.copy()\nfor i in range(WINDOW_SIZE):\n  bitcoin_prices_nbeats[f\"Price+{i+1}\"] = bitcoin_prices_nbeats[\"Price\"].shift(periods=i+1)\nbitcoin_prices_nbeats.dropna().head()\n</pre> # Add windowed columns bitcoin_prices_nbeats = bitcoin_prices.copy() for i in range(WINDOW_SIZE):   bitcoin_prices_nbeats[f\"Price+{i+1}\"] = bitcoin_prices_nbeats[\"Price\"].shift(periods=i+1) bitcoin_prices_nbeats.dropna().head() Out[\u00a0]: Price Price+1 Price+2 Price+3 Price+4 Price+5 Price+6 Price+7 Date 2013-10-08 123.03300 121.79500 120.65533 121.33866 118.67466 108.58483 125.45500 123.65499 2013-10-09 124.04900 123.03300 121.79500 120.65533 121.33866 118.67466 108.58483 125.45500 2013-10-10 125.96116 124.04900 123.03300 121.79500 120.65533 121.33866 118.67466 108.58483 2013-10-11 125.27966 125.96116 124.04900 123.03300 121.79500 120.65533 121.33866 118.67466 2013-10-12 125.92750 125.27966 125.96116 124.04900 123.03300 121.79500 120.65533 121.33866 In\u00a0[\u00a0]: Copied! <pre># Make features and labels\nX = bitcoin_prices_nbeats.dropna().drop(\"Price\", axis=1)\ny = bitcoin_prices_nbeats.dropna()[\"Price\"]\n\n# Make train and test sets\nsplit_size = int(len(X) * 0.8)\nX_train, y_train = X[:split_size], y[:split_size]\nX_test, y_test = X[split_size:], y[split_size:]\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Make features and labels X = bitcoin_prices_nbeats.dropna().drop(\"Price\", axis=1) y = bitcoin_prices_nbeats.dropna()[\"Price\"]  # Make train and test sets split_size = int(len(X) * 0.8) X_train, y_train = X[:split_size], y[:split_size] X_test, y_test = X[split_size:], y[split_size:] len(X_train), len(y_train), len(X_test), len(y_test) Out[\u00a0]: <pre>(2224, 2224, 556, 556)</pre> <p>Train and test sets ready to go!</p> <p>Now let's convert them into TensorFlow <code>tf.data.Dataset</code>'s to ensure they run as fast as possible whilst training.</p> <p>We'll do this by:</p> <ol> <li>Turning the arrays in tensor Datasets using <code>tf.data.Dataset.from_tensor_slices()</code></li> </ol> <ul> <li>Note: <code>from_tensor_slices()</code> works best when your data fits in memory, for extremely large datasets, you'll want to look into using the <code>TFRecord</code> format</li> </ul> <ol> <li>Combine the labels and features tensors into a Dataset using <code>tf.data.Dataset.zip()</code></li> <li>Batch and prefetch the Datasets using <code>batch()</code> and <code>prefetch()</code></li> </ol> <ul> <li>Batching and prefetching ensures the loading time from CPU (preparing data) to GPU (computing on data) is as small as possible</li> </ul> <p>\ud83d\udcd6 Resource: For more on building highly performant TensorFlow data pipelines, I'd recommend reading through the Better performance with the tf.data API guide.</p> In\u00a0[\u00a0]: Copied! <pre># 1. Turn train and test arrays into tensor Datasets\ntrain_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\ntrain_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n\ntest_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\ntest_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n\n# 2. Combine features &amp; labels\ntrain_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\ntest_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))\n\n# 3. Batch and prefetch for optimal performance\nBATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper\ntrain_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntrain_dataset, test_dataset\n</pre> # 1. Turn train and test arrays into tensor Datasets train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train) train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)  test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test) test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)  # 2. Combine features &amp; labels train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset)) test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))  # 3. Batch and prefetch for optimal performance BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE) test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  train_dataset, test_dataset Out[\u00a0]: <pre>(&lt;PrefetchDataset shapes: ((None, 7), (None,)), types: (tf.float64, tf.float64)&gt;,\n &lt;PrefetchDataset shapes: ((None, 7), (None,)), types: (tf.float64, tf.float64)&gt;)</pre> <p>Data prepared! Notice the input shape for the features <code>(None, 7)</code>, the <code>None</code> leaves space for the batch size where as the <code>7</code> represents the <code>WINDOW_SIZE</code>.</p> <p>Time to get create the N-BEATS architecture.</p> In\u00a0[\u00a0]: Copied! <pre># Values from N-BEATS paper Figure 1 and Table 18/Appendix D\nN_EPOCHS = 5000 # called \"Iterations\" in Table 18\nN_NEURONS = 512 # called \"Width\" in Table 18\nN_LAYERS = 4\nN_STACKS = 30\n\nINPUT_SIZE = WINDOW_SIZE * HORIZON # called \"Lookback\" in Table 18\nTHETA_SIZE = INPUT_SIZE + HORIZON\n\nINPUT_SIZE, THETA_SIZE\n</pre> # Values from N-BEATS paper Figure 1 and Table 18/Appendix D N_EPOCHS = 5000 # called \"Iterations\" in Table 18 N_NEURONS = 512 # called \"Width\" in Table 18 N_LAYERS = 4 N_STACKS = 30  INPUT_SIZE = WINDOW_SIZE * HORIZON # called \"Lookback\" in Table 18 THETA_SIZE = INPUT_SIZE + HORIZON  INPUT_SIZE, THETA_SIZE Out[\u00a0]: <pre>(7, 8)</pre> In\u00a0[\u00a0]: Copied! <pre># Make tensors\ntensor_1 = tf.range(10) + 10\ntensor_2 = tf.range(10)\n\n# Subtract\nsubtracted = layers.subtract([tensor_1, tensor_2])\n\n# Add\nadded = layers.add([tensor_1, tensor_2])\n\nprint(f\"Input tensors: {tensor_1.numpy()} &amp; {tensor_2.numpy()}\")\nprint(f\"Subtracted: {subtracted.numpy()}\")\nprint(f\"Added: {added.numpy()}\")\n</pre> # Make tensors tensor_1 = tf.range(10) + 10 tensor_2 = tf.range(10)  # Subtract subtracted = layers.subtract([tensor_1, tensor_2])  # Add added = layers.add([tensor_1, tensor_2])  print(f\"Input tensors: {tensor_1.numpy()} &amp; {tensor_2.numpy()}\") print(f\"Subtracted: {subtracted.numpy()}\") print(f\"Added: {added.numpy()}\") <pre>Input tensors: [10 11 12 13 14 15 16 17 18 19] &amp; [0 1 2 3 4 5 6 7 8 9]\nSubtracted: [10 10 10 10 10 10 10 10 10 10]\nAdded: [10 12 14 16 18 20 22 24 26 28]\n</pre> <p>Both of these layer functions are straight-forward, subtract or add together their inputs.</p> <p>And as mentioned before, they're what powers N-BEATS double residual stacking.</p> <p>The power of residual stacking or residual connections was revealed in Deep Residual Learning for Image Recognition where the authors were able to build a deeper but less complex neural network (this is what introduced the popular ResNet architecture) than previous attempts.</p> <p>This deeper neural network led to state of the art results on the ImageNet challenge in 2015 and different versions of residual connections have been present in deep learning ever since.</p> <p>What is a residual connection?</p> <p>A residual connection (also called skip connections) involves a deeper neural network layer receiving the outputs as well as the inputs of a shallower neural network layer.</p> <p>In the case of N-BEATS, the architecture uses residual connections which:</p> <ul> <li>Subtract the backcast outputs from a previous block from the backcast inputs to the current block</li> <li>Add the forecast outputs from all blocks together in a stack</li> </ul> <p> Annotated version of Figure 1 from the N-BEATS paper highlighting the double residual stacking (section 3.2) of the architecture. Backcast residuals of each block are subtracted from each other and used as the input to the next block where as the forecasts of each block are added together to become the stack forecast.</p> <p>What are the benefits of residual connections?</p> <p>In practice, residual connections have been beneficial for training deeper models (N-BEATS reaches ~150 layers, also see \"These approaches provide clear advantages in improving the trainability of deep architectures\" in section 3.2 of the N-BEATS paper).</p> <p>It's thought that they help avoid the problem of vanishing gradients (patterns learned by a neural network not being passed through to deeper layers).</p> In\u00a0[\u00a0]: Copied! <pre> %%time\n\ntf.random.set_seed(42)\n\n# 1. Setup N-BEATS Block layer\nnbeats_block_layer = NBeatsBlock(input_size=INPUT_SIZE,\n                                 theta_size=THETA_SIZE,\n                                 horizon=HORIZON,\n                                 n_neurons=N_NEURONS,\n                                 n_layers=N_LAYERS,\n                                 name=\"InitialBlock\")\n\n# 2. Create input to stacks\nstack_input = layers.Input(shape=(INPUT_SIZE), name=\"stack_input\")\n\n# 3. Create initial backcast and forecast input (backwards predictions are referred to as residuals in the paper)\nbackcast, forecast = nbeats_block_layer(stack_input)\n# Add in subtraction residual link, thank you to: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/174 \nresiduals = layers.subtract([stack_input, backcast], name=f\"subtract_00\") \n\n# 4. Create stacks of blocks\nfor i, _ in enumerate(range(N_STACKS-1)): # first stack is already creted in (3)\n\n  # 5. Use the NBeatsBlock to calculate the backcast as well as block forecast\n  backcast, block_forecast = NBeatsBlock(\n      input_size=INPUT_SIZE,\n      theta_size=THETA_SIZE,\n      horizon=HORIZON,\n      n_neurons=N_NEURONS,\n      n_layers=N_LAYERS,\n      name=f\"NBeatsBlock_{i}\"\n  )(residuals) # pass it in residuals (the backcast)\n\n  # 6. Create the double residual stacking\n  residuals = layers.subtract([residuals, backcast], name=f\"subtract_{i}\") \n  forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")\n\n# 7. Put the stack model together\nmodel_7 = tf.keras.Model(inputs=stack_input, \n                         outputs=forecast, \n                         name=\"model_7_N-BEATS\")\n\n# 8. Compile with MAE loss and Adam optimizer\nmodel_7.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam(0.001),\n                metrics=[\"mae\", \"mse\"])\n\n# 9. Fit the model with EarlyStopping and ReduceLROnPlateau callbacks\nmodel_7.fit(train_dataset,\n            epochs=N_EPOCHS,\n            validation_data=test_dataset,\n            verbose=0, # prevent large amounts of training outputs\n            # callbacks=[create_model_checkpoint(model_name=stack_model.name)] # saving model every epoch consumes far too much time\n            callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=200, restore_best_weights=True),\n                      tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=100, verbose=1)])\n</pre>  %%time  tf.random.set_seed(42)  # 1. Setup N-BEATS Block layer nbeats_block_layer = NBeatsBlock(input_size=INPUT_SIZE,                                  theta_size=THETA_SIZE,                                  horizon=HORIZON,                                  n_neurons=N_NEURONS,                                  n_layers=N_LAYERS,                                  name=\"InitialBlock\")  # 2. Create input to stacks stack_input = layers.Input(shape=(INPUT_SIZE), name=\"stack_input\")  # 3. Create initial backcast and forecast input (backwards predictions are referred to as residuals in the paper) backcast, forecast = nbeats_block_layer(stack_input) # Add in subtraction residual link, thank you to: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/174  residuals = layers.subtract([stack_input, backcast], name=f\"subtract_00\")   # 4. Create stacks of blocks for i, _ in enumerate(range(N_STACKS-1)): # first stack is already creted in (3)    # 5. Use the NBeatsBlock to calculate the backcast as well as block forecast   backcast, block_forecast = NBeatsBlock(       input_size=INPUT_SIZE,       theta_size=THETA_SIZE,       horizon=HORIZON,       n_neurons=N_NEURONS,       n_layers=N_LAYERS,       name=f\"NBeatsBlock_{i}\"   )(residuals) # pass it in residuals (the backcast)    # 6. Create the double residual stacking   residuals = layers.subtract([residuals, backcast], name=f\"subtract_{i}\")    forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")  # 7. Put the stack model together model_7 = tf.keras.Model(inputs=stack_input,                           outputs=forecast,                           name=\"model_7_N-BEATS\")  # 8. Compile with MAE loss and Adam optimizer model_7.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam(0.001),                 metrics=[\"mae\", \"mse\"])  # 9. Fit the model with EarlyStopping and ReduceLROnPlateau callbacks model_7.fit(train_dataset,             epochs=N_EPOCHS,             validation_data=test_dataset,             verbose=0, # prevent large amounts of training outputs             # callbacks=[create_model_checkpoint(model_name=stack_model.name)] # saving model every epoch consumes far too much time             callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=200, restore_best_weights=True),                       tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=100, verbose=1)]) <pre>\nEpoch 00328: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00428: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nCPU times: user 1min 23s, sys: 4.58 s, total: 1min 28s\nWall time: 3min 44s\n</pre> <p>And would you look at that! N-BEATS algorithm fit to our Bitcoin historical data.</p> <p>How did it perform?</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate N-BEATS model on the test dataset\nmodel_7.evaluate(test_dataset)\n</pre> # Evaluate N-BEATS model on the test dataset model_7.evaluate(test_dataset) <pre>1/1 [==============================] - 0s 46ms/step - loss: 585.4998 - mae: 585.4998 - mse: 1179491.5000\n</pre> Out[\u00a0]: <pre>[585.4998168945312, 585.4998168945312, 1179491.5]</pre> In\u00a0[\u00a0]: Copied! <pre># Make predictions with N-BEATS model\nmodel_7_preds = make_preds(model_7, test_dataset)\nmodel_7_preds[:10]\n</pre> # Make predictions with N-BEATS model model_7_preds = make_preds(model_7, test_dataset) model_7_preds[:10] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([8908.059, 8854.672, 8990.933, 8759.821, 8819.711, 8774.012,\n       8604.187, 8547.038, 8495.928, 8489.514], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate N-BEATS model predictions\nmodel_7_results = evaluate_preds(y_true=y_test,\n                                 y_pred=model_7_preds)\nmodel_7_results\n</pre> # Evaluate N-BEATS model predictions model_7_results = evaluate_preds(y_true=y_test,                                  y_pred=model_7_preds) model_7_results Out[\u00a0]: <pre>{'mae': 585.4998,\n 'mape': 2.7445195,\n 'mase': 1.028561,\n 'mse': 1179491.5,\n 'rmse': 1086.044}</pre> <p>Woah... even with all of those special layers and hand-crafted network, it looks like the N-BEATS model doesn't perform as well as <code>model_1</code> or the original naive forecast.</p> <p>This goes to show the power of smaller networks as well as the fact not all larger models are better suited for a certain type of data.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the N-BEATS model and inspect the architecture\nfrom tensorflow.keras.utils import plot_model\nplot_model(model_7)\n</pre> # Plot the N-BEATS model and inspect the architecture from tensorflow.keras.utils import plot_model plot_model(model_7) Out[\u00a0]: <p>Now that is one good looking model!</p> <p>It even looks similar to the model shown in Figure 1 of the N-BEATS paper.</p> <p> Comparison of <code>model_7</code> (N-BEATS replica model make with Keras Functional API) versus actual N-BEATS architecture diagram.</p> <p>Looks like our Functional API usage did the trick!</p> <p>\ud83d\udd11 Note: Our N-BEATS model replicates the N-BEATS generic architecture, the training setups are largely the same, except for the N-BEATS paper used an ensemble of models to make predictions (multiple different loss functions and multiple different lookback windows), see Table 18 of the N-BEATS paper for more. An extension could be to setup this kind of training regime and see if it improves performance.</p> <p>How about we try and save our version of the N-BEATS model?</p> In\u00a0[\u00a0]: Copied! <pre># This will error out unless a \"get_config()\" method is implemented - this could be extra curriculum\nmodel_7.save(model_7.name)\n</pre> # This will error out unless a \"get_config()\" method is implemented - this could be extra curriculum model_7.save(model_7.name) <pre>WARNING:absl:Found untraced functions such as theta_layer_call_and_return_conditional_losses, theta_layer_call_fn, theta_layer_call_and_return_conditional_losses, theta_layer_call_fn, theta_layer_call_and_return_conditional_losses while saving (showing 5 of 750). These functions will not be directly callable after loading.\n</pre> <pre>INFO:tensorflow:Assets written to: model_7_N-BEATS/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_7_N-BEATS/assets\n/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  category=CustomMaskWarning)\n</pre> <p>You'll notice a warning appears telling us to fully save our model correctly we need to implement a <code>get_config()</code> method in our custom layer class.</p> <p>\ud83d\udcd6 Resource: If you would like to save and load the N-BEATS model or any other custom or subclassed layer/model configuration, you should overwrite the <code>get_config()</code> and optionally <code>from_config()</code> methods. See the TensorFlow Custom Objects documentation for more.</p> In\u00a0[\u00a0]: Copied! <pre>def get_ensemble_models(horizon=HORIZON, \n                        train_data=train_dataset,\n                        test_data=test_dataset,\n                        num_iter=10, \n                        num_epochs=100, \n                        loss_fns=[\"mae\", \"mse\", \"mape\"]):\n  \"\"\"\n  Returns a list of num_iter models each trained on MAE, MSE and MAPE loss.\n\n  For example, if num_iter=10, a list of 30 trained models will be returned:\n  10 * len([\"mae\", \"mse\", \"mape\"]).\n  \"\"\"\n  # Make empty list for trained ensemble models\n  ensemble_models = []\n\n  # Create num_iter number of models per loss function\n  for i in range(num_iter):\n    # Build and fit a new model with a different loss function\n    for loss_function in loss_fns:\n      print(f\"Optimizing model by reducing: {loss_function} for {num_epochs} epochs, model number: {i}\")\n\n      # Construct a simple model (similar to model_1)\n      model = tf.keras.Sequential([\n        # Initialize layers with normal (Gaussian) distribution so we can use the models for prediction\n        # interval estimation later: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal\n        layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"), \n        layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),\n        layers.Dense(HORIZON)                                 \n      ])\n\n      # Compile simple model with current loss function\n      model.compile(loss=loss_function,\n                    optimizer=tf.keras.optimizers.Adam(),\n                    metrics=[\"mae\", \"mse\"])\n      \n      # Fit model\n      model.fit(train_data,\n                epochs=num_epochs,\n                verbose=0,\n                validation_data=test_data,\n                # Add callbacks to prevent training from going/stalling for too long\n                callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                                            patience=200,\n                                                            restore_best_weights=True),\n                           tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n                                                                patience=100,\n                                                                verbose=1)])\n      \n      # Append fitted model to list of ensemble models\n      ensemble_models.append(model)\n\n  return ensemble_models # return list of trained models\n</pre> def get_ensemble_models(horizon=HORIZON,                          train_data=train_dataset,                         test_data=test_dataset,                         num_iter=10,                          num_epochs=100,                          loss_fns=[\"mae\", \"mse\", \"mape\"]):   \"\"\"   Returns a list of num_iter models each trained on MAE, MSE and MAPE loss.    For example, if num_iter=10, a list of 30 trained models will be returned:   10 * len([\"mae\", \"mse\", \"mape\"]).   \"\"\"   # Make empty list for trained ensemble models   ensemble_models = []    # Create num_iter number of models per loss function   for i in range(num_iter):     # Build and fit a new model with a different loss function     for loss_function in loss_fns:       print(f\"Optimizing model by reducing: {loss_function} for {num_epochs} epochs, model number: {i}\")        # Construct a simple model (similar to model_1)       model = tf.keras.Sequential([         # Initialize layers with normal (Gaussian) distribution so we can use the models for prediction         # interval estimation later: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal         layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),          layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),         layers.Dense(HORIZON)                                        ])        # Compile simple model with current loss function       model.compile(loss=loss_function,                     optimizer=tf.keras.optimizers.Adam(),                     metrics=[\"mae\", \"mse\"])              # Fit model       model.fit(train_data,                 epochs=num_epochs,                 verbose=0,                 validation_data=test_data,                 # Add callbacks to prevent training from going/stalling for too long                 callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",                                                             patience=200,                                                             restore_best_weights=True),                            tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",                                                                 patience=100,                                                                 verbose=1)])              # Append fitted model to list of ensemble models       ensemble_models.append(model)    return ensemble_models # return list of trained models <p>Ensemble model creator function created!</p> <p>Let's try it out by running <code>num_iter=5</code> runs for 1000 epochs. This will result in 15 total models (5 for each different loss function).</p> <p>Of course, these numbers could be tweaked to create more models trained for longer.</p> <p>\ud83d\udd11 Note: With ensembles, you'll generally find more total models means better performance. However, this comes with the tradeoff of having to train more models (longer training time) and make predictions with more models (longer prediction time).</p> In\u00a0[\u00a0]: Copied! <pre>%%time\n# Get list of trained ensemble models\nensemble_models = get_ensemble_models(num_iter=5,\n                                      num_epochs=1000)\n</pre> %%time # Get list of trained ensemble models ensemble_models = get_ensemble_models(num_iter=5,                                       num_epochs=1000) <pre>Optimizing model by reducing: mae for 1000 epochs, model number: 0\n\nEpoch 00794: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00928: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nOptimizing model by reducing: mse for 1000 epochs, model number: 0\n\nEpoch 00591: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00707: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 00807: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nOptimizing model by reducing: mape for 1000 epochs, model number: 0\n\nEpoch 00165: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00282: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 00382: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nOptimizing model by reducing: mae for 1000 epochs, model number: 1\nOptimizing model by reducing: mse for 1000 epochs, model number: 1\n\nEpoch 00409: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00509: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nOptimizing model by reducing: mape for 1000 epochs, model number: 1\n\nEpoch 00185: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00726: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 00826: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nOptimizing model by reducing: mae for 1000 epochs, model number: 2\nOptimizing model by reducing: mse for 1000 epochs, model number: 2\nOptimizing model by reducing: mape for 1000 epochs, model number: 2\n\nEpoch 00241: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00341: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nOptimizing model by reducing: mae for 1000 epochs, model number: 3\n\nEpoch 00572: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nOptimizing model by reducing: mse for 1000 epochs, model number: 3\n\nEpoch 00304: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00607: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 00707: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nOptimizing model by reducing: mape for 1000 epochs, model number: 3\n\nEpoch 00301: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00401: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nOptimizing model by reducing: mae for 1000 epochs, model number: 4\nOptimizing model by reducing: mse for 1000 epochs, model number: 4\n\nEpoch 00640: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00740: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nOptimizing model by reducing: mape for 1000 epochs, model number: 4\n\nEpoch 00132: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00609: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 00709: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nCPU times: user 6min 24s, sys: 38.6 s, total: 7min 3s\nWall time: 7min 58s\n</pre> <p>Look at all of those models!</p> <p>How about we now write a function to use the list of trained ensemble models to make predictions and then return a list of predictions (one set of predictions per model)?</p> In\u00a0[\u00a0]: Copied! <pre># Create a function which uses a list of trained models to make and return a list of predictions\ndef make_ensemble_preds(ensemble_models, data):\n  ensemble_preds = []\n  for model in ensemble_models:\n    preds = model.predict(data) # make predictions with current ensemble model\n    ensemble_preds.append(preds)\n  return tf.constant(tf.squeeze(ensemble_preds))\n</pre> # Create a function which uses a list of trained models to make and return a list of predictions def make_ensemble_preds(ensemble_models, data):   ensemble_preds = []   for model in ensemble_models:     preds = model.predict(data) # make predictions with current ensemble model     ensemble_preds.append(preds)   return tf.constant(tf.squeeze(ensemble_preds)) In\u00a0[\u00a0]: Copied! <pre># Create a list of ensemble predictions\nensemble_preds = make_ensemble_preds(ensemble_models=ensemble_models,\n                                     data=test_dataset)\nensemble_preds\n</pre> # Create a list of ensemble predictions ensemble_preds = make_ensemble_preds(ensemble_models=ensemble_models,                                      data=test_dataset) ensemble_preds <pre>WARNING:tensorflow:5 out of the last 22 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdcef255d40&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</pre> <pre>WARNING:tensorflow:5 out of the last 22 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdcef255d40&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</pre> <pre>WARNING:tensorflow:6 out of the last 23 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdc77fed9e0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</pre> <pre>WARNING:tensorflow:6 out of the last 23 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdc77fed9e0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</pre> Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(15, 556), dtype=float32, numpy=\narray([[ 8805.756,  8773.019,  9028.609, ..., 50112.656, 49132.555,\n        46455.695],\n       [ 8764.092,  8740.744,  9051.838, ..., 49355.098, 48502.336,\n        45333.934],\n       [ 8732.57 ,  8719.407,  9093.386, ..., 49921.9  , 47992.15 ,\n        45316.45 ],\n       ...,\n       [ 8938.421,  8773.84 ,  9045.577, ..., 49488.133, 49741.4  ,\n        46536.25 ],\n       [ 8724.761,  8805.311,  9094.972, ..., 49553.086, 48492.86 ,\n        45084.266],\n       [ 8823.311,  8768.297,  9047.492, ..., 49759.902, 48090.945,\n        45874.336]], dtype=float32)&gt;</pre> <p>Now we've got a set of ensemble predictions, we can evaluate them against the ground truth values.</p> <p>However, since we've trained 15 models, there's going to be 15 sets of predictions. Rather than comparing every set of predictions to the ground truth, let's take the median (you could also take the mean too but the median is usually more robust than the mean).</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate ensemble model(s) predictions\nensemble_results = evaluate_preds(y_true=y_test,\n                                  y_pred=np.median(ensemble_preds, axis=0)) # take the median across all ensemble predictions\nensemble_results\n</pre> # Evaluate ensemble model(s) predictions ensemble_results = evaluate_preds(y_true=y_test,                                   y_pred=np.median(ensemble_preds, axis=0)) # take the median across all ensemble predictions ensemble_results Out[\u00a0]: <pre>{'mae': 567.4423,\n 'mape': 2.5843322,\n 'mase': 0.996839,\n 'mse': 1144512.9,\n 'rmse': 1069.8191}</pre> <p>Nice! Looks like the ensemble model is the best performing model on the MAE metric so far.</p> In\u00a0[\u00a0]: Copied! <pre># Find upper and lower bounds of ensemble predictions\ndef get_upper_lower(preds): # 1. Take the predictions of multiple randomly initialized deep learning neural networks\n  \n  # 2. Measure the standard deviation of the predictions\n  std = tf.math.reduce_std(preds, axis=0)\n  \n  # 3. Multiply the standard deviation by 1.96\n  interval = 1.96 * std # https://en.wikipedia.org/wiki/1.96 \n\n  # 4. Get the prediction interval upper and lower bounds\n  preds_mean = tf.reduce_mean(preds, axis=0)\n  lower, upper = preds_mean - interval, preds_mean + interval\n  return lower, upper\n\n# Get the upper and lower bounds of the 95% \nlower, upper = get_upper_lower(preds=ensemble_preds)\n</pre> # Find upper and lower bounds of ensemble predictions def get_upper_lower(preds): # 1. Take the predictions of multiple randomly initialized deep learning neural networks      # 2. Measure the standard deviation of the predictions   std = tf.math.reduce_std(preds, axis=0)      # 3. Multiply the standard deviation by 1.96   interval = 1.96 * std # https://en.wikipedia.org/wiki/1.96     # 4. Get the prediction interval upper and lower bounds   preds_mean = tf.reduce_mean(preds, axis=0)   lower, upper = preds_mean - interval, preds_mean + interval   return lower, upper  # Get the upper and lower bounds of the 95%  lower, upper = get_upper_lower(preds=ensemble_preds) <p>Wonderful, now we've got the upper and lower bounds for the the 95% prediction interval, let's plot them against our ensemble model's predictions.</p> <p>To do so, we can use our plotting function as well as the <code>matplotlib.pyplot.fill_between()</code> method to shade in the space between the upper and lower bounds.</p> In\u00a0[\u00a0]: Copied! <pre># Get the median values of our ensemble preds\nensemble_median = np.median(ensemble_preds, axis=0)\n\n# Plot the median of our ensemble preds along with the prediction intervals (where the predictions fall between)\noffset=500\nplt.figure(figsize=(10, 7))\nplt.plot(X_test.index[offset:], y_test[offset:], \"g\", label=\"Test Data\")\nplt.plot(X_test.index[offset:], ensemble_median[offset:], \"k-\", label=\"Ensemble Median\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"BTC Price\")\nplt.fill_between(X_test.index[offset:], \n                 (lower)[offset:], \n                 (upper)[offset:], label=\"Prediction Intervals\")\nplt.legend(loc=\"upper left\", fontsize=14);\n</pre> # Get the median values of our ensemble preds ensemble_median = np.median(ensemble_preds, axis=0)  # Plot the median of our ensemble preds along with the prediction intervals (where the predictions fall between) offset=500 plt.figure(figsize=(10, 7)) plt.plot(X_test.index[offset:], y_test[offset:], \"g\", label=\"Test Data\") plt.plot(X_test.index[offset:], ensemble_median[offset:], \"k-\", label=\"Ensemble Median\") plt.xlabel(\"Date\") plt.ylabel(\"BTC Price\") plt.fill_between(X_test.index[offset:],                   (lower)[offset:],                   (upper)[offset:], label=\"Prediction Intervals\") plt.legend(loc=\"upper left\", fontsize=14); <p>We've just plotted:</p> <ul> <li>The test data (the ground truth Bitcoin prices)</li> <li>The median of the ensemble predictions</li> <li>The 95% prediction intervals (assuming the data is Gaussian/normal, the model is saying that 95% of the time, predicted value should fall between this range)</li> </ul> <p>What can you tell about the ensemble model from the plot above?</p> <p>It looks like the ensemble predictions are lagging slightly behind the actual data.</p> <p>And the prediction intervals are fairly low throughout.</p> <p>The combination of lagging predictions as well as low prediction intervals indicates that our ensemble model may be overfitting the data, meaning it's basically replicating what a na\u00efve model would do and just predicting the previous timestep value for the next value.</p> <p>This would explain why previous attempts to beat the na\u00efve forecast have been futile.</p> <p>We can test this hypothesis of overfitting by creating a model to make predictions into the future and seeing what they look like.</p> <p>\ud83d\udd11 Note: Our prediction intervals assume that the data we're using come from a Gaussian/normal distribution (also called a bell curve), however, open systems rarely follow the Gaussian. We'll see this later on with the turkey problem \ud83e\udd83. For further reading on this topic, I'd recommend reading The Black Swan by Nassim Nicholas Taleb, especially Part 2 and Chapter 15.</p> In\u00a0[\u00a0]: Copied! <pre>bitcoin_prices_windowed.head()\n</pre> bitcoin_prices_windowed.head() Out[\u00a0]: Price block_reward Price+1 Price+2 Price+3 Price+4 Price+5 Price+6 Price+7 Date 2013-10-01 123.65499 25 NaN NaN NaN NaN NaN NaN NaN 2013-10-02 125.45500 25 123.65499 NaN NaN NaN NaN NaN NaN 2013-10-03 108.58483 25 125.45500 123.65499 NaN NaN NaN NaN NaN 2013-10-04 118.67466 25 108.58483 125.45500 123.65499 NaN NaN NaN NaN 2013-10-05 121.33866 25 118.67466 108.58483 125.45500 123.65499 NaN NaN NaN In\u00a0[\u00a0]: Copied! <pre># Train model on entire data to make prediction for the next day \nX_all = bitcoin_prices_windowed.drop([\"Price\", \"block_reward\"], axis=1).dropna().to_numpy() # only want prices, our future model can be a univariate model\ny_all = bitcoin_prices_windowed.dropna()[\"Price\"].to_numpy()\n</pre> # Train model on entire data to make prediction for the next day  X_all = bitcoin_prices_windowed.drop([\"Price\", \"block_reward\"], axis=1).dropna().to_numpy() # only want prices, our future model can be a univariate model y_all = bitcoin_prices_windowed.dropna()[\"Price\"].to_numpy() <p>Windows and labels ready! Let's turn them into performance optimized TensorFlow Datasets by:</p> <ol> <li>Turning <code>X_all</code> and <code>y_all</code> into tensor Datasets using <code>tf.data.Dataset.from_tensor_slices()</code></li> <li>Combining the features and labels into a Dataset tuple using <code>tf.data.Dataset.zip()</code></li> <li>Batch and prefetch the data using <code>tf.data.Dataset.batch()</code> and <code>tf.data.Dataset.prefetch()</code> respectively</li> </ol> In\u00a0[\u00a0]: Copied! <pre># 1. Turn X and y into tensor Datasets\nfeatures_dataset_all = tf.data.Dataset.from_tensor_slices(X_all)\nlabels_dataset_all = tf.data.Dataset.from_tensor_slices(y_all)\n\n# 2. Combine features &amp; labels\ndataset_all = tf.data.Dataset.zip((features_dataset_all, labels_dataset_all))\n\n# 3. Batch and prefetch for optimal performance\nBATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper\ndataset_all = dataset_all.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ndataset_all\n</pre> # 1. Turn X and y into tensor Datasets features_dataset_all = tf.data.Dataset.from_tensor_slices(X_all) labels_dataset_all = tf.data.Dataset.from_tensor_slices(y_all)  # 2. Combine features &amp; labels dataset_all = tf.data.Dataset.zip((features_dataset_all, labels_dataset_all))  # 3. Batch and prefetch for optimal performance BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper dataset_all = dataset_all.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  dataset_all Out[\u00a0]: <pre>&lt;PrefetchDataset shapes: ((None, 7), (None,)), types: (tf.float64, tf.float64)&gt;</pre> <p>And now let's create a model similar to <code>model_1</code> except with an extra layer, we'll also fit it to the entire dataset for 100 epochs (feel free to play around with the number of epochs or callbacks here, you've got the skills to now).</p> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Create model (nice and simple, just to test)\nmodel_9 = tf.keras.Sequential([\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(HORIZON)\n])\n\n# Compile\nmodel_9.compile(loss=tf.keras.losses.mae,\n                optimizer=tf.keras.optimizers.Adam())\n\n# Fit model on all of the data to make future forecasts\nmodel_9.fit(dataset_all,\n            epochs=100,\n            verbose=0) # don't print out anything, we've seen this all before\n</pre> tf.random.set_seed(42)  # Create model (nice and simple, just to test) model_9 = tf.keras.Sequential([   layers.Dense(128, activation=\"relu\"),   layers.Dense(128, activation=\"relu\"),   layers.Dense(HORIZON) ])  # Compile model_9.compile(loss=tf.keras.losses.mae,                 optimizer=tf.keras.optimizers.Adam())  # Fit model on all of the data to make future forecasts model_9.fit(dataset_all,             epochs=100,             verbose=0) # don't print out anything, we've seen this all before Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdc77ae0dd0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># How many timesteps to predict into the future?\nINTO_FUTURE = 14 # since our Bitcoin data is daily, this is for 14 days\n</pre> # How many timesteps to predict into the future? INTO_FUTURE = 14 # since our Bitcoin data is daily, this is for 14 days <p> Example flow chart representing the loop we're about to create for making forecasts. Not pictured: retraining a forecasting model every time a forecast is made &amp; new data is acquired. For example, if you're predicting the price of Bitcoin daily, you'd want to retrain your model every day, since each day you're going to have a new data point to work with.</p> <p>Alright, let's create a function which returns <code>INTO_FUTURE</code> forecasted values using a trained model.</p> <p>To do so, we'll build the following steps:</p> <ol> <li>Function which takes as input:</li> </ol> <ul> <li>a list of values (the Bitcoin historical data)</li> <li>a trained model (such as <code>model_9</code>)</li> <li>a window into the future to predict (our <code>INTO_FUTURE</code> variable)</li> <li>the window size a model was trained on (<code>WINDOW_SIZE</code>) - the model can only predict on the same kind of data it was trained on</li> </ul> <ol> <li>Creates an empty list for future forecasts (this will be returned at the end of the function) and extracts the last <code>WINDOW_SIZE</code> values from the input values (predictions will start from the last <code>WINDOW_SIZE</code> values of the training data)</li> <li>Loop <code>INTO_FUTURE</code> times making a prediction on <code>WINDOW_SIZE</code> datasets which update to remove the first the value and append the latest prediction</li> </ol> <ul> <li>Eventually future predictions will be made using the model's own previous predictions as input</li> </ul> In\u00a0[\u00a0]: Copied! <pre># 1. Create function to make predictions into the future\ndef make_future_forecast(values, model, into_future, window_size=WINDOW_SIZE) -&gt; list:\n  \"\"\"\n  Makes future forecasts into_future steps after values ends.\n\n  Returns future forecasts as list of floats.\n  \"\"\"\n  # 2. Make an empty list for future forecasts/prepare data to forecast on\n  future_forecast = []\n  last_window = values[-WINDOW_SIZE:] # only want preds from the last window (this will get updated)\n\n  # 3. Make INTO_FUTURE number of predictions, altering the data which gets predicted on each time \n  for _ in range(into_future):\n    \n    # Predict on last window then append it again, again, again (model starts to make forecasts on its own forecasts)\n    future_pred = model.predict(tf.expand_dims(last_window, axis=0))\n    print(f\"Predicting on: \\n {last_window} -&gt; Prediction: {tf.squeeze(future_pred).numpy()}\\n\")\n    \n    # Append predictions to future_forecast\n    future_forecast.append(tf.squeeze(future_pred).numpy())\n    # print(future_forecast)\n\n    # Update last window with new pred and get WINDOW_SIZE most recent preds (model was trained on WINDOW_SIZE windows)\n    last_window = np.append(last_window, future_pred)[-WINDOW_SIZE:]\n  \n  return future_forecast\n</pre> # 1. Create function to make predictions into the future def make_future_forecast(values, model, into_future, window_size=WINDOW_SIZE) -&gt; list:   \"\"\"   Makes future forecasts into_future steps after values ends.    Returns future forecasts as list of floats.   \"\"\"   # 2. Make an empty list for future forecasts/prepare data to forecast on   future_forecast = []   last_window = values[-WINDOW_SIZE:] # only want preds from the last window (this will get updated)    # 3. Make INTO_FUTURE number of predictions, altering the data which gets predicted on each time    for _ in range(into_future):          # Predict on last window then append it again, again, again (model starts to make forecasts on its own forecasts)     future_pred = model.predict(tf.expand_dims(last_window, axis=0))     print(f\"Predicting on: \\n {last_window} -&gt; Prediction: {tf.squeeze(future_pred).numpy()}\\n\")          # Append predictions to future_forecast     future_forecast.append(tf.squeeze(future_pred).numpy())     # print(future_forecast)      # Update last window with new pred and get WINDOW_SIZE most recent preds (model was trained on WINDOW_SIZE windows)     last_window = np.append(last_window, future_pred)[-WINDOW_SIZE:]      return future_forecast <p>Nice! Time to bring BitPredict \ud83d\udcb0\ud83d\udcc8 to life and make future forecasts of the price of Bitcoin.</p> <p>\ud83d\udee0 Exercise: In terms of a forecasting model, what might another approach to our <code>make_future_forecasts()</code> function? Recall, that for making forecasts, you need to retrain a model each time you want to generate a new prediction.</p> <p>So perhaps you could try to: make a prediction (one timestep into the future), retrain a model with this new prediction appended to the data, make a prediction, append the prediction, retrain a model... etc.</p> <p>As it is, the <code>make_future_forecasts()</code> function skips the retraining of a model part.</p> In\u00a0[\u00a0]: Copied! <pre># Make forecasts into future of the price of Bitcoin\n# Note: if you're reading this at a later date, you may already be in the future, so the forecasts \n# we're making may not actually be forecasts, if that's the case, readjust the training data.\nfuture_forecast = make_future_forecast(values=y_all,\n                                       model=model_9,\n                                       into_future=INTO_FUTURE,\n                                       window_size=WINDOW_SIZE)\n</pre> # Make forecasts into future of the price of Bitcoin # Note: if you're reading this at a later date, you may already be in the future, so the forecasts  # we're making may not actually be forecasts, if that's the case, readjust the training data. future_forecast = make_future_forecast(values=y_all,                                        model=model_9,                                        into_future=INTO_FUTURE,                                        window_size=WINDOW_SIZE) <pre>Predicting on: \n [56573.5554719  52147.82118698 49764.1320816  50032.69313676\n 47885.62525472 45604.61575361 43144.47129086] -&gt; Prediction: 55764.46484375\n\nPredicting on: \n [52147.82118698 49764.1320816  50032.69313676 47885.62525472\n 45604.61575361 43144.47129086 55764.46484375] -&gt; Prediction: 50985.9453125\n\nPredicting on: \n [49764.1320816  50032.69313676 47885.62525472 45604.61575361\n 43144.47129086 55764.46484375 50985.9453125 ] -&gt; Prediction: 48522.96484375\n\nPredicting on: \n [50032.69313676 47885.62525472 45604.61575361 43144.47129086\n 55764.46484375 50985.9453125  48522.96484375] -&gt; Prediction: 48137.203125\n\nPredicting on: \n [47885.62525472 45604.61575361 43144.47129086 55764.46484375\n 50985.9453125  48522.96484375 48137.203125  ] -&gt; Prediction: 47880.63671875\n\nPredicting on: \n [45604.61575361 43144.47129086 55764.46484375 50985.9453125\n 48522.96484375 48137.203125   47880.63671875] -&gt; Prediction: 46879.71875\n\nPredicting on: \n [43144.47129086 55764.46484375 50985.9453125  48522.96484375\n 48137.203125   47880.63671875 46879.71875   ] -&gt; Prediction: 48227.6015625\n\nPredicting on: \n [55764.46484375 50985.9453125  48522.96484375 48137.203125\n 47880.63671875 46879.71875    48227.6015625 ] -&gt; Prediction: 53963.69140625\n\nPredicting on: \n [50985.9453125  48522.96484375 48137.203125   47880.63671875\n 46879.71875    48227.6015625  53963.69140625] -&gt; Prediction: 49685.55859375\n\nPredicting on: \n [48522.96484375 48137.203125   47880.63671875 46879.71875\n 48227.6015625  53963.69140625 49685.55859375] -&gt; Prediction: 47596.17578125\n\nPredicting on: \n [48137.203125   47880.63671875 46879.71875    48227.6015625\n 53963.69140625 49685.55859375 47596.17578125] -&gt; Prediction: 48114.4296875\n\nPredicting on: \n [47880.63671875 46879.71875    48227.6015625  53963.69140625\n 49685.55859375 47596.17578125 48114.4296875 ] -&gt; Prediction: 48808.0078125\n\nPredicting on: \n [46879.71875    48227.6015625  53963.69140625 49685.55859375\n 47596.17578125 48114.4296875  48808.0078125 ] -&gt; Prediction: 48623.85546875\n\nPredicting on: \n [48227.6015625  53963.69140625 49685.55859375 47596.17578125\n 48114.4296875  48808.0078125  48623.85546875] -&gt; Prediction: 50178.72265625\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>future_forecast[:10]\n</pre> future_forecast[:10] Out[\u00a0]: <pre>[55764.465,\n 50985.945,\n 48522.965,\n 48137.203,\n 47880.637,\n 46879.72,\n 48227.6,\n 53963.69,\n 49685.56,\n 47596.176]</pre> In\u00a0[\u00a0]: Copied! <pre>def get_future_dates(start_date, into_future, offset=1):\n  \"\"\"\n  Returns array of datetime values from ranging from start_date to start_date+horizon.\n\n  start_date: date to start range (np.datetime64)\n  into_future: number of days to add onto start date for range (int)\n  offset: number of days to offset start_date by (default 1)\n  \"\"\"\n  start_date = start_date + np.timedelta64(offset, \"D\") # specify start date, \"D\" stands for day\n  end_date = start_date + np.timedelta64(into_future, \"D\") # specify end date\n  return np.arange(start_date, end_date, dtype=\"datetime64[D]\") # return a date range between start date and end date\n</pre> def get_future_dates(start_date, into_future, offset=1):   \"\"\"   Returns array of datetime values from ranging from start_date to start_date+horizon.    start_date: date to start range (np.datetime64)   into_future: number of days to add onto start date for range (int)   offset: number of days to offset start_date by (default 1)   \"\"\"   start_date = start_date + np.timedelta64(offset, \"D\") # specify start date, \"D\" stands for day   end_date = start_date + np.timedelta64(into_future, \"D\") # specify end date   return np.arange(start_date, end_date, dtype=\"datetime64[D]\") # return a date range between start date and end date <p>The start date of our forecasted dates will be the last date of our dataset.</p> In\u00a0[\u00a0]: Copied! <pre># Last timestep of timesteps (currently in np.datetime64 format)\nlast_timestep = bitcoin_prices.index[-1]\nlast_timestep\n</pre> # Last timestep of timesteps (currently in np.datetime64 format) last_timestep = bitcoin_prices.index[-1] last_timestep Out[\u00a0]: <pre>Timestamp('2021-05-18 00:00:00')</pre> In\u00a0[\u00a0]: Copied! <pre># Get next two weeks of timesteps\nnext_time_steps = get_future_dates(start_date=last_timestep, \n                                   into_future=INTO_FUTURE)\nnext_time_steps\n</pre> # Get next two weeks of timesteps next_time_steps = get_future_dates(start_date=last_timestep,                                     into_future=INTO_FUTURE) next_time_steps Out[\u00a0]: <pre>array(['2021-05-19', '2021-05-20', '2021-05-21', '2021-05-22',\n       '2021-05-23', '2021-05-24', '2021-05-25', '2021-05-26',\n       '2021-05-27', '2021-05-28', '2021-05-29', '2021-05-30',\n       '2021-05-31', '2021-06-01'], dtype='datetime64[D]')</pre> <p>Look at that! We've now got a list of dates we can use to visualize our future Bitcoin predictions.</p> <p>But to make sure the lines of the plot connect (try not running the cell below and then plotting the data to see what I mean), let's insert the last timestep and Bitcoin price of our training data to the <code>next_time_steps</code> and <code>future_forecast</code> arrays.</p> In\u00a0[\u00a0]: Copied! <pre># Insert last timestep/final price so the graph doesn't look messed\nnext_time_steps = np.insert(next_time_steps, 0, last_timestep)\nfuture_forecast = np.insert(future_forecast, 0, btc_price[-1])\nnext_time_steps, future_forecast\n</pre> # Insert last timestep/final price so the graph doesn't look messed next_time_steps = np.insert(next_time_steps, 0, last_timestep) future_forecast = np.insert(future_forecast, 0, btc_price[-1]) next_time_steps, future_forecast Out[\u00a0]: <pre>(array(['2021-05-18', '2021-05-19', '2021-05-20', '2021-05-21',\n        '2021-05-22', '2021-05-23', '2021-05-24', '2021-05-25',\n        '2021-05-26', '2021-05-27', '2021-05-28', '2021-05-29',\n        '2021-05-30', '2021-05-31', '2021-06-01'], dtype='datetime64[D]'),\n array([43144.473, 55764.465, 50985.945, 48522.965, 48137.203, 47880.637,\n        46879.72 , 48227.6  , 53963.69 , 49685.56 , 47596.176, 48114.43 ,\n        48808.008, 48623.855, 50178.723], dtype=float32))</pre> <p>Time to plot!</p> In\u00a0[\u00a0]: Copied! <pre># Plot future price predictions of Bitcoin\nplt.figure(figsize=(10, 7))\nplot_time_series(bitcoin_prices.index, btc_price, start=2500, format=\"-\", label=\"Actual BTC Price\")\nplot_time_series(next_time_steps, future_forecast, format=\"-\", label=\"Predicted BTC Price\")\n</pre> # Plot future price predictions of Bitcoin plt.figure(figsize=(10, 7)) plot_time_series(bitcoin_prices.index, btc_price, start=2500, format=\"-\", label=\"Actual BTC Price\") plot_time_series(next_time_steps, future_forecast, format=\"-\", label=\"Predicted BTC Price\") <p>Hmmm... how did our model go?</p> <p>It looks like our predictions are starting to form a bit of a cyclic pattern (up and down in the same way).</p> <p>Perhaps that's due to our model overfitting the training data and not generalizing well for future data. Also, as you could imagine, the further you predict into the future, the higher your chance for error (try seeing what happens when you predict 100 days into the future).</p> <p>But of course, we can't measure these predictions as they are because after all, they're predictions into the actual-future (by the time you read this, the future might have already happened, if so, how did the model go?).</p> <p>\ud83d\udd11 Note: A reminder, the predictions we've made here are not financial advice. And by now, you should be well aware of just how poor machine learning models can be at forecasting values in an open system - anyone promising you a model which can \"beat the market\" is likely trying to scam you, oblivious to their errors or very lucky.</p> In\u00a0[\u00a0]: Copied! <pre># Let's introduce a Turkey problem to our BTC data (price BTC falls 100x in one day)\nbtc_price_turkey = btc_price.copy()\nbtc_price_turkey[-1] = btc_price_turkey[-1] / 100\n</pre> # Let's introduce a Turkey problem to our BTC data (price BTC falls 100x in one day) btc_price_turkey = btc_price.copy() btc_price_turkey[-1] = btc_price_turkey[-1] / 100 In\u00a0[\u00a0]: Copied! <pre># Manufacture an extra price on the end (to showcase the Turkey problem)\nbtc_price_turkey[-10:]\n</pre> # Manufacture an extra price on the end (to showcase the Turkey problem) btc_price_turkey[-10:] Out[\u00a0]: <pre>[58788.2096789273,\n 58102.1914262342,\n 55715.5466512869,\n 56573.5554719043,\n 52147.8211869823,\n 49764.1320815975,\n 50032.6931367648,\n 47885.6252547166,\n 45604.6157536131,\n 431.44471290860304]</pre> <p>Notice the last value is 100x lower than what it actually was (remember, this is not a real data point, its only to illustrate the effects of the turkey problem).</p> <p>Now we've got Bitcoin prices including a turkey problem data point, let's get the timesteps.</p> In\u00a0[\u00a0]: Copied! <pre># Get the timesteps for the turkey problem \nbtc_timesteps_turkey = np.array(bitcoin_prices.index)\nbtc_timesteps_turkey[-10:]\n</pre> # Get the timesteps for the turkey problem  btc_timesteps_turkey = np.array(bitcoin_prices.index) btc_timesteps_turkey[-10:] Out[\u00a0]: <pre>array(['2021-05-09T00:00:00.000000000', '2021-05-10T00:00:00.000000000',\n       '2021-05-11T00:00:00.000000000', '2021-05-12T00:00:00.000000000',\n       '2021-05-13T00:00:00.000000000', '2021-05-14T00:00:00.000000000',\n       '2021-05-15T00:00:00.000000000', '2021-05-16T00:00:00.000000000',\n       '2021-05-17T00:00:00.000000000', '2021-05-18T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre> <p>Beautiful! Let's see our artificially created turkey problem Bitcoin data.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 7))\nplot_time_series(timesteps=btc_timesteps_turkey, \n                 values=btc_price_turkey, \n                 format=\"-\", \n                 label=\"BTC Price + Turkey Problem\", \n                 start=2500)\n</pre> plt.figure(figsize=(10, 7)) plot_time_series(timesteps=btc_timesteps_turkey,                   values=btc_price_turkey,                   format=\"-\",                   label=\"BTC Price + Turkey Problem\",                   start=2500) <p>How do you think building a model on this data will go?</p> <p>Remember, all we've changed is a single data point out of our entire dataset.</p> <p>Before we build a model, let's create some windowed datasets with our turkey data.</p> In\u00a0[\u00a0]: Copied! <pre># Create train and test sets for turkey problem data\nfull_windows, full_labels = make_windows(np.array(btc_price_turkey), window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n\nX_train, X_test, y_train, y_test = make_train_test_splits(full_windows, full_labels)\nlen(X_train), len(X_test), len(y_train), len(y_test)\n</pre> # Create train and test sets for turkey problem data full_windows, full_labels = make_windows(np.array(btc_price_turkey), window_size=WINDOW_SIZE, horizon=HORIZON) len(full_windows), len(full_labels)  X_train, X_test, y_train, y_test = make_train_test_splits(full_windows, full_labels) len(X_train), len(X_test), len(y_train), len(y_test) Out[\u00a0]: <pre>(2224, 556, 2224, 556)</pre> In\u00a0[\u00a0]: Copied! <pre># Clone model 1 architecture for turkey model and fit the turkey model on the turkey data\nturkey_model = tf.keras.models.clone_model(model_1)\nturkey_model._name = \"Turkey_Model\"\nturkey_model.compile(loss=\"mae\",\n                     optimizer=tf.keras.optimizers.Adam())\nturkey_model.fit(X_train, y_train,\n                 epochs=100,\n                 verbose=0,\n                 validation_data=(X_test, y_test),\n                 callbacks=[create_model_checkpoint(turkey_model.name)])\n</pre> # Clone model 1 architecture for turkey model and fit the turkey model on the turkey data turkey_model = tf.keras.models.clone_model(model_1) turkey_model._name = \"Turkey_Model\" turkey_model.compile(loss=\"mae\",                      optimizer=tf.keras.optimizers.Adam()) turkey_model.fit(X_train, y_train,                  epochs=100,                  verbose=0,                  validation_data=(X_test, y_test),                  callbacks=[create_model_checkpoint(turkey_model.name)]) <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdc7a4dd550&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate turkey model on test data\nturkey_model.evaluate(X_test, y_test)\n</pre> # Evaluate turkey model on test data turkey_model.evaluate(X_test, y_test) <pre>18/18 [==============================] - 0s 2ms/step - loss: 696.1285\n</pre> Out[\u00a0]: <pre>696.1284790039062</pre> In\u00a0[\u00a0]: Copied! <pre># Load best model and evaluate on test data\nturkey_model = tf.keras.models.load_model(\"model_experiments/Turkey_Model/\")\nturkey_model.evaluate(X_test, y_test)\n</pre> # Load best model and evaluate on test data turkey_model = tf.keras.models.load_model(\"model_experiments/Turkey_Model/\") turkey_model.evaluate(X_test, y_test) <pre>18/18 [==============================] - 0s 2ms/step - loss: 638.3047\n</pre> Out[\u00a0]: <pre>638.3046875</pre> <p>Alright, now let's make some predictions with our model and evaluate them on the test data.</p> In\u00a0[\u00a0]: Copied! <pre># Make predictions with Turkey model\nturkey_preds = make_preds(turkey_model, X_test)\nturkey_preds[:10]\n</pre> # Make predictions with Turkey model turkey_preds = make_preds(turkey_model, X_test) turkey_preds[:10] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([8858.391, 8803.98 , 9039.575, 8785.937, 8778.044, 8735.638,\n       8684.118, 8558.659, 8461.373, 8542.206], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate turkey preds\nturkey_results = evaluate_preds(y_true=y_test,\n                                y_pred=turkey_preds)\nturkey_results\n</pre> # Evaluate turkey preds turkey_results = evaluate_preds(y_true=y_test,                                 y_pred=turkey_preds) turkey_results Out[\u00a0]: <pre>{'mae': 17144.766,\n 'mape': 121.58286,\n 'mase': 26.53158,\n 'mse': 615487800.0,\n 'rmse': 23743.305}</pre> <p>And with just one value change, our error metrics go through the roof.</p> <p>To make sure, let's remind ourselves of how <code>model_1</code> went on unmodified Bitcoin data (no turkey problem).</p> In\u00a0[\u00a0]: Copied! <pre>model_1_results\n</pre> model_1_results Out[\u00a0]: <pre>{'mae': 568.95123,\n 'mape': 2.5448983,\n 'mase': 0.9994897,\n 'mse': 1171744.0,\n 'rmse': 1082.4713}</pre> <p>By changing just one value, the <code>turkey_model</code> MAE increases almost 30x over <code>model_1</code>.</p> <p>Finally, we'll visualize the turkey predictions over the test turkey data.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 7))\n# plot_time_series(timesteps=btc_timesteps_turkey[:split_size], values=btc_price_turkey[:split_size], label=\"Train Data\")\noffset=300\nplot_time_series(timesteps=btc_timesteps_turkey[-len(X_test):], \n                 values=btc_price_turkey[-len(y_test):], \n                 format=\"-\", \n                 label=\"Turkey Test Data\", start=offset)\nplot_time_series(timesteps=btc_timesteps_turkey[-len(X_test):],\n                 values=turkey_preds, \n                 label=\"Turkey Preds\", \n                 start=offset);\n</pre> plt.figure(figsize=(10, 7)) # plot_time_series(timesteps=btc_timesteps_turkey[:split_size], values=btc_price_turkey[:split_size], label=\"Train Data\") offset=300 plot_time_series(timesteps=btc_timesteps_turkey[-len(X_test):],                   values=btc_price_turkey[-len(y_test):],                   format=\"-\",                   label=\"Turkey Test Data\", start=offset) plot_time_series(timesteps=btc_timesteps_turkey[-len(X_test):],                  values=turkey_preds,                   label=\"Turkey Preds\",                   start=offset); <p>Why does this happen?</p> <p>Why does our model fail to capture the turkey problem data point?</p> <p>Think about it like this, just like a turkey who lives 1000 joyful days, based on observation alone has no reason to believe day 1001 won't be as joyful as the last, a model which has been trained on historical data of Bitcoin which has no single event where the price decreased by 100x in a day, has no reason to predict it will in the future.</p> <p>A model cannot predict anything in the future outside of the distribution it was trained on.</p> <p>In turn, highly unlikely price movements (based on historical movements), upward or downward will likely never be part of a forecast.</p> <p>However, as we've seen, despite their unlikeliness, these events can have huuuuuuuuge impacts to the performance of our models.</p> <p>\ud83d\udcd6 Resource: For a great article which discusses Black Swan events and how they often get ignored due to the assumption that historical events come from a certain distribution and that future events will come from the same distribution see Black Swans, Normal Distributions and Supply Chain Risk by Spend Matters.</p> In\u00a0[\u00a0]: Copied! <pre># Compare different model results (w = window, h = horizon, e.g. w=7 means a window size of 7)\nmodel_results = pd.DataFrame({\"naive_model\": naive_results,\n                              \"model_1_dense_w7_h1\": model_1_results,\n                              \"model_2_dense_w30_h1\": model_2_results,\n                              \"model_3_dense_w30_h7\": model_3_results,\n                              \"model_4_CONV1D\": model_4_results,\n                              \"model_5_LSTM\": model_5_results,\n                              \"model_6_multivariate\": model_6_results,\n                              \"model_8_NBEATs\": model_7_results,\n                              \"model_9_ensemble\": ensemble_results,\n                              \"model_10_turkey\": turkey_results}).T\nmodel_results.head(10)\n</pre> # Compare different model results (w = window, h = horizon, e.g. w=7 means a window size of 7) model_results = pd.DataFrame({\"naive_model\": naive_results,                               \"model_1_dense_w7_h1\": model_1_results,                               \"model_2_dense_w30_h1\": model_2_results,                               \"model_3_dense_w30_h7\": model_3_results,                               \"model_4_CONV1D\": model_4_results,                               \"model_5_LSTM\": model_5_results,                               \"model_6_multivariate\": model_6_results,                               \"model_8_NBEATs\": model_7_results,                               \"model_9_ensemble\": ensemble_results,                               \"model_10_turkey\": turkey_results}).T model_results.head(10) Out[\u00a0]: mae mse rmse mape mase naive_model 567.980225 1.147547e+06 1071.236206 2.516525 0.999570 model_1_dense_w7_h1 568.951233 1.171744e+06 1082.471313 2.544898 0.999490 model_2_dense_w30_h1 608.961487 1.281439e+06 1132.006470 2.769339 1.064471 model_3_dense_w30_h7 1237.506348 5.405198e+06 1425.747681 5.558878 2.202074 model_4_CONV1D 570.828308 1.176671e+06 1084.744751 2.559336 1.002787 model_5_LSTM 596.644653 1.273487e+06 1128.488770 2.683845 1.048139 model_6_multivariate 567.587402 1.161688e+06 1077.816528 2.541387 0.997094 model_8_NBEATs 585.499817 1.179492e+06 1086.043945 2.744519 1.028561 model_9_ensemble 567.442322 1.144513e+06 1069.819092 2.584332 0.996839 model_10_turkey 17144.765625 6.154878e+08 23743.304688 121.582863 26.531580 In\u00a0[\u00a0]: Copied! <pre># Sort model results by MAE and plot them\nmodel_results[[\"mae\"]].sort_values(by=\"mae\").plot(figsize=(10, 7), kind=\"bar\");\n</pre> # Sort model results by MAE and plot them model_results[[\"mae\"]].sort_values(by=\"mae\").plot(figsize=(10, 7), kind=\"bar\"); <p>The majority of our deep learning models perform on par or only slightly better than the naive model. And for the turkey model, changing a single data point destroys its performance.</p> <p>\ud83d\udd11 Note: Just because one type of model performs better here doesn't mean it'll perform the best elsewhere (and vice versa, just because one model performs poorly here, doesn't mean it'll perform poorly elsewhere).</p> <p>As I said at the start, this is not financial advice.</p> <p>After what we've gone through, you'll now have some of the skills required to callout BS for any future tutorial or blog post or investment sales guide claiming to have model which is able to predict the futrue.</p> <p>Mark Saroufim's Tweet sums this up nicely (stock market forecasting with a machine learning model is just as reliable as palm reading).</p> <p> Beware the tutorials or trading courses which claim to use some kind of algorithm to beat the market (an open system), they're likely a scam or the creator is very lucky and hasn't yet come across a turkey problem.</p> <p>Don't let these results get you down though, forecasting in a closed system (such as predicting the demand of electricity) often yields quite usable results.</p> <p>If anything, this module teaches anti-knowledge. Knowing that forecasting methods usually don't perform well in open systems.</p> <p>Plus, sometimes not knowing the future is a benefit. A known future is already the past.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#10-milestone-project-3-time-series-forecasting-in-tensorflow-bitpredict","title":"10. Milestone Project 3: Time series forecasting in TensorFlow (BitPredict \ud83d\udcb0\ud83d\udcc8)\u00b6","text":"<p>The goal of this notebook is to get you familiar with working with time series data.</p> <p>We're going to be building a series of models in an attempt to predict the price of Bitcoin.</p> <p>Welcome to Milestone Project 3, BitPredict \ud83d\udcb0\ud83d\udcc8!</p> <p>\ud83d\udd11 Note: \u26a0\ufe0f This is not financial advice, as you'll see time series forecasting for stock market prices is actually quite terrible.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#what-is-a-time-series-problem","title":"What is a time series problem?\u00b6","text":"<p>Time series problems deal with data over time.</p> <p>Such as, the number of staff members in a company over 10-years, sales of computers for the past 5-years, electricity usage for the past 50-years.</p> <p>The timeline can be short (seconds/minutes) or long (years/decades). And the problems you might investigate using can usually be broken down into two categories.</p> <p></p> Problem Type Examples Output Classification Anomaly detection, time series identification (where did this time series come from?) Discrete (a label) Forecasting Predicting stock market prices, forecasting future demand for a product, stocking inventory requirements Continuous (a number) <p>In both cases above, a supervised learning approach is often used. Meaning, you'd have some example data and a label assosciated with that data.</p> <p>For example, in forecasting the price of Bitcoin, your data could be the historical price of Bitcoin for the past month and the label could be today's price (the label can't be tomorrow's price because that's what we'd want to predict).</p> <p>Can you guess what kind of problem BitPredict \ud83d\udcb0\ud83d\udcc8 is?</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Are you ready?</p> <p>We've got a lot to go through.</p> <ul> <li>Get time series data (the historical price of Bitcoin)<ul> <li>Load in time series data using pandas/Python's CSV module</li> </ul> </li> <li>Format data for a time series problem<ul> <li>Creating training and test sets (the wrong way)</li> <li>Creating training and test sets (the right way)</li> <li>Visualizing time series data</li> <li>Turning time series data into a supervised learning problem (windowing)</li> <li>Preparing univariate and multivariate (more than one variable) data</li> </ul> </li> <li>Evaluating a time series forecasting model</li> <li>Setting up a series of deep learning modelling experiments<ul> <li>Dense (fully-connected) networks</li> <li>Sequence models (LSTM and 1D CNN)</li> <li>Ensembling (combining multiple models together)</li> <li>Multivariate models</li> <li>Replicating the N-BEATS algorithm using TensorFlow layer subclassing</li> </ul> </li> <li>Creating a modelling checkpoint to save the best performing model during training</li> <li>Making predictions (forecasts) with a time series model</li> <li>Creating prediction intervals for time series model forecasts</li> <li>Discussing two different types of uncertainty in machine learning (data uncertainty and model uncertainty)</li> <li>Demonstrating why forecasting in an open system is BS (the turkey problem)</li> </ul>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p> <p>\ud83d\udcd6 Resource: Get all of the materials you need for this notebook on the course GitHub.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#check-for-gpu","title":"Check for GPU\u00b6","text":"<p>In order for our deep learning models to run as fast as possible, we'll need access to a GPU.</p> <p>In Google Colab, you can set this up by going to Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>After selecting GPU, you may have to restart the runtime.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#get-data","title":"Get data\u00b6","text":"<p>To build a time series forecasting model, the first thing we're going to need is data.</p> <p>And since we're trying to predict the price of Bitcoin, we'll need Bitcoin data.</p> <p>Specifically, we're going to get the prices of Bitcoin from 01 October 2013 to 18 May 2021.</p> <p>Why these dates?</p> <p>Because 01 October 2013 is when our data source (Coindesk) started recording the price of Bitcoin and 18 May 2021 is when this notebook was created.</p> <p>If you're going through this notebook at a later date, you'll be able to use what you learn to predict on later dates of Bitcoin, you'll just have to adjust the data source.</p> <p>\ud83d\udcd6 Resource: To get the Bitcoin historical data, I went to the Coindesk page for Bitcoin prices, clicked on \"all\" and then clicked on \"Export data\" and selected \"CSV\".</p> <p>You can find the data we're going to use on GitHub.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#importing-time-series-data-with-pandas","title":"Importing time series data with pandas\u00b6","text":"<p>Now we've got some data to work with, let's import it using pandas so we can visualize it.</p> <p>Because our data is in CSV (comma separated values) format (a very common data format for time series), we'll use the pandas <code>read_csv()</code> function.</p> <p>And because our data has a date component, we'll tell pandas to parse the dates using the <code>parse_dates</code> parameter passing it the name our of the date column (\"Date\").</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#importing-time-series-data-with-pythons-csv-module","title":"Importing time series data with Python's CSV module\u00b6","text":"<p>If your time series data comes in CSV form you don't necessarily have to use pandas.</p> <p>You can use Python's in-built <code>csv</code> module. And if you're working with dates, you might also want to use Python's <code>datetime</code>.</p> <p>Let's see how we can replicate the plot we created before except this time using Python's <code>csv</code> and <code>datetime</code> modules.</p> <p>\ud83d\udcd6 Resource: For a great guide on using Python's <code>csv</code> module, check out Real Python's tutorial on Reading and Writing CSV files in Python.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#format-data-part-1-creatining-train-and-test-sets-for-time-series-data","title":"Format Data Part 1: Creatining train and test sets for time series data\u00b6","text":"<p>Alrighty. What's next?</p> <p>If you guessed preparing our data for a model, you'd be right.</p> <p>What's the most important first step for preparing any machine learning dataset?</p> <p>Scaling?</p> <p>No...</p> <p>Removing outliers?</p> <p>No...</p> <p>How about creating train and test splits?</p> <p>Yes!</p> <p>Usually, you could create a train and test split using a function like Scikit-Learn's outstanding <code>train_test_split()</code> but as we'll see in a moment, this doesn't really cut it for time series data.</p> <p>But before we do create splits, it's worth talking about what kind of data we have.</p> <p>In time series problems, you'll either have univariate or multivariate data.</p> <p>Can you guess what our data is?</p> <ul> <li>Univariate time series data deals with one variable, for example, using the price of Bitcoin to predict the price of Bitcoin.</li> <li>Multivariate time series data deals with more than one variable, for example, predicting electricity demand using the day of week, time of year and number of houses in a region.</li> </ul> <p> Example of univariate and multivariate time series data. Univariate involves using the target to predict the target. Multivariate inolves using the target as well as another time series to predict the target.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#create-train-test-sets-for-time-series-the-wrong-way","title":"Create train &amp; test sets for time series (the wrong way)\u00b6","text":"<p>Okay, we've figured out we're dealing with a univariate time series, so we only have to make a split on one variable (for multivariate time series, you will have to split multiple variables).</p> <p>How about we first see the wrong way for splitting time series data?</p> <p>Let's turn our DataFrame index and column into NumPy arrays.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#create-train-test-sets-for-time-series-the-right-way","title":"Create train &amp; test sets for time series (the right way)\u00b6","text":"<p>Of course, there's no way we can actually access data from the future.</p> <p>But we can engineer our test set to be in the future with respect to the training set.</p> <p>To do this, we can create an abitrary point in time to split our data.</p> <p>Everything before the point in time can be considered the training set and everything after the point in time can be considered the test set.</p> <p> Demonstration of time series split. Rather than a traditionaly random train/test split, it's best to split the time series data sequentially. Meaning, the test data should be data from the future when compared to the training data.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#create-a-plotting-function","title":"Create a plotting function\u00b6","text":"<p>Rather than retyping <code>matplotlib</code> commands to continuously plot data, let's make a plotting function we can reuse later.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#modelling-experiments","title":"Modelling Experiments\u00b6","text":"<p>We can build almost any kind of model for our problem as long as the data inputs and outputs are formatted correctly.</p> <p>However, just because we can build almost any kind of model, doesn't mean it'll perform well/should be used in a production setting.</p> <p>We'll see what this means as we build and evaluate models throughout.</p> <p>Before we discuss what modelling experiments we're going to run, there are two terms you should be familiar with, horizon and window.</p> <ul> <li>horizon = number of timesteps to predict into future</li> <li>window = number of timesteps from past used to predict horizon</li> </ul> <p>For example, if we wanted to predict the price of Bitcoin for tomorrow (1 day in the future) using the previous week's worth of Bitcoin prices (7 days in the past), the horizon would be 1 and the window would be 7.</p> <p>Now, how about those modelling experiments?</p> Model Number Model Type Horizon size Window size Extra data 0 Na\u00efve model (baseline) NA NA NA 1 Dense model 1 7 NA 2 Same as 1 1 30 NA 3 Same as 1 7 30 NA 4 Conv1D 1 7 NA 5 LSTM 1 7 NA 6 Same as 1 (but with multivariate data) 1 7 Block reward size 7 N-BEATs Algorithm 1 7 NA 8 Ensemble (multiple models optimized on different loss functions) 1 7 NA 9 Future prediction model (model to predict future values) 1 7 NA 10 Same as 1 (but with turkey \ud83e\udd83 data introduced) 1 7 NA <p>\ud83d\udd11 Note: To reiterate, as you can see, we can build many types of models for the data we're working with. But that doesn't mean that they'll perform well. Deep learning is a powerful technique but it doesn't always work. And as always, start with a simple model first and then add complexity as needed.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-0-naive-forecast-baseline","title":"Model 0: Na\u00efve forecast (baseline)\u00b6","text":"<p>As usual, let's start with a baseline.</p> <p>One of the most common baseline models for time series forecasting, the na\u00efve model (also called the na\u00efve forecast), requires no training at all.</p> <p>That's because all the na\u00efve model does is use the previous timestep value to predict the next timestep value.</p> <p>The formula looks like this:</p> <p>$$\\hat{y}_{t} = y_{t-1}$$</p> <p>In English:</p> <p>The prediction at timestep <code>t</code> (y-hat) is equal to the value at timestep <code>t-1</code> (the previous timestep).</p> <p>Sound simple?</p> <p>Maybe not.</p> <p>In an open system (like a stock market or crypto market), you'll often find beating the na\u00efve forecast with any kind of model is quite hard.</p> <p>\ud83d\udd11 Note: For the sake of this notebook, an open system is a system where inputs and outputs can freely flow, such as a market (stock or crypto). Where as, a closed system the inputs and outputs are contained within the system (like a poker game with your buddies, you know the buy in and you know how much the winner can get). Time series forecasting in open systems is generally quite poor.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#evaluating-a-time-series-model","title":"Evaluating a time series model\u00b6","text":"<p>Time series forecasting often involves predicting a number (in our case, the price of Bitcoin).</p> <p>And what kind of problem is predicting a number?</p> <p>Ten points if you said regression.</p> <p>With this known, we can use regression evaluation metrics to evaluate our time series forecasts.</p> <p>The main thing we will be evaluating is: how do our model's predictions (<code>y_pred</code>) compare against the actual values (<code>y_true</code> or ground truth values)?</p> <p>\ud83d\udcd6 Resource: We're going to be using several metrics to evaluate our different model's time series forecast accuracy. Many of them are sourced and explained mathematically and conceptually in Forecasting: Principles and Practice chapter 5.8, I'd recommend reading through here for a more in-depth overview of what we're going to practice.</p> <p>For all of the following metrics, lower is better (for example an MAE of 0 is better than an MAE 100).</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#scale-dependent-errors","title":"Scale-dependent errors\u00b6","text":"<p>These are metrics which can be used to compare time series values and forecasts that are on the same scale.</p> <p>For example, Bitcoin historical prices in USD veresus Bitcoin forecast values in USD.</p> Metric Details Code MAE (mean absolute error) Easy to interpret (a forecast is X amount different from actual amount). Forecast methods which minimises the MAE will lead to forecasts of the median. <code>tf.keras.metrics.mean_absolute_error()</code> RMSE (root mean square error) Forecasts which minimise the RMSE lead to forecasts of the mean. <code>tf.sqrt(</code><code>tf.keras.metrics.mean_square_error()</code><code>)</code>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#percentage-errors","title":"Percentage errors\u00b6","text":"<p>Percentage errors do not have units, this means they can be used to compare forecasts across different datasets.</p> Metric Details Code MAPE (mean absolute percentage error) Most commonly used percentage error. May explode (not work) if <code>y=0</code>. <code>tf.keras.metrics.mean_absolute_percentage_error()</code> sMAPE (symmetric mean absolute percentage error) Recommended not to be used by Forecasting: Principles and Practice, though it is used in forecasting competitions. Custom implementation"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#scaled-errors","title":"Scaled errors\u00b6","text":"<p>Scaled errors are an alternative to percentage errors when comparing forecast performance across different time series.</p> Metric Details Code MASE (mean absolute scaled error). MASE equals one for the naive forecast (or very close to one). A forecast which performs better than the na\u00efve should get &lt;1 MASE. See sktime's <code>mase_loss()</code> <p>\ud83e\udd14 Question: There are so many metrics... which one should I pay most attention to? It's going to depend on your problem. However, since its ease of interpretation (you can explain it in a sentence to your grandma), MAE is often a very good place to start.</p> <p>Since we're going to be evaluing a lot of models, let's write a function to help us calculate evaluation metrics on their forecasts.</p> <p>First we'll need TensorFlow.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#other-kinds-of-time-series-forecasting-models-which-can-be-used-for-baselines-and-actual-forecasts","title":"Other kinds of time series forecasting models which can be used for baselines and actual forecasts\u00b6","text":"<p>Since we've got a na\u00efve forecast baseline to work with, it's time we start building models to try and beat it.</p> <p>And because this course is focused on TensorFlow and deep learning, we're going to be using TensorFlow to build deep learning models to try and improve on our na\u00efve forecasting results.</p> <p>That being said, there are many other kinds of models you may want to look into for building baselines/performing forecasts.</p> <p>Some of them may even beat our best performing models in this notebook, however, I'll leave trying them out for extra-curriculum.</p> Model/Library Name Resource Moving average https://machinelearningmastery.com/moving-average-smoothing-for-time-series-forecasting-python/ ARIMA (Autoregression Integrated Moving Average) https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/ sktime (Scikit-Learn for time series) https://github.com/alan-turing-institute/sktime TensorFlow Decision Forests (random forest, gradient boosting trees) https://www.tensorflow.org/decision_forests Facebook Kats (purpose-built forecasting and time series analysis library by Facebook) https://github.com/facebookresearch/Kats LinkedIn Greykite (flexible, intuitive and fast forecasts) https://github.com/linkedin/greykite"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#format-data-part-2-windowing-dataset","title":"Format Data Part 2: Windowing dataset\u00b6","text":"<p>Surely we'd be ready to start building models by now?</p> <p>We're so close! Only one more step (really two) to go.</p> <p>We've got to window our time series.</p> <p>Why do we window?</p> <p>Windowing is a method to turn a time series dataset into supervised learning problem.</p> <p>In other words, we want to use windows of the past to predict the future.</p> <p>For example for a univariate time series, windowing for one week (<code>window=7</code>) to predict the next single value (<code>horizon=1</code>) might look like:</p> <pre><code>Window for one week (univariate time series)\n\n[0, 1, 2, 3, 4, 5, 6] -&gt; [7]\n[1, 2, 3, 4, 5, 6, 7] -&gt; [8]\n[2, 3, 4, 5, 6, 7, 8] -&gt; [9]\n</code></pre> <p>Or for the price of Bitcoin, it'd look like:</p> <pre><code>Window for one week with the target of predicting the next day (Bitcoin prices)\n\n[123.654, 125.455, 108.584, 118.674, 121.338, 120.655, 121.795] -&gt; [123.033]\n[125.455, 108.584, 118.674, 121.338, 120.655, 121.795, 123.033] -&gt; [124.049]\n[108.584, 118.674, 121.338, 120.655, 121.795, 123.033, 124.049] -&gt; [125.961]\n</code></pre> <p> Example of windows and horizons for Bitcoin data. Windowing can be used to turn time series data into a supervised learning problem.</p> <p>Let's build some functions which take in a univariate time series and turn it into windows and horizons of specified sizes.</p> <p>We'll start with the default horizon size of 1 and a window size of 7 (these aren't necessarily the best values to use, I've just picked them).</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#turning-windows-into-training-and-test-sets","title":"Turning windows into training and test sets\u00b6","text":"<p>Look how good those windows look! Almost like the stain glass windows on the Sistine Chapel, well, maybe not that good but still.</p> <p>Time to turn our windows into training and test splits.</p> <p>We could've windowed our existing training and test splits, however, with the nature of windowing (windowing often requires an offset at some point in the data), it usually works better to window the data first, then split it into training and test sets.</p> <p>Let's write a function which takes in full sets of windows and their labels and splits them into train and test splits.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#make-a-modelling-checkpoint","title":"Make a modelling checkpoint\u00b6","text":"<p>We're so close to building models. So so so close.</p> <p>Because our model's performance will fluctuate from experiment to experiment, we'll want to make sure we're comparing apples to apples.</p> <p>What I mean by this is in order for a fair comparison, we want to compare each model's best performance against each model's best performance.</p> <p>For example, if <code>model_1</code> performed incredibly well on epoch 55 but its performance fell off toward epoch 100, we want the version of the model from epoch 55 to compare to other models rather than the version of the model from epoch 100.</p> <p>And the same goes for each of our other models: compare the best against the best.</p> <p>To take of this, we'll implement a <code>ModelCheckpoint</code> callback.</p> <p>The <code>ModelCheckpoint</code> callback will monitor our model's performance during training and save the best model to file by setting <code>save_best_only=True</code>.</p> <p>That way when evaluating our model we could restore its best performing configuration from file.</p> <p>\ud83d\udd11 Note: Because of the size of the dataset (smaller than usual), you'll notice our modelling experiment results fluctuate quite a bit during training (hence the implementation of the <code>ModelCheckpoint</code> callback to save the best model).</p> <p>Because we're going to be running multiple experiments, it makes sense to keep track of them by saving models to file under different names.</p> <p>To do this, we'll write a small function to create a <code>ModelCheckpoint</code> callback which saves a model to specified filename.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-1-dense-model-window-7-horizon-1","title":"Model 1: Dense model (window = 7, horizon = 1)\u00b6","text":"<p>Finally!</p> <p>Time to build one of our models.</p> <p>If you think we've been through a fair bit of preprocessing before getting here, you're right.</p> <p>Often, preparing data for a model is one of the largest parts of any machine learning project.</p> <p>And once you've got a good model in place, you'll probably notice far more improvements from manipulating the data (e.g. collecting more, improving the quality) than manipulating the model.</p> <p>We're going to start by keeping it simple, <code>model_1</code> will have:</p> <ul> <li>A single dense layer with 128 hidden units and ReLU (rectified linear unit) activation</li> <li>An output layer with linear activation (or no activation)</li> <li>Adam optimizer and MAE loss function</li> <li>Batch size of 128</li> <li>100 epochs</li> </ul> <p>Why these values?</p> <p>I picked them out of experimentation.</p> <p>A batch size of 32 works pretty well too and we could always train for less epochs but since the model runs so fast (you'll see in a second, it's because the number of samples we have isn't massive) we might as well train for more.</p> <p>\ud83d\udd11 Note: As always, many of the values for machine learning problems are experimental. A reminder that the values you can set yourself in a machine learning algorithm (the hidden units, the batch size, horizon size, window size) are called hyperparameters. And experimenting to find the best values for hyperparameters is called hyperparameter tuning. Where as parameters learned by a model itself (patterns in the data, formally called weights &amp; biases) are referred to as parameters.</p> <p>Let's import TensorFlow and build our first deep learning model for time series.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#making-forecasts-with-a-model-on-the-test-dataset","title":"Making forecasts with a model (on the test dataset)\u00b6","text":"<p>We've trained a model and evaluated the it on the test data, but the project we're working on is called BitPredict \ud83d\udcb0\ud83d\udcc8 so how do you think we could use our model to make predictions?</p> <p>Since we're going to be running more modelling experiments, let's write a function which:</p> <ol> <li>Takes in a trained model (just like <code>model_1</code>)</li> <li>Takes in some input data (just like the data the model was trained on)</li> <li>Passes the input data to the model's <code>predict()</code> method</li> <li>Returns the predictions</li> </ol>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-2-dense-window-30-horizon-1","title":"Model 2: Dense (window = 30, horizon = 1)\u00b6","text":"<p>A na\u00efve model is currently beating our handcrafted deep learning model.</p> <p>We can't let this happen.</p> <p>Let's continue our modelling experiments.</p> <p>We'll keep the previous model architecture but use a window size of 30.</p> <p>In other words, we'll use the previous 30 days of Bitcoin prices to try and predict the next day price.</p> <p> Example of Bitcoin prices windowed for 30 days to predict a horizon of 1.</p> <p>\ud83d\udd11 Note: Recall from before, the window size (how many timesteps to use to fuel a forecast) and the horizon (how many timesteps to predict into the future) are hyperparameters. This means you can tune them to try and find values will result in better performance.</p> <p>We'll start our second modelling experiment by preparing datasets using the functions we created earlier.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-3-dense-window-30-horizon-7","title":"Model 3: Dense (window = 30, horizon = 7)\u00b6","text":"<p>Let's try and predict 7 days ahead given the previous 30 days.</p> <p>First, we'll update the <code>HORIZON</code> and <code>WINDOW_SIZE</code> variables and create windowed data.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#make-our-evaluation-function-work-for-larger-horizons","title":"Make our evaluation function work for larger horizons\u00b6","text":"<p>You'll notice the outputs for <code>model_3_results</code> are multi-dimensional.</p> <p>This is because the predictions are getting evaluated across the <code>HORIZON</code> timesteps (7 predictions at a time).</p> <p>To fix this, let's adjust our <code>evaluate_preds()</code> function to work with multiple shapes of data.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#which-of-our-models-is-performing-best-so-far","title":"Which of our models is performing best so far?\u00b6","text":"<p>So far, we've trained 3 models which use the same architecture but use different data inputs.</p> <p>Let's compare them with the na\u00efve model to see which model is performing the best so far.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-4-conv1d","title":"Model 4: Conv1D\u00b6","text":"<p>Onto the next modelling experiment!</p> <p>This time, we'll be using a Conv1D model. Because as we saw in the sequence modelling notebook, Conv1D models can be used for seq2seq (sequence to sequence) problems.</p> <p>In our case, the input sequence is the previous 7 days of Bitcoin price data and the output is the next day (in seq2seq terms this is called a many to one problem).</p> <p> Framing Bitcoin forecasting in seq2seq (sequence to sequence) terms. Using a window size of 7 and a horizon of one results in a many to one problem. Using a window size of &gt;1 and a horizon of &gt;1 results in a many to many problem. The diagram comes from Andrei Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks.</p> <p>Before we build a Conv1D model, let's recreate our datasets.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-5-rnn-lstm","title":"Model 5: RNN (LSTM)\u00b6","text":"<p>As you might've guessed, we can also use a recurrent neural network to model our sequential time series data.</p> <p>\ud83d\udcd6 Resource: For more on the different types of recurrent neural networks you can use for sequence problems, see the Recurrent Neural Networks section of notebook 08.</p> <p>Let's reuse the same data we used for the Conv1D model, except this time we'll create an LSTM-cell powered RNN to model our Bitcoin data.</p> <p>Once again, one of the most important steps for the LSTM model will be getting our data into the right shape.</p> <p>The <code>tf.keras.layers.LSTM()</code> layer takes a tensor with <code>[batch, timesteps, feature]</code> dimensions.</p> <p>As mentioned earlier, the <code>batch</code> dimension gets taken care of for us but our data is currently only has the <code>feature</code> dimension (<code>WINDOW_SIZE</code>).</p> <p>To fix this, just like we did with the <code>Conv1D</code> model, we can use a <code>tf.keras.layers.Lambda()</code> layer to adjust the shape of our input tensors to the LSTM layer.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#make-a-multivariate-time-series","title":"Make a multivariate time series\u00b6","text":"<p>So far all of our models have barely kept up with the na\u00efve forecast.</p> <p>And so far all of them have been trained on a single variable (also called univariate time series): the historical price of Bitcoin.</p> <p>If predicting the price of Bitcoin using the price of Bitcoin hasn't worked out very well, maybe giving our model more information may help.</p> <p>More information is a vague term because we could actually feed almost anything to our model(s) and they would still try to find patterns.</p> <p>For example, we could use the historical price of Bitcoin as well as anyone with the name Daniel Bourke Tweeted on that day to predict the future price of Bitcoin.</p> <p>But would this help?</p> <p>Porbably not.</p> <p>What would be better is if we passed our model something related to Bitcoin (again, this is quite vauge, since in an open system like a market, you could argue everything is related).</p> <p>This will be different for almost every time series you work on but in our case, we could try to see if the Bitcoin block reward size adds any predictive power to our model(s).</p> <p>What is the Bitcoin block reward size?</p> <p>The Bitcoin block reward size is the number of Bitcoin someone receives from mining a Bitcoin block.</p> <p>At its inception, the Bitcoin block reward size was 50.</p> <p>But every four years or so, the Bitcoin block reward halves.</p> <p>For example, the block reward size went from 50 (starting January 2009) to 25 on November 28 2012.</p> <p>Let's encode this information into our time series data and see if it helps a model's performance.</p> <p>\ud83d\udd11 Note: Adding an extra feature to our dataset such as the Bitcoin block reward size will take our data from univariate (only the historical price of Bitcoin) to multivariate (the price of Bitcoin as well as the block reward size).</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#making-a-windowed-dataset-with-pandas","title":"Making a windowed dataset with pandas\u00b6","text":"<p>Previously, we used some custom made functions to window our univariate time series.</p> <p>However, since we've just added another variable to our dataset, these functions won't work.</p> <p>Not to worry though. Since our data is in a pandas DataFrame, we can leverage the <code>pandas.DataFrame.shift()</code> method to create a windowed multivariate time series.</p> <p>The <code>shift()</code> method offsets an index by a specified number of periods.</p> <p>Let's see it in action.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-6-dense-multivariate-time-series","title":"Model 6: Dense (multivariate time series)\u00b6","text":"<p>To keep things simple, let's the <code>model_1</code> architecture and use it to train and make predictions on our multivariate time series data.</p> <p>By replicating the <code>model_1</code> architecture we'll be able to see whether or not adding the block reward feature improves or detracts from model performance.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-7-n-beats-algorithm","title":"Model 7: N-BEATS algorithm\u00b6","text":"<p>Time to step things up a notch.</p> <p>So far we've tried a bunch of smaller models, models with only a couple of layers.</p> <p>But one of the best ways to improve a model's performance is to increase the number of layers in it.</p> <p>That's exactly what the N-BEATS (Neural Basis Expansion Analysis for Interpretable Time Series Forecasting) algorithm does.</p> <p>The N-BEATS algorithm focuses on univariate time series problems and achieved state-of-the-art performance in the winner of the M4 competition (a forecasting competition).</p> <p>For our next modelling experiment we're going to be replicating the generic architecture of the N-BEATS algorithm (see section 3.3 of the N-BEATS paper).</p> <p>We're not going to go through all of the details in the paper, instead we're going to focus on:</p> <ol> <li>Replicating the model architecture in Figure 1 of the N-BEATS paper</li> </ol> <p> N-BEATS algorithm we're going to replicate with TensorFlow with window (input) and horizon (output) annotations.</p> <ol> <li>Using the same hyperparameters as the paper which can be found in Appendix D of the N-BEATS paper</li> </ol> <p>Doing this will give us an opportunity to practice:</p> <ul> <li>Creating a custom layer for the <code>NBeatsBlock</code> by subclassing <code>tf.keras.layers.Layer</code><ul> <li>Creating a custom layer is helpful for when TensorFlow doesn't already have an existing implementation of a layer or if you'd like to make a layer configuration repeat a number of times (e.g. like a stack of N-BEATS blocks)</li> </ul> </li> <li>Implementing a custom architecture using the Functional API</li> <li>Finding a paper related to our problem and seeing how it goes</li> </ul> <p>\ud83d\udd11 Note: As you'll see in the paper, the authors state \u201cN-BEATS is implemented and trained in TensorFlow\u201d, that's what we'll be doing too!</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#building-and-testing-an-n-beats-block-layer","title":"Building and testing an N-BEATS block layer\u00b6","text":"<p>Let's start by building an N-BEATS block layer, we'll write the code first and then discuss what's going on.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#preparing-data-for-the-n-beats-algorithm-using-tfdata","title":"Preparing data for the N-BEATS algorithm using <code>tf.data</code>\u00b6","text":"<p>We've got the basic building block for the N-BEATS architecture ready to go.</p> <p>But before we use it to replicate the entire N-BEATS generic architecture, let's create some data.</p> <p>This time, because we're going to be using a larger model architecture, to ensure our model training runs as fast as possible, we'll setup our datasets using the <code>tf.data</code> API.</p> <p>And because the N-BEATS algorithm is focused on univariate time series, we'll start by making training and test windowed datasets of Bitcoin prices (just as we've done above).</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#setting-up-hyperparameters-for-n-beats-algorithm","title":"Setting up hyperparameters for N-BEATS algorithm\u00b6","text":"<p>Ho ho, would you look at that! Datasets ready, model building block ready, what'd you say we put things together?</p> <p>Good idea.</p> <p>Okay.</p> <p>Let's go.</p> <p>To begin, we'll create variables for each of the hyperparameters we'll be using for our N-BEATS replica.</p> <p>\ud83d\udcd6 Resource: The following hyperparameters are taken from Figure 1 and Table 18/Appendix D of the N-BEATS paper.</p> <p> Table 18 from N-BEATS paper describing the hyperparameters used for the different variants of N-BEATS. We're using N-BEATS-G which stands for the generic version of N-BEATS.</p> <p>\ud83d\udd11 Note: If you see variables in a machine learning example in all caps, such as \"<code>N_EPOCHS = 100</code>\", these variables are often hyperparameters which are used through the example. You'll usually see them instantiated towards the start of an experiment and then used throughout.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#getting-ready-for-residual-connections","title":"Getting ready for residual connections\u00b6","text":"<p>Beautiful! Hyperparameters ready, now before we create the N-BEATS model, there are two layers to go through which play a large roll in the architecture.</p> <p>They're what make N-BEATS double residual stacking (section 3.2 of the N-BEATS paper) possible:</p> <ul> <li><code>tf.keras.layers.subtract(inputs)</code> - subtracts list of input tensors from each other</li> <li><code>tf.keras.layers.add(inputs)</code> - adds list of input tensors to each other</li> </ul> <p>Let's try them out.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#building-compiling-and-fitting-the-n-beats-algorithm","title":"Building, compiling and fitting the N-BEATS algorithm\u00b6","text":"<p>Okay, we've finally got all of the pieces of the puzzle ready for building and training the N-BEATS algorithm.</p> <p>We'll do so by going through the following:</p> <ol> <li>Setup an instance of the N-BEATS block layer using <code>NBeatsBlock</code> (this'll be the initial block used for the network, the rest will be created as part of stacks)</li> <li>Create an input layer for the N-BEATS stack (we'll be using the Keras Functional API for this)</li> <li>Make the initial backcast and forecasts for the model with the layer created in (1)</li> <li>Use a for loop to create stacks of block layers</li> <li>Use the NBeatsBlock class within the for loop created in (4) to create blocks which return backcasts and block-level forecasts</li> <li>Create the double residual stacking using subtract and add layers</li> <li>Put the model inputs and outputs together using <code>tf.keras.Model()</code></li> <li>Compile the model with MAE loss (the paper uses multiple losses but we'll use MAE to keep it inline with our other models) and Adam optimizer with default settings as per section 5.2 of N-BEATS paper)</li> <li>Fit the N-BEATS model for 5000 epochs and since it's fitting for so many epochs, we'll use a couple of callbacks:</li> </ol> <ul> <li><code>tf.keras.callbacks.EarlyStopping()</code> - stop the model from training if it doesn't improve validation loss for 200 epochs and restore the best performing weights using <code>restore_best_weights=True</code> (this'll prevent the model from training for loooongggggg period of time without improvement)</li> <li><code>tf.keras.callbacks.ReduceLROnPlateau()</code> - if the model's validation loss doesn't improve for 100 epochs, reduce the learning rate by 10x to try and help it make incremental improvements (the smaller the learning rate, the smaller updates a model tries to make)</li> </ul> <p>Woah. A bunch of steps. But I'm sure you're up to it.</p> <p>Let's do it!</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#plotting-the-n-beats-architecture-weve-created","title":"Plotting the N-BEATS architecture we've created\u00b6","text":"<p>You know what would be cool?</p> <p>If we could plot the N-BEATS model we've crafted.</p> <p>Well it turns out we can using <code>tensorflow.keras.utils.plot_model()</code>.</p> <p>Let's see what it looks like.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-8-creating-an-ensemble-stacking-different-models-together","title":"Model 8: Creating an ensemble (stacking different models together)\u00b6","text":"<p>After all that effort, the N-BEATS algorithm's performance was underwhelming.</p> <p>But again, this is part of the parcel of machine learning. Not everything will work.</p> <p>That's when we refer back to the motto: experiment, experiment, experiment.</p> <p>Our next experiment is creating an ensemble of models.</p> <p>An ensemble involves training and combining multiple different models on the same problem. Ensemble models are often the types of models you'll see winning data science competitions on websites like Kaggle.</p> <p> Example of the power of ensembling. One Daniel model makes a decision with a smart level of 7 but when a Daniel model teams up with multiple different people, together (ensembled) they make a decision with a smart level of 10. The key here is combining the decision power of people with different backgrounds, if you combined multiple Daniel models, you'd end up with an average smart level of 7. Note: smart level is not an actual measurement of decision making, it is for demonstration purposes only.</p> <p>For example, in the N-BEATS paper, they trained an ensemble of models (180 in total, see section 3.4) to achieve the results they did using a combination of:</p> <ul> <li>Different loss functions (sMAPE, MASE and MAPE)</li> <li>Different window sizes (2 x horizon, 3 x horizon, 4 x horizon...)</li> </ul> <p>The benefit of ensembling models is you get the \"decision of the crowd effect\". Rather than relying on a single model's predictions, you can take the average or median of many different models.</p> <p>The keyword being: different.</p> <p>It wouldn't make sense to train the same model 10 times on the same data and then average the predictions.</p> <p>Fortunately, due to their random initialization, even deep learning models with the same architecture can produce different results.</p> <p>What I mean by this is each time you create a deep learning model, it starts with random patterns (weights &amp; biases) and then it adjusts these random patterns to better suit the dataset it's being trained on.</p> <p>However, the process it adjusts these patterns is often a form of guided randomness as well (the SGD optimizer stands for stochastic or random gradient descent).</p> <p>To create our ensemble models we're going to be using a combination of:</p> <ul> <li>Different loss functions (MAE, MSE, MAPE)</li> <li>Randomly initialized models</li> </ul> <p>Essentially, we'll be creating a suite of different models all attempting to model the same data.</p> <p>And hopefully the combined predictive power of each model is better than a single model on its own.</p> <p>Let's find out!</p> <p>We'll start by creating a function to produce a list of different models trained with different loss functions. Each layer in the ensemble models will be initialized with a random normal (Gaussian) distribution using He normal initialization, this'll help estimating the prediction intervals later on.</p> <p>\ud83d\udd11 Note: In your machine leanring experiments, you may have already dealt with examples of ensemble models. Algorithms such as the random forest model are a form of ensemble, it uses a number of randomly created decision trees where each individual tree may perform poorly but when combined gives great results.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#constructing-and-fitting-an-ensemble-of-models-using-different-loss-functions","title":"Constructing and fitting an ensemble of models (using different loss functions)\u00b6","text":""},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#making-predictions-with-an-ensemble-model","title":"Making predictions with an ensemble model\u00b6","text":""},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#plotting-the-prediction-intervals-uncertainty-estimates-of-our-ensemble","title":"Plotting the prediction intervals (uncertainty estimates) of our ensemble\u00b6","text":"<p>Right now all of our model's (prior to the ensemble model) are predicting single points.</p> <p>Meaning, given a set of <code>WINDOW_SIZE=7</code> values, the model will predict <code>HORIZION=1</code>.</p> <p>But what might be more helpful than a single value?</p> <p>Perhaps a range of values?</p> <p>For example, if a model is predicting the price of Bitcoin to be 50,000USD tomorrow, would it be helpful to know it's predicting the 50,000USD because it's predicting the price to be between 48,000 and 52,000USD? (note: \"$\" has been omitted from the previous sentence due to formatting issues)</p> <p>Knowing the range of values a model is predicting may help you make better decisions for your forecasts.</p> <p>You'd know that although the model is predicting 50,000USD (a point prediction, or single value in time), the value could actually be within the range 48,000USD to 52,000USD (of course, the value could also be outside of this range as well, but we'll get to that later).</p> <p>These kind of prediction ranges are called prediction intervals or uncertainty estimates. And they're often as important as the forecast itself.</p> <p>Why?</p> <p>Because point predictions are almost always going to be wrong. So having a range of values can help with decision making.</p> <p>\ud83d\udcd6 Resource(s):</p> <ul> <li>The steps we're about to take have been inspired by the Machine Learning Mastery blog post Prediction Intervals for Deep Learning Neural Networks. Check out the post for more options to measure uncertainty with neural networks.</li> <li>For an example of uncertainty estimates being used in the wild, I'd also refer to Uber's Engineering Uncertainty Estimation in Neural Networks for Time Series Prediction at Uber blog post.</li> </ul> <p> Example of how uncertainty estimates and predictions intervals can give an understanding of where point predictions (a single number) may not include all of useful information you'd like to know. For example, your model's point prediction for Uber trips on New Years Eve might be 100 (a made up number) but really, the prediction intervals are between 55 and 153 (both made up for the example). In this case, preparing 100 rides might end up being 53 short (it could even be more, like the point prediction, the prediction intervals are also estimates). The image comes from Uber's blog post on uncertainty estimation in neural networks.</p> <p>One way of getting the 95% condfidnece prediction intervals for a deep learning model is the bootstrap method:</p> <ol> <li>Take the predictions from a number of randomly initialized models (we've got this thanks to our ensemble model)</li> <li>Measure the standard deviation of the predictions</li> <li>Multiply standard deviation by 1.96 (assuming the distribution is Gaussian, 95% of observations fall within 1.96 standard deviations of the mean, this is why we initialized our neural networks with a normal distribution)</li> <li>To get the prediction interval upper and lower bounds, add and subtract the value obtained in (3) to the mean/median of the predictions made in (1)</li> </ol>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#aside-two-types-of-uncertainty-coconut-and-subway","title":"Aside: two types of uncertainty (coconut and subway)\u00b6","text":"<p>Inheritly, you know you cannot predict the future.</p> <p>That doesn't mean trying to isn't valuable.</p> <p>For many things, future predictions are helpful. Such as knowing the bus you're trying to catch to the library leaves at 10:08am. The time 10:08am is a point prediction, if the bus left at a random time every day, how helpful would it be?</p> <p>Just like saying the price of Bitcoin tomorrow will be 50,000USD is a point prediction.</p> <p>However, as we've discussed knowing a prediction interval or uncertainty estimate can be as helpful or even more helpful than a point prediction itself.</p> <p>Uncertainty estimates seek out to qualitatively and quantitatively answer the questions:</p> <ul> <li>What can my model know? (with perfect data, what's possible to learn?)</li> <li>What doesn't my model know? (what can a model never predict?)</li> </ul> <p>There are two types of uncertainty in machine learning you should be aware of:</p> <ul> <li><p>Aleatoric uncertainty - this type of uncertainty cannot be reduced, it is also referred to as \"data\" or \"subway\" uncertainty.</p> <ul> <li>Let's say your train is scheduled to arrive at 10:08am but very rarely does it arrive at exactly 10:08am. You know it's usually a minute or two either side and perhaps up to 10-minutes late if traffic is bad. Even with all the data you could imagine, this level of uncertainty is still going to be present (much of it being noise).</li> <li>When we measured prediction intervals, we were measuring a form of subway uncertainty for Bitcoin price predictions (a little either side of the point prediction).</li> </ul> </li> <li><p>Epistemic uncertainty - this type of uncertainty can be reduced, it is also referred to as \"model\" or \"coconut\" uncertainty, it is very hard to calculate.</p> <ul> <li>The analogy for coconut uncertainty involves whether or not you'd get hit on the head by a coconut when going to a beach.<ul> <li>If you were at a beach with coconuts trees, as you could imagine, this would be very hard to calculate. How often does a coconut fall of a tree? Where are you standing?</li> <li>But you could reduce this uncertainty to zero by going to a beach without coconuts (collect more data about your situation).</li> </ul> </li> <li>Model uncertainty can be reduced by collecting more data samples/building a model to capture different parameters about the data you're modelling.</li> </ul> </li> </ul> <p>The lines between these are blurred (one type of uncertainty can change forms into the other) and they can be confusing at first but are important to keep in mind for any kind of time series prediction.</p> <p>If you ignore the uncertanties, are you really going to get a reliable prediction?</p> <p>Perhaps another example might help.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#uncertainty-in-dating","title":"Uncertainty in dating\u00b6","text":"<p>Let's say you're going on a First Date Feedback Radio Show to help improve your dating skills.</p> <p>Where you go on a blind first date with a girl (feel free to replace girl with your own preference) and the radio hosts record the date and then playback snippets of where you could've improved.</p> <p>And now let's add a twist.</p> <p>Last week your friend went on the same show. They told you about the girl they met and how the conversation went.</p> <p>Because you're now a machine learning engineer, you decide to build a machine learning model to help you with first date conversations.</p> <p>What levels of uncertainty do we have here?</p> <p>From an aleatory uncertainty (data) point of view, no matter how many conversations of first dates you collect, the conversation you end up having will likely be different to the rest (the best conversations have no subject and appear random).</p> <p>From an epistemic uncertainty (model) point of view, if the date is truly blind and both parties don't know who they're seeing until they meet in person, the epistemic uncertainty would be high. Because now you have no idea who the person you're going to meet is nor what you might talk about.</p> <p>However, the level of epistemic uncertainty would be reduced if your friend told about the girl they went on a date with last week on the show and it turns out you're going on a date with the same girl.</p> <p>But even though you know a little bit about the girl, your aleatory uncertainty (or subway uncertainty) is still high because you're not sure where the conversation will go.</p> <p>If you're wondering where above scenario came from, it happened to me this morning. Good timing right?</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#learning-more-on-uncertainty","title":"Learning more on uncertainty\u00b6","text":"<p>The field of quantifying uncertainty estimation in machine learning is a growing area of research.</p> <p>If you'd like to learn more I'd recommend the following.</p> <p>\ud83d\udcd6 Resources: Places to learn more about uncertainty in machine learning/forecasting:</p> <ul> <li>\ud83c\udfa5 MIT 6.S191: Evidential Deep Learning and Uncertainty</li> <li>Uncertainty quantification on Wikipedia</li> <li>Why you should care about the Nate Silver vs. Nassim Taleb Twitter war by Isaac Faber - a great insight into the role of uncertainty in the example of election prediction.</li> <li>3 facts about time series forecasting that surprise experienced machine learning practitioners by Skander Hannachi - fantastic outline of some of the main mistakes people make when building forecasting models, especially forgetting about uncertainty estimates.</li> <li>Engineering Uncertainty Estimation in Neural Networks for Time Series Prediction at Uber - a discussion on techniques Uber used to engineer uncertainty estimates into their time sereis neural networks.</li> </ul>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-9-train-a-model-on-the-full-historical-data-to-make-predictions-into-future","title":"Model 9: Train a model on the full historical data to make predictions into future\u00b6","text":"<p>What would a forecasting model be worth if we didn't use it to predict into the future?</p> <p>It's time we created a model which is able to make future predictions on the price of Bitcoin.</p> <p>To make predictions into the future, we'll train a model on the full dataset and then get to make predictions to some future horizon.</p> <p>Why use the full dataset?</p> <p>Previously, we split our data into training and test sets to evaluate how our model did on pseudo-future data (the test set).</p> <p>But since the goal of a forecasting model is to predict values into the actual-future, we won't be using a test set.</p> <p>\ud83d\udd11 Note: Forecasting models need to be retrained every time a forecast is made. Why? Because if Bitcoin prices are updated daily and you predict the price for tomorrow. Your model is only really valid for one day. When a new price comes out (e.g. the next day), you'll have to retrain your model to incorporate that new price to predict the next forecast.</p> <p>Let's get some data ready.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#make-predictions-on-the-future","title":"Make predictions on the future\u00b6","text":"<p>Let's predict the future and get rich!</p> <p>Well... maybe not.</p> <p>As you've seen so far, our machine learning models have performed quite poorly at predicting the price of Bitcoin (time series forecasting in open systems is typically a game of luck), often worse than the naive forecast.</p> <p>That doesn't mean we can't use our models to try and predict into the future right?</p> <p>To do so, let's start by defining a variable <code>INTO_FUTURE</code> which decides how many timesteps we'd like to predict into the future.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#plot-future-forecasts","title":"Plot future forecasts\u00b6","text":"<p>This is so exciting! Forecasts made!</p> <p>But right now, they're just numbers on a page.</p> <p>Let's bring them to life by adhering to the data explorer's motto: visualize, visualize, visualize!</p> <p>To plot our model's future forecasts against the historical data of Bitcoin, we're going to need a series of future dates (future dates from the final date of where our dataset ends).</p> <p>How about we create a function to return a date range from some specified start date to a specified number of days into the future (<code>INTO_FUTURE</code>).</p> <p>To do so, we'll use a combination of NumPy's <code>datetime64</code> datatype (our Bitcoin dates are already in this datatype) as well as NumPy's <code>timedelta64</code> method which helps to create date ranges.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#model-10-why-forecasting-is-bs-the-turkey-problem","title":"Model 10: Why forecasting is BS (the turkey problem \ud83e\udd83)\u00b6","text":"<p>When creating any kind of forecast, you must keep the turkey problem in mind.</p> <p>The turkey problem is an analogy for when your observational data (your historical data) fails to capture a future event which is catostrophic and could lead you to ruin.</p> <p>The story goes, a turkey lives a good life for 1000 days, being fed every day and taken care of by its owners until the evening before Thanksgiving.</p> <p>Based on the turkey's observational data, it has no reason to believe things shouldn't keep going the way they are.</p> <p>In other words, how could a turkey possibly predict that on day 1001, after 1000 consectutive good days, it was about to have a far from ideal day.</p> <p> Example of the turkey problem. A turkey might live 1000 good days and none of them would be a sign of what's to happen on day 1001. Similar with forecasting, your historical data may not have any indication of a change which is about to come. The graph image is from page 41 of The Black Swan by Nassim Taleb (I added in the turkey graphics).</p> <p>How does this relate to predicting the price of Bitcoin (or the price of any stock or figure in an open market)?</p> <p>You could have the historical data of Bitcoin for its entire existence and build a model which predicts it perfectly.</p> <p>But then one day for some unknown and unpredictable reason, the price of Bitcoin plummets 100x in a single day.</p> <p>Of course, this kind of scenario is unlikely.</p> <p>But that doesn't take away from its significance.</p> <p>Think about it in your own life, how many times have the most significant events happened seemingly out of the blue?</p> <p>As in, you could go to a cafe and run into the love of your life, despite visiting the same cafe for 10-years straight and never running into this person before.</p> <p>The same thing goes for predicting the price of Bitcoin, you could make money for 10-years straight and then lose it all in a single day.</p> <p>It doesn't matter how many times you get paid, it matters the amount you get paid.</p> <p>\ud83d\udcd6 Resource: If you'd like to learn more about the turkey problem, I'd recommend the following:</p> <ul> <li>Explaining both the XIV trade and why forecasting is BS by Nassim Taleb</li> <li>The Black Swan by Nassim Taleb (epsecially Chapter 4 which outlines and discusses the turkey problem)</li> </ul> <p>Let's get specific and see how the turkey problem effects us modelling the historical and future price of Bitcoin.</p> <p>To do so, we're going to manufacture a highly unlikely data point into the historical price of Bitcoin, the price falling 100x in one day.</p> <p>\ud83d\udd11 Note: A very unlikely and unpredictable event such as the price of Bitcoin falling 100x in a single day (note: the adjective \"unlikely\" is based on the historical price changes of Bitcoin) is also referred to a Black Swan event. A Black Swan event is an unknown unknown, you have no way of predicting whether or not it will happen but these kind of events often have a large impact.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#building-a-turkey-model-model-to-predict-on-turkey-data","title":"Building a turkey model (model to predict on turkey data)\u00b6","text":"<p>With our updated data, we only changed 1 value.</p> <p>Let's see how it effects a model.</p> <p>To keep things comparable to previous models, we'll create a <code>turkey_model</code> which is a clone of <code>model_1</code> (same architecture, but different data).</p> <p>That way, when we evaluate the <code>turkey_model</code> we can compare its results to <code>model_1_results</code> and see how much a single data point can influence a model's performance.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#compare-models","title":"Compare Models\u00b6","text":"<p>We've trained a bunch of models.</p> <p>And if anything, we've seen just how poorly machine learning and deep learning models are at forecasting the price of Bitcoin (or any kind of open market value).</p> <p>To highlight this, let's compare the results of all of the modelling experiments we've performed so far.</p>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 &amp; 1)</li> </ol> <ul> <li>Try doing this for a univariate model (e.g. <code>model_1</code>) and a multivariate model (e.g. <code>model_6</code>) and see if it effects model training or evaluation results.</li> </ul> <ol> <li>Get the most up to date data on Bitcoin, train a model &amp; see how it goes (our data goes up to May 18 2021).</li> </ol> <ul> <li>You can download the Bitcoin historical data for free from coindesk.com/price/bitcoin and clicking \"Export Data\" -&gt; \"CSV\".</li> </ul> <ol> <li>For most of our models we used <code>WINDOW_SIZE=7</code>, but is there a better window size?</li> </ol> <ul> <li>Setup a series of experiments to find whether or not there's a better window size.</li> <li>For example, you might train 10 different models with <code>HORIZON=1</code> but with window sizes ranging from 2-12.</li> </ul> <ol> <li>Create a windowed dataset just like the ones we used for <code>model_1</code> using <code>tf.keras.preprocessing.timeseries_dataset_from_array()</code> and retrain <code>model_1</code> using the recreated dataset.</li> <li>For our multivariate modelling experiment, we added the Bitcoin block reward size as an extra feature to make our time series multivariate.</li> </ol> <ul> <li>Are there any other features you think you could add?</li> <li>If so, try it out, how do these affect the model?</li> </ul> <ol> <li>Make prediction intervals for future forecasts. To do so, one way would be to train an ensemble model on all of the data, make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for <code>model_8</code>.</li> <li>For future predictions, try to make a prediction, retrain a model on the predictions, make a prediction, retrain a model, make a prediction, retrain a model, make a prediction (retrain a model each time a new prediction is made). Plot the results, how do they look compared to the future predictions where a model wasn't retrained for every forecast (<code>model_9</code>)?</li> <li>Throughout this notebook, we've only tried algorithms we've handcrafted ourselves. But it's worth seeing how a purpose built forecasting algorithm goes.</li> </ol> <ul> <li>Try out one of the extra algorithms listed in the modelling experiments part such as:<ul> <li>Facebook's Kats library - there are many models in here, remember the machine learning practioner's motto: experiment, experiment, experiment.</li> <li>LinkedIn's Greykite library</li> </ul> </li> </ul>"},{"location":"Learning/Tensorflow/10_time_series_forecasting_in_tensorflow/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<p>We've only really scratched the surface with time series forecasting and time series modelling in general. But the good news is, you've got plenty of hands-on coding experience with it already.</p> <p>If you'd like to dig deeper in to the world of time series, I'd recommend the following:</p> <ul> <li>Forecasting: Principles and Practice is an outstanding online textbook which discusses at length many of the most important concepts in time series forecasting. I'd especially recommend reading at least Chapter 1 in full.<ul> <li>I'd definitely recommend at least checking out chapter 1 as well as the chapter on forecasting accuracy measures.</li> </ul> </li> <li>\ud83c\udfa5 Introduction to machine learning and time series by Markus Loning goes through different time series problems and how to approach them. It focuses on using the <code>sktime</code> library (Scikit-Learn for time series), though the principles are applicable elsewhere.</li> <li>Why you should care about the Nate Silver vs. Nassim Taleb Twitter war by Isaac Faber is an outstanding discussion insight into the role of uncertainty in the example of election prediction.</li> <li>TensorFlow time series tutorial - A tutorial on using TensorFlow to forecast weather time series data with TensorFlow.</li> <li>\ud83d\udcd5 The Black Swan by Nassim Nicholas Taleb - Nassim Taleb was a pit trader (a trader who trades on their own behalf) for 25 years, this book compiles many of the lessons he learned from first-hand experience. It changed my whole perspective on our ability to predict.</li> <li>3 facts about time series forecasting that surprise experienced machine learning practitioners by Skander Hannachi, Ph.D - time series data is different to other kinds of data, if you've worked on other kinds of machine learning problems before, getting into time series might require some adjustments, Hannachi outlines 3 of the most common.</li> <li>\ud83c\udfa5 World-class lectures by Jordan Kern, watching these will take you from 0 to 1 with time series problems:<ul> <li>Time Series Analysis - how to analyse time series data.</li> <li>Time Series Modelling - different techniques for modelling time series data (many of which aren't deep learning).</li> </ul> </li> </ul>"},{"location":"Learning/Tensorflow/geturl/","title":"Geturl","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport pyperclip\n</pre> import os import pyperclip In\u00a0[\u00a0]: Copied! <pre>pre_url = 'Learning/Tensorflow/'\n</pre> pre_url = 'Learning/Tensorflow/' In\u00a0[\u00a0]: Copied! <pre># L\u1ea5y t\u1ea5t c\u1ea3 c\u00e1c t\u1ec7p trong th\u01b0 m\u1ee5c hi\u1ec7n t\u1ea1i v\u00e0 thay th\u1ebf d\u1ea5u c\u00e1ch b\u1eb1ng d\u1ea5u _\nfiles = [pre_url + f.replace(\" \", \"_\") for f in os.listdir() if os.path.isfile(f)]\n</pre> # L\u1ea5y t\u1ea5t c\u1ea3 c\u00e1c t\u1ec7p trong th\u01b0 m\u1ee5c hi\u1ec7n t\u1ea1i v\u00e0 thay th\u1ebf d\u1ea5u c\u00e1ch b\u1eb1ng d\u1ea5u _ files = [pre_url + f.replace(\" \", \"_\") for f in os.listdir() if os.path.isfile(f)] In\u00a0[\u00a0]: Copied! <pre># Gh\u00e9p c\u00e1c t\u00ean t\u1ec7p th\u00e0nh m\u1ed9t chu\u1ed7i, m\u1ed7i t\u00ean t\u1ec7p c\u00e1ch nhau m\u1ed9t d\u00f2ng\nfile_names = \"\\n\".join(files)\n</pre> # Gh\u00e9p c\u00e1c t\u00ean t\u1ec7p th\u00e0nh m\u1ed9t chu\u1ed7i, m\u1ed7i t\u00ean t\u1ec7p c\u00e1ch nhau m\u1ed9t d\u00f2ng file_names = \"\\n\".join(files) In\u00a0[\u00a0]: Copied! <pre># Sao ch\u00e9p danh s\u00e1ch t\u00ean t\u1ec7p v\u00e0o clipboard\npyperclip.copy(file_names)\n</pre> # Sao ch\u00e9p danh s\u00e1ch t\u00ean t\u1ec7p v\u00e0o clipboard pyperclip.copy(file_names) In\u00a0[\u00a0]: Copied! <pre>print(\"\u0110\u00e3 sao ch\u00e9p t\u00ean c\u00e1c t\u1ec7p v\u00e0o clipboard:\")\nprint(file_names)\n</pre> print(\"\u0110\u00e3 sao ch\u00e9p t\u00ean c\u00e1c t\u1ec7p v\u00e0o clipboard:\") print(file_names)"},{"location":"Problem/general/","title":"T\u1ed5ng quan","text":""},{"location":"Problem/general/#trang-nay-chi-cung-cap-e-bai-noi-nop-bai-se-o-tren-trang-codeforces","title":"Trang n\u00e0y ch\u1ec9 cung c\u1ea5p \u0111\u1ec1 b\u00e0i, n\u01a1i n\u1ed9p b\u00e0i s\u1ebd \u1edf tr\u00ean trang codeforces.","text":""},{"location":"Problem/general/#link-tham-gia-group-codeforces-e-nop-bai","title":"Link tham gia group codeforces \u0111\u1ec3 n\u1ed9p b\u00e0i.","text":"<ul> <li>link group codeforces</li> </ul>"},{"location":"Problem/general/#tai-khoan-codeforces-mau-e-truy-cap-nop-bai-trong-truong-hop-chua-tao-tai-khoan","title":"T\u00e0i kho\u1ea3n codeforces m\u1eabu \u0111\u1ec3 truy c\u1eadp n\u1ed9p b\u00e0i (trong tr\u01b0\u1eddng h\u1ee3p ch\u01b0a t\u1ea1o t\u00e0i kho\u1ea3n).","text":"<ul> <li>L\u01b0u \u00fd: \u0111\u1ec3 tr\u00e1nh b\u1ecb phi\u1ec1n, xin \u0111\u1eebng \u0111\u1ed5i m\u1eadt kh\u1ea9u t\u00e0i kho\u1ea3n d\u01b0\u1edbi \u0111\u00e2y.</li> </ul> T\u00e0i kho\u1ea3n t\u1ea1m th\u1eddi \u0111\u0103ng nh\u1eadp codeforces <pre><code>orzntg\n</code></pre> <pre><code>123456\n</code></pre>"},{"location":"Problem/coci0607/Overview/","title":"M\u00f4 t\u1ea3 v\u1ec1 COCI 2006 - 2007","text":""},{"location":"Problem/coci0607/Overview/#tai-lieu-pdf","title":"T\u00e0i li\u1ec7u PDF:","text":"<p>Contest 1</p> <p>Link n\u1ed9p b\u00e0i Contest 1.</p> <p>Contest 2</p> <p>Contest 3</p> <p>Contest 4</p> <p>Contest 5</p> <p>Contest 6</p> <p>Regional</p> <p>Olympiad</p>"},{"location":"Problem/coci0607/Overview/#cac-tai-lieu-ve-cuoc-thi","title":"C\u00e1c t\u00e0i li\u1ec7u v\u1ec1 cu\u1ed9c thi","text":"<p>COCI 2006-2007</p>"},{"location":"Problem/coci0607/Contest1/P1_MODULO/","title":"MODULO","text":""},{"location":"Problem/coci0607/Contest1/P1_MODULO/#modulo","title":"MODULO","text":"<p>Cho hai s\u1ed1 nguy\u00ean \\(A\\) v\u00e0 \\(B\\), \\(A\\%B\\) l\u00e0 s\u1ed1 d\u01b0 khi chia \\(A\\) cho \\(B\\). V\u00ed d\u1ee5, c\u00e1c s\u1ed1 \\(7, 14, 27\\) v\u00e0 \\(38\\) c\u00f3 s\u1ed1 d\u01b0 l\u1ea7n l\u01b0\u1ee3t l\u00e0 \\(1, 2, 0\\) v\u00e0 \\(2\\) khi chia cho \\(3\\). Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh nh\u1eadn v\u00e0o \\(10\\) s\u1ed1 nguy\u00ean v\u00e0 in ra s\u1ed1 l\u01b0\u1ee3ng s\u1ed1 d\u01b0 kh\u00e1c nhau khi chia cho \\(42\\).  </p>"},{"location":"Problem/coci0607/Contest1/P1_MODULO/#input","title":"Input","text":"<ul> <li>G\u1ed3m \\(10\\) s\u1ed1 nguy\u00ean kh\u00f4ng \u00e2m, m\u1ed7i s\u1ed1 nh\u1ecf h\u01a1n \\(1000\\), m\u1ed7i s\u1ed1 \u0111\u01b0\u1ee3c nh\u1eadp tr\u00ean m\u1ed9t d\u00f2ng ri\u00eang.  </li> </ul>"},{"location":"Problem/coci0607/Contest1/P1_MODULO/#output","title":"Output","text":"<ul> <li>In ra m\u1ed9t s\u1ed1 nguy\u00ean duy nh\u1ea5t l\u00e0 s\u1ed1 l\u01b0\u1ee3ng s\u1ed1 d\u01b0 kh\u00e1c nhau khi chia cho \\(42\\).  </li> </ul>"},{"location":"Problem/coci0607/Contest1/P1_MODULO/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>1  \n2  \n3  \n4  \n5  \n6  \n7  \n8  \n9  \n10  \n</code></pre> Output <pre><code>10  \n</code></pre> Note <p>C\u00e1c s\u1ed1 d\u01b0 c\u00f3 l\u00e0: \\(1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\).  </p> <p>Test 2</p> Input <pre><code>42  \n84  \n252  \n420  \n840  \n126  \n42  \n84  \n420  \n126  \n</code></pre> Output <pre><code>1  \n</code></pre> Note <p>T\u1ea5t c\u1ea3 c\u00e1c s\u1ed1 \u0111\u1ec1u chia h\u1ebft cho \\(42\\).</p> <p>Test 3</p> Input <pre><code>39  \n40  \n41  \n42  \n43  \n44  \n82  \n83  \n84  \n85  \n</code></pre> Output <pre><code>6  \n</code></pre> Note <p>C\u00e1c s\u1ed1 d\u01b0 c\u00f3 l\u00e0: \\(39, 40, 41, 0, 1, 2, 40, 41, 0, 1\\). C\u00f3 \\(6\\) s\u1ed1 kh\u00e1c nhau.  </p>"},{"location":"Problem/coci0607/Contest1/P2_HERMAN/","title":"HERMAN","text":""},{"location":"Problem/coci0607/Contest1/P2_HERMAN/#herman","title":"HERMAN","text":"<p>Nh\u00e0 to\u00e1n h\u1ecdc ng\u01b0\u1eddi \u0110\u1ee9c th\u1ebf k\u1ef7 19, Hermann Minkowski, \u0111\u00e3 nghi\u00ean c\u1ee9u m\u1ed9t lo\u1ea1i h\u00ecnh h\u1ecdc phi Euclid \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 h\u00ecnh h\u1ecdc taxicab. Trong h\u00ecnh h\u1ecdc taxicab, kho\u1ea3ng c\u00e1ch gi\u1eefa hai \u0111i\u1ec3m \\(T_1(x_1, y_1)\\) v\u00e0 \\(T_2(x_2, y_2)\\) \u0111\u01b0\u1ee3c x\u00e1c \u0111\u1ecbnh nh\u01b0 sau:  </p> \\[D(T_1,T_2) = |x_1 - x_2| + |y_1 - y_2|\\] <p>T\u1ea5t c\u1ea3 c\u00e1c \u0111\u1ecbnh ngh\u0129a kh\u00e1c v\u1eabn gi\u1eef nguy\u00ean nh\u01b0 trong h\u00ecnh h\u1ecdc Euclid, bao g\u1ed3m c\u1ea3 \u0111\u1ecbnh ngh\u0129a v\u1ec1 \u0111\u01b0\u1eddng tr\u00f2n: M\u1ed9t \u0111\u01b0\u1eddng tr\u00f2n l\u00e0 t\u1eadp h\u1ee3p t\u1ea5t c\u1ea3 c\u00e1c \u0111i\u1ec3m trong m\u1eb7t ph\u1eb3ng c\u00f3 kho\u1ea3ng c\u00e1ch c\u1ed1 \u0111\u1ecbnh (b\u00e1n k\u00ednh) t\u1eeb m\u1ed9t \u0111i\u1ec3m c\u1ed1 \u0111\u1ecbnh (t\u00e2m \u0111\u01b0\u1eddng tr\u00f2n).  </p> <p>Ch\u00fang ta quan t\u00e2m \u0111\u1ebfn s\u1ef1 kh\u00e1c bi\u1ec7t v\u1ec1 di\u1ec7n t\u00edch c\u1ee7a hai \u0111\u01b0\u1eddng tr\u00f2n c\u00f3 b\u00e1n k\u00ednh \\(R\\), m\u1ed9t \u0111\u01b0\u1eddng tr\u00f2n trong h\u00ecnh h\u1ecdc Euclid th\u00f4ng th\u01b0\u1eddng v\u00e0 m\u1ed9t \u0111\u01b0\u1eddng tr\u00f2n trong h\u00ecnh h\u1ecdc taxicab. Nhi\u1ec7m v\u1ee5 gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 kh\u00f3 kh\u0103n n\u00e0y \u0111\u01b0\u1ee3c giao cho b\u1ea1n.  </p>"},{"location":"Problem/coci0607/Contest1/P2_HERMAN/#input","title":"Input","text":"<p>D\u00f2ng \u0111\u1ea7u ti\u00ean v\u00e0 duy nh\u1ea5t c\u1ee7a \u0111\u1ea7u v\u00e0o ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(R\\), c\u00f3 gi\u00e1 tr\u1ecb nh\u1ecf h\u01a1n ho\u1eb7c b\u1eb1ng \\(10000\\).  </p>"},{"location":"Problem/coci0607/Contest1/P2_HERMAN/#output","title":"Output","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean in ra di\u1ec7n t\u00edch c\u1ee7a m\u1ed9t \u0111\u01b0\u1eddng tr\u00f2n c\u00f3 b\u00e1n k\u00ednh \\(R\\) trong h\u00ecnh h\u1ecdc Euclid th\u00f4ng th\u01b0\u1eddng.  </li> <li>D\u00f2ng th\u1ee9 hai in ra di\u1ec7n t\u00edch c\u1ee7a m\u1ed9t \u0111\u01b0\u1eddng tr\u00f2n c\u00f3 b\u00e1n k\u00ednh \\(R\\) trong h\u00ecnh h\u1ecdc taxicab.  </li> </ul> <p>L\u01b0u \u00fd: K\u1ebft qu\u1ea3 trong kho\u1ea3ng sai s\u1ed1 \u00b1\\(0.0001\\) so v\u1edbi \u0111\u00e1p \u00e1n ch\u00ednh th\u1ee9c s\u1ebd \u0111\u01b0\u1ee3c ch\u1ea5p nh\u1eadn.  </p>"},{"location":"Problem/coci0607/Contest1/P2_HERMAN/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>1  \n</code></pre> Output <pre><code>3.141593  \n2.000000  \n</code></pre> <p>Test 2</p> Input <pre><code>21  \n</code></pre> Output <pre><code>1385.442360  \n882.000000  \n</code></pre> <p>Test 3</p> Input <pre><code>42  \n</code></pre> Output <pre><code>5541.769441  \n3528.000000  \n</code></pre>"},{"location":"Problem/coci0607/Contest1/P3_OKVIRI/","title":"OKVIRI","text":""},{"location":"Problem/coci0607/Contest1/P3_OKVIRI/#okviri","title":"OKVIRI","text":"<p>\u201cKhung Peter Pan\u201d l\u00e0 m\u1ed9t c\u00e1ch trang tr\u00ed v\u0103n b\u1ea3n trong \u0111\u00f3 m\u1ed7i k\u00fd t\u1ef1 \u0111\u01b0\u1ee3c bao quanh b\u1edfi m\u1ed9t khung h\u00ecnh kim c\u01b0\u01a1ng, v\u1edbi c\u00e1c khung c\u1ee7a c\u00e1c k\u00fd t\u1ef1 li\u1ec1n k\u1ec1 \u0111an xen nhau. M\u1ed9t khung Peter Pan cho m\u1ed9t ch\u1eef c\u00e1i tr\u00f4ng nh\u01b0 sau (k\u00fd t\u1ef1 <code>X</code> l\u00e0 k\u00fd t\u1ef1 c\u1ea7n \u0111\u00f3ng khung): <pre><code>..#..\n.#.#.\n#.X.#\n.#.#.\n..#..\n</code></pre></p> <p>Tuy nhi\u00ean, ch\u1ec9 s\u1eed d\u1ee5ng m\u1ed9t ki\u1ec3u khung nh\u01b0 v\u1eady s\u1ebd kh\u00e1 \u0111\u01a1n \u0111i\u1ec7u, v\u00ec v\u1eady ch\u00fang ta s\u1ebd \u0111\u00f3ng khung m\u1ed7i k\u00fd t\u1ef1 th\u1ee9 ba b\u1eb1ng \u201ckhung Wendy\u201d. Khung Wendy tr\u00f4ng nh\u01b0 sau:  </p> <pre><code>..*..\n.*.*.\n*.X.*\n.*.*.\n..*..\n</code></pre> <p>Khi khung Wendy ch\u1ed3ng l\u00ean khung Peter Pan, khung Wendy (v\u00ec tr\u00f4ng \u0111\u1eb9p h\u01a1n) s\u1ebd \u0111\u01b0\u1ee3c \u0111\u1eb7t l\u00ean tr\u00ean. \u0110\u1ec3 hi\u1ec3u r\u00f5 h\u01a1n v\u1ec1 c\u00e1ch \u0111an xen gi\u1eefa c\u00e1c khung, h\u00e3y xem c\u00e1c v\u00ed d\u1ee5 b\u00ean d\u01b0\u1edbi.  </p>"},{"location":"Problem/coci0607/Contest1/P3_OKVIRI/#input","title":"Input","text":"<p>D\u00f2ng \u0111\u1ea7u ti\u00ean v\u00e0 duy nh\u1ea5t ch\u1ee9a t\u1ed1i \u0111a \\(15\\) ch\u1eef c\u00e1i in hoa c\u1ee7a b\u1ea3ng ch\u1eef c\u00e1i ti\u1ebfng Anh.  </p>"},{"location":"Problem/coci0607/Contest1/P3_OKVIRI/#output","title":"Output","text":"<p>In ra t\u1eeb \u0111\u01b0\u1ee3c vi\u1ebft b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng khung Peter Pan v\u00e0 Wendy tr\u00ean \\(5\\) d\u00f2ng.  </p>"},{"location":"Problem/coci0607/Contest1/P3_OKVIRI/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>A  \n</code></pre> Output <pre><code>..#..  \n.#.#.  \n#.A.#  \n.#.#.  \n..#..  \n</code></pre> <p>Test 2</p> Input <pre><code>DOG  \n</code></pre> Output <pre><code>..#...#...*..  \n.#.#.#.#.*.*.  \n#.D.#.O.*.G.*  \n.#.#.#.#.*.*.  \n..#...#...*..  \n</code></pre> <p>Test 3</p> Input <pre><code>ABCD  \n</code></pre> Output <pre><code>..#...#...*...#..  \n.#.#.#.#.*.*.#.#.  \n#.A.#.B.*.C.*.D.#  \n.#.#.#.#.*.*.#.#.  \n..#...#...*...#..  \n</code></pre>"},{"location":"Problem/coci0607/Contest1/P4_SLIKAR/","title":"SLIKAR","text":""},{"location":"Problem/coci0607/Contest1/P4_SLIKAR/#slikar","title":"SLIKAR","text":"<p>Ho\u00e0ng \u0111\u1ebf \u0111\u1ed9c \u00e1c Cactus s\u1edf h\u1eefu Th\u00f9ng Bia Ma Thu\u1eadt v\u00e0 \u0111\u00e3 l\u00e0m ng\u1eadp r\u1eebng Enchanted! H\u1ecda s\u0129 v\u00e0 ba ch\u00fa nh\u00edm nh\u1ecf ph\u1ea3i nhanh ch\u00f3ng tr\u1edf v\u1ec1 hang c\u1ee7a H\u1ea3i ly \u0111\u1ec3 tr\u00e1nh n\u01b0\u1edbc l\u0169!  </p> <p>B\u1ea3n \u0111\u1ed3 c\u1ee7a r\u1eebng Enchanted g\u1ed3m \\(R\\) h\u00e0ng v\u00e0 \\(C\\) c\u1ed9t. C\u00e1c \u00f4 tr\u1ed1ng \u0111\u01b0\u1ee3c k\u00fd hi\u1ec7u b\u1eb1ng k\u00fd t\u1ef1 <code>.</code>, c\u00e1c \u00f4 b\u1ecb ng\u1eadp n\u01b0\u1edbc k\u00fd hi\u1ec7u l\u00e0 <code>*</code>, v\u00e0 c\u00e1c t\u1ea3ng \u0111\u00e1 \u0111\u01b0\u1ee3c k\u00fd hi\u1ec7u l\u00e0 <code>X</code>. Ngo\u00e0i ra, hang c\u1ee7a H\u1ea3i ly \u0111\u01b0\u1ee3c k\u00fd hi\u1ec7u l\u00e0 <code>D</code> v\u00e0 H\u1ecda s\u0129 c\u00f9ng ba ch\u00fa nh\u00edm nh\u1ecf \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1eb1ng <code>S</code>.  </p> <p>M\u1ed7i ph\u00fat, H\u1ecda s\u0129 v\u00e0 ba ch\u00fa nh\u00edm nh\u1ecf c\u00f3 th\u1ec3 di chuy\u1ec3n \u0111\u1ebfn m\u1ed9t trong b\u1ed1n \u00f4 l\u00e2n c\u1eadn (l\u00ean, xu\u1ed1ng, tr\u00e1i ho\u1eb7c ph\u1ea3i). \u0110\u1ed3ng th\u1eddi, n\u01b0\u1edbc l\u0169 c\u0169ng lan r\u1ed9ng, l\u00e0m cho t\u1ea5t c\u1ea3 c\u00e1c \u00f4 tr\u1ed1ng c\u00f3 \u00edt nh\u1ea5t m\u1ed9t c\u1ea1nh chung v\u1edbi \u00f4 b\u1ecb ng\u1eadp n\u01b0\u1edbc s\u1ebd b\u1ecb ng\u1eadp theo.  </p> <p>C\u1ea3 n\u01b0\u1edbc l\u0169 l\u1eabn nh\u00f3m H\u1ecda s\u0129 kh\u00f4ng th\u1ec3 \u0111i v\u00e0o \u00f4 c\u00f3 \u0111\u00e1. \u0110\u1eb7c bi\u1ec7t, nh\u00f3m H\u1ecda s\u0129 kh\u00f4ng th\u1ec3 \u0111i v\u00e0o c\u00e1c \u00f4 \u0111\u00e3 b\u1ecb ng\u1eadp n\u01b0\u1edbc, v\u00e0 n\u01b0\u1edbc l\u0169 c\u0169ng kh\u00f4ng th\u1ec3 tr\u00e0n v\u00e0o hang c\u1ee7a H\u1ea3i ly.  </p> <p>Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh nh\u1eadn v\u00e0o b\u1ea3n \u0111\u1ed3 c\u1ee7a r\u1eebng Enchanted v\u00e0 in ra th\u1eddi gian ng\u1eafn nh\u1ea5t \u0111\u1ec3 nh\u00f3m H\u1ecda s\u0129 \u0111\u1ebfn \u0111\u01b0\u1ee3c hang c\u1ee7a H\u1ea3i ly m\u1ed9t c\u00e1ch an to\u00e0n.  </p> <p>L\u01b0u \u00fd: Nh\u00f3m H\u1ecda s\u0129 kh\u00f4ng th\u1ec3 di chuy\u1ec3n v\u00e0o \u00f4 s\u1ebd b\u1ecb ng\u1eadp n\u01b0\u1edbc trong c\u00f9ng m\u1ed9t ph\u00fat.  </p>"},{"location":"Problem/coci0607/Contest1/P4_SLIKAR/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(R\\) v\u00e0 \\(C\\) \\((R, C \\leq 50)\\).  </li> <li>\\(R\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a \\(C\\) k\u00fd t\u1ef1 thu\u1ed9c t\u1eadp <code>{., *, X, D, S}</code>.  </li> <li>B\u1ea3n \u0111\u1ed3 lu\u00f4n ch\u1ee9a \u0111\u00fang m\u1ed9t k\u00fd t\u1ef1 <code>D</code> v\u00e0 m\u1ed9t k\u00fd t\u1ef1 <code>S</code>.  </li> </ul>"},{"location":"Problem/coci0607/Contest1/P4_SLIKAR/#output","title":"Output","text":"<ul> <li>In ra th\u1eddi gian ng\u1eafn nh\u1ea5t \u0111\u1ec3 nh\u00f3m H\u1ecda s\u0129 \u0111\u1ebfn \u0111\u01b0\u1ee3c hang c\u1ee7a H\u1ea3i ly.  </li> <li>N\u1ebfu kh\u00f4ng th\u1ec3 \u0111\u1ebfn \u0111\u01b0\u1ee3c, in ra <code>KAKTUS</code>.  </li> </ul>"},{"location":"Problem/coci0607/Contest1/P4_SLIKAR/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>3 3  \nD.*  \n...  \n.S.  \n</code></pre> Output <pre><code>3  \n</code></pre> <p>Test 2</p> Input <pre><code>3 3  \nD.*  \n...  \n..S  \n</code></pre> Output <pre><code>KAKTUS  \n</code></pre> Note <p>Nh\u00f3m H\u1ecda s\u0129 c\u00f3 th\u1ec3 \u0111i d\u1ecdc theo bi\u00ean d\u01b0\u1edbi v\u00e0 sau \u0111\u00f3 l\u00e0 bi\u00ean tr\u00e1i, nh\u01b0ng s\u1ebd b\u1ecb n\u01b0\u1edbc l\u0169 nh\u1ea5n ch\u00ecm m\u1ed9t ph\u00fat tr\u01b0\u1edbc khi \u0111\u1ebfn n\u01a1i.  </p> <p>Test 3</p> Input <pre><code>3 6  \nD...*.  \n.X.X..  \n....S.  \n</code></pre> Output <pre><code>6  \n</code></pre>"},{"location":"Problem/coci0607/Contest1/P5_BOND/","title":"BOND","text":""},{"location":"Problem/coci0607/Contest1/P5_BOND/#bond","title":"BOND","text":"<p>M\u1ecdi ng\u01b0\u1eddi \u0111\u1ec1u bi\u1ebft \u0111i\u1ec7p vi\u00ean b\u00ed m\u1eadt 007, Bond (James Bond). Tuy nhi\u00ean, \u00edt ai bi\u1ebft r\u1eb1ng th\u1ef1c t\u1ebf anh \u1ea5y kh\u00f4ng t\u1ef1 th\u1ef1c hi\u1ec7n h\u1ea7u h\u1ebft c\u00e1c nhi\u1ec7m v\u1ee5 c\u1ee7a m\u00ecnh; thay v\u00e0o \u0111\u00f3, nh\u1eefng nhi\u1ec7m v\u1ee5 \u0111\u00f3 \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n b\u1edfi c\u00e1c ng\u01b0\u1eddi anh em h\u1ecd c\u1ee7a anh \u1ea5y, Jimmy Bonds. Bond (James Bond) \u0111\u00e3 m\u1ec7t m\u1ecfi v\u1edbi vi\u1ec7c ph\u1ea3i t\u1ef1 ph\u00e2n c\u00f4ng nhi\u1ec7m v\u1ee5 cho Jimmy Bonds m\u1ed7i khi c\u00f3 nhi\u1ec7m v\u1ee5 m\u1edbi, v\u00ec v\u1eady anh \u1ea5y \u0111\u00e3 nh\u1edd b\u1ea1n gi\u00fap \u0111\u1ee1.  </p> <p>M\u1ed7i th\u00e1ng, Bond (James Bond) nh\u1eadn \u0111\u01b0\u1ee3c m\u1ed9t danh s\u00e1ch c\u00e1c nhi\u1ec7m v\u1ee5. D\u1ef1a tr\u00ean kinh nghi\u1ec7m t\u1eeb c\u00e1c nhi\u1ec7m v\u1ee5 tr\u01b0\u1edbc \u0111\u00f3, anh \u1ea5y t\u00ednh to\u00e1n x\u00e1c su\u1ea5t th\u00e0nh c\u00f4ng c\u1ee7a t\u1eebng nhi\u1ec7m v\u1ee5 khi m\u1ed7i Jimmy Bond th\u1ef1c hi\u1ec7n. Nhi\u1ec7m v\u1ee5 c\u1ee7a b\u1ea1n l\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u n\u00e0y v\u00e0 t\u00ecm ra c\u00e1ch ph\u00e2n c\u00f4ng nhi\u1ec7m v\u1ee5 sao cho x\u00e1c su\u1ea5t t\u1ea5t c\u1ea3 nhi\u1ec7m v\u1ee5 \u0111\u01b0\u1ee3c ho\u00e0n th\u00e0nh th\u00e0nh c\u00f4ng l\u00e0 cao nh\u1ea5t.  </p> <p>L\u01b0u \u00fd: X\u00e1c su\u1ea5t \u0111\u1ec3 t\u1ea5t c\u1ea3 nhi\u1ec7m v\u1ee5 \u0111\u01b0\u1ee3c ho\u00e0n th\u00e0nh th\u00e0nh c\u00f4ng l\u00e0 t\u00edch c\u1ee7a x\u00e1c su\u1ea5t ho\u00e0n th\u00e0nh t\u1eebng nhi\u1ec7m v\u1ee5 ri\u00eang l\u1ebb.  </p>"},{"location":"Problem/coci0607/Contest1/P5_BOND/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 20)\\), s\u1ed1 l\u01b0\u1ee3ng Jimmy Bonds v\u00e0 s\u1ed1 nhi\u1ec7m v\u1ee5.  </li> <li>\\(N\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a \\(N\\) s\u1ed1 nguy\u00ean t\u1eeb \\(0\\) \u0111\u1ebfn \\(100\\), s\u1ed1 th\u1ee9 \\(j\\) trong d\u00f2ng th\u1ee9 \\(i\\) l\u00e0 x\u00e1c su\u1ea5t (t\u00ednh theo ph\u1ea7n tr\u0103m) \u0111\u1ec3 Jimmy Bond th\u1ee9 \\(i\\) ho\u00e0n th\u00e0nh nhi\u1ec7m v\u1ee5 th\u1ee9 \\(j\\).  </li> </ul>"},{"location":"Problem/coci0607/Contest1/P5_BOND/#output","title":"Output","text":"<ul> <li>In ra x\u00e1c su\u1ea5t t\u1ed1i \u0111a \u0111\u1ec3 t\u1ea5t c\u1ea3 c\u00e1c nhi\u1ec7m v\u1ee5 \u0111\u01b0\u1ee3c ho\u00e0n th\u00e0nh th\u00e0nh c\u00f4ng, t\u00ednh theo ph\u1ea7n tr\u0103m.  </li> <li>K\u1ebft qu\u1ea3 ph\u1ea3i c\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c trong kho\u1ea3ng sai s\u1ed1 \u00b1\\(0.000001\\).  </li> </ul>"},{"location":"Problem/coci0607/Contest1/P5_BOND/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>2  \n100 100  \n50 50  \n</code></pre> Output <pre><code>50.000000  \n</code></pre> <p>Test 2</p> Input <pre><code>2  \n0 50  \n50 0  \n</code></pre> Output <pre><code>25.000000  \n</code></pre> <p>Test 3</p> Input <pre><code>3  \n25 60 100  \n13 0 50  \n12 70 90  \n</code></pre> Output <pre><code>9.100000  \n</code></pre> Note <p>N\u1ebfu Jimmy Bond 1 th\u1ef1c hi\u1ec7n nhi\u1ec7m v\u1ee5 th\u1ee9 3, Jimmy Bond 2 th\u1ef1c hi\u1ec7n nhi\u1ec7m v\u1ee5 th\u1ee9 1 v\u00e0 Jimmy Bond 3 th\u1ef1c hi\u1ec7n nhi\u1ec7m v\u1ee5 th\u1ee9 2, x\u00e1c su\u1ea5t th\u00e0nh c\u00f4ng l\u00e0: \\(1.0 \\times 0.13 \\times 0.7 = 0.091 = 9.1\\%\\)  T\u1ea5t c\u1ea3 c\u00e1c c\u00e1ch s\u1eafp x\u1ebfp kh\u00e1c \u0111\u1ec1u cho x\u00e1c su\u1ea5t th\u00e0nh c\u00f4ng nh\u1ecf h\u01a1n.  </p>"},{"location":"Problem/coci0607/Contest1/P6_DEBUG/","title":"DEBUG","text":""},{"location":"Problem/coci0607/Contest1/P6_DEBUG/#debug","title":"DEBUG","text":"<p>Khi g\u1ee1 l\u1ed7i m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh, Mirko nh\u1eadn th\u1ea5y r\u1eb1ng m\u1ed9t l\u1ed7i trong ch\u01b0\u01a1ng tr\u00ecnh c\u00f3 th\u1ec3 li\u00ean quan \u0111\u1ebfn s\u1ef1 t\u1ed3n t\u1ea1i c\u1ee7a c\u00e1c \"h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c\" trong b\u1ed9 nh\u1edb ch\u01b0\u01a1ng tr\u00ecnh. B\u1ed9 nh\u1edb ch\u01b0\u01a1ng tr\u00ecnh l\u00e0 m\u1ed9t ma tr\u1eadn g\u1ed3m \\(R\\) h\u00e0ng v\u00e0 \\(C\\) c\u1ed9t, ch\u1ec9 ch\u1ee9a c\u00e1c s\u1ed1 kh\u00f4ng (<code>0</code>) v\u00e0 m\u1ed9t (<code>1</code>). M\u1ed9t h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c l\u00e0 m\u1ed9t ma tr\u1eadn con h\u00ecnh vu\u00f4ng trong b\u1ed9 nh\u1edb (c\u00f3 k\u00edch th\u01b0\u1edbc l\u1edbn h\u01a1n \\(1 \\times 1\\)), khi xoay \\(180^\\circ\\) v\u1eabn gi\u1eef nguy\u00ean. V\u00ed d\u1ee5, ma tr\u1eadn sau ch\u1ee9a 3 h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c:  </p> \\[ \\begin{array}{c|c|c|c} \\texttt{101010} &amp; \\texttt{....10} &amp; \\texttt{......} &amp; \\texttt{101...} \\\\ \\texttt{111001} &amp; \\texttt{....01} &amp; \\texttt{...00.} &amp; \\texttt{111...} \\\\ \\texttt{101001} &amp; \\texttt{......} &amp; \\texttt{...00.} &amp; \\texttt{101...} \\\\ \\hline \\text{input} &amp; \\text{h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c} &amp; \\text{h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c} &amp; \\text{h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c} \\\\ \\end{array} \\] <p>Mirko \u0111ang th\u1eafc m\u1eafc li\u1ec7u c\u00f3 m\u1ed1i li\u00ean h\u1ec7 n\u00e0o gi\u1eefa k\u00edch th\u01b0\u1edbc c\u1ee7a h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c l\u1edbn nh\u1ea5t v\u00e0 l\u1ed7i trong ch\u01b0\u01a1ng tr\u00ecnh hay kh\u00f4ng. H\u00e3y gi\u00fap Mirko b\u1eb1ng c\u00e1ch vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh nh\u1eadn v\u00e0o d\u1eef li\u1ec7u v\u00e0 in ra k\u00edch th\u01b0\u1edbc c\u1ee7a h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c l\u1edbn nh\u1ea5t. K\u00edch th\u01b0\u1edbc c\u1ee7a h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c l\u00e0 s\u1ed1 h\u00e0ng (ho\u1eb7c s\u1ed1 c\u1ed9t) m\u00e0 s\u00e1t th\u1ee7 bao g\u1ed3m. Trong v\u00ed d\u1ee5 tr\u00ean, c\u00e1c k\u00edch th\u01b0\u1edbc h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c l\u1ea7n l\u01b0\u1ee3t l\u00e0 \\(2\\), \\(2\\) v\u00e0 \\(3\\).  </p>"},{"location":"Problem/coci0607/Contest1/P6_DEBUG/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(R\\) v\u00e0 \\(C\\) \\((R, C \\leq 300)\\).  </li> <li>\\(R\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a \\(C\\) k\u00fd t\u1ef1 (<code>0</code> ho\u1eb7c <code>1</code>), kh\u00f4ng c\u00f3 d\u1ea5u c\u00e1ch.  </li> </ul>"},{"location":"Problem/coci0607/Contest1/P6_DEBUG/#output","title":"Output","text":"<ul> <li>In ra k\u00edch th\u01b0\u1edbc c\u1ee7a h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c l\u1edbn nh\u1ea5t tr\u00ean m\u1ed9t d\u00f2ng.  </li> <li>N\u1ebfu kh\u00f4ng c\u00f3 h\u00ecnh vu\u00f4ng ch\u1ebft ch\u00f3c n\u00e0o, in ra <code>-1</code>.  </li> </ul>"},{"location":"Problem/coci0607/Contest1/P6_DEBUG/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>3 6  \n101010  \n111001  \n101001  \n</code></pre> Output <pre><code>3  \n</code></pre> <p>Test 2</p> Input <pre><code>4 5  \n10010  \n01010  \n10101  \n01001  \n</code></pre> Output <pre><code>3  \n</code></pre> <p>Test 3</p> Input <pre><code>3 3  \n101  \n111  \n100  \n</code></pre> Output <pre><code>-1  \n</code></pre>"},{"location":"Problem/coci0607/Contest2/P1_R2/","title":"R2","text":""},{"location":"Problem/coci0607/Contest2/P1_R2/#r2","title":"R2","text":"<p>S\u1ed1 \\(S\\) \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 trung b\u00ecnh c\u1ed9ng c\u1ee7a hai s\u1ed1 \\(R1\\) v\u00e0 \\(R2\\) n\u1ebfu \\(S\\) b\u1eb1ng \\((R1+R2)/2\\). Qu\u00e0 sinh nh\u1eadt c\u1ee7a Mirko cho Slavko l\u00e0 hai s\u1ed1 nguy\u00ean \\(R1\\) v\u00e0 \\(R2\\). Slavko nhanh ch\u00f3ng t\u00ecm ra trung b\u00ecnh c\u1ed9ng c\u1ee7a ch\u00fang c\u0169ng l\u00e0 m\u1ed9t s\u1ed1 nguy\u00ean nh\u01b0ng l\u1ea1i l\u00e0m m\u1ea5t \u0111i \\(R2\\). Gi\u00fap Slavko t\u00ecm \\(R2\\).</p>"},{"location":"Problem/coci0607/Contest2/P1_R2/#input","title":"Input","text":"<ul> <li>M\u1ed9t d\u00f2ng duy nh\u1ea5t ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(R1\\) v\u00e0 \\(S\\), c\u1ea3 hai trong ph\u1ea1m vi \\(-1000\\) \u0111\u1ebfn \\(1000\\).  </li> </ul>"},{"location":"Problem/coci0607/Contest2/P1_R2/#output","title":"Output","text":"<ul> <li>In ra \\(R2\\) trong m\u1ed9t d\u00f2ng</li> </ul>"},{"location":"Problem/coci0607/Contest2/P1_R2/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>11 15\n</code></pre> Output <pre><code>19\n</code></pre> <p>Test 2</p> Input <pre><code>4 3 \n</code></pre> Output <pre><code>2\n</code></pre>"},{"location":"Problem/coci0607/Contest2/P2_ABC/","title":"ABC","text":""},{"location":"Problem/coci0607/Contest2/P2_ABC/#abc","title":"ABC","text":"<p>B\u1ea1n s\u1ebd \u0111\u01b0\u1ee3c cung c\u1ea5p ba s\u1ed1 nguy\u00ean \\(A\\), \\(B\\) v\u00e0 \\(C\\). C\u00e1c s\u1ed1 n\u00e0y kh\u00f4ng \u0111\u01b0\u1ee3c cho theo \u0111\u00fang th\u1ee9 t\u1ef1, nh\u01b0ng ta bi\u1ebft r\u1eb1ng \\(A\\) nh\u1ecf h\u01a1n \\(B\\) v\u00e0 \\(B\\) nh\u1ecf h\u01a1n \\(C\\).</p> <p>\u0110\u1ec3 d\u1ec5 nh\u00ecn h\u01a1n, ch\u00fang ta c\u1ea7n s\u1eafp x\u1ebfp ch\u00fang theo th\u1ee9 t\u1ef1 \u0111\u00e3 cho.</p>"},{"location":"Problem/coci0607/Contest2/P2_ABC/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a ba s\u1ed1 nguy\u00ean d\u01b0\u01a1ng \\(A\\), \\(B\\) v\u00e0 \\(C\\), kh\u00f4ng nh\u1ea5t thi\u1ebft theo \u0111\u00fang th\u1ee9 t\u1ef1. C\u1ea3 ba s\u1ed1 \u0111\u1ec1u kh\u00f4ng v\u01b0\u1ee3t qu\u00e1 \\(100\\).</li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a ba ch\u1eef c\u00e1i in hoa <code>'A'</code>, <code>'B'</code> v\u00e0 <code>'C'</code> (kh\u00f4ng c\u00f3 kho\u1ea3ng tr\u1eafng) th\u1ec3 hi\u1ec7n th\u1ee9 t\u1ef1 mong mu\u1ed1n.</li> </ul>"},{"location":"Problem/coci0607/Contest2/P2_ABC/#output","title":"Output","text":"<ul> <li>In ra ba s\u1ed1 \\(A\\), \\(B\\), \\(C\\) theo th\u1ee9 t\u1ef1 mong mu\u1ed1n, c\u00e1ch nhau b\u1edfi m\u1ed9t kho\u1ea3ng tr\u1eafng.</li> </ul>"},{"location":"Problem/coci0607/Contest2/P2_ABC/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>1 5 3  \nABC  \n</code></pre> Output <pre><code>1 3 5  \n</code></pre> <p>Test 2</p> Input <pre><code>6 4 2  \nCAB  \n</code></pre> Output <pre><code>6 2 4  \n</code></pre>"},{"location":"Problem/coci0607/Contest2/P3_KOLONE/","title":"KOLONE","text":""},{"location":"Problem/coci0607/Contest2/P3_KOLONE/#kolone","title":"KOLONE","text":"<p>Khi di chuy\u1ec3n, ki\u1ebfn t\u1ea1o th\u00e0nh h\u00e0ng sao cho m\u1ed7i con ki\u1ebfn (tr\u1eeb con \u0111\u1ea7u ti\u00ean) \u0111\u1ec1u \u0111\u1ee9ng sau m\u1ed9t con ki\u1ebfn kh\u00e1c. Tuy nhi\u00ean, \u00edt ai bi\u1ebft \u0111i\u1ec1u g\u00ec s\u1ebd x\u1ea3y ra khi hai h\u00e0ng ki\u1ebfn di chuy\u1ec3n theo h\u01b0\u1edbng ng\u01b0\u1ee3c nhau g\u1eb7p nhau trong m\u1ed9t l\u1ed1i \u0111i qu\u00e1 h\u1eb9p \u0111\u1ec3 c\u1ea3 hai c\u00f3 th\u1ec3 \u0111i qua. </p> <p>M\u1ed9t gi\u1ea3 thuy\u1ebft cho r\u1eb1ng, trong t\u00ecnh hu\u1ed1ng n\u00e0y, ki\u1ebfn s\u1ebd nh\u1ea3y qua nhau. </p> <p>T\u1eeb th\u1eddi \u0111i\u1ec3m hai h\u00e0ng g\u1eb7p nhau, m\u1ed7i gi\u00e2y, m\u1ed7i con ki\u1ebfn s\u1ebd nh\u1ea3y qua (ho\u1eb7c b\u1ecb nh\u1ea3y qua, theo s\u1ef1 \u0111\u1ed3ng thu\u1eadn c\u1ee7a ch\u00fang) con ki\u1ebfn tr\u01b0\u1edbc m\u1eb7t n\u00f3 \u0111\u1ec3 hai con ki\u1ebfn \u0111\u1ed5i ch\u1ed7 cho nhau, nh\u01b0ng ch\u1ec9 khi con ki\u1ebfn kia \u0111ang di chuy\u1ec3n theo h\u01b0\u1edbng ng\u01b0\u1ee3c l\u1ea1i. H\u00e3y t\u00ecm th\u1ee9 t\u1ef1 c\u1ee7a c\u00e1c con ki\u1ebfn sau \\(T\\) gi\u00e2y.</p>"},{"location":"Problem/coci0607/Contest2/P3_KOLONE/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(N1\\) v\u00e0 \\(N2\\), l\u1ea7n l\u01b0\u1ee3t l\u00e0 s\u1ed1 l\u01b0\u1ee3ng ki\u1ebfn trong h\u00e0ng th\u1ee9 nh\u1ea5t v\u00e0 h\u00e0ng th\u1ee9 hai.</li> <li>Hai d\u00f2ng ti\u1ebfp theo ch\u1ee9a th\u1ee9 t\u1ef1 c\u00e1c con ki\u1ebfn trong h\u00e0ng th\u1ee9 nh\u1ea5t v\u00e0 h\u00e0ng th\u1ee9 hai (t\u1eeb \u0111\u1ea7u \u0111\u1ebfn cu\u1ed1i). M\u1ed7i con ki\u1ebfn \u0111\u01b0\u1ee3c x\u00e1c \u0111\u1ecbnh duy nh\u1ea5t b\u1edfi m\u1ed9t ch\u1eef c\u00e1i in hoa c\u1ee7a b\u1ea3ng ch\u1eef c\u00e1i ti\u1ebfng Anh (m\u1ed7i ch\u1eef c\u00e1i l\u00e0 duy nh\u1ea5t trong c\u1ea3 hai h\u00e0ng).</li> <li>D\u00f2ng cu\u1ed1i c\u00f9ng ch\u1ee9a s\u1ed1 nguy\u00ean \\(T\\) (\\(0 \\leq T \\leq 50\\)).</li> </ul>"},{"location":"Problem/coci0607/Contest2/P3_KOLONE/#output","title":"Output","text":"<ul> <li>In ra th\u1ee9 t\u1ef1 c\u1ee7a c\u00e1c con ki\u1ebfn sau \\(T\\) gi\u00e2y tr\u00ean m\u1ed9t d\u00f2ng duy nh\u1ea5t. G\u00f3c nh\u00ecn c\u1ee7a ch\u00fang ta l\u00e0 h\u00e0ng ki\u1ebfn th\u1ee9 nh\u1ea5t \u0111\u1ebfn t\u1eeb b\u00ean tr\u00e1i v\u00e0 h\u00e0ng c\u00f2n l\u1ea1i \u0111\u1ebfn t\u1eeb b\u00ean ph\u1ea3i.</li> </ul>"},{"location":"Problem/coci0607/Contest2/P3_KOLONE/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>3 3  \nABC  \nDEF  \n0  \n</code></pre> Output <pre><code>CBADEF  \n</code></pre> <p>Test 2</p> Input <pre><code>3 3  \nABC  \nDEF  \n2  \n</code></pre> Output <pre><code>CDBEAF  \n</code></pre> <p>Test 3</p> Input <pre><code>3 4  \nJLA  \nCRUO  \n3  \n</code></pre> Output <pre><code>CARLUJO  \n</code></pre>"},{"location":"Problem/coci0607/Contest2/P4_SJECISTA/","title":"SJECISTA","text":""},{"location":"Problem/coci0607/Contest2/P4_SJECISTA/#sjecista","title":"SJECI\u0160TA","text":"<p>X\u00e9t m\u1ed9t \u0111a gi\u00e1c l\u1ed3i c\u00f3 \\(N\\) \u0111\u1ec9nh, v\u1edbi t\u00ednh ch\u1ea5t b\u1ed5 sung r\u1eb1ng kh\u00f4ng c\u00f3 ba \u0111\u01b0\u1eddng ch\u00e9o n\u00e0o c\u1eaft nhau t\u1ea1i c\u00f9ng m\u1ed9t \u0111i\u1ec3m. H\u00e3y t\u00ecm s\u1ed1 giao \u0111i\u1ec3m gi\u1eefa c\u00e1c c\u1eb7p \u0111\u01b0\u1eddng ch\u00e9o trong \u0111a gi\u00e1c \u0111\u00f3.</p> <p>H\u00ecnh d\u01b0\u1edbi \u0111\u00e2y minh h\u1ecda m\u1ed9t \u0111a gi\u00e1c nh\u01b0 v\u1eady v\u1edbi \\(6\\) \u0111\u1ec9nh.</p> <p></p> <p>L\u01b0u \u00fd: M\u1ed9t \u0111a gi\u00e1c \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 l\u1ed3i n\u1ebfu t\u1ea5t c\u1ea3 c\u00e1c g\u00f3c b\u00ean trong c\u1ee7a n\u00f3 \u0111\u1ec1u nh\u1ecf h\u01a1n \\(180\\) \u0111\u1ed9.</p>"},{"location":"Problem/coci0607/Contest2/P4_SJECISTA/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean v\u00e0 duy nh\u1ea5t ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\), v\u1edbi \\(3 \\leq N \\leq 100\\).</li> </ul>"},{"location":"Problem/coci0607/Contest2/P4_SJECISTA/#output","title":"Output","text":"<ul> <li>In ra s\u1ed1 giao \u0111i\u1ec3m gi\u1eefa c\u00e1c c\u1eb7p \u0111\u01b0\u1eddng ch\u00e9o tr\u00ean m\u1ed9t d\u00f2ng duy nh\u1ea5t.</li> </ul>"},{"location":"Problem/coci0607/Contest2/P4_SJECISTA/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>3  \n</code></pre> Output <pre><code>0  \n</code></pre> <p>Test 2</p> Input <pre><code>4  \n</code></pre> Output <pre><code>1  \n</code></pre> <p>Test 3</p> Input <pre><code>6  \n</code></pre> Output <pre><code>15  \n</code></pre>"},{"location":"Problem/coci0607/Contest2/P5_STOL/","title":"STOL","text":""},{"location":"Problem/coci0607/Contest2/P5_STOL/#stol","title":"STOL","text":"<p>Mirko v\u1eeba mua m\u1ed9t c\u0103n h\u1ed9 v\u00e0 mu\u1ed1n m\u1eddi c\u00e0ng nhi\u1ec1u ng\u01b0\u1eddi c\u00e0ng t\u1ed1t \u0111\u1ebfn \u0103n t\u1ed1i \u0111\u1ec3 c\u00f9ng ch\u00fac m\u1eebng. \u0110\u1ec3 l\u00e0m \u0111\u01b0\u1ee3c \u0111i\u1ec1u n\u00e0y, anh \u1ea5y c\u1ea7n m\u1ed9t chi\u1ebfc b\u00e0n g\u1ed7 h\u00ecnh ch\u1eef nh\u1eadt l\u1edbn \u0111\u1ec3 c\u00f3 th\u1ec3 ng\u1ed3i c\u00f9ng kh\u00e1ch m\u1eddi.</p> <p>S\u1ed1 ng\u01b0\u1eddi c\u00f3 th\u1ec3 ng\u1ed3i quanh b\u00e0n b\u1eb1ng chu vi c\u1ee7a n\u00f3 (t\u1ed5ng \u0111\u1ed9 d\u00e0i c\u1ee7a c\u1ea3 b\u1ed1n c\u1ea1nh). Mirko mu\u1ed1n mua m\u1ed9t chi\u1ebfc b\u00e0n sao cho n\u00f3 c\u00f3 th\u1ec3 \u0111\u1eb7t v\u1eeba trong c\u0103n h\u1ed9 v\u00e0 c\u00f3 th\u1ec3 m\u1eddi \u0111\u01b0\u1ee3c nhi\u1ec1u ng\u01b0\u1eddi nh\u1ea5t c\u00f3 th\u1ec3. B\u00e0n ph\u1ea3i \u0111\u01b0\u1ee3c \u0111\u1eb7t sao cho c\u00e1c c\u1ea1nh c\u1ee7a n\u00f3 song song v\u1edbi c\u00e1c c\u1ea1nh c\u1ee7a c\u0103n h\u1ed9.</p> <p>D\u1ef1a tr\u00ean b\u1ed1 c\u1ee5c c\u1ee7a c\u0103n h\u1ed9, h\u00e3y t\u00ecm s\u1ed1 ng\u01b0\u1eddi t\u1ed1i \u0111a m\u00e0 Mirko c\u00f3 th\u1ec3 m\u1eddi \u0111\u1ebfn \u0103n t\u1ed1i.</p>"},{"location":"Problem/coci0607/Contest2/P5_STOL/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(R\\) v\u00e0 \\(C\\) (\\(1 \\leq R, C \\leq 400\\)), l\u00e0 k\u00edch th\u01b0\u1edbc c\u1ee7a c\u0103n h\u1ed9.</li> <li>M\u1ed7i trong \\(R\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a \u0111\u00fang \\(C\\) k\u00fd t\u1ef1 (kh\u00f4ng c\u00f3 kho\u1ea3ng tr\u1eafng), m\u1ed7i \u00f4 c\u00f3 th\u1ec3 l\u00e0 \u00f4 tr\u1ed1ng (<code>.</code>) ho\u1eb7c b\u1ecb ch\u1eb7n (<code>X</code>).</li> <li>Mirko ch\u1ec9 c\u00f3 th\u1ec3 \u0111\u1eb7t b\u00e0n tr\u00ean c\u00e1c \u00f4 tr\u1ed1ng.</li> </ul>"},{"location":"Problem/coci0607/Contest2/P5_STOL/#output","title":"Output","text":"<ul> <li>In ra s\u1ed1 kh\u00e1ch t\u1ed1i \u0111a m\u00e0 Mirko c\u00f3 th\u1ec3 m\u1eddi \u0111\u1ebfn \u0103n t\u1ed1i sau khi mua b\u00e0n tr\u00ean m\u1ed9t d\u00f2ng duy nh\u1ea5t.</li> </ul>"},{"location":"Problem/coci0607/Contest2/P5_STOL/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>2 2  \n..  \n..  \n</code></pre> Output <pre><code>7  \n</code></pre> Note <p>L\u01b0u \u00fd r\u1eb1ng Mirko ch\u1ec9 c\u00f3 th\u1ec3 m\u1eddi \\(7\\) ng\u01b0\u1eddi v\u00ec trong s\u1ed1 ng\u01b0\u1eddi ng\u1ed3i v\u00e0o b\u00e0n c\u00f3 c\u1ea3 anh \u1ea5y.</p> <p>Test 2</p> Input <pre><code>4 4  \nX.XX  \nX..X  \n..X.  \n..XX  \n</code></pre> Output <pre><code>9  \n</code></pre> <p>Test 3</p> Input <pre><code>3 3  \nX.X  \n.X.  \nX.X  \n</code></pre> Output <pre><code>3  \n</code></pre>"},{"location":"Problem/coci0607/Contest2/P6_STRAZA/","title":"STRAZA","text":""},{"location":"Problem/coci0607/Contest2/P6_STRAZA/#straza","title":"STRA\u017dA","text":"<p>G\u1ea7n m\u1ed9t c\u0103n c\u1ee9 qu\u00e2n s\u1ef1 c\u00f3 m\u1ed9t h\u1ec7 th\u1ed1ng h\u00e0o \u0111\u01b0\u1ee3c m\u00f4 h\u00ecnh h\u00f3a d\u01b0\u1edbi d\u1ea1ng c\u00e1c \u0111o\u1ea1n th\u1eb3ng tr\u00ean m\u1eb7t ph\u1eb3ng. Ban \u0111\u00eam, khi ph\u1ea7n l\u1edbn binh s\u0129 \u0111ang ng\u1ee7 say, ba l\u00ednh canh \u0111\u1ee9ng g\u00e1c \u0111\u1ec3 gi\u00e1m s\u00e1t c\u00e1c h\u00e0o. Hai l\u00ednh canh c\u00f3 th\u1ec3 nh\u00ecn th\u1ea5y nhau n\u1ebfu t\u1ed3n t\u1ea1i m\u1ed9t h\u00e0o (ho\u1eb7c m\u1ed9t chu\u1ed7i c\u00e1c h\u00e0o n\u1ed1i ti\u1ebfp nhau) tr\u1ea3i d\u00e0i to\u00e0n b\u1ed9 \u0111o\u1ea1n th\u1eb3ng n\u1ed1i gi\u1eefa h\u1ecd v\u00e0 kh\u00f4ng c\u00f3 l\u00ednh canh th\u1ee9 ba n\u1eb1m tr\u00ean \u0111o\u1ea1n th\u1eb3ng \u0111\u00f3. </p> <p>V\u00ec l\u00fd do an ninh, c\u00e1c l\u00ednh canh ph\u1ea3i \u0111\u01b0\u1ee3c \u0111\u1eb7t sao cho m\u1ed7i ng\u01b0\u1eddi c\u00f3 th\u1ec3 nh\u00ecn th\u1ea5y hai ng\u01b0\u1eddi c\u00f2n l\u1ea1i. H\u1ecfi c\u00f3 bao nhi\u00eau c\u00e1ch s\u1eafp x\u1ebfp c\u00e1c l\u00ednh canh theo y\u00eau c\u1ea7u tr\u00ean c\u00e1c h\u00e0o?</p>"},{"location":"Problem/coci0607/Contest2/P6_STRAZA/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a s\u1ed1 nguy\u00ean \\(N\\) (\\(1 \\leq N \\leq 20\\)), s\u1ed1 l\u01b0\u1ee3ng h\u00e0o.</li> <li>M\u1ed7i trong \\(N\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a b\u1ed1n s\u1ed1 nguy\u00ean d\u01b0\u01a1ng \\(X_1\\), \\(Y_1\\), \\(X_2\\), \\(Y_2\\) (\\(1 \\leq X_1, Y_1, X_2, Y_2 \\leq 1000\\)), trong \u0111\u00f3 \\((X_1, Y_1)\\) l\u00e0 t\u1ecda \u0111\u1ed9 m\u1ed9t \u0111\u1ea7u h\u00e0o v\u00e0 \\((X_2, Y_2)\\) l\u00e0 t\u1ecda \u0111\u1ed9 \u0111\u1ea7u c\u00f2n l\u1ea1i.</li> <li>C\u00e1c h\u00e0o trong d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o c\u00f3 th\u1ec3 tr\u00f9ng nhau ho\u1eb7c c\u00f3 chung \u0111i\u1ec3m \u0111\u1ea7u m\u00fat.</li> </ul>"},{"location":"Problem/coci0607/Contest2/P6_STRAZA/#output","title":"Output","text":"<ul> <li>In ra s\u1ed1 c\u00e1ch \u0111\u1eb7t l\u00ednh canh th\u1ecfa m\u00e3n \u0111i\u1ec1u ki\u1ec7n tr\u00ean m\u1ed9t d\u00f2ng duy nh\u1ea5t.</li> </ul>"},{"location":"Problem/coci0607/Contest2/P6_STRAZA/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>6\n0 0 1 0  \n0 0 0 1 \n1 0 1 1 \n0 1 1 1 \n0 0 1 1 \n1 0 0 1 \n</code></pre> Output <pre><code>8\n</code></pre> <p>Test 2</p> Input <pre><code>4\n5 1 7 1 \n1 1 5 1 \n4 0 4 4 \n7 0 3 4 \n</code></pre> Output <pre><code>1\n</code></pre> <p>Test 3</p> Input <pre><code>3\n2 2 3 2 \n3 2 3 3 \n3 3 2 3 \n</code></pre> Output <pre><code>0\n</code></pre>"},{"location":"Problem/coci0607/Contest3/P1_PATULJCI/","title":"PATULJCI","text":""},{"location":"Problem/coci0607/Contest3/P1_PATULJCI/#patuljci","title":"PATULJCI","text":"<p>M\u1ed7i ng\u00e0y, trong khi nh\u1eefng ch\u00fa l\u00f9n \u0111ang b\u1eadn r\u1ed9n trong m\u1ecf, B\u1ea1ch Tuy\u1ebft chu\u1ea9n b\u1ecb b\u1eefa t\u1ed1i cho h\u1ecd, b\u1ea3y c\u00e1i gh\u1ebf, b\u1ea3y c\u00e1i \u0111\u0129a, 7 c\u00e1i n\u0129a v\u00e0 7 c\u00e1i dao cho 7 ch\u00fa l\u00f9n \u0111ang \u0111\u00f3i.</p> <p>M\u1ed9t ng\u00e0y n\u1ecd, 9 ch\u00fa l\u00f9n \u0111\u1ebfn t\u1eeb m\u1ecf thay v\u00ec 7 ch\u00fa l\u00f9n (kh\u00f4ng ai bi\u1ebft nh\u01b0 n\u00e0o v\u00e0 t\u1ea1i sao), m\u1ed7i ng\u01b0\u1eddi h\u1ecd t\u1ef1 x\u01b0ng l\u00e0 m\u1ed9t trong 7 ch\u00fa l\u00f9n c\u1ee7a B\u1ea1ch Tuy\u1ebft.</p> <p>May m\u1eafn thay, m\u1ed7i ch\u00fa l\u00f9n \u0111\u1ed9i m\u1ed9t chi\u1ebfc m\u0169 v\u1edbi s\u1ed1 nguy\u00ean d\u01b0\u01a1ng kh\u00f4ng qu\u00e1 100. B\u1ea1ch Tuy\u1ebft l\u00e0 m\u1ed9t nh\u00e0 to\u00e1n h\u1ecdc n\u1ed5i ti\u1ebfng, do \u0111\u00f3 \u0111\u00e3 nh\u1eadn ra t\u1eeb l\u00e2u r\u1eb1ng t\u1ed5ng c\u00e1c s\u1ed1 \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 tr\u00ean c\u00e1c m\u0169 c\u1ee7a 7 ch\u00fa l\u00f9n ch\u00ednh x\u00e1c l\u00e0 100.</p> <p>Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh 7 ch\u00fa l\u00f9n ban \u0111\u1ea7u, ch\u1ecdn 7 ch\u00fa l\u00f9n trong s\u1ed1 9 ch\u00fa l\u00f9n \u0111\u00f3 \u0111\u1ec3 t\u1ed5ng l\u00e0 100.</p>"},{"location":"Problem/coci0607/Contest3/P1_PATULJCI/#input","title":"Input:","text":"<ul> <li>\u0110\u1ea7u v\u00e0o c\u00f3 9 d\u00f2ng. M\u1ed7i d\u00f2ng ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean c\u00f3 gi\u00e1 tr\u1ecb t\u1eeb 1 \u0111\u1ebfn 99. T\u1ea5t c\u1ea3 c\u00e1c s\u1ed1 \u0111\u01b0\u1ee3c cho \u0111\u00f3 l\u00e0 ph\u00e2n bi\u1ec7t.</li> <li>L\u01b0u \u00fd: T\u1eadp d\u1eef li\u1ec7u lu\u00f4n c\u00f3 m\u1ed9t l\u1eddi gi\u1ea3i \u0111\u1ed9c nh\u1ea5t.</li> </ul>"},{"location":"Problem/coci0607/Contest3/P1_PATULJCI/#output","title":"Output:","text":"<ul> <li>Ch\u01b0\u01a1ng tr\u00ecnh c\u1ee7a b\u1ea1n ph\u1ea3i in ra ch\u00ednh x\u00e1c 7 d\u00f2ng - l\u00e0 s\u1ed1 \u0111\u01b0\u1ee3c ghi tr\u00ean m\u1ed7i m\u0169 c\u1ee7a 7 ch\u00fa l\u00fan ban \u0111\u1ea7u.</li> </ul>"},{"location":"Problem/coci0607/Contest3/P1_PATULJCI/#example","title":"Example:","text":"<p>Test 1</p> Input <pre><code>7\n8\n10\n13\n15\n19\n20\n23\n25\n</code></pre> Output <pre><code>8\n6\n5\n1\n37\n30\n28\n22\n36\n</code></pre> <p>Test 2</p> Input <pre><code>7\n8\n10\n13\n19\n20\n23\n</code></pre> Output <pre><code>8\n6\n5\n1\n30\n28\n22\n</code></pre>"},{"location":"Problem/coci0607/Contest3/P2_NPUZZLE/","title":"NPUZZLE","text":""},{"location":"Problem/coci0607/Contest3/P2_NPUZZLE/#npuzzle","title":"NPUZZLE","text":"<p>N-puzzle l\u00e0 m\u1ed9t c\u00e2u \u0111\u1ed1 c\u00f3 nhi\u1ec1u bi\u1ebfn t\u00ean v\u00e0 bi\u1ebfn th\u1ec3 kh\u00e1c nhau. Trong b\u00e0i n\u00e0y ch\u00fang ta ch\u1ec9 quan t\u00e2m \u0111\u1ebfn bi\u1ebfn th\u1ec3 15-puzzle. N\u00f3 bao g\u1ed3m m\u1ed9t l\u01b0\u1edbi \\(4 \\times 4\\) c\u00e1c \u00f4 tr\u01b0\u1ee3t, trong s\u1ed1 \u0111\u00f3 c\u00f3 m\u1ed9t \u00f4 vu\u00f4ng b\u1ecb thi\u1ebfu. C\u00e1c \u00f4 vu\u00f4ng \u0111\u01b0\u1ee3c g\u00e1n nh\u00e3n b\u1eb1ng c\u00e1c ch\u1eef c\u00e1i in hoa t\u1eeb 'A' \u0111\u1ebfn 'O' v\u1edbi b\u1ed1 c\u1ee5c mon mu\u1ed1n l\u00e0 nh\u01b0 sau:</p> <p></p> <p>Ta \u0111\u1ecbnh ngh\u0129a \"\u0111\u1ed9 ph\u00e2n t\u00e1n c\u1ee7a c\u00e2u \u0111\u1ed1 l\u00e0 t\u1ed5ng c\u1ee7a c\u00e1c kho\u1ea3ng c\u00e1ch gi\u1eefa nh\u1eefng \u00f4 hi\u1ec7n t\u1ea1i v\u00e0 v\u1ecb tr\u00ed mong mu\u1ed1n c\u1ee7a c\u00e1c \u00f4 \u0111\u00f3. Kho\u1ea3ng c\u00e1ch gi\u1eefa hai \u00f4 vu\u00f4ng \u0111\u01b0\u1ee3c t\u00ednh theo kho\u1ea3ng c\u00e1ch Manhattan (t\u1ee9c l\u00e0 kho\u1ea3ng c\u00e1ch Manhattan gi\u1eefa 2 \u00f4 \\((x, y)\\) v\u00e0 \\((u, v)\\) l\u00e0 \\(|x - u| + |y - v|\\)).</p> <p>Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh t\u00ednh \u0111\u1ed9 ph\u00e2n t\u00e1n c\u1ee7a c\u00e2u \u0111\u1ed1 b\u1ea1n nh\u1eadn \u0111\u01b0\u1ee3c.</p>"},{"location":"Problem/coci0607/Contest3/P2_NPUZZLE/#input","title":"Input:","text":"<ul> <li>G\u1ed3m 4 d\u00f2ng, m\u1ed7i d\u00f2ng ch\u1ee9a 4 k\u00ed t\u1ef1 bi\u1ec3u di\u1ec5n tr\u1ea1ng th\u00e1i c\u1ee7a c\u00e2u \u0111\u1ed1.</li> </ul>"},{"location":"Problem/coci0607/Contest3/P2_NPUZZLE/#output","title":"Output:","text":"<ul> <li>D\u1eef li\u1ec7u \u0111\u1ea7u ra g\u1ed3m m\u1ed9t d\u00f2ng duy nh\u1ea5t l\u00e0 \u0111\u1ed9 ph\u00e2n t\u00e1n b\u1ea1n t\u00ecm \u0111\u01b0\u1ee3c.</li> </ul>"},{"location":"Problem/coci0607/Contest3/P2_NPUZZLE/#example","title":"Example:","text":"<p>Test 1</p> Input <pre><code>ABCD\nEFGH\nIJKL\nM.NO\n</code></pre> Output <pre><code>2\n</code></pre> <p>Test 2</p> Input <pre><code>.BCD\nEAGH\nIJFL\nMNOK\n</code></pre> Output <pre><code>6\n</code></pre>"},{"location":"Problem/coci0607/Contest3/P3_TROJKE/","title":"TROJKE","text":""},{"location":"Problem/coci0607/Contest3/P3_TROJKE/#trojke","title":"TROJKE","text":"<p>Mirko v\u00e0 Slavko \u0111ang ch\u01a1i m\u1ed9t tr\u00f2 ch\u01a1i m\u1edbi c\u00f3 t\u00ean l\u00e0 \"Trojke\" (B\u1ed9 ba). \u0110\u1ea7u ti\u00ean, h\u1ecd d\u00f9ng ph\u1ea5n \u0111\u1ec3 v\u1ebd m\u1ed9t l\u01b0\u1edbi \\(n \\times n\\) \u00f4 vu\u00f4ng tr\u00ean m\u1eb7t \u0111\u01b0\u1eddng. Sau \u0111\u00f3 h\u1ecd vi\u1ebft c\u00e1c ch\u1eef c\u00e1i v\u00e0o m\u1ed7i \u00f4 vu\u00f4ng \u0111\u00f3. \u0110\u1ea3m b\u1ea3o r\u1eb1ng kh\u00f4ng c\u00f3 ch\u1eef c\u00e1i n\u00e0o \u0111\u01b0\u1ee3c vi\u1ebft nhi\u1ec1u h\u01a1n m\u1ed9t l\u1ea7n trong l\u01b0\u1edbi.</p> <p>Tr\u00f2 ch\u01a1i y\u00eau c\u1ea7u t\u00ecm ba k\u00fd t\u1ef1 n\u1eb1m tr\u00ean c\u00f9ng m\u1ed9t \u0111\u01b0\u1eddng th\u1eb3ng nhanh nh\u1ea5t c\u00f3 th\u1ec3. Ba k\u00fd t\u1ef1 \u0111\u01b0\u1ee3c coi l\u00e0 n\u1eb1m tr\u00ean m\u1ed9t \u0111\u01b0\u1eddng th\u1eb3ng n\u1ebfu nh\u01b0 c\u00f3 m\u1ed9t \u0111\u01b0\u1eddng th\u1eb3ng \u0111i qua ch\u00ednh gi\u1eefa t\u00e2m c\u1ee7a t\u1eebng \u00f4 vu\u00f4ng ch\u1ee9a ch\u1ee9a ch\u00fang.</p> <p>Sau m\u1ed9t kho\u1ea3ng th\u1eddi gian, vi\u1ec7c t\u00ecm ra b\u1ed9 ba m\u1edbi tr\u1edf n\u00ean kh\u00f3 kh\u0103n h\u01a1n. Mirko v\u00e0 Slavko c\u1ea7n m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh \u0111\u1ec3 \u0111\u1ebfm t\u1ea5t c\u1ea3 c\u00e1c b\u1ed9 ba c\u00f3 th\u1ec3 c\u00f3 \u0111\u1ec3 bi\u1ebft li\u1ec7u h\u1ecd \u0111\u00e3 t\u00ecm th\u1ea5y h\u1ebft hay ch\u01b0a.</p>"},{"location":"Problem/coci0607/Contest3/P3_TROJKE/#input","title":"Input:","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(n\\) \\((3 \\le n \\le 100)\\) l\u00e0 k\u00edch th\u01b0\u1edbc c\u1ee7a l\u01b0\u1edbi.</li> <li>M\u1ed7i d\u00f2ng trong \\(n\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a \\(n\\) k\u00fd t\u1ef1 m\u00f4 t\u1ea3 l\u01b0\u1edbi g\u1ed3m c\u00e1c k\u00fd t\u1ef1 in hoa v\u00e0 k\u00fd t\u1ef1 '.' (m\u00f4 t\u1ea3 \u00f4 vu\u00f4ng tr\u1ed1ng).</li> </ul>"},{"location":"Problem/coci0607/Contest3/P3_TROJKE/#output","title":"Output:","text":"<ul> <li>\u0110\u1ea7u ra l\u00e0 m\u1ed9t d\u00f2ng duy nh\u1ea5t m\u00f4 t\u1ea3 s\u1ed1 l\u01b0\u1ee3ng b\u1ed9 ba c\u1ee7a l\u01b0\u1edbi.</li> </ul>"},{"location":"Problem/coci0607/Contest3/P3_TROJKE/#example","title":"Example:","text":"<p>Test 1</p> Input <pre><code>4\n...D\n..C.\n.B..\nA...\n</code></pre> Output <pre><code>4\n</code></pre> <p>Test 2</p> Input <pre><code>5\n..T..\nA....\n.FE.R\n....X\nS....\n</code></pre> Output <pre><code>3\n</code></pre> <p>Test 3</p> Input <pre><code>10\n....AB....\n..C....D..\n.E......F.\n...G..H...\nI........J\nK........L\n...M..N...\n.O......P.\n..Q....R..\n....ST....\n</code></pre> Output <pre><code>0\n</code></pre>"},{"location":"Problem/coci0607/Contest3/P4_TENKICI/","title":"TENKICI","text":""},{"location":"Problem/coci0607/Contest3/P4_TENKICI/#tenkici","title":"TENKICI","text":"<p>Mirko t\u00ecm th\u1ea5y m\u1ed9t b\u1ed9 s\u01b0u t\u1eadp \\(N\\) chi\u1ebfc xe t\u0103ng \u0111\u1ed3 ch\u01a1i c\u0169 t\u1eeb th\u1eddi Th\u1ebf chi\u1ebfn II tr\u00ean g\u00e1c x\u00e9p c\u1ee7a \u00f4ng n\u1ed9i. C\u1eadu l\u1eadp t\u1ee9c g\u1ecdi b\u1ea1n m\u00ecnh l\u00e0 Slavko \u0111\u1ebfn \u0111\u1ec3 ch\u01a1i c\u00f9ng. H\u1ecd t\u1ea1o ra m\u1ed9t chi\u1ebfn tr\u01b0\u1eddng l\u00e0 m\u1ed9t t\u1ea5m v\u00e1n g\u1ed7 bao g\u1ed3m c\u00e1c \u00f4 vu\u00f4ng v\u1edbi \\(N\\) h\u00e0ng v\u00e0 \\(N\\) c\u1ed9t.</p> <p>M\u1ed7i chi\u1ebfc xe t\u0103ng c\u00f3 th\u1ec3 di chuy\u1ec3n \u0111\u1ebfn m\u1ed9t trong b\u1ed1n \u00f4 k\u1ec1 c\u1ea1nh. M\u1ed9t chi\u1ebfc xe t\u0103ng c\u00f3 th\u1ec3 b\u1eafn v\u00e0o b\u1ea5t k\u1ef3 \u00f4 vu\u00f4ng n\u00e0o tr\u00ean c\u00f9ng h\u00e0ng ho\u1eb7c c\u00f9ng c\u1ed9t m\u00e0 n\u00f3 \u0111ang \u0111\u1ee9ng. M\u1ed9t xe t\u0103ng \u0111\u01b0\u1ee3c coi l\u00e0 \"b\u1ea3o v\u1ec7\" h\u00e0ng v\u00e0 c\u1ed9t m\u00e0 n\u00f3 \u0111ang \u0111\u1ee9ng.</p> <p>Ngo\u00e0i ra, kh\u00f4ng c\u00f3 2 xe t\u0103ng n\u00e0o n\u1eb1m trong c\u00f9ng m\u1ed9t \u00f4 vu\u00f4ng t\u1ea1i b\u1ea5t k\u1ef3 m\u1ed9t th\u1eddi \u0111i\u1ec3m n\u00e0o.</p> <p>Sauu nhi\u1ec1u gi\u1edd ch\u01a1i, m\u1eb9 c\u1ee7a Mirko la xu\u1ed1ng \u0103n tr\u01b0a, v\u00ec th\u1ebf h\u1ecd quy\u1ebft \u0111\u1ecbnh s\u1eafp x\u1ebfp l\u1ea1i nh\u1eefng chi\u1ebfc xe t\u0103ng sao cho m\u1ed7i chi\u1ebfc xe t\u0103ng \u0111\u1ec1u b\u1ea3o v\u1ec7 c\u00e1c h\u00e0ng v\u00e0 c\u00e1c c\u1ed9t kh\u00e1c nhau (t\u1ee9c l\u00e0 m\u1ed7i h\u00e0ng v\u00e0 m\u1ed7i c\u1ed9t \u0111\u00eau ch\u1ec9 ch\u1ee9a duy nh\u1ea5t m\u1ed9t xe t\u0103ng).</p> <p>Tuy nhi\u00ean, h\u1ecd mu\u1ed1n l\u00e0m \u0111i\u1ec1u \u0111\u00f3 v\u1edbi s\u1ed1 l\u1ea7n di chuy\u1ec3n c\u00e1c xe t\u0103ng l\u00e0 \u00edt nh\u1ea5t c\u00f3 th\u1ec3.</p> <p>Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh t\u00ecm s\u1ed1 l\u1ea7n di chuy\u1ec3n c\u00e1c xe t\u0103ng t\u1ed1i thi\u1ec3u \u0111\u1ec3 s\u1eafp x\u1ebfp l\u1ea1i c\u00e1c xe t\u0103ng sao cho m\u1ed7i h\u00e0ng v\u00e0 m\u1ed7i c\u1ed9t ch\u1ee9a duy nh\u1ea5t m\u1ed9t xe, v\u00e0 in ra m\u1ed9t chu\u1ed7i c\u00e1c b\u01b0\u1edbc di chuy\u1ec3n ng\u1eafn nh\u1ea5t t\u00ecm \u0111\u01b0\u1ee3c.</p>"},{"location":"Problem/coci0607/Contest3/P4_TENKICI/#input","title":"Input:","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) \\((5 \\le N \\le 500)\\). </li> <li>M\u1ed7i d\u00f2ng trong s\u1ed1 \\(N\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(R\\) v\u00e0 \\(C\\) \\((1 \\le R, C \\le N)\\) m\u00f4 t\u1ea3 v\u1ecb tr\u00ed h\u00e0ng v\u00e0 c\u1ed9t c\u1ee7a m\u1ed7i xe t\u0103ng t\u1ea1i th\u1eddi \u0111i\u1ec3m b\u1ecb m\u1eb9 c\u1ee7a Mirko g\u1ecdi \u0111i \u0103n c\u01a1m. \u0110\u1ea3m b\u1ea3o r\u1eb1ng kh\u00f4ng c\u00f3 b\u1ea5t k\u1ef3 hai xe t\u0103ng n\u00e0o n\u1eb1m trong c\u00f9ng m\u1ed9t \u00f4 vu\u00f4ng. C\u00e1c h\u00e0ng v\u00e0 c\u00e1c c\u1ed9t \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(N\\), t\u1eeb tr\u00ean xu\u1ed1ng d\u01b0\u1edbi v\u00e0 t\u1eeb tr\u00e1i qua ph\u1ea3i.</li> </ul>"},{"location":"Problem/coci0607/Contest3/P4_TENKICI/#output","title":"Output:","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean in ra s\u1ed1 l\u1ea7n di chuy\u1ec3n t\u1ed1i thi\u1ec3u (k\u00fd hi\u1ec7u l\u00e0 \\(K\\)) tr\u00ean d\u00f2ng \u0111\u1ea7u ti\u00ean. </li> <li>M\u1ed7i d\u00f2ng trong s\u1ed1 \\(K\\) d\u00f2ng ti\u1ebfp theo, in ra ch\u1ec9 s\u1ed1 c\u1ee7a xe t\u0103ng \u0111\u01b0\u1ee3c di chuy\u1ec3n v\u00e0 h\u01b0\u1edbng di chuy\u1ec3n c\u1ee7a n\u00f3 (s\u1ed1 th\u1ee9 t\u1ef1 c\u1ee7a xe t\u0103ng v\u00e0 h\u01b0\u1edbng di chuy\u1ec3n \u0111\u01b0\u1ee3c ph\u00e2n c\u00e1ch b\u1eb1ng m\u1ed9t kho\u1ea3ng tr\u1eafng). H\u01b0\u1edbng di chuy\u1ec3n c\u1ee7a xe t\u0103ng \u0111\u01b0\u1ee3c k\u00fd hi\u1ec7u b\u1edfi b\u1ed1n ch\u1eef c\u00e1i in hoa: 'L', 'R', 'U', 'D' l\u1ea7n l\u01b0\u1ee3t l\u00e0 tr\u00e1i, ph\u1ea3i, tr\u00ean, d\u01b0\u1edbi.</li> <li>L\u01b0u \u00fd r\u1eb1ng chu\u1ed7i c\u00e1c b\u01b0\u1edbc di chuy\u1ec3n c\u1ee7a xe t\u0103ng c\u00f3 th\u1ec3 c\u00f3 nhi\u1ec1u c\u00e1ch.</li> </ul>"},{"location":"Problem/coci0607/Contest3/P4_TENKICI/#scoring","title":"Scoring","text":"<p>N\u1ebfu c\u1ea3 hai s\u1ed1 \\(K\\) v\u00e0 c\u00e1ch di chuy\u1ec3n l\u00e0 \u0111\u00fang, b\u1ea1n s\u1ebd nh\u1eadn \u0111\u01b0\u1ee3c to\u00e0n b\u1ed9 s\u1ed1 \u0111i\u1ec3m cho m\u1ed7i test. N\u1ebfu ch\u01b0\u01a1ng tr\u00ecnh c\u1ee7a b\u1ea1n ch\u1ec9 \u0111\u00fang \\(K\\) v\u00e0 kh\u00f4ng in ra c\u00e1ch di chuy\u1ec3n ho\u1eb7c c\u00e1ch di chuy\u1ec3n l\u00e0 sai th\u00ec b\u1ea1n s\u1ebd nh\u1eadn \u0111\u01b0\u1ee3c \\(60\\%\\) cho m\u1ed7i test.</p>"},{"location":"Problem/coci0607/Contest3/P4_TENKICI/#example","title":"Example:","text":"<p>Test 1</p> Input <pre><code>5 \n1 1 \n1 2 \n1 3 \n1 4 \n1 5\n</code></pre> Output <pre><code>10 \n1 D \n2 D \n3 D \n4 D \n1 D \n2 D \n3 D \n1 D \n2 D \n1 D\n</code></pre> <p>Test 2</p> Input <pre><code>5 \n2 3 \n3 2 \n3 3 \n3 4 \n4 3\n</code></pre> Output <pre><code>8 \n1 R \n1 R \n2 U \n2 U \n4 D \n4 D \n5 L \n5 L\n</code></pre> <p>Test 3</p> Input <pre><code>6 \n1 1 \n1 2 \n2 1 \n5 6 \n6 5 \n6 6\n</code></pre> Output <pre><code>8 \n2 R \n2 D \n3 D \n3 R \n4 U \n4 L \n5 L \n5 U\n</code></pre>"},{"location":"Problem/coci0607/Contest3/P5_BICIKLI/","title":"BICIKLI","text":""},{"location":"Problem/coci0607/Contest3/P5_BICIKLI/#bicikli","title":"BICIKLI","text":"<p>M\u1ed9t cu\u1ed9c \u0111ua xe \u0111\u1ea1p \u0111ang \u0111\u01b0\u1ee3c t\u1ed5 th\u1ee9c t\u1ea1i v\u00f9ng \u0111\u1ea5t xa x\u00f4i. V\u00f9ng \u0111\u1ea5t n\u00e0y c\u00f3 \\(N\\) th\u1ecb tr\u1ea5n, \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(N\\). C\u00f3 \\(M\\) con \u0111\u01b0\u1eddng m\u1ed9t chi\u1ec1u gi\u1eefa c\u00e1c th\u1ecb tr\u1ea5n. Cu\u1ed9c \u0111ua s\u1ebd b\u1eaft \u0111\u1ea7u t\u1ea1i th\u1ecb tr\u1ea5n \\(1\\) v\u00e0 k\u1ebft th\u00fac t\u1ea1i th\u1ecb tr\u1ea5n \\(2\\). </p> <p>C\u00f3 bao nhi\u00eau c\u00e1ch ch\u1ecdn tuy\u1ebfn \u0111\u01b0\u1eddng \u0111ua kh\u00e1c nhau? Hai tuy\u1ebfn \u0111\u01b0\u1eddng \u0111\u01b0\u1ee3c coi l\u00e0 kh\u00e1c nhau n\u1ebfu ch\u00fang kh\u00f4ng \u0111\u01b0\u1ee3c t\u1ea1o b\u1edfi ch\u00ednh x\u00e1c m\u1ed9t t\u1eadp h\u1ee3p c\u00e1c con \u0111\u01b0\u1eddng.</p>"},{"location":"Problem/coci0607/Contest3/P5_BICIKLI/#input","title":"Input:","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(N\\) v\u00e0 \\(M\\) \\((1 \\le N \\le 10^4; 1 \\le M \\le 10^5)\\) m\u00f4 t\u1ea3 s\u1ed1 th\u1ecb tr\u1ea5n v\u00e0 s\u1ed1 con \u0111\u01b0\u1eddng.</li> <li>M\u1ed7i d\u00f2ng trong s\u1ed1 \\(M\\) d\u00f2ng ti\u1ebfp hteo ch\u1ee9a 2 s\u1ed1 nguy\u00ean ph\u00e2n bi\u1ec7t \\(A\\) v\u00e0 \\(B\\), m\u00f4 t\u1ea3 con \u0111\u01b0\u1eddng \u0111i t\u1eeb \\(A\\) \u0111\u1ebfn \\(B\\). C\u00f3 th\u1ec3 c\u00f3 nhi\u1ec1u con \u0111\u01b0\u1eddng gi\u1eefa hai th\u1ecb tr\u1ea5n.</li> </ul>"},{"location":"Problem/coci0607/Contest3/P5_BICIKLI/#output","title":"Output:","text":"<p>In ra s\u1ed1 l\u01b0\u1ee3ng tuy\u1ebfn \u0111\u01b0\u1eddng kh\u00e1c nhau tr\u00ean m\u1ed9t d\u00f2ng. N\u1ebfu s\u1ed1 c\u1ea7n in ra c\u00f3 nhi\u1ec1u h\u01a1n 9 ch\u1eef s\u1ed1, h\u00e3y in ra 9 ch\u1eef s\u1ed1 cu\u1ed1i c\u00f9ng (t\u1ee9c l\u00e0 ph\u1ea7n d\u01b0 c\u1ee7a \\(10^9\\)). N\u1ebfu c\u00f3 v\u00f4 s\u1ed1 tuy\u1ebfn \u0111\u01b0\u1eddng, in ra <code>inf</code>.</p>"},{"location":"Problem/coci0607/Contest3/P5_BICIKLI/#example","title":"Example:","text":"<p>Test 1</p> Input <pre><code>6 7 \n1 3 \n1 4 \n3 2 \n4 2 \n5 6 \n6 5 \n3 4\n</code></pre> Output <pre><code>3\n</code></pre> <p>Test 2</p> Input <pre><code>6 8 \n1 3 \n1 4 \n3 2 \n4 2 \n5 6 \n6 5 \n3 4 \n4 3 \n</code></pre> Output <pre><code>inf\n</code></pre> <p>Test 3</p> Input <pre><code>31 60 \n1 3 \n1 3 \n3 4 \n3 4 \n4 5 \n4 5 \n5 6 \n5 6 \n6 7 \n6 7 \n... \n... \n... \n28 29 \n28 29 \n29 30 \n29 30 \n30 31 \n30 31 \n31 2 \n31 2\n</code></pre> Output <pre><code>073741824\n</code></pre>"},{"location":"Problem/coci0607/Contest3/P6_LISTA/","title":"LISTA","text":""},{"location":"Problem/coci0607/Contest3/P6_LISTA/#lista","title":"LISTA","text":"<p>Mirko nh\u1eadn \u0111\u01b0\u1ee3c m\u00f3n qu\u00e0 sinh nh\u1eadt t\u1eeb d\u00ec c\u1ee7a m\u00ecnh \u1edf M\u1ef9, m\u00f3n qu\u00e0 l\u00e0 m\u1ed9t danh s\u00e1ch li\u00ean k\u1ebft \u0111\u00f4i ho\u00e0n to\u00e0n m\u1edbi. Danh s\u00e1ch n\u00e0y ch\u1ee9a \\(N\\) n\u00fat, \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(N\\). C\u00f3 hai lo\u1ea1i thao t\u00e1c tr\u00ean danh s\u00e1ch:</p> <ul> <li>A: Di chuy\u1ec3n n\u00fat \\(X\\) \u0111\u1ebfn tr\u01b0\u1edbc n\u00fat \\(Y\\).</li> <li>B: Di chuy\u1ec3n n\u00fat \\(X\\) ra sau n\u00fat \\(Y\\).</li> </ul> <p></p> <ul> <li>V\u00ed d\u1ee5 v\u1ec1 danh s\u00e1ch li\u00ean k\u1ebft \u0111\u00f4i v\u1edbi 6 n\u00fat.</li> </ul> <p></p> <ul> <li>Danh s\u00e1ch sau thao t\u00e1c <code>A 1 4</code>. </li> </ul> <p></p> <ul> <li>Danh s\u00e1ch sau thao t\u00e1c <code>B 3 5</code>.</li> </ul> <p>Mirko \u0111\u00e3 ch\u01a1i ch\u00fang trong nhi\u1ec1u gi\u1edd, v\u00e0 ghi l\u1ea1i t\u1eebng thao t\u00e1c tr\u00ean m\u1ed9t t\u1edd gi\u1ea5y \u0111\u1ec3 c\u00f3 th\u1ec3 kh\u00f4i ph\u1ee5c l\u1ea1i tr\u1ea1ng th\u00e1i ban \u0111\u1ea7u (c\u00e1c n\u00fat t\u1eeb \\(1\\) \u0111\u1ebfn \\(N\\) c\u00f3 th\u1ee9 t\u1ef1 t\u1eeb tr\u00e1i qua ph\u1ea3i).</p> <p>Khi c\u1eadu \u1ea5y quy\u1ebft \u0111\u1ecbnh kh\u00f4i ph\u1ee5c l\u1ea1i danh s\u00e1ch, Mirko ng\u1ea1c nhi\u00ean khi nh\u1eadn ra r\u1eb1ng n\u00f3 kh\u00f4ng h\u1ec1 d\u1ec5 \u0111\u1ec3 \u0111\u1ea3o ng\u01b0\u1ee3c c\u00e1c thao t\u00e1c v\u00e0 kh\u1ed5i ph\u1ee5c l\u1ea1i tr\u1ea1ng th\u00e1i ban \u0111\u1ea7u. Mirko c\u00f4ng th\u1ec3 bi\u1ebft v\u1ecb tr\u00ed v\u1ecb tr\u00ed ban \u0111\u1ea7u c\u1ee7a n\u00fat X tr\u01b0\u1edbc m\u1ed7i thao t\u00e1c m\u00e0 ch\u1ec9 bi\u1ebft n\u00f3 khi k\u1ebft th\u00fac thao t\u00e1c.</p> <p>Mirko v\u1eabn ch\u01b0a h\u1ebft b\u00e0ng ho\u00e0ng, v\u00ec v\u1eady h\u00e3y vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh gi\u00fap c\u1eadu \u1ea5y t\u00ecm th\u1ea5y chu\u1ed7i c\u00e1c thao t\u00e1c ng\u1eafn nh\u1ea5t c\u00f3 th\u1ec3 \u0111\u1ec3 kh\u00f4i ph\u1ee5c l\u1ea1i tr\u1ea1ng th\u00e1i ban \u0111\u1ea7u c\u1ee7a danh s\u00e1ch t\u1eeb c\u00e1c thao t\u00e1c \u0111\u00e3 ghi l\u1ea1i.</p>"},{"location":"Problem/coci0607/Contest3/P6_LISTA/#input","title":"Input:","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(N\\) v\u00e0 \\(K\\) \\((2 \\le N \\le 5 \\times 10^5; 0 \\le M \\le 10^5)\\) l\u00e0 s\u1ed1 l\u01b0\u1ee3ng n\u00fat trong danh s\u00e1ch v\u00e0 s\u1ed1 l\u01b0\u1ee3ng thao t\u00e1c m\u00e0 Mirko t\u1ea1o ra. </li> <li>M\u1ed7i d\u00f2ng trong s\u1ed1 \\(M\\) d\u00f2ng ti\u1ebfp theo m\u00f4 t\u1ea3 m\u1ed9t thao t\u00e1c m\u00e0 Mirko t\u1ea1o ra g\u1ed3m lo\u1ea1i thao t\u00e1c (<code>A</code> ho\u1eb7c <code>B</code>) v\u00e0 2 s\u1ed1 nguy\u00ean \\(X\\) v\u00e0 \\(Y\\).</li> </ul>"},{"location":"Problem/coci0607/Contest3/P6_LISTA/#output","title":"Output:","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean in ra s\u1ed1 l\u01b0\u1ee3ng thao t\u00e1c \u00edt nh\u1ea5t (k\u00ed hi\u1ec7u l\u00e0 \\(K\\)) t\u00ecm \u0111\u01b0\u1ee3c.</li> <li>M\u1ed7i d\u00f2ng trong s\u1ed1 \\(K\\) d\u00f2ng ti\u1ebfp theo in ra m\u1ed9t thao t\u00e1c (v\u1edbi \u0111\u1ecbnh d\u1ea1ng gi\u1ed1ng nh\u01b0 \u0111\u1ecbnh d\u1ea1ng thao t\u00e1c tr\u00ean Input). L\u01b0u \u00fd: Chu\u1ed7i thao t\u00e1c c\u00f3 th\u1ec3 c\u00f3 nhi\u1ec1u \u0111\u00e1p \u00e1n.</li> </ul>"},{"location":"Problem/coci0607/Contest3/P6_LISTA/#scoring","title":"Scoring:","text":"<ul> <li>N\u1ebfu c\u1ea3 2 s\u1ed1 \\(K\\) v\u00e0 chu\u1ed7i c\u00e1c thao t\u00e1c l\u00e0 \u0111\u00fang, b\u1ea1n s\u1ebd \u0111\u01b0\u1ee3c t\u1ed1i \u0111a \u0111i\u1ec3m cho m\u1ed7i test. N\u1ebfu ch\u01b0\u01a1ng tr\u00ecnh c\u1ee7a b\u1ea1n in ra \u0111\u00fang s\u1ed1 \\(K\\) v\u00e0 kh\u00f4ng in ra chu\u1ed7i c\u00e1c thao t\u00e1c ho\u1eb7c in ra chu\u1ed7i c\u00e1c thao t\u00e1c b\u1ecb sai, b\u1ea1n s\u1ebd nh\u1eadn \u0111\u01b0\u1ee3c \\(60\\%\\) \u0111i\u1ec3m cho m\u1ed7i test.</li> </ul>"},{"location":"Problem/coci0607/Contest3/P6_LISTA/#sample","title":"Sample:","text":"<p>Test 1</p> Input <pre><code>2 1 \nA 2 1\n</code></pre> Output <pre><code>1 \nA 1 2\n</code></pre> <p>Test 1</p> Input <pre><code>4 3 \nB 1 2 \nA 4 3 \nB 1 4\n</code></pre> Output <pre><code>2 \nA 1 2 \nB 4 3 \n</code></pre> <p>Test 2</p> Input <pre><code>6 5 \nA 1 4 \nB 2 5 \nB 4 2 \nB 6 3 \nA 3 5\n</code></pre> Output <pre><code>3 \nA 4 5 \nB 6 5 \nA 2 3\n</code></pre>"},{"location":"Problem/coci0607/Contest4/P1_Sibice/","title":"SIBICE","text":""},{"location":"Problem/coci0607/Contest4/P1_Sibice/#sibice","title":"Sibice","text":"<p>Mirko ngh\u1ecbch di\u00eam v\u00e0 l\u00e0m r\u01a1i h\u1ebft di\u00eam ra ph\u00f2ng c\u1ee7a c\u1eadu. M\u1eb9 c\u1ee7a c\u1eadu kh\u00f4ng th\u00edch \u0111i\u1ec1u \u0111\u00f3 v\u00e0 y\u00eau c\u1ea7u c\u1eadu d\u1ecdn h\u1ebft c\u00e1c que di\u00eam v\u00e0o m\u1ed9t chi\u1ebfc h\u1ed9p. Tuy nhi\u00ean, Mirko \u0111\u1ec3 \u00fd r\u1eb1ng c\u00f3 m\u1ed9t s\u1ed1 que di\u00eam qu\u00e1 d\u00e0i so v\u1edbi c\u00e1i h\u1ed9p v\u00e0 c\u1eadu quy\u1ebft \u0111\u1ecbnh x\u1eed l\u00ed nh\u1eefng que di\u00eam \u0111\u00f3 b\u1eb1ng c\u00e1ch n\u00e9m v\u00e0o th\u00f9ng r\u00e1c c\u1ee7a nh\u00e0 h\u00e0ng x\u00f3m. </p> <p>M\u1ed9t que di\u00eam \u0111\u01b0\u1ee3c coi l\u00e0 v\u1eeba v\u1edbi chi\u1ebfc h\u1ed9p n\u1ebfu que di\u00eam \u0111\u00f3 c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c \u0111\u1eb7t ho\u00e0n to\u00e0n xu\u1ed1ng \u0111\u00e1y c\u1ee7a chi\u1ebfc h\u1ed9p.</p> <p>Cho \\(N\\) \\((1 \\leq N \\leq 50)\\) l\u00e0 s\u1ed1 que di\u00eam \u1edf tr\u00ean s\u00e0n. V\u1edbi m\u1ed7i que di\u00eam, h\u00e3y gi\u00fap Mirko x\u00e1c \u0111\u1ecbnh xem n\u00f3 c\u00f3 v\u1eeba v\u1edbi chi\u1ebfc h\u1ed9p kh\u00f4ng nh\u00e9!</p>"},{"location":"Problem/coci0607/Contest4/P1_Sibice/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a ba s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 50)\\), \\(W\\) v\u00e0 \\(H\\) \\((1 \\leq W,H \\leq 100)\\) l\u1ea7n l\u01b0\u1ee3t l\u00e0 s\u1ed1 que di\u00eam c\u1ea7n ki\u1ec3m tra v\u00e0 k\u00edch th\u01b0\u1edbc c\u1ee7a chi\u1ebfc h\u1ed9p.</li> <li>\\(N\\) d\u00f2ng ti\u1ebfp theo, d\u00f2ng th\u1ee9 \\(i\\) ch\u1ee9a s\u1ed1 nguy\u00ean \\(x_i\\) \\((1 \\leq x_i \\leq 1000)\\) l\u00e0 \u0111\u1ed9 d\u00e0i c\u1ee7a que di\u00eam th\u1ee9 \\(i\\).</li> </ul>"},{"location":"Problem/coci0607/Contest4/P1_Sibice/#output","title":"Output","text":"<ul> <li>V\u1edbi m\u1ed7i que di\u00eam, in ra tr\u00ean m\u1ed9t d\u00f2ng:<ul> <li>DA n\u1ebfu que di\u00eam \u0111\u00f3 v\u1eeba v\u1edbi chi\u1ebfc h\u1ed9p.</li> <li>NE trong tr\u01b0\u1eddng h\u1ee3p c\u00f2n l\u1ea1i.</li> </ul> </li> </ul> <p>Test 1</p> Input <pre><code>5 3 4\n3\n4\n5\n6\n7\n</code></pre> Output <pre><code>DA\nDA\nDA\nNE\nNE\n</code></pre>"},{"location":"Problem/coci0607/Contest4/P2_Skener/","title":"SKENER","text":""},{"location":"Problem/coci0607/Contest4/P2_Skener/#skener","title":"Skener","text":"<p>Sau khi gi\u1ea3i quy\u1ebft xong \u0111\u1ed1ng di\u00eam, Mirko l\u1ea1i ph\u1ea3i \u0111\u1ed1i m\u1eb7t v\u1edbi m\u1ed9t nhi\u1ec7m v\u1ee5 m\u1edbi. M\u1eb9 c\u1ee7a c\u1eadu y\u00eau c\u1ea7u c\u1eadu \u0111\u1ecdc m\u1ed9t b\u00e0i b\u00e1o v\u1ec1 nh\u1eefng c\u1eb7p \u0111\u00f4i trong showbiz. Tuy nhi\u00ean, c\u1ee1 ch\u1eef c\u1ee7a b\u00e0i b\u00e1o qu\u00e1 nh\u1ecf \u0111\u1ec3 Mirko c\u00f3 th\u1ec3 \u0111\u1ecdc. May m\u1eafn thay, c\u1eadu c\u00f3 m\u1ed9t c\u00e1i m\u00e1y scan trong t\u1ee7 \u0111\u1ec3 ph\u00f3ng to c\u1ee1 ch\u1eef l\u00ean.</p> <p>B\u00e0i b\u00e1o \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n d\u01b0\u1edbi d\u1ea1ng m\u1ed9t ma tr\u1eadn c\u00e1c k\u00ed t\u1ef1 v\u1edbi \\(R\\) h\u00e0ng v\u00e0 \\(C\\) c\u1ed9t. C\u00e1c k\u00ed t\u1ef1 bao g\u1ed3m c\u00e1c k\u00ed t\u1ef1 b\u1ea3ng ch\u1eef c\u00e1i ti\u1ebfng Anh, ch\u1eef s\u1ed1 v\u00e0 '.' (d\u1ea5u ch\u1ea5m). M\u00e1y scan c\u1ee7a Mirko nh\u1eadn v\u00e0o 2 tham s\u1ed1 \\(ZR\\) v\u00e0 \\(ZC\\). Sau \u0111\u00f3, n\u00f3 thay th\u1ebf t\u1ea5t c\u1ea3 c\u00e1c k\u00ed t\u1ef1 \u0111\u01b0\u1ee3c scan b\u1eb1ng m\u1ed9t ma tr\u1eadn \\(ZR * ZC\\), v\u1edbi t\u1ea5t c\u1ea3 ph\u1ea7n t\u1eed c\u1ee7a ma tr\u1eadn \u0111\u1ec1u l\u00e0 k\u00ed t\u1ef1 ban \u0111\u1ea7u.</p> <p>Mirko ch\u1ea1y v\u00e0o ph\u00f2ng, b\u1eadt m\u00e1y l\u00ean v\u00e0 nh\u1eadn ra ph\u1ea7n m\u1ec1m scan c\u1ee7a m\u00e1y c\u1eadu \u0111\u00e3 h\u1ecfng. V\u00ec v\u1eady, c\u1eadu \u1ea5y c\u1ea7n t\u1edbi s\u1ef1 gi\u00fap \u0111\u1ee1 c\u1ee7a b\u1ea1n!</p>"},{"location":"Problem/coci0607/Contest4/P2_Skener/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a b\u1ed1n s\u1ed1 nguy\u00ean \\(R\\), \\(C\\), \\(ZR\\), \\(ZC\\) \\((1 \\leq R, C \\leq 50, 1 \\leq ZR, ZC \\leq 5)\\)</li> </ul>"},{"location":"Problem/coci0607/Contest4/P2_Skener/#output","title":"Output","text":"<ul> <li>In ra ma tr\u1eadn bi\u1ec3u di\u1ec5n b\u00e0i b\u00e1o sau khi \u0111\u01b0\u1ee3c ph\u00f3ng to. </li> </ul> <p>Test 1</p> Input <pre><code>3 3 1 2\n.x.\nx.x\n.x. \n</code></pre> Output <pre><code>..xx..\nxx..xx\n..xx.. \n</code></pre> <p>Test 2</p> Input <pre><code>3 3 2 1\n.x.\nx.x\n.x. \n</code></pre> Output <pre><code>.x.\n.x.\nx.x\nx.x\n.x.\n.x.\n</code></pre>"},{"location":"Problem/coci0607/Contest4/P3_Prsteni/","title":"PRSTENI","text":""},{"location":"Problem/coci0607/Contest4/P3_Prsteni/#prsteni","title":"Prsteni","text":"<p>Sau m\u1ed9t bu\u1ed5i s\u00e1ng m\u1ec7t m\u1ecfi, Mirko \u0111\u00e3 \u0111i ng\u1ee7. Tuy nhi\u00ean, Stanko, em c\u1ee7a c\u1eadu l\u1ea1i v\u1eeba th\u1ee9c d\u1eady v\u00e0 b\u1eaft \u0111\u1ea7u ki\u1ebfm th\u1ee9 g\u00ec \u0111\u00f3 \u0111\u1ec3 ch\u01a1i. Stanko t\u00ecm th\u1ea5y \\(N\\) chi\u1ebfn nh\u1eabn v\u1edbi k\u00edch c\u1ee1 kh\u00e1c nhau trong garage v\u00e0 x\u1ebfp ch\u00fang sao cho m\u1ed7i chi\u1ebfc nh\u1eabn \u0111\u1ec1u ti\u1ebfp x\u00fac v\u1edbi hai chi\u1ebfc nh\u1eabn n\u1eb1m tr\u01b0\u1edbc v\u00e0 sau n\u00f3 (Tr\u1eeb chi\u1ebfc nh\u1eabn \u0111\u1ea7u ti\u00ean v\u00e0 cu\u1ed1i c\u00f9ng ch\u1ec9 ti\u1ebfp x\u00fac v\u1edbi m\u1ed9t chi\u1ebfc nh\u1eabn k\u1ebf b\u00ean). </p> <p>C\u1eadu \u1ea5y xoay chi\u1ebfc nh\u1eabn \u0111\u1ea7u ti\u00ean v\u00e0 th\u1ea5y r\u1eb1ng nh\u1eefng chi\u1ebfc nh\u1eabn kh\u00e1c c\u0169ng xoay theo v\u1edbi t\u1ed1c \u0111\u1ed9 kh\u00e1c nhau. C\u1ea3m th\u1ea5y th\u00edch th\u00fa v\u1edbi ph\u00e1t hi\u1ec7n n\u00e0y, Stanko \u0111\u00e3 \u0111\u1ebfm xem nh\u1eefng chi\u1ebfc nh\u1eabn kh\u00e1c xoay \u0111\u01b0\u1ee3c bao nhi\u00eau v\u00f2ng n\u1ebfu c\u1eadu \u1ea5y xoay chi\u1ebfc nh\u1eabn \u0111\u1ea7u ti\u00ean m\u1ed9t v\u00f2ng. Tuy nhi\u00ean, c\u1eadu \u1ea5y b\u1ecf cu\u1ed9c khi nh\u1eadn ra s\u1ed1 v\u00f2ng nh\u1eefng chi\u1ebfc nh\u1eabn xoay \u0111\u01b0\u1ee3c c\u00f3 th\u1ec3 kh\u00f4ng ph\u1ea3i s\u1ed1 nguy\u00ean v\u00e0 c\u1eadu \u1ea5y kh\u00f4ng bi\u1ebft l\u00e0m th\u1ebf n\u00e0o c\u1ea3! </p> <p>H\u00e3y vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh gi\u00fap Stanko \u0111\u1ebfm s\u1ed1 v\u00f2ng nh\u1eefng chi\u1ebfc nh\u1eabn kh\u00e1c xoay \u0111\u01b0\u1ee3c khi chi\u1ebfc nh\u1eabn \u0111\u1ea7u ti\u00ean \u0111\u01b0\u1ee3c xoay m\u1ed9t v\u00f2ng.</p>"},{"location":"Problem/coci0607/Contest4/P3_Prsteni/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 100)\\), s\u1ed1 nh\u1eabn m\u00e0 Stanko t\u00ecm \u0111\u01b0\u1ee3c.</li> <li>D\u00f2ng ti\u1ebfp theo ch\u1ee9a \\(N\\) s\u1ed1 nguy\u00ean \\(a_i\\) \\((1 \\leq a_i \\leq 1000)\\) l\u00e0 b\u00e1n k\u00ednh c\u1ee7a chi\u1ebfc nh\u1eabn th\u1ee9 \\(i\\).</li> </ul>"},{"location":"Problem/coci0607/Contest4/P3_Prsteni/#output","title":"Output","text":"<ul> <li>In ra \\(N - 1\\) d\u00f2ng, d\u00f2ng th\u1ee9 \\(i\\) l\u00e0 s\u1ed1 v\u00f2ng m\u00e0 chi\u1ebfc nh\u1eabn th\u1ee9 \\(i + 1\\) xoay \u0111\u01b0\u1ee3c khi chi\u1ebfc nh\u1eabn \u0111\u1ea7u ti\u00ean xoay 1 v\u00f2ng, bi\u1ec3u di\u1ec5n d\u01b0\u1edbi d\u1ea1ng ph\u00e2n s\u1ed1 t\u1ed1i gi\u1ea3n \\(A/B\\).</li> </ul>"},{"location":"Problem/coci0607/Contest4/P3_Prsteni/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>3\n8 4 2  \n</code></pre> Output <pre><code>2/1\n4/1   \n</code></pre> <p>Test 2</p> Input <pre><code>4\n12 3 8 4   \n</code></pre> Output <pre><code>4/1\n3/2\n3/1\n</code></pre> <p>Test 3</p> Input <pre><code>4\n300 1 1 300\n</code></pre> Output <pre><code>300/1\n300/1\n1/1 \n</code></pre>"},{"location":"Problem/coci0607/Contest4/P4_Zbrka/","title":"ZBRKA","text":""},{"location":"Problem/coci0607/Contest4/P4_Zbrka/#zbrka","title":"Zbrka","text":"<p>Cho m\u1ed9t d\u00e3y c\u00f3 \\(N\\) ph\u1ea7n t\u1eed, m\u1ed7i ph\u1ea7n t\u1eed n\u1eb1m trong kho\u1ea3ng t\u1eeb 1 \u0111\u1ebfn \\(N\\) v\u00e0 xu\u1ea5t hi\u1ec7n duy nh\u1ea5t m\u1ed9t l\u1ea7n.</p> <p>M\u1ed9t c\u1eb7p s\u1ed1 trong d\u00e3y \u0111\u01b0\u1ee3c coi l\u00e0 g\u00e2y l\u00fa n\u1ebfu s\u1ed1 n\u1eb1m \u1edf ph\u00eda tr\u01b0\u1edbc l\u1edbn h\u01a1n s\u1ed1 n\u1eb1m \u1edf ph\u00eda sau. \u0110\u1ed9 l\u00fa c\u1ee7a m\u1ed9t d\u00e3y l\u00e0 s\u1ed1 l\u01b0\u1ee3ng c\u00e1c c\u1eb7p g\u00e2y l\u00fa c\u00f3 trong d\u00e3y \u0111\u00f3. V\u00ed d\u1ee5, \u0111\u1ed9 l\u00fa c\u1ee7a d\u00e3y \\((1, 4, 3, 2)\\) l\u00e0 \\(3\\) v\u00ec trong d\u00e3y c\u00f3 3 c\u1eb7p g\u00e2y l\u00fa: \\((4, 3)\\), \\((4, 2)\\) v\u00e0 \\((3, 2)\\).</p> <p>Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh \u0111\u1ebfm s\u1ed1 l\u01b0\u1ee3ng d\u00e3y \u0111\u1ed9 d\u00e0i \\(N\\) c\u00f3 \u0111\u1ed9 l\u00fa b\u1eb1ng \\(C\\).</p>"},{"location":"Problem/coci0607/Contest4/P4_Zbrka/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(N\\) v\u00e0 \\(C\\).</li> </ul>"},{"location":"Problem/coci0607/Contest4/P4_Zbrka/#output","title":"Output","text":"<ul> <li>In ra s\u1ed1 d\u00e3y \u0111\u1ed9 d\u00e0i \\(N\\) c\u00f3 \u0111\u1ed9 l\u00fa b\u1eb1ng \\(C\\), modulo \\(1000000007\\)</li> </ul> <p>Test 1</p> Input <pre><code>10 1\n</code></pre> Output <pre><code>9\n</code></pre> <p>Test 2</p> Input <pre><code>4 3\n</code></pre> Output <pre><code>6\n</code></pre> <p>Test 3</p> Input <pre><code>9 13\n</code></pre> Output <pre><code>17957\n</code></pre>"},{"location":"Problem/coci0607/Contest4/P5_Jogurt/","title":"JOGURT","text":""},{"location":"Problem/coci0607/Contest4/P5_Jogurt/#jogurt","title":"Jogurt","text":"<p>M\u1ed9t c\u00e2y nh\u1ecb ph\u00e2n \u0111\u1ea7y \u0111\u1ee7 bao g\u1ed3m c\u00e1c \u0111\u1ec9nh \u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp theo c\u1ea5u tr\u00fac ph\u00e2n t\u1ea7ng. Trong \u0111\u00f3 c\u00f3 m\u1ed9t \u0111\u1ec9nh g\u1ed1c \u1edf t\u1ea7ng \\(0\\). \u0110\u1ec9nh g\u1ed1c c\u00f3 \\(2\\) \u0111\u1ec9nh con \u1edf t\u1ea7ng \\(1\\), m\u1ed7i \u0111\u1ec9nh con \u1edf t\u1ea7ng \\(1\\) l\u1ea1i c\u00f3 \\(2\\) \u0111\u1ec9nh con \u1edf t\u1ea7ng \\(2\\), v.v.</p> <p>T\u1ed5ng qu\u00e1t h\u01a1n, m\u1ed9t c\u00e2y nh\u1ecb ph\u00e2n \u0111\u1ea7y \u0111\u1ee7 c\u00f3 \\(N\\) t\u1ea7ng s\u1ebd c\u00f3 \\(2^{N}-1\\) \u0111\u1ec9nh, m\u1ed7i \u0111\u1ec9nh c\u00f3 \u0111\u00fang \\(2\\) \u0111\u1ec9nh con (Tr\u1eeb nh\u1eefng \u0111\u1ec9nh \u1edf t\u1ea7ng \\(N-1\\)).</p> <p>Cho m\u1ed9t c\u00e2y nh\u1ecb ph\u00e2n \u0111\u1ea7y \u0111\u1ee7 \\(N\\) t\u1ea7ng. H\u00e3y \u0111\u00e1nh s\u1ed1 m\u1ed7i \u0111\u1ec9nh c\u1ee7a c\u00e2y b\u1eb1ng m\u1ed9t s\u1ed1 trong kho\u1ea3ng t\u1eeb \\(1\\) \u0111\u1ebfn \\(2^{N}-1\\), m\u1ed7i s\u1ed1 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng duy nh\u1ea5t m\u1ed9t l\u1ea7n sao cho v\u1edbi m\u1ed7i \u0111\u1ec9nh \u1edf t\u1ea7ng \\(D\\), gi\u00e1 tr\u1ecb tuy\u1ec7t \u0111\u1ed1i c\u1ee7a hi\u1ec7u gi\u1eefa t\u1ed5ng c\u00e1c s\u1ed1 trong c\u00e2y con b\u00ean tr\u00e1i v\u00e0 t\u1ed5ng c\u00e1c s\u1ed1 trong c\u00e2y con b\u00ean ph\u1ea3i b\u1eb1ng \\(2^{D}\\). N\u1ebfu c\u00f3 nhi\u1ec1u c\u00e1ch \u0111\u00e1nh s\u1ed1 th\u1ecfa m\u00e3n, in ra m\u1ed9t c\u00e1ch b\u1ea5t k\u00ec.</p> <p>V\u00ed d\u1ee5, hi\u1ec7u gi\u1eefa t\u1ed5ng c\u00e1c s\u1ed1 trong c\u00e2y con b\u00ean tr\u00e1i v\u00e0 c\u00e2y con b\u00ean ph\u1ea3i c\u1ee7a \u0111\u1ec9nh g\u1ed1c ph\u1ea3i b\u1eb1ng \\(1\\). V\u1edbi c\u00e1c \u0111\u1ec9nh \u1edf t\u1ea7ng \\(1\\), hi\u1ec7u gi\u1eefa t\u1ed5ng c\u00e1c s\u1ed1 trong hai c\u00e2y con c\u1ee7a m\u1ed7i \u0111\u1ec9nh ph\u1ea3i b\u1eb1ng \\(2\\).</p>"},{"location":"Problem/coci0607/Contest4/P5_Jogurt/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 15)\\), s\u1ed1 t\u1ea7ng c\u1ee7a c\u00e2y.</li> </ul>"},{"location":"Problem/coci0607/Contest4/P5_Jogurt/#output","title":"Output","text":"<ul> <li>In ra \\(2^{N}-1\\) s\u1ed1 tr\u00ean m\u1ed9t d\u00f2ng theo th\u1ee9 t\u1ef1 duy\u1ec7t c\u00e2y nh\u1ecb ph\u00e2n (S\u1ed1 \u0111\u1ea7u ti\u00ean l\u00e0 c\u1ee7a \u0111\u1ec9nh g\u1ed1c, ti\u1ebfp theo l\u00e0 c\u00e2y con b\u00ean tr\u00e1i, r\u1ed3i \u0111\u1ebfn c\u00e2y con b\u00ean ph\u1ea3i) </li> </ul> <p>Test 1</p> Input <pre><code>2\n</code></pre> Output <pre><code>3 1 2\n</code></pre> <p>Test 2</p> Input <pre><code>3\n</code></pre> Output <pre><code>3 1 7 5 6 2 4\n</code></pre>"},{"location":"Problem/coci0607/Contest4/P6_Ispiti/","title":"ISPITI","text":""},{"location":"Problem/coci0607/Contest4/P6_Ispiti/#ispiti","title":"Ispiti","text":"<p>L\u00e0ng c\u1ee7a Mirko \u0111ang t\u1ed5 ch\u1ee9c m\u1ed9t cu\u1ed9c thi. Ai c\u0169ng mu\u1ed1n v\u01b0\u1ee3t qua k\u1ef3 thi v\u1edbi n\u1ed7 l\u1ef1c \u00edt nh\u1ea5t c\u00f3 th\u1ec3, nh\u01b0ng \u0111i\u1ec1u \u0111\u00f3 kh\u00f4ng h\u1ec1 d\u1ec5 d\u00e0ng. Mirko nh\u1eadn ra r\u1eb1ng c\u00e1ch t\u1ed1t nh\u1ea5t l\u00e0 t\u00ecm m\u1ed9t ng\u01b0\u1eddi gi\u1ecfi h\u01a1n m\u00ecnh v\u00e0 h\u1ecdc h\u1ecfi t\u1eeb h\u1ecd. M\u1ecdi ng\u01b0\u1eddi trong l\u00e0ng c\u0169ng l\u00e0m theo, v\u00e0 b\u00e2y gi\u1edd ai c\u0169ng \u0111ang t\u00ecm m\u1ed9t ng\u01b0\u1eddi \u0111\u1ec3 h\u1ecdc c\u00f9ng.</p> <p>Ch\u00fang ta c\u00f3 th\u1ec3 \u0111\u00e1nh gi\u00e1 xem m\u1ed9t ng\u01b0\u1eddi \u0111\u00e3 chu\u1ea9n b\u1ecb cho cu\u1ed9c thi t\u1ed1t \u0111\u1ebfn m\u1ee9c n\u00e0o b\u1eb1ng \\(2\\) s\u1ed1 nguy\u00ean \\(A\\) v\u00e0 \\(B\\). S\u1ed1 \\(A\\) th\u1ec3 hi\u1ec7n \u0111\u1ed9 hi\u1ec3u bi\u1ebft c\u1ee7a ng\u01b0\u1eddi \u0111\u00f3 v\u1ec1 m\u00f4n h\u1ecdc v\u00e0 s\u1ed1 \\(B\\) th\u1ec3 hi\u1ec7n l\u01b0\u1ee3ng ki\u1ebfn th\u1ee9c c\u1ee7a ng\u01b0\u1eddi \u0111\u00f3.</p> <p>L\u00e0 ng\u01b0\u1eddi \u0111\u1ee9ng \u0111\u1ea7u c\u1ee7a ng\u00f4i l\u00e0ng, Mirko ra lu\u1eadt r\u1eb1ng m\u1ed9t ng\u01b0\u1eddi ch\u1ec9 \u0111\u01b0\u1ee3c nh\u1edd m\u1ed9t ng\u01b0\u1eddi kh\u00e1c gi\u00fap \u0111\u1ee1 ch\u1ec9 khi ng\u01b0\u1eddi \u0111\u00f3 c\u00f3 c\u1ea3 hai ch\u1ec9 s\u1ed1 l\u1edbn h\u01a1n ho\u1eb7c b\u1eb1ng ch\u1ec9 s\u1ed1 c\u1ee7a m\u00ecnh (kh\u00f4ng ai mu\u1ed1n h\u1ecfi m\u1ed9t ng\u01b0\u1eddi hi\u1ec3u bi\u1ebft k\u00e9m h\u01a1n ho\u1eb7c c\u00f3 \u00edt ki\u1ebfn th\u1ee9c h\u01a1n m\u00ecnh).</p> <p>Ngo\u00e0i ra, m\u1ecdi ng\u01b0\u1eddi s\u1ebd \u01b0u ti\u00ean ch\u1ecdn ng\u01b0\u1eddi c\u00f3 s\u1ef1 ch\u00eanh l\u1ec7ch v\u1ec1 l\u01b0\u1ee3ng ki\u1ebfn th\u1ee9c \\(B\\) nh\u1ecf nh\u1ea5t so v\u1edbi m\u00ecnh, \u0111\u1ec3 tr\u00e1nh l\u00e0m phi\u1ec1n nh\u1eefng ng\u01b0\u1eddi qu\u00e1 gi\u1ecfi. N\u1ebfu c\u00f3 nhi\u1ec1u l\u1ef1a ch\u1ecdn th\u1ecfa m\u00e3n \u0111i\u1ec1u ki\u1ec7n n\u00e0y, h\u1ecd s\u1ebd ti\u1ebfp t\u1ee5c \u01b0u ti\u00ean ng\u01b0\u1eddi c\u00f3 s\u1ef1 ch\u00eanh l\u1ec7ch v\u1ec1 m\u1ee9c \u0111\u1ed9 hi\u1ec3u bi\u1ebft \\(A\\) nh\u1ecf nh\u1ea5t.</p> <p>G\u1ea7n \u0111\u00e2y, l\u00e0ng c\u1ee7a Mirko ng\u00e0y c\u00e0ng tr\u1edf n\u00ean n\u1ed5i ti\u1ebfng v\u00e0 nhi\u1ec1u h\u1ecdc sinh m\u1edbi li\u00ean t\u1ee5c chuy\u1ec3n \u0111\u1ebfn ngay tr\u01b0\u1edbc k\u1ef3 thi. V\u00ec kh\u00f4ng quen v\u1edbi c\u00e1c quy t\u1eafc c\u1ee7a Mirko, h\u1ecd b\u1ed1i r\u1ed1i v\u00e0 kh\u00f4ng bi\u1ebft ph\u1ea3i nh\u1edd ai gi\u00fap \u0111\u1ee1. V\u00ec v\u1eady, h\u1ecd quy\u1ebft \u0111\u1ecbnh nh\u1edd m\u1ed9t l\u1eadp tr\u00ecnh vi\u00ean \u1edf l\u00e0ng k\u1ebf b\u00ean gi\u00fap \u0111\u1ee1!</p>"},{"location":"Problem/coci0607/Contest4/P6_Ispiti/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 200000)\\), s\u1ed1 l\u01b0\u1ee3ng truy v\u1ea5n.</li> <li>N d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng c\u00f3 th\u1ec3 ch\u1ee9a:<ul> <li>\"D A B\": M\u1ed9t h\u1ecdc sinh c\u00f3 \u0111\u1ed9 hi\u1ec3u bi\u1ebft l\u00e0 \\(A\\) v\u00e0 l\u01b0\u1ee3ng ki\u1ebfn th\u1ee9c l\u00e0 \\(B\\) chuy\u1ec3n \u0111\u1ebfn l\u00e0ng \\((1 \\leq A, B \\leq 2.10^{9})\\). D\u1eef li\u1ec7u v\u00e0o \u0111\u1ea3m b\u1ea3o ch\u1ec9 s\u1ed1 \\(A\\) v\u00e0 \\(B\\) c\u1ee7a m\u1ed7i h\u1ecdc sinh l\u00e0 duy nh\u1ea5t.</li> <li>\"P i\": H\u1ecdc sinh th\u1ee9 \\(i\\) chuy\u1ec3n v\u00e0o mu\u1ed1n t\u00ecm m\u1ed9t ng\u01b0\u1eddi \u0111\u1ec3 h\u1ecdc c\u00f9ng.</li> </ul> </li> </ul>"},{"location":"Problem/coci0607/Contest4/P6_Ispiti/#output","title":"Output","text":"<ul> <li>V\u1edbi m\u1ed7i truy v\u1ea5n d\u1ea1ng \"P i\", in ra h\u1ecdc sinh ph\u00f9 h\u1ee3p \u0111\u1ec3 h\u1ecdc c\u00f9ng h\u1ecdc sinh th\u1ee9 \\(i\\). C\u00e1c h\u1ecdc sinh \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 theo th\u1ee9 t\u1ef1 di chuy\u1ec3n t\u1edbi l\u00e0ng (b\u1eaft \u0111\u1ea7u t\u1eeb 1). N\u1ebfu kh\u00f4ng c\u00f3 h\u1ecdc sinh n\u00e0o ph\u00f9 h\u1ee3p, in ra \"NE\"</li> </ul> <p>Test 1</p> Input <pre><code>6\nD 3 1\nD 2 2\nD 1 3\nP 1\nP 2\nP 3 \n</code></pre> Output <pre><code>NE\nNE\nNE\n</code></pre> <p>Test 2</p> Input <pre><code>6\nD 8 8\nD 2 4\nD 5 6\nP 2\nD 6 2\nP 4 \n</code></pre> Output <pre><code>3\n1\n</code></pre> <p>Test 3</p> Input <pre><code>7\nD 5 2\nD 5 3\nP 1\nD 7 1\nD 8 7\nP 3\nP 2\n</code></pre> Output <pre><code>2\n4\n4\n</code></pre>"},{"location":"Problem/coci0607/Contest5/P1_Trik/","title":"TRIK","text":""},{"location":"Problem/coci0607/Contest5/P1_Trik/#trik","title":"Trik","text":"<p>Ghen t\u1ecb v\u1edbi v\u1ecb tr\u00ed tr\u01b0\u1edfng l\u00e0ng c\u1ee7a Mirko, Borko ch\u1ea1y th\u1eb3ng v\u00e0o l\u1ec1u v\u00e0 c\u1ed1 g\u1eafng ch\u1ee9ng t\u1ecf Mirko kh\u00f4ng ph\u00f9 h\u1ee3p \u0111\u1ec3 l\u00e0m m\u1ed9t ch\u1ec9 huy.</p> <p>Borko \u0111\u1eb7t \\(3\\) chi\u1ebfc c\u1ed1c nh\u1ef1a l\u00ean b\u00e0n, \u0111\u01b0\u1ee3c \u0111\u1eb7t th\u1eb3ng h\u00e0ng v\u00e0 \u00fap xu\u1ed1ng, \u0111\u1ed3ng th\u1eddi m\u1ed9t qu\u1ea3 b\u00f3ng nh\u1ecf \u0111\u01b0\u1ee3c \u0111\u1eb7t b\u00ean d\u01b0\u1edbi chi\u1ebfc c\u1ed1c tr\u00e1i nh\u1ea5t. Sau \u0111\u00f3 anh ta \u0111\u1ed5i ch\u1ed7 hai chi\u1ebfc c\u1ed1c b\u1eb1ng \\(1\\) trong \\(3\\) c\u00e1ch (nh\u01b0 h\u00ecnh v\u1ebd d\u01b0\u1edbi) m\u1ed9t s\u1ed1 l\u1ea7n tu\u1ef3 \u00fd. Mirko ph\u1ea3i ch\u1ec9 ra xem qu\u1ea3 b\u00f3ng n\u1eb1m d\u01b0\u1edbi chi\u1ebfc c\u1ed1c n\u00e0o sau khi Borko ho\u00e0n th\u00e0nh vi\u1ec7c \u0111\u1ed5i ch\u1ed7.</p> <p></p> <p>Mirko th\u00f4ng minh khoanh tay m\u1ec9m c\u01b0\u1eddi trong khi Borko th\u00ec kh\u00f3 kh\u0103n h\u01a1n \u0111\u1ec3 di chuy\u1ec3n c\u00e1c chi\u1ebfc c\u1ed1c c\u00e0ng nhanh c\u00e0ng t\u1ed1t. Nh\u01b0ng anh ta \u0111\u00e2u ng\u1edd \u0111\u01b0\u1ee3c r\u1eb1ng m\u1ed9t l\u1eadp tr\u00ecnh vi\u00ean \u1edf \u0111\u1eb1ng sau \u0111\u00e3 ghi l\u1ea1i t\u1ea5t c\u1ea3 c\u00e1c b\u01b0\u1edbc di chuy\u1ec3n v\u00e0 s\u1eed d\u1ee5ng m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh xem qu\u1ea3 b\u00f3ng \u1edf \u0111\u00e2u. H\u00e3y vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh \u0111\u00f3.</p>"},{"location":"Problem/coci0607/Contest5/P1_Trik/#input","title":"Input","text":"<p>G\u1ed3m m\u1ed9t d\u00f2ng ch\u1ee9a x\u00e2u k\u00ed t\u1ef1 \u0111\u1ed9 d\u00e0i kh\u00f4ng qu\u00e1 \\(50\\) th\u1ec3 hi\u1ec7n c\u00e1c b\u01b0\u1edbc di chuy\u1ec3n c\u1ee7a Borko ('A', 'B' ho\u1eb7c 'C' nh\u01b0 h\u00ecnh v\u1ebd b\u00ean tr\u00ean v\u00e0 kh\u00f4ng ch\u1ee9a d\u1ea5u nh\u00e1y \u0111\u01a1n).</p>"},{"location":"Problem/coci0607/Contest5/P1_Trik/#output","title":"Output","text":"<p>In ra ch\u1ec9 s\u1ed1 c\u1ee7a chi\u1ebfc c\u1ed1c ch\u1ee9a qu\u1ea3 b\u00f3ng trong \u0111\u00f3, theo quy t\u1eafc: \\(1\\) l\u00e0 chi\u1ebfc c\u1ed1c b\u00ean tr\u00e1i, \\(2\\) l\u00e0 chi\u1ebfc c\u1ed1c \u1edf gi\u1eefa v\u00e0 \\(3\\) l\u00e0 chi\u1ebfc c\u1ed1c b\u00ean ph\u1ea3i.</p> <p>Test 1</p> Input <pre><code>AB\n</code></pre> Output <pre><code>3\n</code></pre> <p>Test 2</p> Input <pre><code>CBABCACCC\n</code></pre> Output <pre><code>1\n</code></pre>"},{"location":"Problem/coci0607/Contest5/P2_Natrij/","title":"NATRIJ","text":""},{"location":"Problem/coci0607/Contest5/P2_Natrij/#natrij","title":"Natrij","text":"<p>Sau th\u1ea5t b\u1ea1i trong vi\u1ec7c \u0111o\u1ea1t l\u1ea5y quy\u1ec1n l\u1ef1c trong y\u00ean b\u00ecnh, Borko \u0111\u00e3 quy\u1ebft \u0111\u1ecbnh s\u1ebd ph\u00e1 hu\u1ef7 \u0111\u00ecnh l\u00e0ng c\u1ee7a Mirko - \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng cho anh ta b\u1eb1ng nh\u1eefng t\u1ea5m b\u00eca c\u1ee9ng b\u1edfi v\u00f4 s\u1ed1 b\u1ea7y t\u00f4i trung th\u00e0nh.</p> <p>\u0110\u1ec3 l\u00e0m \u0111\u01b0\u1ee3c \u0111i\u1ec1u \u0111\u00f3, Borko s\u1ebd s\u1eed d\u1ee5ng m\u1ed9t con chip vi x\u1eed l\u00ed c\u1ee7a Mirko (\u0111\u01b0\u1ee3c tr\u1ed9m b\u1edfi m\u1ed9t ng\u01b0\u1eddi b\u1ea1n c\u1ee7a Borko - Zvonko), m\u1ed9t x\u00f4 n\u01b0\u1edbc v\u00e0 m\u1ed9t t\u00fai Natri (Sodium). Anh ta s\u1ebd th\u1eddi gian h\u1eb9n c\u1ee7a \"v\u1ee5 n\u1ed5\" v\u00e0o trong con chip vi x\u1eed l\u00ed, v\u00e0 th\u1ea3 Natri (Sodium) v\u00e0o trong n\u01b0\u1edbc sau khi b\u1ed9 \u0111\u1ebfm v\u1ec1 \\(0\\).</p> <p>Borko bi\u1ebft th\u1eddi \u0111i\u1ec3m hi\u1ec7n t\u1ea1i l\u00e0 m\u1ea5y gi\u1edd v\u00e0 th\u1eddi \u0111i\u1ec3m anh ta mu\u1ed1n v\u1ee5 n\u1ed5 x\u1ea3y ra. L\u00e0 m\u1ed9t ng\u01b0\u1eddi kh\u00f4ng y\u00eau th\u00edch s\u1ed1 h\u1ecdc cho l\u1eafm v\u00e0 t\u00ean Zvonko th\u00ec \u0111ang \u0111i ch\u01a1i v\u1edbi m\u1ea5y h\u00f2n bi \u1edf s\u00e2n sau r\u1ed3i n\u00ean c\u0169ng ch\u1eb3ng gi\u00fap \u00edch \u0111\u01b0\u1ee3c g\u00ec, do \u0111\u00f3, anh ta mu\u1ed1n nh\u1edd b\u1ea1n vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh \u0111\u1ec3 t\u00ednh to\u00e1n th\u1eddi gian \u0111\u1ebfn l\u00fac x\u1ea3y ra v\u1ee5 n\u1ed5 (l\u00e0 th\u1eddi gian \u0111\u1ebfm ng\u01b0\u1ee3c m\u00e0 Borko s\u1ebd nh\u1eadp v\u00e0o con chip) bi\u1ebft r\u1eb1ng th\u1eddi gian t\u1ed1i thi\u1ec3u l\u00e0 \\(1\\) gi\u00e2y v\u00e0 t\u1ed1i \u0111a l\u00e0 \\(24\\) gi\u1edd.</p>"},{"location":"Problem/coci0607/Contest5/P2_Natrij/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean bao g\u1ed3m th\u1eddi gian hi\u1ec7n t\u1ea1i vi\u1ebft d\u01b0\u1edbi \u0111\u1ecbnh d\u1ea1ng \\(hh:mm:ss\\) (gi\u1edd, ph\u00fat, gi\u00e2y). \u0110\u1ea3m b\u1ea3o r\u1eb1ng gi\u1edd s\u1ebd n\u1eb1m trong kho\u1ea3ng t\u1eeb \\(0\\) \u0111\u1ebfn \\(23\\) v\u00e0 ph\u00fat s\u1ebd n\u1eb1m trong kho\u1ea3ng t\u1eeb \\(0\\) \u0111\u1ebfn \\(59\\).</li> <li>D\u00f2ng th\u1ee9 hai l\u00e0 th\u1eddi gian x\u1ea3y ra v\u1ee5 n\u1ed5, c\u00f3 c\u00f9ng v\u1edbi \u0111\u1ecbnh d\u1ea1ng nh\u01b0 tr\u00ean.</li> </ul>"},{"location":"Problem/coci0607/Contest5/P2_Natrij/#output","title":"Output","text":"<ul> <li>G\u1ed3m m\u1ed9t d\u00f2ng l\u00e0 th\u1eddi gian \u0111\u1ebfm ng\u01b0\u1ee3c \u0111\u1ebfn khi v\u1ee5 n\u1ed5 x\u1ea3y ra, c\u00f3 c\u00f9ng v\u1edbi \u0111\u1ecbnh d\u1ea1ng nh\u01b0 \u0111\u1ea7u v\u00e0o.</li> </ul> <p>Test 1</p> Input <pre><code>20:00:00\n04:00:00\n</code></pre> Output <pre><code>08:00:00\n</code></pre> <p>Test 2</p> Input <pre><code>12:34:56\n14:36:22 \n</code></pre> Output <pre><code>02:01:26\n</code></pre>"},{"location":"Problem/coci0607/Contest5/P3_Tenis/","title":"TENIS","text":""},{"location":"Problem/coci0607/Contest5/P3_Tenis/#tenis","title":"Tenis","text":"<p>Sau khi k\u1ebf ho\u1ea1ch B \u0111\u1ec3 chi\u1ebfm quy\u1ec1n ki\u1ec3m so\u00e1t ng\u00f4i l\u00e0ng c\u1ee7a Borko th\u1ea5t b\u1ea1i (b\u1edfi v\u00ec anh em c\u1ee7a Mirko - Stanko - u\u1ed1ng s\u1ea1ch ch\u1ed7 n\u01b0\u1edbc), Zvonko tr\u1ebb tu\u1ed5i quy\u1ebft \u0111\u1ecbnh th\u01b0 gi\u00e3n m\u1ed9t s\u1ed1 m\u00f4n th\u1ec3 thao. V\u00ec v\u1eady anh ta \u0111\u00e3 d\u1ef1 tr\u1eef n\u01b0\u1edbc \u00e9p c\u00e0 r\u1ed1t v\u00e0 \u0111\u1eadu ph\u1ed9ng, v\u00e0 lao th\u1eb3ng l\u00ean chi\u1ebfc gh\u1ebf \u0111i v\u0103ng, s\u1eb5n s\u00e0ng \u0111\u1ec3 theo d\u00f5i tr\u1eadn tennis ti\u1ebfp theo tr\u00ean m\u00e0n \u1ea3nh nh\u1ecf.</p> <p>Trong l\u00fac hai tuy\u1ec3n th\u1ee7 \u0111ang kh\u1edfi \u0111\u1ed9ng, th\u00f4ng s\u1ed1 c\u1ee7a c\u1ea3 hai hi\u1ec7n l\u00ean tr\u00ean m\u00e0n h\u00ecnh. Zvonko \u0111\u1ec3 \u00fd th\u1ea5y l\u1ecbch s\u1eed \u0111\u1ea5u c\u1ee7a ng\u01b0\u1eddi ch\u01a1i c\u00f3 m\u1ed9t s\u1ed1 k\u1ebft qu\u1ea3 kh\u00f4ng h\u1ee3p l\u1ec7. Anh ta n\u1ea3y ra m\u1ed9t \u00fd t\u01b0\u1edfng si\u00eau c\u1ea5p v\u0169 tr\u1ee5 b\u1eb1ng c\u00e1ch nh\u1edd b\u1ea1n vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh m\u00e1y t\u00ednh c\u00f3 kh\u1ea3 n\u0103ng ki\u1ec3m tra \u0111\u1ed9 h\u1ee3p l\u1ec7 c\u1ee7a k\u1ebft qu\u1ea3, sau \u0111\u00f3 anh ta s\u1ebd b\u00e1n ch\u00fang cho nh\u1eefng ng\u01b0\u1eddi qu\u1ea3n l\u00fd s\u1ed5 s\u00e1ch v\u00e0 t\u1eadn h\u01b0\u1edfng cu\u1ed9c s\u1ed1ng vinh hoa.</p> <p>M\u1ed9t tr\u1eadn tennis bao g\u1ed3m nhi\u1ec1u set, m\u1ed7i set g\u1ed3m nhi\u1ec1u v\u00e1n. Lu\u1eadt ch\u01a1i nh\u01b0 sau:</p> <ul> <li>M\u1ed9t ng\u01b0\u1eddi ch\u01a1i th\u1eafng m\u1ed9t set n\u1ebfu anh ta th\u1eb1ng h\u01a1n \\(6\\) v\u00e1n v\u00e0 d\u1eabn tr\u01b0\u1edbc \u0111\u1ed1i th\u1ee7 \u00edt nh\u1ea5t \\(2\\) v\u00e1n.</li> <li>Th\u00eam n\u1eefa, n\u1ebfu k\u1ebft qu\u1ea3 l\u00e0 \\(6:6\\) trong set \u0111\u1ea7u ho\u1eb7c set th\u1ee9 hai, m\u1ed9t v\u00e1n quy\u1ebft \u0111\u1ecbnh s\u1ebd \u0111\u01b0\u1ee3c \u0111\u01b0a ra x\u00e1c \u0111\u1ecbnh xem ai l\u00e0 ng\u01b0\u1eddi chi\u1ebfn th\u1eafng c\u1ee7a c\u1ea3 set (v\u00e1n tie-break).</li> <li>Tr\u1eadn \u0111\u1ea5u k\u1ebft th\u00fac khi m\u1ed9t trong hai tuy\u1ec3n th\u1ee7 th\u1eafng \u0111\u01b0\u1ee3c \\(2\\) set, v\u00e0 ng\u01b0\u1eddi \u0111\u00f3 s\u1ebd l\u00e0 ng\u01b0\u1eddi chi\u1ebfn th\u1eafng.</li> </ul> <p>K\u1ebft qu\u1ea3 c\u1ee7a m\u1ed9t tr\u1eadn \u0111\u1ea5u \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 h\u1ee3p l\u1ec7 n\u1ebfu nh\u01b0 tr\u1eadn \u0111\u1ea5u \u0111\u00f3 \u0111\u01b0\u1ee3c ch\u01a1i theo lu\u1eadt v\u00e0 k\u1ebft th\u00fac \u0111\u00fang lu\u1eadt.</p> <p>H\u01a1n n\u1eefa, n\u1ebfu c\u00f3 m\u1ed9t tuy\u1ec3n th\u1ee7 t\u00ean l\u00e0 Roger Federer (\u0111\u01b0\u1ee3c ch\u1ec9 \u0111\u1ecbnh l\u00e0 \"federer\" trong \u0111\u1ea7u v\u00e0o) th\u00ec n\u1ebfu k\u1ebft qu\u1ea3 c\u00f3 m\u1ed9t set m\u00e0 anh ta thua th\u00ec l\u00e0 kh\u00f4ng h\u1ee3p l\u1ec7. M\u1ed7i tr\u1eadn \u0111\u1ea5u s\u1ebd c\u00f3 kho\u1ea3ng t\u1eeb \\(1\\) \u0111\u1ebfn \\(5\\) set.</p> <p>H\u00e3y vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh \u0111\u1ec3 ki\u1ec3m tra \u0111\u1ed9 h\u1ee3p l\u1ec7  c\u1ee7a t\u1ea5t c\u1ea3 tr\u1eadn \u0111\u1ea5u gi\u1eefa hai tuy\u1ec3n th\u1ee7</p>"},{"location":"Problem/coci0607/Contest5/P3_Tenis/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean g\u1ed3m t\u00ean c\u1ee7a hai tuy\u1ec3n th\u1ee7 \u0111\u01b0\u1ee3c ng\u0103n c\u00e1ch b\u1edfi d\u1ea5u kho\u1ea3ng tr\u1eafng. C\u1ea3 hai c\u00e1i t\u00ean \u0111\u1ec1u ch\u1ee9a c\u00e1c ch\u1eef c\u00e1i in th\u01b0\u1eddng v\u00e0 c\u00f3 \u0111\u1ed9 d\u00e0i t\u1ed1i \u0111a \\(20\\). C\u00e1c c\u00e1i t\u00ean s\u1ebd kh\u00e1c nhau.</li> <li>D\u00f2ng th\u1ee9 hai g\u1ed3m m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\le N \\le 50)\\), m\u00f4 t\u1ea3 s\u1ed1 tr\u1eadn m\u00e0 hai tuy\u1ec3n th\u1ee7 \u0111\u00e3 thi \u0111\u1ea5u.</li> <li>M\u1ed7i d\u00f2ng trong \\(N\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a k\u1ebft qu\u1ea3 c\u1ee7a m\u1ed9t tr\u1eadn \u0111\u1ea5u, bao g\u1ed3m k\u1ebft qu\u1ea3 c\u1ee7a m\u1ed9t s\u1ed1 set \u0111\u01b0\u1ee3c ng\u0103n c\u00e1ch b\u1edfi d\u1ea5u kho\u1ea3ng tr\u1eafng. M\u1ed7i tr\u1eadn \u0111\u1ea5u s\u1ebd g\u1ed3m \\(1\\) \u0111\u1ebfn \\(5\\) set.</li> <li>M\u1ed9t set \u0111\u01b0\u1ee3c cho d\u01b0\u1edbi \u0111\u1ecbnh d\u1ea1ng \"A:B\", v\u1edbi A v\u00e0 B l\u00e0 s\u1ed1 v\u00e1n th\u1eafng b\u1edfi m\u1ed7i ng\u01b0\u1eddi ch\u01a1i, c\u00e1c con s\u1ed1 n\u00e0y s\u1ebd n\u1eb1m trong kho\u1ea3ng \\([0, 99]\\).</li> </ul>"},{"location":"Problem/coci0607/Contest5/P3_Tenis/#output","title":"Output","text":"<p>V\u1edbi m\u1ed7i tr\u1eadn \u0111\u1ea5u theo th\u1ee9 t\u1ef1 \u0111\u01b0\u1ee3c nh\u1eadp v\u00e0o, in ra \"da\" n\u1ebfu k\u1ebft qu\u1ea3 h\u1ee3p l\u1ec7, ho\u1eb7c \"ne\" n\u1ebfu kh\u00f4ng h\u1ee3p l\u1ec7.</p> <p>Test 1</p> Input <pre><code>sampras agassi\n6\n6:2 6:4\n3:6 7:5 2:6\n6:5 7:4\n7:6 7:6\n6:2 3:6\n6:2 1:6 6:8 \n</code></pre> Output <pre><code>da\nda\nne\nda\nne\nda\n</code></pre> <p>Test 2</p> Input <pre><code>federer roddick\n1\n2:6 4:6 \n</code></pre> Output <pre><code>ne\n</code></pre>"},{"location":"Problem/coci0607/Contest5/P4_Liga/","title":"LIGA","text":""},{"location":"Problem/coci0607/Contest5/P4_Liga/#liga","title":"Liga","text":"<p>Trong khi c\u00e1c c\u1ea7u th\u1ee7 \u0111ang ngh\u1ec9 gi\u1eefa c\u00e1c set, \u0111\u00e0i truy\u1ec1n h\u00ecnh ph\u00e1t nh\u1eefng qu\u1ea3ng c\u00e1o t\u1ebb nh\u1ea1t. Zvonko b\u1eadt t\u00ednh n\u0103ng teletext tr\u00ean TV c\u1ee7a m\u00ecnh v\u00e0 xem qua c\u00e1c t\u1ef7 s\u1ed1 v\u00e0 b\u1ea3ng x\u1ebfp h\u1ea1ng m\u1edbi nh\u1ea5t c\u1ee7a gi\u1ea3i b\u00f3ng \u0111\u00e1. L\u00e0 m\u1ed9t ng\u01b0\u1eddi nh\u1ea1y b\u00e9n, anh ta n\u1ea3y ra \u00fd t\u01b0\u1edfng cho m\u1ed9t tr\u00f2 ch\u01a1i to\u00e1n h\u1ecdc ho\u00e0n to\u00e0n m\u1edbi.</p> <p>B\u1ea3ng x\u1ebfp h\u1ea1ng bao g\u1ed3m n\u0103m th\u00f4ng s\u1ed1 cho m\u1ed7i \u0111\u1ed9i: T\u1ed5ng s\u1ed1 tr\u1eadn thi \u0111\u1ea5u, s\u1ed1 tr\u1eadn th\u1eafng, ho\u00e0 v\u00e0 thua, v\u00e0 s\u1ed1 \u0111i\u1ec3m ki\u1ebfm \u0111\u01b0\u1ee3c. M\u1ed9t \u0111\u1ed9i c\u00f3 th\u1ec3 ki\u1ebfm \u0111\u01b0\u1ee3c \\(3\\) \u0111i\u1ec3m cho m\u1ed7i tr\u1eadn th\u1eafng v\u00e0 \\(1\\) \u0111i\u1ec3m cho m\u1ed7i tr\u1eadn ho\u00e0.</p> <p>Zvonko nh\u1eadn th\u1ea5y r\u1eb1ng gi\u00e1 tr\u1ecb \u1edf m\u1ed9t v\u00e0i th\u00f4ng s\u1ed1 c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c x\u00e1c \u0111\u1ecbnh d\u1ef1a tr\u00ean c\u00e1c th\u00f4ng s\u1ed1 kh\u00e1c.</p> <p>Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh v\u1edbi \u0111\u1ea7u v\u00e0o l\u00e0 m\u1ed9t b\u1ea3ng x\u1ebfp h\u1ea1ng trong \u0111\u00f3 m\u1ed9t s\u1ed1 gi\u00e1 tr\u1ecb \u1edf m\u1ed9t v\u00e0i th\u00f4ng s\u1ed1 \u0111ang kh\u00f4ng \u0111\u01b0\u1ee3c x\u00e1c \u0111\u1ecbnh v\u00e0 x\u00e1c \u0111\u1ecbnh c\u00e1c gi\u00e1 tr\u1ecb \u0111\u00f3.</p> <p>D\u1eef li\u1ec7u t\u1eeb c\u00e1c \u0111\u1ed9i kh\u00e1c nhau kh\u00f4ng li\u00ean quan t\u1edbi nhau. V\u00ed d\u1ee5, c\u00f3 th\u1ec3 c\u00f3 b\u1ea3ng x\u1ebfp h\u1ea1ng m\u00e0 t\u1ea5t c\u1ea3 c\u00e1c \u0111\u1ed9i \u0111\u1ec1u th\u1eafng tr\u1eadn \u0111\u1ea5u m\u00e0 h\u1ecd thi \u0111\u1ea5u (m\u1eb7c d\u00f9 kh\u00f4ng th\u1ec3 x\u1ea3y ra \u1edf c\u00e1c gi\u1ea3i \u0111\u1ea5u th\u1eadt).</p> <p>M\u1ed7i \u0111\u1ed9i thi \u0111\u1ea5u t\u1ed1i \u0111a \\(100\\) tr\u1eadn (d\u00f9 cho \u0111\u00f3 c\u00f3 th\u1ec3 l\u00e0 m\u1ed9t trong c\u00e1c th\u00f4ng s\u1ed1 b\u1ecb m\u1ea5t).</p>"},{"location":"Problem/coci0607/Contest5/P4_Liga/#input","title":"Input","text":"<ul> <li> <p>D\u00f2ng \u0111\u1ea7u ti\u00ean g\u1ed3m s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\le N \\le 1000)\\) cho bi\u1ebft s\u1ed1 l\u01b0\u1ee3ng \u0111\u1ed9i thi \u0111\u1ea5u trong gi\u1ea3i b\u00f3ng.</p> </li> <li> <p>M\u1ed7i d\u00f2ng trong \\(N\\) d\u00f2ng ti\u1ebfp theo g\u1ed3m \\(5\\) th\u00f4ng s\u1ed1 cho m\u1ed7i team \u0111\u01b0\u1ee3c ph\u00e2n bi\u1ec7t b\u1eb1ng c\u00e1c d\u1ea5u kho\u1ea3ng tr\u1eafng (th\u1ee9 t\u1ef1 c\u00e1c th\u00f4ng s\u1ed1 nh\u01b0 tr\u00ean \u0111\u1ec1 b\u00e0i). M\u1ed7i th\u00f4ng s\u1ed1 l\u00e0 m\u1ed9t s\u1ed1 nguy\u00ean kh\u00f4ng \u00e2m ho\u1eb7c l\u00e0 k\u00ed t\u1ef1 '?' n\u1ebfu nh\u01b0 gi\u00e1 tr\u1ecb \u1edf \u0111\u00f3 kh\u00f4ng \u0111\u01b0\u1ee3c cho bi\u1ebft.</p> </li> <li> <p>M\u1ed7i \u0111\u1ea7u v\u00e0o \u0111\u1ec1u tho\u1ea3 m\u00e3n c\u00e1c \u0111i\u1ec1u ki\u1ec7n \u0111\u01b0a ra v\u00e0 t\u1ed3n t\u1ea1i m\u1ed9t \u0111\u00e1p \u00e1n duy nh\u1ea5t \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh c\u00e1c gi\u00e1 tr\u1ecb trong c\u00e1c th\u00f4ng s\u1ed1 b\u1ecb m\u1ea5t.</p> </li> </ul>"},{"location":"Problem/coci0607/Contest5/P4_Liga/#output","title":"Output","text":"<ul> <li>In ra b\u1ea3ng v\u1edbi \u0111\u1ea7y \u0111\u1ee7 c\u00e1c th\u00f4ng s\u1ed1.</li> </ul>"},{"location":"Problem/coci0607/Contest5/P4_Liga/#note","title":"Note","text":"<ul> <li>V\u1edbi m\u1ed7i tr\u01b0\u1eddng h\u1ee3p, ch\u01b0\u01a1ng tr\u00ecnh c\u1ee7a b\u1ea1n s\u1ebd \u0111\u01b0\u1ee3c nh\u1eadn s\u1ed1 \u0111i\u1ec3m t\u01b0\u01a1ng \u1ee9ng v\u1edbi s\u1ed1 \u0111\u1ed9i b\u00f3ng \u0111\u01b0\u1ee3c x\u00e1c \u0111\u1ecbnh c\u00e1c th\u00f4ng s\u1ed1 m\u1ed9t c\u00e1ch ch\u00ednh x\u00e1c, l\u00e0m tr\u00f2n xu\u1ed1ng. N\u1ebfu ch\u01b0\u01a1ng tr\u00ecnh ch\u1ea1y qu\u00e1 th\u1eddi gian ho\u1eb7c c\u00f3 l\u1ed7i kh\u00e1c x\u1ea3y ra, \u0111i\u1ec3m c\u1ee7a b\u1ea1n \u1ee9ng v\u1edbi tr\u01b0\u1eddng h\u1ee3p \u0111\u00f3 s\u1ebd l\u00e0 \\(0\\).</li> </ul> <p>Test 1</p> Input <pre><code>5\n27 21 3 3 66\n27 18 6 3 ?\n? 15 5 7 50\n? 14 7 5 ?\n? 14 ? 8 47 \n</code></pre> Output <pre><code>27 21 3 3 66\n27 18 6 3 60\n27 15 5 7 50\n26 14 7 5 49\n27 14 5 8 47 \n</code></pre>"},{"location":"Problem/coci0607/Contest5/P5_Ivana/","title":"IVANA","text":""},{"location":"Problem/coci0607/Contest5/P5_Ivana/#ivana","title":"Ivana","text":"<p>M\u1eb7c d\u00f9 c\u00f4 n\u00e0ng Ivana \u0111\u00e3 th\u1ea5y Zvonko tr\u1ed9m con chip vi x\u1eed l\u00ed c\u1ee7a Mirko trong b\u00e0i \"Natrij\", c\u00f4 n\u00e0ng - d\u01b0\u1edbi c\u01b0\u01a1ng v\u1ecb c\u1ee7a m\u1ed9t ng\u01b0\u1eddi em g\u00e1i - kh\u00f4ng h\u1ec1 n\u00f3i cho Mirko bi\u1ebft \u0111i\u1ec1u \u0111\u00e3 x\u1ea3y ra \u0111\u01a1n gi\u1ea3n v\u00ec n\u00e0ng ta \u0111\u00e3 th\u00edch Zvonko m\u1ea5t tiu. C\u00f4 n\u00e0ng \u0111\u00e3 y\u00eau c\u1ea7u m\u1ed9t bu\u1ed5i \u0111i xem phim v\u1edbi ch\u00e0ng ta nh\u01b0 l\u00e0 \u0111i\u1ec1u ki\u1ec7n \u0111\u1ec3 nh\u1eafm m\u1eaft l\u00e0m ng\u01a1 nh\u1eefng \u0111i\u1ec1u m\u00e0 c\u00f4 \u1ea3 \u0111\u00e3 ch\u1ee9ng ki\u1ebfn.</p> <p>Zvonko v\u1ed1n kh\u00f4ng quan t\u00e2m \u0111\u1ebfn n\u1eef nhi b\u1edfi n\u1ebfu y\u00eau \u0111\u01b0\u01a1ng th\u00ec l\u1ea5y \u0111\u00e2u ra th\u1eddi gian \u0111\u1ec3 t\u00f4i v\u0103n luy\u1ec7n to\u00e1n? V\u00ec v\u1eady, ch\u00e0ng ta \u0111\u00e3 \u0111\u01b0a ra m\u1ed9t l\u1eddi \u0111\u1ec1 ngh\u1ecb r\u1eb1ng c\u1ea3 hai s\u1ebd c\u00f9ng ch\u01a1i m\u1ed9t tr\u00f2 ch\u01a1i v\u00e0 n\u1ebfu Ivana th\u1eafng, h\u1ecd s\u1ebd \u0111i xem phim c\u00f9ng nhau. N\u00e0ng ta th\u1ea5y v\u1eady li\u1ec1n g\u1eadt \u0111\u1ea7u h\u1edbn h\u1edf do c\u00f3 t\u00e0i nh\u1ea3y d\u00e2y v\u00e0 l\u00e2u l\u00e2u l\u1ea5n s\u00e2n sang t\u00fap c\u1ea7u v\u1edbi hai ng\u01b0\u1eddi anh trai.</p> <p>Zvonko \u0111\u1eb7t \\(N\\) s\u1ed1 nguy\u00ean d\u01b0\u01a1ng th\u00e0nh m\u1ed9t v\u00f2ng tr\u00f2n v\u00e0 b\u1eaft \u0111\u1ea7u gi\u1ea3i th\u00edch c\u00e1c lu\u1eadt l\u1ec7:</p> <ul> <li>Ng\u01b0\u1eddi ch\u01a1i tr\u01b0\u1edbc s\u1ebd b\u1eaft \u0111\u1ea7u ch\u1ecdn m\u1ed9t s\u1ed1 b\u1ea5t k\u00ec.</li> <li>Ng\u01b0\u1eddi ch\u01a1i c\u00f2n l\u1ea1i sau \u0111\u00f3 s\u1ebd ch\u1ecdn m\u1ed9t trong hai s\u1ed1 \u1edf b\u00ean c\u1ea1nh s\u1ed1 \u0111\u01b0\u1ee3c ng\u01b0\u1eddi \u0111i tr\u01b0\u1edbc ch\u1ecdn.</li> <li>Sau \u0111\u00f3 l\u1ea7n l\u01b0\u1ee3t t\u1eebng ng\u01b0\u1eddi ch\u1ecdn m\u1ed9t s\u1ed1 m\u00e0 k\u1ec1 c\u1ea1nh v\u1edbi b\u1ea5t k\u00ec s\u1ed1 n\u00e0o \u0111\u00e3 \u0111\u01b0\u1ee3c ch\u1ecdn tr\u01b0\u1edbc \u0111\u00f3 cho \u0111\u1ebfn khi h\u1ebft s\u1ed1 tr\u00ean s\u00e0n. Ai l\u1ea5y \u0111\u01b0\u1ee3c nhi\u1ec1u s\u1ed1 l\u1ebb h\u01a1n th\u00ec th\u1eafng.</li> </ul> <p>Zvonko ch\u01a1i t\u1ed1i \u01b0u h\u1ebft m\u1ee9c c\u00f3 th\u1ec3; ch\u00e0ng ta lu\u00f4n t\u00ecm ki\u1ebfm m\u1ed9t chi\u1ebfn thu\u1eadt m\u00e0 d\u1eabn t\u1edbi ho\u1eb7c l\u00e0 th\u1eafng ho\u1eb7c l\u00e0 ho\u00e0. Zvonko kh\u00f4ng bi\u1ebft Ivana ch\u01a1i hay \u0111\u1ebfn \u0111\u00e2u. D\u00f9 v\u1eady v\u1edbi c\u00e1i t\u00f4i l\u1edbn, anh ch\u00e0ng c\u1ee7a ch\u00fang ta \u0111\u00e3 nh\u01b0\u1eddng cho Ivana \u0111i tr\u01b0\u1edbc.</p> <p>Nh\u01b0ng Ivana ch\u1ec9 quan t\u00e2m t\u1edbi vi\u1ec7c \u0111\u01b0\u1ee3c ng\u1ed3i k\u1ebf b\u00ean ch\u00e0ng Zvonko tr\u01b0\u1edbc m\u00e0n h\u00ecnh l\u1edbn n\u00ean c\u00f4 n\u00e0ng t\u00ecm ki\u1ebfm s\u1ef1 gi\u00fap \u0111\u1ee1 khi ch\u01a1i.</p> <p>Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh ki\u1ec3m tra xem bao nhi\u00eau n\u01b0\u1edbc \u0111i \u0111\u1ea7u ti\u00ean kh\u00e1c nhau Ivana c\u00f3 th\u1ec3 \u0111i \u0111\u1ec3 c\u00f4 n\u00e0ng c\u00f3 c\u01a1 h\u1ed9i chi\u1ebfn th\u1eafng v\u1ec1 sau.</p>"},{"location":"Problem/coci0607/Contest5/P5_Ivana/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean g\u1ed3m s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\le N \\le 100)\\) th\u1ec3 hi\u1ec7n s\u1ed1 l\u01b0\u1ee3ng s\u1ed1 trong v\u00f2ng tr\u00f2n.</li> <li>D\u00f2ng th\u1ee9 hai g\u1ed3m \\(N\\) s\u1ed1 nguy\u00ean \u0111\u01b0\u1ee3c ng\u0103n c\u00e1ch nhau b\u1edfi m\u1ed9t d\u1ea5u kho\u1ea3ng tr\u1eafng. M\u1ed7i s\u1ed1 n\u1eb1m trong \u0111o\u1ea1n \\([1, 1000]\\). Kh\u00f4ng c\u00f3 hai s\u1ed1 n\u00e0o gi\u1ed1ng nhau.</li> </ul>"},{"location":"Problem/coci0607/Contest5/P5_Ivana/#output","title":"Output","text":"<p>S\u1ed1 l\u01b0\u1ee3ng n\u01b0\u1edbc \u0111i \u0111\u1ea7u ti\u00ean kh\u00e1c nhau sao cho Ivana c\u00f3 th\u1ec3 chi\u1ebfn th\u1eafng v\u1ec1 sau.</p> <p>Test 1</p> Input <pre><code>3\n3 1 5 \n</code></pre> Output <pre><code>3\n</code></pre> Note <pre><code>D\u00f9 ch\u1ecdn s\u1ed1 n\u00e0o \u0111\u1ec3 \u0111i tr\u01b0\u1edbc th\u00ec Ivana \u0111\u1ec1u k\u1ebft th\u00fac v\u00e1n v\u1edbi $2$ s\u1ed1 l\u1ebb trong tay v\u00e0 d\u00e0nh chi\u1ebfn th\u1eafng d\u1ec5 d\u00e0ng.\n</code></pre> <p>Test 2</p> Input <pre><code>4\n1 2 3 4 \n</code></pre> Output <pre><code>2\n</code></pre> Note <pre><code>N\u1ebfu Ivana ch\u1ecdn m\u1ed9t trong hai s\u1ed1 ch\u1eb5n, Zvonko ch\u1eafc ch\u1eafn s\u1ebd ch\u1ecdn s\u1ed1 l\u1ebb v\u00e0 d\u1eabn \u0111\u1ebfn k\u1ebft qu\u1ea3 ho\u00e0. Ng\u01b0\u1ee3c l\u1ea1i, n\u1ebfu c\u00f4 n\u00e0ng ch\u1ecdn m\u1ed9t trong hai s\u1ed1 l\u1ebb, Zvonko s\u1ebd ph\u1ea3i ch\u1ecdn m\u1ed9t trong hai s\u1ed1 ch\u1eb5n, n\u00e0ng ta ch\u1ec9 c\u1ea7n ch\u1ecdn s\u1ed1 l\u1ebb c\u00f2n l\u1ea1i v\u00e0 chi\u1ebfn th\u1eafng.\n</code></pre> <p>Test 3</p> Input <pre><code>8\n4 10 5 2 9 8 1 7\n</code></pre> Output <pre><code>5\n</code></pre>"},{"location":"Problem/coci0607/Contest5/P6_Dvaput/","title":"DVAPUT","text":""},{"location":"Problem/coci0607/Contest5/P6_Dvaput/#dvaput","title":"Dvaput","text":"<p>Ivana th\u1eafng v\u1ee5 c\u00e1 c\u01b0\u1ee3c r\u1ed3i (Zvonko c\u0169ng kh\u00f4ng ng\u1edd \u0111\u01b0\u1ee3c vi\u1ec7c n\u00e0y v\u00e0 nghi ng\u1edd r\u1eb1ng \u0111\u00e3 c\u00f3 s\u1ef1 can thi\u1ec7p t\u1eeb b\u00ean ngo\u00e0i) v\u00e0 b\u00e2y gi\u1edd Zvonko \u0111ang \u0111\u1ee3i c\u00f4 n\u00e0ng \u1edf r\u1ea1p ph\u00edm. Trong khi ch\u00e0ng ta \u0111\u1ee3i, ch\u00e0ng ta quan s\u00e1t th\u00f4ng b\u00e1o \u1edf m\u00e0n h\u00ecnh b\u00ean tr\u00ean.</p> <p>Ivana \u0111\u00e3 \u0111\u1ebfn mu\u1ed9n, Zvonko \u0111\u00e3 nh\u00ecn m\u00e0n h\u00ecnh \u0111\u01b0\u1ee3c m\u1ed9t l\u00fac v\u00e0 \u0111\u1ec3 \u00fd r\u1eb1ng m\u1ed9t s\u1ed1 th\u00f4ng b\u00e1o \u0111\u01b0\u1ee3c hi\u1ec7n l\u00ean nhi\u1ec1u h\u01a1n m\u1ed9t l\u1ea7n. Theo ph\u1ea3n x\u1ea1, anh ch\u00e0ng n\u00e0y ghi l\u1ea1i t\u1ea5t c\u1ea3 c\u00e1c th\u00f4ng b\u00e1o l\u00ean m\u1ed9t m\u1ea3nh gi\u1ea5y. Anh ta mu\u1ed1n bi\u1ebft \u0111\u1ed9 d\u00e0i c\u1ee7a x\u00e2u con d\u00e0i nh\u1ea5t xu\u1ea5t hi\u1ec7n \u00edt nh\u1ea5t hai l\u1ea7n l\u00e0 bao nhi\u00eau (xu\u1ea5t hi\u1ec7n \u1edf hai v\u1ecb tr\u00ed kh\u00e1c nhau tr\u00ean t\u1edd gi\u1ea5y).</p>"},{"location":"Problem/coci0607/Contest5/P6_Dvaput/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean l\u00e0 s\u1ed1 \\(L\\) \\((1 \\le L \\le 2 \\times 10^5)\\), \u0111\u1ed9 d\u00e0i c\u1ee7a x\u00e2u m\u00e0 Zvonko \u0111\u00e3 ghi l\u1ea1i.</li> <li>D\u00f2ng th\u1ee9 hai g\u1ed3m \\(L\\) k\u00ed t\u1ef1 in th\u01b0\u1eddng trong b\u1ea3ng ch\u1eef c\u00e1i ti\u1ebfng Anh.</li> </ul>"},{"location":"Problem/coci0607/Contest5/P6_Dvaput/#output","title":"Output","text":"<ul> <li>\u0110\u1ed9 d\u00e0i c\u1ee7a x\u00e2u con d\u00e0i nh\u1ea5t xu\u1ea5t hi\u1ec7n \u00edt nh\u1ea5t hai l\u1ea7n. N\u1ebfu kh\u00f4ng c\u00f3 th\u00ec in ra \\(0\\).</li> </ul> <p>Test 1</p> Input <pre><code>11\nsabcabcfabc\n</code></pre> Output <pre><code>3\n</code></pre> <p>Test 2</p> Input <pre><code>18\ntrutrutiktiktappop \n</code></pre> Output <pre><code>4\n</code></pre> <p>Test 3</p> Input <pre><code>6\nabcdef\n</code></pre> Output <pre><code>0\n</code></pre>"},{"location":"Problem/coci0607/Contest6/P1_PRASE/","title":"PRASE","text":""},{"location":"Problem/coci0607/Contest6/P1_PRASE/#prase","title":"PRASE","text":"<p>C\u00f3 \\(N\\) \u0111\u1ee9a tr\u1ebb \u0111ang \u0103n tr\u01b0a c\u00f9ng nhau. L\u1ea7n l\u01b0\u1ee3t t\u1eebng \u0111\u1ee9a tr\u1ebb l\u1ea5y th\u1ee9c \u0103n t\u1eeb b\u00e0n.  </p> <p>Tuy nhi\u00ean, m\u1ed9t s\u1ed1 \u0111\u1ee9a ch\u01b0a \u0111\u01b0\u1ee3c d\u1ea1y c\u00e1ch c\u01b0 x\u1eed \u0111\u00fang m\u1ef1c n\u00ean ch\u00fang tranh gi\u00e0nh th\u1ee9c \u0103n m\u00e0 kh\u00f4ng nh\u01b0\u1eddng cho ng\u01b0\u1eddi kh\u00e1c. N\u1ebfu t\u1ea1i m\u1ed9t th\u1eddi \u0111i\u1ec3m n\u00e0o \u0111\u00f3, m\u1ed9t \u0111\u1ee9a tr\u1ebb l\u1ea5y m\u1ed9t mi\u1ebfng th\u1ee9c \u0103n v\u00e0 t\u1ed5ng s\u1ed1 th\u1ee9c \u0103n m\u00e0 \u0111\u1ee9a tr\u1ebb \u0111\u00f3 \u0111\u00e3 l\u1ea5y (kh\u00f4ng t\u00ednh mi\u1ebfng m\u1edbi n\u00e0y) nhi\u1ec1u h\u01a1n t\u1ed5ng s\u1ed1 th\u1ee9c \u0103n m\u00e0 t\u1ea5t c\u1ea3 c\u00e1c \u0111\u1ee9a tr\u1ebb kh\u00e1c \u0111\u00e3 l\u1ea5y c\u1ed9ng l\u1ea1i, th\u00ec m\u1eb9 c\u1ee7a ch\u00fang s\u1ebd nh\u1eafc nh\u1edf \u0111\u1ee9a tr\u1ebb \u0111\u00f3 ph\u1ea3i c\u01b0 x\u1eed \u0111\u00fang m\u1ef1c.  </p> <p>B\u1ea1n s\u1ebd \u0111\u01b0\u1ee3c cung c\u1ea5p th\u1ee9 t\u1ef1 m\u00e0 c\u00e1c \u0111\u1ee9a tr\u1ebb l\u1ea5y th\u1ee9c \u0103n. H\u00e3y vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh t\u00ednh xem m\u1eb9 \u0111\u00e3 ph\u1ea3i nh\u1eafc nh\u1edf bao nhi\u00eau l\u1ea7n.  </p>"},{"location":"Problem/coci0607/Contest6/P1_PRASE/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 100)\\), l\u00e0 s\u1ed1 mi\u1ebfng th\u1ee9c \u0103n m\u00e0 c\u00e1c \u0111\u1ee9a tr\u1ebb \u0111\u00e3 l\u1ea5y.  </li> <li>\\(N\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a t\u00ean c\u1ee7a m\u1ed9t \u0111\u1ee9a tr\u1ebb \u0111\u00e3 l\u1ea5y m\u1ed9t mi\u1ebfng th\u1ee9c \u0103n. T\u00ean c\u1ee7a m\u1ed7i \u0111\u1ee9a tr\u1ebb l\u00e0 m\u1ed9t x\u00e2u g\u1ed3m t\u1ed1i \u0111a 20 ch\u1eef c\u00e1i th\u01b0\u1eddng c\u1ee7a b\u1ea3ng ch\u1eef c\u00e1i ti\u1ebfng Anh.  </li> </ul>"},{"location":"Problem/coci0607/Contest6/P1_PRASE/#output","title":"Output","text":"<ul> <li>In ra s\u1ed1 l\u1ea7n m\u1eb9 \u0111\u00e3 nh\u1eafc nh\u1edf b\u1ecdn tr\u1ebb tr\u00ean m\u1ed9t d\u00f2ng duy nh\u1ea5t.  </li> </ul>"},{"location":"Problem/coci0607/Contest6/P1_PRASE/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>4  \nmirko  \nstanko  \nstanko  \nstanko  \n</code></pre> Output <pre><code>1  \n</code></pre> <p>Test 2</p> Input <pre><code>17  \na  \nb  \nb  \na  \na  \na  \nc  \na  \nb  \nb  \nc  \nb  \nb  \nb  \nb  \nb  \nb  \n</code></pre> Output <pre><code>4  \n</code></pre>"},{"location":"Problem/coci0607/Contest6/P2_MAGIJA/","title":"MAGIJA","text":""},{"location":"Problem/coci0607/Contest6/P2_MAGIJA/#magija","title":"MAGIJA","text":"<p>Nh\u00e0 \u1ea3o thu\u1eadt n\u1ed5i ti\u1ebfng Al'Dimi Kartimi c\u1ea7n m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh \u0111\u1ec3 gi\u00fap \u00f4ng thi\u1ebft k\u1ebf m\u1eb7t sau c\u1ee7a nh\u1eefng l\u00e1 b\u00e0i.  </p> <p>Al'Dimi tr\u01b0\u1edbc ti\u00ean v\u1ebd g\u00f3c ph\u1ea7n t\u01b0 tr\u00e1i tr\u00ean c\u1ee7a l\u00e1 b\u00e0i, sau \u0111\u00f3 v\u1ebd g\u00f3c ph\u1ea3i tr\u00ean \u0111\u1ed1i x\u1ee9ng v\u1edbi g\u00f3c tr\u00e1i tr\u00ean qua tr\u1ee5c d\u1ecdc c\u1ee7a l\u00e1 b\u00e0i. Ti\u1ebfp theo, v\u1ebd n\u1eeda d\u01b0\u1edbi l\u00e1 b\u00e0i \u0111\u1ed1i x\u1ee9ng v\u1edbi n\u1eeda tr\u00ean qua tr\u1ee5c ngang c\u1ee7a l\u00e1 b\u00e0i.</p> <p>Sau khi v\u1ebd, Al'Dimi c\u00f2n th\u00eam m\u1ed9t l\u1ed7i nh\u1ecf (thay \u0111\u1ed5i m\u1ed9t \u00f4 vu\u00f4ng) \u0111\u1ec3 gi\u00fap \u00f4ng nh\u1eadn di\u1ec7n t\u1eebng l\u00e1 b\u00e0i (\u0111\u1ec3 gian l\u1eadn).  </p> <p>H\u00e3y gi\u00fap Al'Dimi vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh v\u1ebd to\u00e0n b\u1ed9 m\u1eb7t sau c\u1ee7a l\u00e1 b\u00e0i, d\u1ef1a tr\u00ean h\u00ecnh v\u1ebd g\u00f3c ph\u1ea7n t\u01b0 tr\u00e1i tr\u00ean v\u00e0 v\u1ecb tr\u00ed l\u1ed7i.  </p> <p>\u0110\u00e2y l\u00e0 ba v\u00ed d\u1ee5 v\u1ec1 l\u00e1 b\u00e0i c\u1ee7a Al'Dimi.</p> <p><pre><code>###.##.###\n##########\n.########.\n..######..\n####.#####\n##########\n..######..\n.########.\n##########\n###.##.###\n</code></pre> \u00d4 l\u1ed7i l\u00e0 \u00f4 \\((5,5)\\).</p> <p><pre><code>#.#..#.#\n#.####.#\n#.####.#\n........\n.#.##.#.\n.#.##.#.\n........\n#.####.#\n#.####.#\n#.#.##.#\n</code></pre> \u00d4 l\u1ed7i l\u00e0 \u00f4 \\((10,5)\\).</p> <p><pre><code>.#.##.##\n#.#..#.#\n........\n..#..#..\n..#..#..\n........\n#.#..#.#\n##.##.##\n</code></pre> \u00d4 l\u1ed7i l\u00e0 \u00f4 \\((1,1)\\).</p>"},{"location":"Problem/coci0607/Contest6/P2_MAGIJA/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(R\\) v\u00e0 \\(C\\) \\((1 \\leq R, C \\leq 50)\\), l\u00e0 s\u1ed1 h\u00e0ng v\u00e0 s\u1ed1 c\u1ed9t c\u1ee7a g\u00f3c ph\u1ea7n t\u01b0 tr\u00e1i tr\u00ean c\u1ee7a l\u00e1 b\u00e0i.  </li> <li>\\(R\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a \\(C\\) k\u00fd t\u1ef1 <code>.</code> ho\u1eb7c <code>#</code>, bi\u1ec3u di\u1ec5n h\u00ecnh v\u1ebd c\u1ee7a g\u00f3c ph\u1ea7n t\u01b0 tr\u00e1i tr\u00ean.</li> <li>D\u00f2ng ti\u1ebfp theo ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(A\\) v\u00e0 \\(B\\) \\((1 \\leq A \\leq 2R, 1 \\leq B \\leq 2C)\\), l\u00e0 t\u1ecda \u0111\u1ed9 c\u1ee7a l\u1ed7i.  </li> </ul>"},{"location":"Problem/coci0607/Contest6/P2_MAGIJA/#output","title":"Output","text":"<ul> <li>In ra \\(2R\\) d\u00f2ng, m\u1ed7i d\u00f2ng ch\u1ee9a \\(2C\\) k\u00fd t\u1ef1, bi\u1ec3u di\u1ec5n to\u00e0n b\u1ed9 m\u1eb7t sau c\u1ee7a l\u00e1 b\u00e0i.  </li> </ul>"},{"location":"Problem/coci0607/Contest6/P2_MAGIJA/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>2 2\n#.\n.#\n3 3\n</code></pre> Output <pre><code>#..#\n.##.\n.#..\n#..#\n</code></pre> <p>Test 2</p> Input <pre><code>3 3\n###\n###\n###\n1 4 \n</code></pre> Output <pre><code>###.##\n######\n######\n######\n######\n######  \n</code></pre> <p>Test 3</p> Input <pre><code>5 4\n#.#.\n#.##\n#.##\n....\n.#.#\n10 5\n</code></pre> Output <pre><code>#.#..#.#\n#.####.#\n#.####.#\n........\n.#.##.#.\n.#.##.#.\n........\n#.####.#\n#.####.#\n#.#.##.#\n</code></pre>"},{"location":"Problem/coci0607/Contest6/P3_MARATON/","title":"MARATON","text":""},{"location":"Problem/coci0607/Contest6/P3_MARATON/#maraton","title":"MARATON","text":"<p>Albert, Barbara, Casper, Dinko, v\u00e0 Eustahije \u0111ang ch\u01a1i m\u1ed9t tr\u1eadn c\u1edd caro marathon tr\u00ean b\u00e0n c\u1edd k\u00edch th\u01b0\u1edbc \\(N \\times N\\).  </p> <p>Ban \u0111\u1ea7u, t\u1ea5t c\u1ea3 c\u00e1c \u00f4 tr\u00ean b\u00e0n c\u1edd \u0111\u1ec1u tr\u1ed1ng, v\u00e0 c\u00e1c ng\u01b0\u1eddi ch\u01a1i l\u1ea7n l\u01b0\u1ee3t vi\u1ebft ch\u1eef c\u00e1i \u0111\u1ea7u ti\u00ean trong t\u00ean c\u1ee7a h\u1ecd v\u00e0o b\u1ea5t k\u1ef3 \u00f4 tr\u1ed1ng n\u00e0o (do \u0111\u00e2y l\u00e0 nh\u1eefng ng\u01b0\u1eddi ch\u01a1i \u0111\u1eb3ng c\u1ea5p, kh\u00f4ng ai c\u00f3 c\u00f9ng ch\u1eef c\u00e1i \u0111\u1ea7u ti\u00ean).  </p> <p>Tr\u00f2 ch\u01a1i k\u1ebft th\u00fac khi m\u1ed9t ng\u01b0\u1eddi ch\u01a1i \u0111\u1eb7t \u0111\u01b0\u1ee3c 3 ch\u1eef c\u00e1i li\u00ean ti\u1ebfp theo h\u00e0ng, c\u1ed9t ho\u1eb7c \u0111\u01b0\u1eddng ch\u00e9o. Ng\u01b0\u1eddi \u0111\u00f3 s\u1ebd chi\u1ebfn th\u1eafng.  </p> <p>Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh ki\u1ec3m tra tr\u1ea1ng th\u00e1i c\u1ee7a b\u00e0n c\u1edd v\u00e0 x\u00e1c \u0111\u1ecbnh ai l\u00e0 ng\u01b0\u1eddi chi\u1ebfn th\u1eafng, ho\u1eb7c th\u00f4ng b\u00e1o n\u1ebfu tr\u00f2 ch\u01a1i v\u1eabn \u0111ang di\u1ec5n ra.  </p>"},{"location":"Problem/coci0607/Contest6/P3_MARATON/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 30)\\), k\u00edch th\u01b0\u1edbc c\u1ee7a b\u00e0n c\u1edd.  </li> <li>\\(N\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a \\(N\\) k\u00fd t\u1ef1 l\u00e0 ch\u1eef c\u00e1i in hoa ho\u1eb7c <code>.</code> (\u00f4 tr\u1ed1ng).  </li> <li>D\u1eef li\u1ec7u \u0111\u1ea3m b\u1ea3o c\u00f3 t\u1ed1i \u0111a m\u1ed9t ng\u01b0\u1eddi chi\u1ebfn th\u1eafng.  </li> </ul>"},{"location":"Problem/coci0607/Contest6/P3_MARATON/#output","title":"Output","text":"<ul> <li>N\u1ebfu c\u00f3 ng\u01b0\u1eddi th\u1eafng, in ra ch\u1eef c\u00e1i \u0111\u1ea7u ti\u00ean trong t\u00ean c\u1ee7a h\u1ecd.  </li> <li>N\u1ebfu kh\u00f4ng c\u00f3 ng\u01b0\u1eddi chi\u1ebfn th\u1eafng, in ra <code>ongoing</code>.  </li> </ul>"},{"location":"Problem/coci0607/Contest6/P3_MARATON/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>3  \nXOC  \nXOC  \nX..  \n</code></pre> Output <pre><code>X  \n</code></pre> <p>Test 2</p> Input <pre><code>4  \n....  \n..A.  \nAAB.  \n.B.B  \n</code></pre> Output <pre><code>ongoing  \n</code></pre> <p>Test 3</p> Input <pre><code>3  \nABB  \nAAA  \nBBA  \n</code></pre> Output <pre><code>A  \n</code></pre>"},{"location":"Problem/coci0607/Contest6/P4_KAMEN/","title":"KAMEN","text":""},{"location":"Problem/coci0607/Contest6/P4_KAMEN/#kamen","title":"KAMEN","text":"<p>Domeniko \u0111\u00e3 ph\u1ea3i n\u1eb1m tr\u00ean gi\u01b0\u1eddng g\u1ea7n hai tu\u1ea7n nay v\u00ec b\u1ea1n c\u1ee7a c\u1eadu, Nedjeljko, v\u00f4 t\u00ecnh th\u1ea3 m\u1ed9t h\u00f2n \u0111\u00e1 l\u1edbn l\u00ean ch\u00e2n tr\u00e1i c\u1ee7a c\u1eadu. V\u00ec \u0111\u00e3 gi\u1ea3i h\u1ebft c\u00e1c b\u00e0i t\u1eeb k\u1ef3 thi qu\u1ed1c gia c\u1ee7a Croatia t\u1eeb n\u0103m 1998, Domeniko ph\u1ea3i t\u00ecm m\u1ed9t c\u00e1ch m\u1edbi \u0111\u1ec3 gi\u1ebft th\u1eddi gian.</p> <p>Tr\u00f2 ch\u01a1i m\u1edbi c\u1ee7a Domeniko \u0111\u01b0\u1ee3c ch\u01a1i tr\u00ean m\u1ed9t b\u1ea3ng c\u00f3 k\u00edch th\u01b0\u1edbc \\( R \\times C \\). Ban \u0111\u1ea7u, m\u1ed7i \u00f4 vu\u00f4ng tr\u00ean b\u1ea3ng ho\u1eb7c l\u00e0 tr\u1ed1ng ho\u1eb7c b\u1ecb ch\u1eb7n b\u1edfi m\u1ed9t b\u1ee9c t\u01b0\u1eddng. Domeniko n\u00e9m m\u1ed9t h\u00f2n \u0111\u00e1 v\u00e0o b\u1ea3ng b\u1eb1ng c\u00e1ch \u0111\u1eb7t n\u00f3 v\u00e0o h\u00e0ng tr\u00ean c\u00f9ng c\u1ee7a m\u1ed9t c\u1ed9t r\u1ed3i \u0111\u1ec3 tr\u1ecdng l\u1ef1c l\u00e0m ph\u1ea7n c\u00f2n l\u1ea1i.</p> <p>Quy t\u1eafc c\u1ee7a tr\u1ecdng l\u1ef1c nh\u01b0 sau:</p> <ul> <li>N\u1ebfu \u00f4 vu\u00f4ng ngay b\u00ean d\u01b0\u1edbi h\u00f2n \u0111\u00e1 l\u00e0 m\u1ed9t b\u1ee9c t\u01b0\u1eddng ho\u1eb7c h\u00f2n \u0111\u00e1 \u0111ang \u1edf h\u00e0ng d\u01b0\u1edbi c\u00f9ng c\u1ee7a c\u1ed9t, h\u00f2n \u0111\u00e1 s\u1ebd \u0111\u1ee9ng y\u00ean.</li> <li>N\u1ebfu \u00f4 vu\u00f4ng b\u00ean d\u01b0\u1edbi h\u00f2n \u0111\u00e1 tr\u1ed1ng, h\u00f2n \u0111\u00e1 s\u1ebd r\u01a1i xu\u1ed1ng \u00f4 \u0111\u00f3.</li> <li>N\u1ebfu \u00f4 b\u00ean d\u01b0\u1edbi h\u00f2n \u0111\u00e1 ch\u1ee9a m\u1ed9t h\u00f2n \u0111\u00e1 kh\u00e1c, h\u00f2n \u0111\u00e1 \u0111ang r\u01a1i c\u00f3 th\u1ec3 tr\u01b0\u1ee3t sang hai b\u00ean:<ul> <li>N\u1ebfu c\u00e1c \u00f4 b\u00ean tr\u00e1i v\u00e0 ch\u00e9o xu\u1ed1ng b\u00ean tr\u00e1i \u0111\u1ec1u tr\u1ed1ng, h\u00f2n \u0111\u00e1 tr\u01b0\u1ee3t sang tr\u00e1i m\u1ed9t \u00f4.</li> <li>N\u1ebfu kh\u00f4ng th\u1ec3 tr\u01b0\u1ee3t sang tr\u00e1i nh\u01b0ng c\u00e1c \u00f4 b\u00ean ph\u1ea3i v\u00e0 ch\u00e9o xu\u1ed1ng b\u00ean ph\u1ea3i \u0111\u1ec1u tr\u1ed1ng, h\u00f2n \u0111\u00e1 tr\u01b0\u1ee3t sang ph\u1ea3i m\u1ed9t \u00f4.</li> <li>N\u1ebfu c\u1ea3 hai b\u00ean \u0111\u1ec1u b\u1ecb ch\u1eb7n, h\u00f2n \u0111\u00e1 s\u1ebd \u0111\u1ee9ng y\u00ean v\u00e0 kh\u00f4ng di chuy\u1ec3n n\u1eefa.</li> </ul> </li> </ul> <p>Domeniko s\u1ebd kh\u00f4ng bao gi\u1edd n\u00e9m m\u1ed9t h\u00f2n \u0111\u00e1 m\u1edbi cho \u0111\u1ebfn khi h\u00f2n \u0111\u00e1 tr\u01b0\u1edbc \u0111\u00f3 \u0111\u00e3 ho\u00e0n to\u00e0n d\u1eebng l\u1ea1i.</p> <p>H\u00e3y vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh m\u00f4 ph\u1ecfng tr\u1ea1ng th\u00e1i c\u1ee7a b\u1ea3ng sau khi Domeniko n\u00e9m h\u1ebft t\u1ea5t c\u1ea3 c\u00e1c h\u00f2n \u0111\u00e1, bi\u1ebft r\u1eb1ng b\u1ea1n \u0111\u01b0\u1ee3c cung c\u1ea5p th\u1ee9 t\u1ef1 c\u00e1c c\u1ed9t m\u00e0 Domeniko \u0111\u00e3 n\u00e9m \u0111\u00e1 v\u00e0o.</p> <p>L\u01b0u \u00fd: Domeniko s\u1ebd kh\u00f4ng bao gi\u1edd n\u00e9m m\u1ed9t h\u00f2n \u0111\u00e1 v\u00e0o m\u1ed9t c\u1ed9t c\u00f3 \u00f4 tr\u00ean c\u00f9ng kh\u00f4ng tr\u1ed1ng.</p>"},{"location":"Problem/coci0607/Contest6/P4_KAMEN/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\( R \\) v\u00e0 \\( C \\) \\((1 \\leq R \\leq 30000, 1 \\leq C \\leq 30)\\) \u2014 k\u00edch th\u01b0\u1edbc c\u1ee7a b\u1ea3ng.</li> <li>M\u1ed7i d\u00f2ng trong s\u1ed1 \\( R \\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a \\( C \\) k\u00fd t\u1ef1, th\u1ec3 hi\u1ec7n tr\u1ea1ng th\u00e1i ban \u0111\u1ea7u c\u1ee7a b\u1ea3ng. K\u00fd t\u1ef1 <code>.</code> bi\u1ec3u th\u1ecb m\u1ed9t \u00f4 tr\u1ed1ng, k\u00fd t\u1ef1 <code>X</code> bi\u1ec3u th\u1ecb m\u1ed9t b\u1ee9c t\u01b0\u1eddng.</li> <li>D\u00f2ng ti\u1ebfp theo ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\( N \\) \\((1 \\leq N \\leq 100000)\\) \u2014 s\u1ed1 l\u01b0\u1ee3ng h\u00f2n \u0111\u00e1 Domeniko n\u00e9m.</li> <li>\\( N \\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean t\u1eeb 1 \u0111\u1ebfn \\( C \\), th\u1ec3 hi\u1ec7n c\u1ed9t Domeniko \u0111\u00e3 n\u00e9m \u0111\u00e1 v\u00e0o (c\u1ed9t b\u00ean tr\u00e1i nh\u1ea5t l\u00e0 c\u1ed9t 1).</li> </ul> <p>Ghi ch\u00fa: Trong 60% s\u1ed1 test, \\( R \\) s\u1ebd kh\u00f4ng v\u01b0\u1ee3t qu\u00e1 30.</p>"},{"location":"Problem/coci0607/Contest6/P4_KAMEN/#output","title":"Output","text":"<p>In ra \\( R \\) d\u00f2ng, m\u1ed7i d\u00f2ng ch\u1ee9a \\( C \\) k\u00fd t\u1ef1, th\u1ec3 hi\u1ec7n tr\u1ea1ng th\u00e1i cu\u1ed1i c\u00f9ng c\u1ee7a b\u1ea3ng. K\u00fd t\u1ef1 <code>O</code> \u0111\u1ea1i di\u1ec7n cho h\u00f2n \u0111\u00e1.</p>"},{"location":"Problem/coci0607/Contest6/P4_KAMEN/#examples","title":"Examples","text":"<p>Test 1</p> Input <pre><code>5 4\n....\n....\nX...\n....\n....\n4\n1\n1\n1\n1\n</code></pre> Output <pre><code>....\nO...\nX...\n....\nOOO.\n</code></pre> Note <ul> <li>H\u00f2n \u0111\u00e1 \u0111\u1ea7u ti\u00ean r\u01a1i xu\u1ed1ng v\u00e0 d\u1eebng l\u1ea1i \u1edf tr\u00ean b\u1ee9c t\u01b0\u1eddng.</li> <li>H\u00f2n \u0111\u00e1 th\u1ee9 hai r\u01a1i tr\u00ean h\u00f2n \u0111\u00e1 \u0111\u1ea7u ti\u00ean, tr\u01b0\u1ee3t sang ph\u1ea3i v\u00e0 r\u01a1i xu\u1ed1ng \u0111\u00e1y c\u1ed9t th\u1ee9 hai.</li> <li>H\u00f2n \u0111\u00e1 th\u1ee9 ba r\u01a1i tr\u00ean h\u00f2n \u0111\u00e1 \u0111\u1ea7u ti\u00ean v\u00e0 h\u00f2n \u0111\u00e1 th\u1ee9 hai, tr\u01b0\u1ee3t sang tr\u00e1i v\u00e0 r\u01a1i xu\u1ed1ng \u0111\u00e1y c\u1ed9t \u0111\u1ea7u ti\u00ean.</li> <li>H\u00f2n \u0111\u00e1 th\u1ee9 t\u01b0 r\u01a1i tr\u00ean h\u00f2n \u0111\u00e1 \u0111\u1ea7u ti\u00ean, h\u00f2n \u0111\u00e1 th\u1ee9 hai, v\u00e0 cu\u1ed1i c\u00f9ng tr\u01b0\u1ee3t sang ph\u1ea3i.</li> </ul> <p>Test 2</p> Input <pre><code>7 6\n......\n......\n...XX.\n......\n......\n.XX...\n......\n6\n1\n4\n4\n6\n4\n4\n</code></pre> Output <pre><code>......\n...O..\n...XX.\n......\n.OO...\n.XX...\nO..O.O\n</code></pre>"},{"location":"Problem/coci0607/Contest6/P5_V/","title":"V","text":""},{"location":"Problem/coci0607/Contest6/P5_V/#v","title":"V","text":"<p>Zvonko l\u1ea1i \u0111ang ch\u01a1i v\u1edbi c\u00e1c ch\u1eef s\u1ed1, m\u1eb7c d\u00f9 m\u1eb9 c\u1eadu \u0111\u00e3 c\u1ea3nh b\u00e1o r\u1eb1ng c\u1eadu l\u00e0m to\u00e1n qu\u00e1 nhi\u1ec1u v\u00e0 n\u00ean ra ngo\u00e0i ch\u01a1i v\u1edbi b\u1ea1n b\u00e8.</p> <p>Trong tr\u00f2 ch\u01a1i m\u1edbi nh\u1ea5t c\u1ee7a m\u00ecnh, Zvonko t\u00ecm c\u00e1c b\u1ed9i c\u1ee7a m\u1ed9t s\u1ed1 nguy\u00ean \\( X \\), ch\u1ec9 bao g\u1ed3m c\u00e1c ch\u1eef s\u1ed1 cho tr\u01b0\u1edbc. M\u1ed9t b\u1ed9i c\u1ee7a \\( X \\) l\u00e0 m\u1ed9t s\u1ed1 chia h\u1ebft cho \\( X \\).</p> <p>\u0110\u1ec3 ph\u00e1 h\u1ecfng ni\u1ec1m vui c\u1ee7a Zvonko, m\u1eb9 c\u1eadu quy\u1ebft \u0111\u1ecbnh vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh gi\u1ea3i b\u00e0i to\u00e1n n\u00e0y. H\u00e3y vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh t\u00ednh c\u00f3 bao nhi\u00eau b\u1ed9i c\u1ee7a \\( X \\) n\u1eb1m gi\u1eefa \\( A \\) v\u00e0 \\( B \\) (bao g\u1ed3m c\u1ea3 \\( A \\) v\u00e0 \\( B \\)), sao cho khi vi\u1ebft \u1edf h\u1ec7 th\u1eadp ph\u00e2n, ch\u00fang ch\u1ec9 ch\u1ee9a c\u00e1c ch\u1eef s\u1ed1 \u0111\u01b0\u1ee3c ph\u00e9p.</p>"},{"location":"Problem/coci0607/Contest6/P5_V/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a ba s\u1ed1 nguy\u00ean \\( X, A, B \\) \\((1 \\leq X &lt; 10^{11}, 1 \\leq A \\leq B &lt; 10^{11})\\).</li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a c\u00e1c ch\u1eef s\u1ed1 \u0111\u01b0\u1ee3c ph\u00e9p. C\u00e1c ch\u1eef s\u1ed1 n\u00e0y \u0111\u01b0\u1ee3c cho theo th\u1ee9 t\u1ef1 t\u0103ng d\u1ea7n, kh\u00f4ng c\u00f3 kho\u1ea3ng tr\u1eafng v\u00e0 kh\u00f4ng tr\u00f9ng l\u1eb7p.</li> </ul>"},{"location":"Problem/coci0607/Contest6/P5_V/#output","title":"Output","text":"<p>In ra m\u1ed9t d\u00f2ng duy nh\u1ea5t \u2014 s\u1ed1 l\u01b0\u1ee3ng b\u1ed9i c\u1ee7a \\( X \\) th\u1ecfa m\u00e3n y\u00eau c\u1ea7u.</p>"},{"location":"Problem/coci0607/Contest6/P5_V/#examples","title":"Examples","text":"<p>Test 1</p> Input <pre><code>2 1 20\n0123456789\n</code></pre> Output <pre><code>10\n</code></pre> <p>Test 2</p> Input <pre><code>6 100 9294\n23689\n</code></pre> Output <pre><code>111\n</code></pre> <p>Test 3</p> Input <pre><code>5 4395 9999999999\n12346789\n</code></pre> Output <pre><code>0\n</code></pre>"},{"location":"Problem/coci0607/Contest6/P6_PROSTOR/","title":"PROSTOR","text":""},{"location":"Problem/coci0607/Contest6/P6_PROSTOR/#prostor","title":"PROSTOR","text":"<p>T\u1eeb r\u1ea5t l\u00e2u tr\u01b0\u1edbc \u0111\u00e2y, trong m\u1ed9t kh\u00f4ng gian ba chi\u1ec1u xa x\u00f4i, c\u00f3 m\u1ed9t b\u1ed9 t\u1ed9c c\u00e1c h\u00ecnh ch\u1eef nh\u1eadt s\u1ed1ng h\u1ea1nh ph\u00fac. Nh\u1eefng h\u00ecnh ch\u1eef nh\u1eadt n\u00e0y s\u1ed1ng trong h\u00f2a b\u00ecnh, lu\u00f4n song song v\u1edbi m\u1ed9t trong c\u00e1c m\u1eb7t ph\u1eb3ng t\u1ecda \u0111\u1ed9.</p> <p>M\u1ed9t ng\u00e0y n\u1ecd, m\u1ed9t h\u00ecnh h\u1ed9p ch\u1eef nh\u1eadt xu\u1ea5t hi\u1ec7n, c\u01b0\u1ee1i tr\u00ean m\u1ed9t kh\u1ed1i 20 m\u1eb7t (icosahedron), khoe nh\u1eefng g\u00f3c nh\u1ecdn v\u00e0 th\u1ec3 t\u00edch d\u01b0\u01a1ng c\u1ee7a m\u00ecnh. C\u00e1c h\u00ecnh ch\u1eef nh\u1eadt nh\u00ecn ch\u1eb1m ch\u1eb1m trong s\u1ef1 kinh ng\u1ea1c v\u00e0 b\u1eaft \u0111\u1ea7u m\u01a1 \u01b0\u1edbc tr\u1edf th\u00e0nh nh\u1eefng h\u00ecnh h\u1ed9p ch\u1eef nh\u1eadt. K\u1ec3 t\u1eeb ng\u00e0y h\u00f4m \u0111\u00f3, m\u1ecdi th\u1ee9 kh\u00f4ng bao gi\u1edd c\u00f2n nh\u01b0 c\u0169. C\u00e1c h\u00ecnh ch\u1eef nh\u1eadt b\u1eaft \u0111\u1ea7u so s\u00e1nh nhau v\u1ec1 di\u1ec7n t\u00edch, chu vi v\u00e0 th\u1eadm ch\u00ed c\u1ea3 t\u1ef7 l\u1ec7 gi\u1eefa c\u00e1c c\u1ea1nh.</p> <p>Cu\u1ed1i c\u00f9ng, xung \u0111\u1ed9t \u0111\u1ea7u ti\u00ean x\u1ea3y ra v\u1ec1 quy\u1ec1n s\u1edf h\u1eefu c\u00e1c \u0111i\u1ec3m chung. Theo th\u1eddi gian, m\u1ed7i c\u1eb7p h\u00ecnh ch\u1eef nh\u1eadt c\u00f3 \u00edt nh\u1ea5t m\u1ed9t \u0111i\u1ec3m chung (bao g\u1ed3m c\u1ea3 nh\u1eefng h\u00ecnh ch\u1ec9 ch\u1ea1m v\u00e0o nhau) tr\u1edf th\u00e0nh k\u1ebb th\u00f9.</p> <p>Nhi\u1ec7m v\u1ee5 c\u1ee7a b\u1ea1n l\u00e0 kh\u00f4i ph\u1ee5c h\u00f2a b\u00ecnh b\u1eb1ng c\u00e1ch x\u00e1c \u0111\u1ecbnh s\u1ed1 l\u01b0\u1ee3ng c\u1eb7p h\u00ecnh ch\u1eef nh\u1eadt \u0111ang c\u00f3 xung \u0111\u1ed9t.</p>"},{"location":"Problem/coci0607/Contest6/P6_PROSTOR/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\( N \\) \\((1 \\leq N \\leq 100000)\\) \u2014 s\u1ed1 l\u01b0\u1ee3ng h\u00ecnh ch\u1eef nh\u1eadt.</li> <li>\\( N \\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a 6 s\u1ed1 nguy\u00ean c\u00e1ch nhau b\u1edfi d\u1ea5u c\u00e1ch. Ba s\u1ed1 \u0111\u1ea7u ti\u00ean l\u00e0 t\u1ecda \u0111\u1ed9 c\u1ee7a m\u1ed9t \u0111\u1ec9nh c\u1ee7a h\u00ecnh ch\u1eef nh\u1eadt. Ba s\u1ed1 c\u00f2n l\u1ea1i l\u00e0 t\u1ecda \u0111\u1ed9 c\u1ee7a \u0111\u1ec9nh \u0111\u1ed1i di\u1ec7n.</li> <li>C\u00e1c t\u1ecda \u0111\u1ed9 l\u00e0 s\u1ed1 nguy\u00ean trong kho\u1ea3ng t\u1eeb 1 \u0111\u1ebfn 999 (bao g\u1ed3m c\u1ea3 1 v\u00e0 999).</li> <li>M\u1ed7i h\u00ecnh ch\u1eef nh\u1eadt song song v\u1edbi m\u1ed9t trong c\u00e1c m\u1eb7t ph\u1eb3ng t\u1ecda \u0111\u1ed9.</li> </ul>"},{"location":"Problem/coci0607/Contest6/P6_PROSTOR/#output","title":"Output","text":"<ul> <li>In ra m\u1ed9t d\u00f2ng duy nh\u1ea5t \u2014 t\u1ed5ng s\u1ed1 c\u1eb7p h\u00ecnh ch\u1eef nh\u1eadt c\u00f3 xung \u0111\u1ed9t.</li> </ul>"},{"location":"Problem/coci0607/Contest6/P6_PROSTOR/#examples","title":"Examples","text":"<p>Test 1</p> Input <pre><code>3\n1 1 1 1 3 3\n1 3 3 1 6 6\n1 4 4 1 5 5\n</code></pre> Output <pre><code>2\n</code></pre> <p>Test 2</p> Input <pre><code>3\n15 10 10 15 20 20\n10 15 10 20 15 20\n10 10 15 20 20 15\n</code></pre> Output <pre><code>3\n</code></pre> <p>Test 3</p> Input <pre><code>5\n4 4 5 4 3 2\n5 3 2 4 3 1\n5 4 3 1 1 3\n1 4 3 1 5 4\n5 5 4 5 4 2\n</code></pre> Output <pre><code>4\n</code></pre>"},{"location":"Problem/coci0607/Olympiad/P1_PATRIK/","title":"PATRIK","text":""},{"location":"Problem/coci0607/Olympiad/P1_PATRIK/#patrik","title":"PATRIK","text":"<p>C\u00f3 \\(N\\) ng\u01b0\u1eddi \u0111ang x\u1ebfp h\u00e0ng \u0111\u1ec3 v\u00e0o m\u1ed9t bu\u1ed5i h\u00f2a nh\u1ea1c. M\u1ecdi ng\u01b0\u1eddi c\u1ea3m th\u1ea5y bu\u1ed3n ch\u00e1n n\u00ean h\u1ecd quy\u1ebft \u0111\u1ecbnh quay sang t\u00ecm ki\u1ebfm m\u1ed9t ng\u01b0\u1eddi quen trong h\u00e0ng.</p> <p>Hai ng\u01b0\u1eddi A v\u00e0 B \u0111\u1ee9ng trong h\u00e0ng c\u00f3 th\u1ec3 nh\u00ecn th\u1ea5y nhau n\u1ebfu h\u1ecd \u0111\u1ee9ng ngay c\u1ea1nh nhau ho\u1eb7c n\u1ebfu kkhoong c\u00f3 ai \u1edf gi\u1eefa h\u1ecd cao h\u01a1n h\u1eb3n so v\u1edbi A ho\u1eb7c B.</p> <p>Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh x\u00e1c \u0111\u1ecbnh s\u1ed1 c\u1eb7p c\u00f3 th\u1ec3 nh\u00ecn th\u1ea5y nhau.</p>"},{"location":"Problem/coci0607/Olympiad/P1_PATRIK/#input","title":"Input:","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\le N \\le 5*10^5)\\) l\u00e0 s\u1ed1 l\u01b0\u1ee3ng ng\u01b0\u1eddi \u0111\u1ee9ng x\u1ebfp h\u00e0ng.</li> <li>M\u1ed7i d\u00f2ng trong s\u1ed1 \\(N\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean duy nh\u1ea5t m\u00f4 t\u1ea3 \u0111\u1ed9 cao c\u1ee7a m\u1ed7i ng\u01b0\u1eddi (\u0111\u01a1n v\u1ecb nanomet).</li> <li>M\u1ecdi ngu\u1eddi \u0111\u1ec1u c\u00f3 chi\u1ec1u cao th\u1ea5p h\u01a1n 231 nanomet.</li> <li>Chi\u1ec1u cao \u0111\u01b0\u1ee3c \u0111\u01b0a ra theo th\u1ee9 t\u1ef1 m\u00e0 m\u1ecdi ng\u01b0\u1eddi \u0111ang \u0111\u1ee9ng trong h\u00e0ng.</li> </ul>"},{"location":"Problem/coci0607/Olympiad/P1_PATRIK/#output","title":"Output:","text":"<ul> <li>D\u1eef li\u1ec7u \u0111\u1ea7u ra l\u00e0 m\u1ed9t s\u1ed1 nguy\u00ean duy nh\u1ea5t m\u00f4 t\u1ea3 s\u1ed1 c\u1eb7p c\u00f3 th\u1ec3 nh\u00ecn th\u1ea5y nhau.</li> </ul>"},{"location":"Problem/coci0607/Olympiad/P1_PATRIK/#example","title":"Example:","text":"<p>Test 1</p> Input <pre><code>7\n2\n4\n1\n2\n2\n5\n1 \n</code></pre> Output <pre><code>10\n</code></pre>"},{"location":"Problem/coci0607/Olympiad/P2_POLICIJA/","title":"POLICIJA","text":""},{"location":"Problem/coci0607/Olympiad/P2_POLICIJA/#policija","title":"POLICIJA","text":"<p>Nh\u1eb1m gi\u00fap b\u1eaft gi\u1eef t\u1ed9i ph\u1ea1m \u0111ang ch\u1ea1y tr\u1ed1n, c\u1ea3nh s\u00e1t \u0111ang tri\u1ec3n khai m\u1ed9t h\u1ec7 th\u1ed1ng m\u00e1y t\u00ednh m\u1edbi. Khu v\u1ef1c do c\u1ea3nh s\u00e1t ki\u1ec3m so\u00e1t c\u00f3 \\(N\\) th\u00e0nh ph\u1ed1 v\u00e0 \\(E\\) con \u0111\u01b0\u1eddng hai chi\u00eau k\u1ebft n\u1ed1i c\u00e1c th\u00e0nh ph\u1ed1. C\u00e1c th\u00e0nh ph\u1ed1 \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(N\\).</p> <p>C\u1ea3nh s\u00e1t th\u01b0\u1eddng mu\u1ed1n b\u1eaft t\u1ed9i ph\u1ea1m \u0111ang c\u1ed1 g\u1eafng di chuy\u1ec3n t\u1eeb th\u00e0nh ph\u1ed1 n\u00e0y qua th\u00e0nh ph\u1ed1 kh\u00e1c. C\u00e1c thanh tra, khi nh\u00ecn v\u00e0o b\u1ea3n \u0111\u1ed3, h\u1ecd s\u1ebd c\u1ed1 g\u1eafng x\u00e1c \u0111\u1ecbnh n\u01a1i \u0111\u1ec3 d\u1ef1ng r\u00e0o ch\u1eafn v\u00e0 tr\u1ea1m ki\u1ec3m so\u00e1t. H\u1ec7 th\u1ed1ng m\u00e1y t\u00ednh m\u1edbi c\u00f3 th\u1ec3 tr\u1ea3 l\u1eddi c\u00e1c truy v\u1ea5n sau:</p> <ol> <li> <p>X\u00e9t hai th\u00e0nh ph\u1ed1 \\(A\\) v\u00e0 \\(B\\), v\u00e0 con \u0111\u01b0\u1eddng k\u1ebft n\u1ed1i gi\u1eefa th\u00e0nh ph\u1ed1 \\(G1\\) v\u00e0 \\(G2\\). N\u1ebfu con \u0111\u01b0\u1eddng gi\u1eefa th\u00e0nh ph\u1ed1 \\(G1\\) v\u00e0 \\(G2\\) b\u1ecb ch\u1eb7n th\u00ec nh\u1eefng t\u00ean t\u1ed9i ph\u1ea1m c\u00f3 th\u1ec3 \u0111i t\u1eeb th\u00e0nh ph\u1ed1 \\(A\\) \u0111\u1ebfn th\u00e0nh ph\u1ed1 \\(B\\) hay kh\u00f4ng.</p> </li> <li> <p>X\u00e9t ba th\u00e0nh ph\u1ed1 \\(A\\), \\(B\\) v\u00e0 \\(C\\). Li\u1ec7u t\u00ean t\u1ed9i ph\u1ea1m c\u00f3 th\u1ec3 \u0111i t\u1eeb th\u00e0nh ph\u1ed1 \\(A\\) \u0111\u1ebfn th\u00e0nh ph\u1ed1 \\(B\\) n\u1ebfu to\u00e0n b\u1ed9 th\u00e0nh ph\u1ed1 \\(C\\) b\u1ecb phong t\u1ecfa v\u00e0 t\u00ean t\u1ed9i ph\u1ea1m kh\u00f4ng th\u1ec3 ti\u1ebfp c\u1eadn th\u00e0nh ph\u1ed1 \\(C\\) kh\u00f4ng.</p> </li> </ol> <p>Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh c\u00f3 th\u1ec3 th\u1ef1c hi\u1ec7n m\u00f4 t\u1ea3 c\u1ee7a h\u1ec7 th\u1ed1ng.</p>"},{"location":"Problem/coci0607/Olympiad/P2_POLICIJA/#input","title":"Input:","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a 2 s\u1ed1 nguy\u00ean \\(N\\) v\u00e0 \\(E\\) \\((2 \\le N \\le 10^5; 1 \\le E \\le 5 \\times 10^5)\\) l\u00e0 s\u1ed1 l\u01b0\u1ee3ng th\u00e0nh ph\u1ed1 v\u00e0 s\u1ed1 con \u0111\u01b0\u1eddng.</li> <li>M\u1ed7i d\u00f2ng trong s\u1ed1 \\(E\\) d\u00f2ng ti\u1ebfp theo ch\u1eefa hai s\u1ed1 nguy\u00ean ph\u00e2n bi\u1ec7t t\u1eeb \\(1\\) \u0111\u1ebfn \\(N\\) m\u00f4 t\u1ea3 ch\u1ec9 s\u1ed1 c\u1ee7a 2 th\u00e0nh ph\u1ed1 \u0111\u01b0\u1ee3c k\u1ebft n\u1ed1i v\u1edbi nhau qua con \u0111\u01b0\u1eddng (c\u00f3 nhi\u1ec1u nh\u1ea5t m\u1ed9t con \u0111\u01b0\u1eddng \u0111\u01b0\u1ee3c k\u1ebft n\u1ed1i gi\u1eefa m\u1ed7i c\u1eb7p th\u00e0nh ph\u1ed1).</li> <li>D\u00f2ng ti\u1ebfp theo ch\u1ee9a s\u1ed1 nguy\u00ean \\(Q\\) \\((1 \\le Q \\le 3 \\times 10^5)\\) th\u1ec3 hi\u1ec7n s\u1ed1 truy v\u1ea5n c\u1ee7a h\u1ec7 th\u1ed1ng.</li> <li>M\u1ed7i d\u00f2ng trong s\u1ed1 \\(Q\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a \\(4\\) ho\u1eb7c \\(5\\) s\u1ed1 nguy\u00ean:<ul> <li>S\u1ed1 \u0111\u1ea7u ti\u00ean l\u00e0 m\u1ed9t s\u1ed1 nguy\u00ean l\u00e0 \\(1\\) ho\u1eb7c \\(2\\) th\u1ec3 hi\u1ec7n lo\u1ea1i c\u1ee7a truy v\u1ea5n.</li> <li>N\u1ebfu l\u00e0 quy v\u1ea5n lo\u1ea1i 1, th\u00ec ti\u1ebfp theo g\u1ed3m \\(4\\) s\u1ed1 nguy\u00ean \\(A\\), \\(B\\), \\(G1\\), \\(G2\\). D\u1eef li\u1ec7u \u0111\u1ea3m b\u1ea3o r\u1eb1ng \\(A\\) v\u00e0 \\(B\\) l\u00e0 kh\u00e1c nhau, v\u00e0 \\(G1\\), \\(G2\\) l\u00e0 con \u0111\u01b0\u1eddng t\u1ed3n t\u1ea1i.</li> <li>N\u1ebfu l\u00e0 truy v\u1ea5n lo\u1ea1i 2, th\u00ec ti\u1ebfp theo g\u1ed3m \\(3\\) s\u1ed1 nguy\u00ean \\(A\\), \\(B\\) v\u00e0 \\(C\\). D\u1eef li\u1ec7u \u0111\u1ea3m b\u1ea3o r\u1eb1ng \\(A\\), \\(B\\) v\u00e0 \\(C\\) \u0111\u00f4i m\u1ed9t ph\u00e2n bi\u1ec7t.</li> </ul> </li> <li>D\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o s\u1ebd \u0111\u1ea3m b\u1ea3o sao cho gi\u1eefa hai th\u00e0nh ph\u1ed1 b\u1ea5t k\u1ef3 lu\u00f4n t\u1ed3n t\u1ea1i \u00edt nh\u1ea5t m\u1ed9t \u0111\u01b0\u1eddng \u0111i.</li> </ul>"},{"location":"Problem/coci0607/Olympiad/P2_POLICIJA/#output","title":"Output:","text":"<ul> <li>In ra \u0111\u00e1p \u00e1n c\u1ee7a \\(Q\\) truy v\u1eabn tr\u00ean m\u1ed7i d\u00f2ng. \u0110\u00e1p \u00e1n c\u1ee7a t\u1eebng truy v\u1ea5n in ra thoe \u0111\u1ecbnh d\u1ea1ng <code>yes</code> ho\u1eb7c l\u00e0 <code>no</code>.</li> </ul> <p>L\u01b0u \u00fd: N\u1ebfu ch\u01b0\u01a1ng tr\u00ecnh c\u1ee7a b\u1ea1n tr\u1ea3 l\u00f2i ch\u00ednh x\u00e1c t\u1ea5t c\u1ea3 c\u00e1c truy v\u1ea5n c\u1ee7a m\u1ed9t lo\u1ea1i nh\u01b0ng sai lo\u1ea1i c\u00f2n l\u1ea1i, b\u1ea1n s\u1ebd ch\u1ec9 nh\u1eadn \\(50\\%\\) s\u1ed1 \u0111i\u1ec3m cho test \u0111\u00f3. Tuy nhi\u00ean ngay c\u1ea3 v\u1eady, b\u1ea1n v\u1eabn ph\u1ea3i in ra \u0111\u1ea7y \u0111\u1ee7 \u0111\u00e1p \u00e1n c\u1ee7a c\u1ea3 \\(Q\\) truy v\u1ea5n (c\u00e1c truy v\u1ea5n c\u00f2n l\u1ea1i c\u00f3 th\u1ec3 tr\u1ea3 l\u1eddi tu\u1ef3 \u00fd).</p>"},{"location":"Problem/coci0607/Olympiad/P2_POLICIJA/#example","title":"Example:","text":"<p>Test 1</p> Input <pre><code>13 15\n1 2\n2 3\n3 5\n2 4\n4 6\n2 6\n1 4\n1 7\n7 8\n7 9\n7 10\n8 11\n8 12\n9 12\n12 13\n5\n1 5 13 1 2\n1 6 2 1 4\n1 13 6 7 8\n2 13 6 7\n2 13 6 8 \n</code></pre> Output <pre><code>yes\nyes\nyes\nno\nyes \n</code></pre>"},{"location":"Problem/coci0607/Olympiad/P3_SABOR/","title":"SABOR","text":""},{"location":"Problem/coci0607/Olympiad/P3_SABOR/#sabor","title":"SABOR","text":"<p>Ch\u1ee7 t\u1ecbch c\u1ee7a \u0111\u1ea3ng c\u1ea7m quy\u1ec1n \u0111ang t\u1ed5 ch\u1ee9c m\u1ed9t cu\u1ed9c h\u1ee3p t\u1ea1i tr\u1ee5 s\u1edf c\u1ee7a \u0111\u1ea3ng. C\u00e1c ch\u00ednh tr\u1ecb gia, l\u00e0 th\u00e0nh vi\u00ean c\u1ee7a \u0111\u1ea3ng, \u0111ang s\u1ed1ng tr\u00ean m\u1ed9t l\u01b0\u1edbi hai chi\u1ec1u, m\u1ed7i \u00f4 c\u00f3 m\u1ed9t ch\u00ednh tr\u1ecb gia s\u1ed1ng \u1edf \u0111\u00f3 (ngo\u1ea1i tr\u1eeb c\u00e1c \u00f4 c\u00f3 v\u1eadt c\u1ea3n). Tr\u1ee5 s\u1edf n\u1eb1m \u1edf \u00f4 \\((0, 0)\\), v\u00e0 c\u0169ng l\u00e0 n\u01a1i m\u00e0 ch\u1ee7 t\u1ecbch s\u1ed1ng.</p> <p>C\u00e1c ch\u00ednh tr\u1ecb gia co th\u1ec3 di chuy\u1ec3n m\u1ed9t trong b\u1ed1n h\u01b0\u1edbng (tr\u00ean, d\u01b0\u1edbi, tr\u00e1i, ph\u1ea3i), m\u1ed7i b\u01b0\u1edbc h\u1ecd s\u1ebd \u0111i \u0111\u1ebfn m\u1ed9t trong b\u1ed1n \u00f4 k\u1ec1 c\u1ea1nh. H\u1ecd kh\u00f4ng th\u1ec3 \u0111i v\u00e0o \u00f4 c\u00f3 v\u1eadt c\u1ea3n.cu\u1ed9c h\u1ecdp s\u1ebd c\u00f3 s\u1ef1 tham d\u1ef1 c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c th\u00e0nh vi\u00ean c\u1ee7a \u0111\u1ea3ng c\u00e1ch tr\u1ee5 s\u1edf nhi\u1ec1u nh\u1ea5t \\(S\\) b\u01b0\u1edbc. M\u1ed7i th\u00e0nh vi\u00ean t\u1edbi cu\u1ed9c h\u1ecdp s\u1ebd ph\u1ea3i \u0111i theo tuy\u1ebfn \u0111\u01b0\u1eddng ng\u1eafn nh\u1ea5t \u0111\u1ec3 \u0111\u1ebfn tr\u1ee5 s\u1edf (c\u00f3 th\u1ec3 \u0111i theo b\u1ea5t k\u1ef3 tuy\u1ebfn \u0111\u01b0\u1eddng n\u00e0o c\u00f9ng \u0111\u1ed9 d\u00e0i v\u1edbi tuy\u1ebfn ng\u1eafn nh\u1ea5t \u0111\u00f3). </p> <p>Ch\u1ee7 t\u1ecbch nh\u1eadn th\u1ea5y r\u1eb1ng c\u00e1c ch\u00ednh tr\u1ecb gia s\u1ebd \u0111\u1ed5i phe v\u1edbi m\u1ed7i b\u01b0\u1edbc m\u00e0 h\u1ecd di chuy\u1ec3n, t\u1ee9c l\u00e0 h\u1ecd tr\u1eed th\u00e0nh th\u00e0nh vi\u00ean c\u1ee7a \u0111\u1ea3ng \u0111\u1ed5i l\u1eadp (tr\u00ean ch\u00ednh tr\u01b0\u1eddng ch\u1ec9 c\u00f3 hai \u0111\u1ea3ng).</p> <p>Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh x\u00e1c \u0111\u1ecbnh r\u1eb1ng c\u00f3 bao nhi\u00eau ch\u00ednh tr\u1ecb gia \u0111\u1ebfn cu\u1ed9c h\u1ecdp l\u00e0 th\u00e0nh vi\u00ean c\u1ee7a \u0111\u1ea3ng c\u1ea7m quy\u1ec1n, v\u00e0 bao nhi\u00eau l\u00e0 c\u1ee7a \u0111\u1ea3ng \u0111\u1ed1i l\u1eadp.</p>"},{"location":"Problem/coci0607/Olympiad/P3_SABOR/#input","title":"Input:","text":"<ul> <li>D\u00f2ng \u0111\u00e0u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(B\\) v\u00e0 \\(S\\) \\((0 \\le B \\le 10^4; 1 \\le S \\le 10^4)\\) m\u00f4 t\u1ea3 s\u1ed1 l\u01b0\u1ee3ng v\u1eadt c\u1ea3n v\u00e0 s\u1ed1 b\u01b0\u1edbc xa nh\u1ea5t m\u00e0 m\u1ed9t ch\u00ednh tr\u1ecb gia c\u00f3 th\u1ec3 \u0111i.</li> <li>M\u1ed7i d\u00f2ng trong s\u1ed1 \\(B\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a hai s\u1ed1 nguy\u00ean l\u00e0 t\u1ecda \u0111\u1ed9 c\u1ee7a m\u1ed7i v\u1eadt c\u1ea3n. Gi\u00e1 tr\u1ecb tuy\u1ec7t \u0111\u1ed1i c\u1ee7a c\u1ea3 hai t\u1ecda \u0111\u1ed9 \u0111\u1ec1u b\u00e9 h\u01a1n \\(1000\\).</li> <li>D\u1eef li\u1ec7u \u0111\u1ea3m b\u1ea3o r\u1eb1ng kh\u00f4ng c\u00f3 hai v\u1eadt c\u1ea3n n\u00e0o n\u1eb1m c\u00f9ng m\u1ed9t \u00f4, v\u00e0 kh\u00f4ng c\u00f3 v\u1eadt c\u1ea3n n\u00e0o n\u1eb1m \u1edf \u00f4 \\((0, 0)\\).</li> </ul>"},{"location":"Problem/coci0607/Olympiad/P3_SABOR/#output","title":"Output:","text":"<ul> <li>\u0110\u00e2u ra tr\u00ean m\u1ed9t d\u00f2ng l\u00e0 hai s\u1ed1 nguy\u00ean d\u01b0\u01a1ng c\u00e1ch nhau m\u1ed9t kho\u1ea3ng tr\u1eafng l\u00e0 s\u1ed1 l\u01b0\u1ee3ng ch\u00ednh tr\u1ecb gia \u0111\u1ebfn cu\u1ed9c h\u1ecdp thu\u1ed9c phe \u0111\u1ea3ng c\u1ea7m quy\u1ec1n v\u00e0 s\u1ed1 l\u01b0\u1ee3ng ch\u00ednh tr\u1ecb gia \u0111\u1ebfn cu\u1ed9c h\u1ecdp l\u00e0 c\u1ee7a \u0111\u1ea3ng \u0111\u1ed1i l\u1eadp.</li> </ul>"},{"location":"Problem/coci0607/Olympiad/P3_SABOR/#example","title":"Example:","text":"<p>Test 1</p> Input <pre><code>0 2\n</code></pre> Output <pre><code>9 4\n</code></pre> <p>Test 2</p> Input <pre><code>4 5\n-1 1\n0 -1\n0 1\n1 0 \n</code></pre> Output <pre><code>10 16\n</code></pre> <p>Test 3</p> Input <pre><code>4 50000\n1 1\n-1 -1\n1 -1\n-1 1 \n</code></pre> Output <pre><code>2500099997 2500000000\n</code></pre>"},{"location":"Problem/coci0607/Regional/P1_BARD/","title":"BARD","text":""},{"location":"Problem/coci0607/Regional/P1_BARD/#bard","title":"BARD","text":"<p>M\u1ed7i bu\u1ed5i t\u1ed1i, d\u00e2n l\u00e0ng trong m\u1ed9t ng\u00f4i l\u00e0ng nh\u1ecf t\u1ee5 h\u1ecdp quanh m\u1ed9t \u0111\u1ed1ng l\u1eeda l\u1edbn v\u00e0 h\u00e1t nh\u1eefng b\u00e0i ca. M\u1ed9t nh\u00e2n v\u1eadt quan tr\u1ecdng trong c\u1ed9ng \u0111\u1ed3ng l\u00e0 ng\u01b0\u1eddi h\u00e1t rong. M\u1ed7i bu\u1ed5i t\u1ed1i, n\u1ebfu ng\u01b0\u1eddi h\u00e1t rong c\u00f3 m\u1eb7t, anh \u1ea5y s\u1ebd h\u00e1t m\u1ed9t b\u00e0i h\u00e1t m\u1edbi m\u00e0 ch\u01b0a ai t\u1eebng nghe tr\u01b0\u1edbc \u0111\u00f3, v\u00e0 kh\u00f4ng b\u00e0i h\u00e1t n\u00e0o kh\u00e1c \u0111\u01b0\u1ee3c h\u00e1t trong \u0111\u00eam \u0111\u00f3. N\u1ebfu ng\u01b0\u1eddi h\u00e1t rong kh\u00f4ng c\u00f3 m\u1eb7t, c\u00e1c d\u00e2n l\u00e0ng kh\u00e1c s\u1ebd h\u00e1t c\u00f9ng nhau v\u00e0 trao \u0111\u1ed5i t\u1ea5t c\u1ea3 nh\u1eefng b\u00e0i h\u00e1t m\u00e0 h\u1ecd bi\u1ebft.</p> <p>D\u1ef1a v\u00e0o danh s\u00e1ch nh\u1eefng d\u00e2n l\u00e0ng c\u00f3 m\u1eb7t trong \\(E\\) bu\u1ed5i t\u1ed1i li\u00ean ti\u1ebfp, h\u00e3y x\u00e1c \u0111\u1ecbnh t\u1ea5t c\u1ea3 nh\u1eefng d\u00e2n l\u00e0ng bi\u1ebft t\u1ea5t c\u1ea3 c\u00e1c b\u00e0i h\u00e1t \u0111\u00e3 \u0111\u01b0\u1ee3c h\u00e1t trong kho\u1ea3ng th\u1eddi gian \u0111\u00f3.</p>"},{"location":"Problem/coci0607/Regional/P1_BARD/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) (\\(1 \\leq N \\leq 100\\)), s\u1ed1 l\u01b0\u1ee3ng d\u00e2n l\u00e0ng. D\u00e2n l\u00e0ng \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(N\\). D\u00e2n l\u00e0ng s\u1ed1 \\(1\\) l\u00e0 ng\u01b0\u1eddi h\u00e1t rong.</li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(E\\) (\\(1 \\leq E \\leq 50\\)), s\u1ed1 l\u01b0\u1ee3ng bu\u1ed5i t\u1ed1i.</li> <li>M\u1ed7i trong \\(E\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a danh s\u00e1ch c\u00e1c d\u00e2n l\u00e0ng c\u00f3 m\u1eb7t trong bu\u1ed5i t\u1ed1i \u0111\u00f3. M\u1ed7i d\u00f2ng b\u1eaft \u0111\u1ea7u v\u1edbi m\u1ed9t s\u1ed1 nguy\u00ean d\u01b0\u01a1ng \\(K\\) (\\(2 \\leq K \\leq N\\)), s\u1ed1 l\u01b0\u1ee3ng d\u00e2n l\u00e0ng c\u00f3 m\u1eb7t v\u00e0o bu\u1ed5i t\u1ed1i \u0111\u00f3, sau \u0111\u00f3 l\u00e0 \\(K\\) s\u1ed1 nguy\u00ean d\u01b0\u01a1ng, c\u00e1ch nhau b\u1edfi kho\u1ea3ng tr\u1eafng, \u0111\u1ea1i di\u1ec7n cho c\u00e1c d\u00e2n l\u00e0ng.</li> <li>Kh\u00f4ng c\u00f3 d\u00e2n l\u00e0ng n\u00e0o xu\u1ea5t hi\u1ec7n hai l\u1ea7n trong m\u1ed9t bu\u1ed5i t\u1ed1i.</li> <li>Ng\u01b0\u1eddi h\u00e1t rong s\u1ebd xu\u1ea5t hi\u1ec7n \u00edt nh\u1ea5t m\u1ed9t l\u1ea7n trong to\u00e0n b\u1ed9 danh s\u00e1ch c\u00e1c bu\u1ed5i t\u1ed1i.</li> </ul>"},{"location":"Problem/coci0607/Regional/P1_BARD/#output","title":"Output","text":"<ul> <li>In ra t\u1ea5t c\u1ea3 c\u00e1c d\u00e2n l\u00e0ng bi\u1ebft t\u1ea5t c\u1ea3 c\u00e1c b\u00e0i h\u00e1t, bao g\u1ed3m c\u1ea3 ng\u01b0\u1eddi h\u00e1t rong, m\u1ed7i s\u1ed1 nguy\u00ean tr\u00ean m\u1ed9t d\u00f2ng theo th\u1ee9 t\u1ef1 t\u0103ng d\u1ea7n.</li> </ul>"},{"location":"Problem/coci0607/Regional/P1_BARD/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>4\n3\n2 1 2\n3 2 3 4\n3 4 2 1\n</code></pre> Output <pre><code>1\n2\n4\n</code></pre> <p>Test 2</p> Input <pre><code>8\n5\n4 1 3 5 4\n2 5 6\n3 6 7 8\n2 6 2\n4 2 6 8 1\n</code></pre> Output <pre><code>1\n2\n6\n8\n</code></pre> <p>Test 3</p> Input <pre><code>5\n3\n2 1 3\n2 2 1\n4 2 1 4 5\n</code></pre> Output <pre><code>1\n</code></pre>"},{"location":"Problem/coci0607/Regional/P2_TETRIS/","title":"TETRIS","text":""},{"location":"Problem/coci0607/Regional/P2_TETRIS/#tetris","title":"TETRIS","text":"<p>Tetris l\u00e0 m\u1ed9t tr\u00f2 ch\u01a1i m\u00e1y t\u00ednh ph\u1ed5 bi\u1ebfn \u0111\u01b0\u1ee3c ch\u01a1i tr\u00ean m\u1ed9t s\u00e2n g\u1ed3m \\(C\\) c\u1ed9t v\u00e0 s\u1ed1 h\u00e0ng kh\u00f4ng gi\u1edbi h\u1ea1n. Trong m\u1ed7i l\u01b0\u1ee3t, m\u1ed9t trong b\u1ea3y kh\u1ed1i h\u00ecnh sau s\u1ebd \u0111\u01b0\u1ee3c th\u1ea3 v\u00e0o s\u00e2n:</p> <p></p> <p>Ng\u01b0\u1eddi ch\u01a1i c\u00f3 th\u1ec3 xoay kh\u1ed1i 90, 180 ho\u1eb7c 270 \u0111\u1ed9 v\u00e0 di chuy\u1ec3n n\u00f3 sang tr\u00e1i ho\u1eb7c ph\u1ea3i, mi\u1ec5n l\u00e0 kh\u1ed1i n\u1eb1m ho\u00e0n to\u00e0n trong s\u00e2n. Sau \u0111\u00f3, kh\u1ed1i r\u01a1i xu\u1ed1ng cho \u0111\u1ebfn khi ch\u1ea1m v\u00e0o \u0111\u00e1y s\u00e2n ho\u1eb7c c\u00e1c \u00f4 \u0111\u00e3 b\u1ecb chi\u1ebfm. Trong bi\u1ebfn th\u1ec3 Tetris c\u1ee7a ch\u00fang ta, kh\u1ed1i ph\u1ea3i r\u01a1i xu\u1ed1ng sao cho t\u1ea5t c\u1ea3 c\u00e1c ph\u1ea7n c\u1ee7a n\u00f3 \u0111\u1ec1u n\u1eb1m tr\u00ean \u0111\u00e1y s\u00e2n ho\u1eb7c tr\u00ean c\u00e1c \u00f4 \u0111\u00e3 b\u1ecb chi\u1ebfm. N\u00f3i c\u00e1ch kh\u00e1c, sau khi kh\u1ed1i r\u01a1i xu\u1ed1ng, kh\u00f4ng \u0111\u01b0\u1ee3c c\u00f3 \u00f4 tr\u1ed1ng n\u00e0o m\u00e0 ph\u00eda tr\u00ean n\u00f3 c\u00f3 \u00f4 b\u1ecb chi\u1ebfm.</p> <p>V\u00ed d\u1ee5, n\u1ebfu s\u00e2n c\u00f3 \\(6\\) c\u1ed9t v\u1edbi \u0111\u1ed9 cao ban \u0111\u1ea7u (s\u1ed1 \u00f4 \u0111\u00e3 b\u1ecb chi\u1ebfm trong m\u1ed7i c\u1ed9t) l\u1ea7n l\u01b0\u1ee3t l\u00e0 \\(2, 1, 1, 1, 0, 1\\), th\u00ec kh\u1ed1i s\u1ed1 \\(5\\) c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c th\u1ea3 theo \\(5\\) c\u00e1ch kh\u00e1c nhau.</p> <p></p> <p>B\u1ea1n \u0111\u01b0\u1ee3c cung c\u1ea5p \u0111\u1ed9 cao ban \u0111\u1ea7u c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c c\u1ed9t v\u00e0 s\u1ed1 hi\u1ec7u c\u1ee7a kh\u1ed1i h\u00ecnh \u0111\u01b0\u1ee3c th\u1ea3 xu\u1ed1ng. H\u00e3y vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh t\u00ednh s\u1ed1 c\u00e1ch kh\u00e1c nhau c\u00f3 th\u1ec3 th\u1ea3 kh\u1ed1i v\u00e0o s\u00e2n, t\u1ee9c l\u00e0 s\u1ed1 c\u1ea5u h\u00ecnh kh\u00e1c nhau c\u1ee7a s\u00e2n c\u00f3 th\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c sau khi th\u1ea3 kh\u1ed1i.</p>"},{"location":"Problem/coci0607/Regional/P2_TETRIS/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(C\\) v\u00e0 \\(P\\) (\\(1 \\leq C \\leq 100\\), \\(1 \\leq P \\leq 7\\)), l\u1ea7n l\u01b0\u1ee3t l\u00e0 s\u1ed1 c\u1ed9t v\u00e0 s\u1ed1 hi\u1ec7u c\u1ee7a kh\u1ed1i h\u00ecnh \u0111\u01b0\u1ee3c th\u1ea3 xu\u1ed1ng.</li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a \\(C\\) s\u1ed1 nguy\u00ean c\u00e1ch nhau b\u1edfi d\u1ea5u c\u00e1ch, m\u1ed7i s\u1ed1 n\u1eb1m trong kho\u1ea3ng t\u1eeb \\(0\\) \u0111\u1ebfn \\(100\\), bi\u1ec3u di\u1ec5n \u0111\u1ed9 cao ban \u0111\u1ea7u c\u1ee7a c\u00e1c c\u1ed9t.</li> </ul>"},{"location":"Problem/coci0607/Regional/P2_TETRIS/#output","title":"Output","text":"<ul> <li>In ra m\u1ed9t s\u1ed1 nguy\u00ean duy nh\u1ea5t, l\u00e0 s\u1ed1 c\u00e1ch kh\u00e1c nhau \u0111\u1ec3 th\u1ea3 kh\u1ed1i v\u00e0o s\u00e2n.</li> </ul>"},{"location":"Problem/coci0607/Regional/P2_TETRIS/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>6 5\n2 1 1 1 0 1\n</code></pre> Output <pre><code>5\n</code></pre> <p>Test 2</p> Input <pre><code>5 1\n0 0 0 0 0\n</code></pre> Output <pre><code>7\n</code></pre> <p>Test 3</p> Input <pre><code>9 4\n4 3 5 4 6 5 7 6 6\n</code></pre> Output <pre><code>1\n</code></pre>"},{"location":"Problem/coci0607/Regional/P4_CIRCLE/","title":"CIRCLE","text":""},{"location":"Problem/coci0607/Regional/P4_CIRCLE/#circle","title":"CIRCLE","text":"<p>M\u1ed9t ng\u00e0y h\u00e8 \u0111\u1eb9p tr\u1eddi, khi Mirko \u0111ang u\u1ed1ng n\u01b0\u1edbc chanh trong ph\u00f2ng...  </p> <p>\"Anh trai!\", Stanko h\u00e9t l\u00ean.  </p> <p>\"\u0110\u00f4i khi anh t\u1ef1 h\u1ecfi ai m\u1edbi l\u00e0 anh l\u1edbn h\u01a1n gi\u1eefa hai ch\u00fang ta. C\u00f3 chuy\u1ec7n g\u00ec th\u1ebf?\", Mirko h\u1ecfi.  </p> <p>\"Nghe k\u1ef9 n\u00e0y! \u1ede s\u00e2n sau, em c\u00f3 \\(N\\) vi\u00ean s\u1ecfi \u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp th\u00e0nh m\u1ed9t v\u00f2ng tr\u00f2n. M\u1ed9t s\u1ed1 vi\u00ean s\u1ecfi m\u00e0u \u0111en, m\u1ed9t s\u1ed1 vi\u00ean m\u00e0u tr\u1eafng. Em s\u1ebd th\u1ef1c hi\u1ec7n \u0111i\u1ec1u sau: gi\u1eefa hai vi\u00ean s\u1ecfi li\u1ec1n k\u1ec1 c\u00f9ng m\u00e0u, em ch\u00e8n m\u1ed9t vi\u00ean s\u1ecfi \u0111en, c\u00f2n gi\u1eefa hai vi\u00ean s\u1ecfi li\u1ec1n k\u1ec1 kh\u00e1c m\u00e0u, em ch\u00e8n m\u1ed9t vi\u00ean s\u1ecfi tr\u1eafng. L\u00fac n\u00e0y, s\u1ed1 vi\u00ean s\u1ecfi s\u1ebd t\u0103ng g\u1ea5p \u0111\u00f4i th\u00e0nh \\(2N\\). Sau \u0111\u00f3, em lo\u1ea1i b\u1ecf \\(N\\) vi\u00ean s\u1ecfi ban \u0111\u1ea7u, ch\u1ec9 gi\u1eef l\u1ea1i \\(N\\) vi\u00ean s\u1ecfi m\u1edbi th\u00eam v\u00e0o. Em s\u1ebd l\u00e0m \u0111i\u1ec1u n\u00e0y \u0111\u00fang \\(K\\) l\u1ea7n. V\u00e0 anh ph\u1ea3i x\u00e1c \u0111\u1ecbnh v\u00f2ng tr\u00f2n ban \u0111\u1ea7u c\u1ee7a em.\"**, Stanko gi\u1ea3i th\u00edch.  </p> <p>\"Ha! Anh s\u1ebd kh\u00f4ng b\u1ecb l\u1eeba d\u1ec5 d\u00e0ng \u0111\u00e2u! Anh nh\u1eadn ra r\u1eb1ng kh\u00f4ng ph\u1ea3i l\u00fac n\u00e0o c\u0169ng c\u00f3 th\u1ec3 bi\u1ebft ch\u00ednh x\u00e1c v\u00f2ng tr\u00f2n ban \u0111\u1ea7u, nh\u01b0ng anh c\u00f3 th\u1ec3 \u0111\u1ebfm s\u1ed1 l\u01b0\u1ee3ng v\u00f2ng tr\u00f2n ban \u0111\u1ea7u kh\u00e1c nhau c\u00f3 th\u1ec3 t\u1ea1o ra c\u00f9ng m\u1ed9t k\u1ebft qu\u1ea3 sau \u0111\u00fang \\(K\\) l\u1ea7n bi\u1ebfn \u0111\u1ed5i nh\u01b0 v\u00f2ng tr\u00f2n c\u1ee7a em.\", Mirko tr\u1ea3 l\u1eddi.  </p> <p>B\u1ea1n \u0111\u01b0\u1ee3c cung c\u1ea5p c\u1ea5u h\u00ecnh c\u1ee7a v\u00f2ng tr\u00f2n tr\u01b0\u1edbc khi Stanko th\u1ef1c hi\u1ec7n bi\u1ebfn \u0111\u1ed5i \\(K\\) l\u1ea7n. Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh x\u00e1c \u0111\u1ecbnh s\u1ed1 l\u01b0\u1ee3ng v\u00f2ng tr\u00f2n ban \u0111\u1ea7u kh\u00e1c nhau c\u00f3 th\u1ec3 t\u1ea1o ra c\u00f9ng m\u1ed9t k\u1ebft qu\u1ea3 sau \\(K\\) bi\u1ebfn \u0111\u1ed5i.  </p> <p>Hai c\u1ea5u h\u00ecnh s\u1ecfi \u0111\u01b0\u1ee3c coi l\u00e0 gi\u1ed1ng nhau n\u1ebfu c\u00f3 th\u1ec3 xoay v\u00f2ng n\u00e0y th\u00e0nh v\u00f2ng kia. V\u00ed d\u1ee5, BBW v\u00e0 BWB \u0111\u01b0\u1ee3c coi l\u00e0 gi\u1ed1ng nhau, nh\u01b0ng BBWWBW v\u00e0 WWBBWB th\u00ec kh\u00f4ng.  </p>"},{"location":"Problem/coci0607/Regional/P4_CIRCLE/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(N\\) v\u00e0 \\(K\\) (\\(3 \\leq N \\leq 100\\), \\(1 \\leq K \\leq 10\\)), t\u01b0\u01a1ng \u1ee9ng l\u00e0 s\u1ed1 l\u01b0\u1ee3ng vi\u00ean s\u1ecfi trong v\u00f2ng tr\u00f2n v\u00e0 s\u1ed1 l\u1ea7n bi\u1ebfn \u0111\u1ed5i c\u1ee7a Stanko.  </li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a \u0111\u00fang \\(N\\) k\u00fd t\u1ef1 'B' ho\u1eb7c 'W', \u0111\u1ea1i di\u1ec7n cho v\u00f2ng tr\u00f2n s\u1ecfi ban \u0111\u1ea7u c\u1ee7a Stanko.  </li> </ul>"},{"location":"Problem/coci0607/Regional/P4_CIRCLE/#output","title":"Output","text":"<ul> <li>In ra m\u1ed9t s\u1ed1 nguy\u00ean duy nh\u1ea5t: s\u1ed1 l\u01b0\u1ee3ng v\u00f2ng tr\u00f2n ban \u0111\u1ea7u kh\u00e1c nhau c\u00f3 th\u1ec3 t\u1ea1o ra c\u00f9ng m\u1ed9t v\u00f2ng tr\u00f2n sau \\(K\\) l\u1ea7n bi\u1ebfn \u0111\u1ed5i.  </li> </ul>"},{"location":"Problem/coci0607/Regional/P4_CIRCLE/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>3 1\nBBW\n</code></pre> Output <pre><code>2\n</code></pre> Note <p>Hai v\u00f2ng tr\u00f2n ban \u0111\u1ea7u BBW v\u00e0 WBW \u0111\u1ec1u tr\u1edf th\u00e0nh BWW sau m\u1ed9t l\u1ea7n bi\u1ebfn \u0111\u1ed5i.  </p> <p>Test 2</p> Input <pre><code>6 2\nWBWWBW\n</code></pre> Output <pre><code>3\n</code></pre>"},{"location":"Problem/coci0708/Overview/","title":"M\u00f4 t\u1ea3 v\u1ec1 COCI 2007 - 2008","text":""},{"location":"Problem/coci0708/Overview/#tai-lieu-pdf","title":"T\u00e0i li\u1ec7u PDF:","text":"<p>Contest 1</p> <p>Contest 2</p> <p>Contest 3</p> <p>Contest 4</p> <p>Contest 5</p> <p>Contest 6</p> <p>Regional</p> <p>Olympiad</p>"},{"location":"Problem/coci0708/Overview/#cac-tai-lieu-ve-cuoc-thi","title":"C\u00e1c t\u00e0i li\u1ec7u v\u1ec1 cu\u1ed9c thi","text":"<p>COCI 2007-2008</p>"},{"location":"Problem/coci0708/Contest1/P1_CETVRTA/","title":"CETVRTA","text":""},{"location":"Problem/coci0708/Contest1/P1_CETVRTA/#cetvrta","title":"CETVRTA","text":"<p>Mirko c\u1ea7n ch\u1ecdn ra b\u1ed1n \u0111i\u1ec3m tr\u00ean m\u1eb7t ph\u1eb3ng sao cho b\u1ed1n \u0111i\u1ec3m \u0111\u00f3 t\u1ea1o th\u00e0nh m\u1ed9t h\u00ecnh ch\u1eef nh\u1eadt v\u1edbi c\u00e1c c\u1ea1nh song song v\u1edbi c\u00e1c tr\u1ee5c t\u1ecda \u0111\u1ed9. C\u1eadu \u0111\u00e3 ch\u1ecdn \u0111\u01b0\u1ee3c ba \u0111i\u1ec3m v\u00e0 t\u1ef1 tin r\u1eb1ng ba \u0111i\u1ec3m \u0111\u00f3 l\u00e0 ch\u00ednh x\u00e1c. Tuy nhi\u00ean, c\u1eadu kh\u00f4ng th\u1ec3 x\u00e1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c \u0111i\u1ec3m cu\u1ed1i c\u00f9ng. H\u00e3y gi\u00fap c\u1eadu \u1ea5y nh\u00e9!</p>"},{"location":"Problem/coci0708/Contest1/P1_CETVRTA/#input","title":"Input","text":"<ul> <li>G\u1ed3m \\(3\\) d\u00f2ng, m\u1ed7i d\u00f2ng l\u00e0 g\u1ed3m \\(2\\) s\u1ed1 nguy\u00ean l\u00e0 t\u1ecda \u0111\u1ed9 c\u1ee7a c\u00e1c \u0111i\u1ec3m m\u00e0 Mirko \u0111\u00e3 ch\u1ecdn. C\u00e1c t\u1ecda \u0111\u1ed9 n\u1eb1m trong kho\u1ea3ng t\u1eeb \\(1\\) \u0111\u1ebfn \\(1000\\).</li> </ul>"},{"location":"Problem/coci0708/Contest1/P1_CETVRTA/#output","title":"Output","text":"<ul> <li>In ra t\u1ecda \u0111\u1ed9 c\u1ee7a \u0111i\u1ec3m c\u00f2n l\u1ea1i.</li> </ul> <p>Test 1</p> Input <pre><code>5 5\n5 7\n7 5\n</code></pre> Output <pre><code>7 7\n</code></pre> <p>Test 2</p> Input <pre><code>30 20\n10 10\n10 20\n</code></pre> Output <pre><code>30 10\n</code></pre>"},{"location":"Problem/coci0708/Contest1/P2_PEG/","title":"PEG","text":""},{"location":"Problem/coci0708/Contest1/P2_PEG/#peg","title":"PEG","text":"<p>Trong t\u1ef1a game logic n\u1ed5i ti\u1ebfng Peg, c\u00e1c qu\u00e2n c\u1edd nh\u1ea3y qua c\u00e1c qu\u00e2n c\u1edd kh\u00e1c \u0111\u1ec3 lo\u1ea1i qu\u00e2n c\u1edd \u0111\u00f3 ra kh\u1ecfi tr\u00f2 ch\u01a1i cho \u0111\u1ebfn khi ch\u1ec9 c\u00f2n m\u1ed9t qu\u00e2n duy nh\u1ea5t.  \u0110\u00e2y l\u00e0 tr\u1ea1ng th\u00e1i ban \u0111\u1ea7u c\u1ee7a b\u00e0n c\u1edd:</p> <pre><code>    ooo\n    ooo\n  ooooooo\n  ooo.ooo\n  ooooooo\n    ooo\n    ooo\n</code></pre> <p>C\u00e1c k\u00ed t\u1ef1 'o' bi\u1ec3u di\u1ec5n m\u1ed9t qu\u00e2n c\u1edd v\u00e0 k\u00ed t\u1ef1 '.' bi\u1ec3u di\u1ec5n m\u1ed9t \u00f4 tr\u1ed1ng. Trong m\u1ed9t n\u01b0\u1edbc, m\u1ed9t ng\u01b0\u1eddi ch\u01a1i c\u00f3 th\u1ec3 ch\u1ecdn \\(1\\) qu\u00e2n c\u1edd \u0111\u1ec3 di chuy\u1ec3n theo h\u01b0\u1edbng l\u00ean tr\u00ean, xu\u1ed1ng d\u01b0\u1edbi, sang tr\u00e1i ho\u1eb7c sang ph\u1ea3i n\u1ebfu \u1edf h\u01b0\u1edbng \u0111\u00f3 c\u00f3 m\u1ed9t qu\u00e2n c\u1edd kh\u00e1c v\u00e0 m\u1ed9t \u00f4 tr\u1ed1ng \u1edf ph\u00eda sau qu\u00e2n c\u1edd \u0111\u00f3. Qu\u00e2n c\u1edd \u0111\u01b0\u1ee3c ch\u1ecdn s\u1ebd nh\u1ea3y \u0111\u1ebfn \u00f4 tr\u1ed1ng v\u00e0 lo\u1ea1i qu\u00e2n c\u1edd b\u1ecb nh\u1ea3y qua ra kh\u1ecfi tr\u00f2 ch\u01a1i.</p> <p>Cho bi\u1ebft tr\u1ea1ng th\u00e1i ban \u0111\u1ea7u c\u1ee7a b\u00e0n c\u1edd, h\u00e3y vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh \u0111\u1ebfm s\u1ed1 l\u01b0\u1ee3ng n\u01b0\u1edbc \u0111i h\u1ee3p l\u1ec7.</p>"},{"location":"Problem/coci0708/Contest1/P2_PEG/#input","title":"Input","text":"<ul> <li>Tr\u1ea1ng th\u00e1i ban \u0111\u1ea7u c\u1ee7a b\u00e0n c\u1edd, \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n tr\u00ean \\(7\\) d\u00f2ng, m\u1ed7i d\u00f2ng \\(7\\) k\u00ed t\u1ef1. \\(2\\) k\u00ed t\u1ef1 \u0111\u1ea7u ti\u00ean v\u00e0 cu\u1ed1i c\u00f9ng c\u1ee7a \\(2\\) h\u00e0ng \u0111\u1ea7u ti\u00ean v\u00e0 cu\u1ed1i c\u00f9ng l\u00e0 kho\u1ea3ng tr\u1eafng, t\u1ea5t c\u1ea3 c\u00e1c k\u00ed t\u1ef1 c\u00f2n l\u1ea1i l\u00e0 'o' ho\u1eb7c '.'.</li> </ul>"},{"location":"Problem/coci0708/Contest1/P2_PEG/#output","title":"Output","text":"<ul> <li>S\u1ed1 l\u01b0\u1ee3ng n\u01b0\u1edbc \u0111i h\u1ee3p l\u1ec7.</li> </ul> <p>Test 1</p> Input <pre><code>  ooo  \n  ooo  \nooooooo\nooo.ooo\nooooooo\n  ooo  \n  ooo  \n</code></pre> Output <pre><code>4\n</code></pre> <p>Test 2</p> Input <pre><code>  ooo  \n  ooo  \n..ooo..\noo...oo\n..ooo..\n  ooo  \n  ooo\n</code></pre> Output <pre><code>12\n</code></pre>"},{"location":"Problem/coci0708/Contest1/P3_PRINOVA/","title":"PRINOVA","text":""},{"location":"Problem/coci0708/Contest1/P3_PRINOVA/#prinova","title":"PRINOVA","text":"<p>Brojko v\u00e0 Brojana l\u00e0 m\u1ed9t c\u1eb7p v\u1ee3 ch\u1ed3ng. H\u1ecd c\u00f3 \\(N\\) b\u00e9 trai, \u0111\u01b0\u1ee3c \u0111\u1eb7t t\u00ean b\u1eb1ng \\(N\\) s\u1ed1 nguy\u00ean ch\u1eb5n ph\u00e2n bi\u1ec7t \\(P_1, P_2, ..., P_N\\).</p> <p>Gia \u0111\u00ecnh c\u1ee7a Brojko v\u00e0 Brojana chu\u1ea9n b\u1ecb \u0111\u00f3n th\u00eam m\u1ed9t b\u00e9 g\u00e1i v\u00e0 h\u1ecd mu\u1ed1n chu\u1ea9n b\u1ecb m\u1ed9t c\u00e1i t\u00ean th\u1eadt \u0111\u1eb9p cho c\u00f4 b\u00e9. H\u1ecd \u0111\u00e3 quy\u1ebft \u0111\u1ecbnh t\u00ean c\u1ee7a b\u00e9 s\u1ebd l\u00e0 m\u1ed9t s\u1ed1 l\u1ebb n\u0103m trong kho\u1ea3ng \\([A, B]\\). H\u1ecd c\u1ea3m th\u1ea5y m\u1ecdi s\u1ed1 trong kho\u1ea3ng \\([A, B]\\) \u0111\u1ec1u \u0111\u1eb9p nh\u01b0 nhau n\u00ean h\u1ecd mu\u1ed1n ch\u1ecdn s\u1ed1 c\u00f3 kho\u1ea3ng c\u00e1ch l\u1edbn nh\u1ea5t t\u1edbi s\u1ed1 g\u1ea7n nh\u1ea5t trong danh s\u00e1ch t\u00ean c\u1ee7a c\u00e1c c\u1eadu con trai.</p> <p>C\u1ee5 th\u1ec3 h\u01a1n, h\u1ecd mu\u1ed1n t\u00ecm m\u1ed9t s\u1ed1 l\u1ebb \\(X \\in [A, B]\\) sao cho \\(min\\)\\(\\{|X - P_i|, i \\in [1, N]\\}\\) l\u00e0 l\u1edbn nh\u1ea5t c\u00f3 th\u1ec3.</p> <p>Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh x\u00e1c \u0111\u1ecbnh t\u00ean cho b\u00e9 g\u00e1i. N\u1ebfu c\u00f3 nhi\u1ec1u \u0111\u00e1p \u00e1n, in ra b\u1ea5t k\u00ec.</p>"},{"location":"Problem/coci0708/Contest1/P3_PRINOVA/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 100)\\), s\u1ed1 l\u01b0\u1ee3ng b\u00e9 trai.</li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a \\(N\\) s\u1ed1 ch\u1eb5n ph\u00e2n bi\u1ec7t, th\u1ec3 hi\u1ec7n t\u00ean c\u1ee7a c\u00e1c b\u00e9 trai. C\u00e1c s\u1ed1 \u0111\u1ea3m b\u1ea3o b\u00e9 h\u01a1n \\(10^{9}\\).</li> <li>D\u00f2ng th\u1ee9 ba ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(A\\) v\u00e0 \\(B\\) \\((1 \\leq A \\le B \\leq 10^{9})\\), kho\u1ea3ng t\u00ean h\u1ecd mu\u1ed1n ch\u1ecdn cho b\u00e9 g\u00e1i.</li> </ul>"},{"location":"Problem/coci0708/Contest1/P3_PRINOVA/#output","title":"Output","text":"<ul> <li>In ra m\u1ed9t s\u1ed1 nguy\u00ean l\u00e0 t\u00ean c\u1ee7a b\u00e9 g\u00e1i.</li> </ul> <p>Test 1</p> Input <pre><code>3\n2 6 16\n20 50\n</code></pre> Output <pre><code>49\n</code></pre> <p>Test 2</p> Input <pre><code>3\n2 6 16\n3 15\n</code></pre> Output <pre><code>11\n</code></pre> <p>Test 3</p> Input <pre><code>3\n2 6 16\n1 7\n</code></pre> Output <pre><code>5\n</code></pre>"},{"location":"Problem/coci0708/Contest1/P4_ZAPIS/","title":"ZAPIS","text":""},{"location":"Problem/coci0708/Contest1/P4_ZAPIS/#zapis","title":"ZAPIS","text":"<p>M\u1ed9t d\u00e3y ngo\u1eb7c \u0111\u00fang l\u00e0 m\u1ed9t d\u00e3y ngo\u1eb7c ch\u1ec9 ch\u1ee9a c\u00e1c d\u1ea5u ngo\u1eb7c \u0111\u00f3ng ho\u1eb7c m\u1edf v\u00e0 th\u1ecfa m\u00e3n c\u00e1c \u0111i\u1ec1u ki\u1ec7n sau:  - M\u1ed9t x\u00e2u r\u1ed7ng l\u00e0 m\u1ed9t d\u00e3y ngo\u1eb7c \u0111\u00fang.  - N\u1ebfu \\(A\\) l\u00e0 m\u1ed9t d\u00e3y ngo\u1eb7c \u0111\u00fang th\u00ec \\((A)\\), \\([A]\\) v\u00e0 \\(\\{A\\}\\) c\u0169ng l\u00e0 d\u00e3y ngo\u1eb7c \u0111\u00fang.  - N\u1ebfu \\(A\\) v\u00e0 \\(B\\) \u0111\u1ec1u l\u00e0 d\u00e3y ngo\u1eb7c \u0111\u00fang th\u00ec \\(AB\\) c\u0169ng l\u00e0 d\u00e3y ngo\u1eb7c \u0111\u00fang. V\u00ed d\u1ee5, c\u00e1c d\u00e3y ngo\u1eb7c \\([(\\{\\})]\\), \\([]()\\{\\}\\) l\u00e0 d\u00e3y ngo\u1eb7c \u0111\u00fang, nh\u01b0ng c\u00e1c d\u00e3y \\([(\\{\\{([\\), \\([](\\{)\\}\\) v\u00e0 \\([\\{\\}])([\\{\\}]\\) th\u00ec kh\u00f4ng.</p> <p>Ivica t\u00ecm th\u1ea5y m\u1ed9t x\u00e2u c\u00f3 v\u1ebb l\u00e0 m\u1ed9t d\u00e3y ngo\u1eb7c \u0111\u00fang, m\u1ed9t s\u1ed1 k\u00ed t\u1ef1 trong x\u00e2u \u0111\u00e3 b\u1ecb m\u1edd v\u00e0 kh\u00f4ng th\u1ec3 \u0111\u1ecdc \u0111\u01b0\u1ee3c.</p> <p>Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh \u0111\u1ebfm xem c\u00f3 bao nhi\u00eau c\u00e1ch thay th\u1ebf c\u00e1c k\u00ed t\u1ef1 b\u1ecb m\u1edd trong x\u00e2u sao cho x\u00e2u \u0111\u00f3 l\u00e0 m\u1ed9t d\u00e3y ngo\u1eb7c \u0111\u00fang. Do k\u1ebft qu\u1ea3 c\u00f3 th\u1ec3 r\u1ea5t l\u1edbn, b\u1ea1n ch\u1ec9 c\u1ea7n in ra 5 ch\u1eef s\u1ed1 cu\u1ed1i c\u1ee7a k\u1ebft qu\u1ea3.</p>"},{"location":"Problem/coci0708/Contest1/P4_ZAPIS/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean ch\u1eb5n \\(N\\) \\((2\\leq N \\leq 200)\\), \u0111\u1ed9 d\u00e0i c\u1ee7a x\u00e2u.</li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a m\u1ed9t d\u00e3y ngo\u1eb7c, c\u00e1c k\u00ed t\u1ef1 b\u1ecb m\u1edd \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n b\u1edfi d\u1ea5u '?'.</li> </ul>"},{"location":"Problem/coci0708/Contest1/P4_ZAPIS/#output","title":"Output","text":"<ul> <li>In ra s\u1ed1 c\u00e1ch thay th\u1ebf c\u00e1c k\u00ed t\u1ef1 b\u1ecb m\u1edd \u0111\u1ec3 t\u1ea1o th\u00e0nh d\u00e3y ngo\u1eb7c \u0111\u00fang.</li> </ul> <p>Test 1</p> Input <pre><code>6\n()()()\n</code></pre> Output <pre><code>1\n</code></pre> <p>Test 2</p> Input <pre><code>10\n(?([?)]?}? \n</code></pre> Output <pre><code>3\n</code></pre> <p>Test 3</p> Input <pre><code>16\n???[???????]????\n</code></pre> Output <pre><code>92202\n</code></pre> Note <p>\u0110\u1ed1i v\u1edbi test th\u1ee9 2, ba d\u00e3y ngo\u1eb7c \u0111\u00fang c\u00f3 th\u1ec3 l\u00e0 \\((\\{([()])\\})\\), \\(()([()]\\{\\})\\) v\u00e0 \\(([([])]\\{\\})\\).</p>"},{"location":"Problem/coci0708/Contest1/P5_SREDNJI/","title":"SREDNJI","text":""},{"location":"Problem/coci0708/Contest1/P5_SREDNJI/#srednji","title":"SREDNJI","text":"<p>Cho m\u1ed9t d\u00e3y s\u1ed1 nguy\u00ean \\(A\\) c\u00f3 \\(N\\) ph\u1ea7n t\u1eed bao g\u1ed3m c\u00e1c s\u1ed1 nguy\u00ean t\u1eeb \\(1\\) \u0111\u1ebfn \\(N\\), m\u1ed7i s\u1ed1 xu\u1ea5t hi\u1ec7n \u0111\u00fang m\u1ed9t l\u1ea7n. </p> <p>M\u1ed9t d\u00e3y con c\u1ee7a \\(A\\) l\u00e0 m\u1ed9t d\u00e3y nh\u1eadn \u0111\u01b0\u1ee3c khi ta x\u00f3a m\u1ed9t v\u00e0i ph\u1ea7n t\u1eed \u1edf \u0111\u1ea7u ho\u1eb7c cu\u1ed1i d\u00e3y \\(A\\) (c\u00f3 th\u1ec3 l\u00e0 \\(0\\)).</p> <p>\u0110\u1ebfm s\u1ed1 d\u00e3y con c\u00f3 \u0111\u1ed9 d\u00e0i l\u1ebb c\u1ee7a \\(A\\) c\u00f3 trung v\u1ecb l\u00e0 \\(B\\). Trung v\u1ecb c\u1ee7a m\u1ed9t d\u00e3y l\u00e0 ph\u1ea7n t\u1eed \u1edf ch\u00ednh gi\u1eefa c\u1ee7a d\u00e3y sau khi \u0111\u00e3 s\u1eafp x\u1ebfp. V\u00ed d\u1ee5, trung v\u1ecb c\u1ee7a d\u00e3y \\(\\{5, 1, 3\\}\\) l\u00e0 3.</p>"},{"location":"Problem/coci0708/Contest1/P5_SREDNJI/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 100000)\\) v\u00e0 \\(B\\) \\((1 \\leq B \\leq N)\\).</li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a \\(N\\) s\u1ed1 nguy\u00ean l\u00e0 ph\u1ea7n t\u1eed c\u1ee7a d\u00e3y \\(A\\), c\u00e1c s\u1ed1 c\u00e1ch nhau b\u1edfi m\u1ed9t d\u1ea5u c\u00e1ch.</li> </ul>"},{"location":"Problem/coci0708/Contest1/P5_SREDNJI/#output","title":"Output","text":"<ul> <li>In ra s\u1ed1 d\u00e3y con c\u00f3 \u0111\u1ed9 d\u00e0i l\u1ebb c\u1ee7a \\(A\\) c\u00f3 trung v\u1ecb l\u00e0 \\(B\\).</li> </ul> <p>Test 1</p> Input <pre><code>5 4\n1 2 3 4 5 \n</code></pre> Output <pre><code>2\n</code></pre> <p>Test 2</p> Input <pre><code>6 3\n1 2 4 5 6 3 \n</code></pre> Output <pre><code>1\n</code></pre> <p>Test 3</p> Input <pre><code>7 4\n5 7 2 4 3 1 6 \n</code></pre> Output <pre><code>4\n</code></pre> Note <p>Trong test th\u1ee9 4, c\u00e1c d\u00e3y con th\u1ecfa m\u00e3n l\u00e0 \\(\\{4\\}, \\{7, 2, 4\\}, \\{5, 7, 2, 4, 3\\}\\) v\u00e0 \\(\\{5, 7, 2, 4, 3, 1, 6\\}\\)</p>"},{"location":"Problem/coci0708/Contest1/P6_STAZA/","title":"STAZA","text":""},{"location":"Problem/coci0708/Contest1/P6_STAZA/#staza","title":"STAZA","text":"<p>M\u1ed9t \u0111\u1ea5t n\u01b0\u1edbc n\u1ecd \u0111ang t\u1ed5 ch\u1ee9c m\u1ed9t cu\u1ed9c \u0111ua xe \u0111\u1ea1p. M\u1ea1ng l\u01b0\u1edbi giao th\u00f4ng c\u1ee7a qu\u1ed1c gia n\u00e0y bao g\u1ed3m \\(N\\) th\u00e0nh ph\u1ed1 \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(N\\) v\u1edbi \\(M\\) con \u0111\u01b0\u1eddng hai chi\u1ec1u k\u1ebft n\u1ed1i gi\u1eefa c\u00e1c th\u00e0nh ph\u1ed1. Ch\u00fang ta c\u00f3 m\u1ed9t s\u1ed1 \u0111\u1ecbnh ngh\u0129a nh\u01b0 sau:  - M\u1ed9t \u0111\u01b0\u1eddng \u0111i l\u00e0 m\u1ed9t c\u00e1ch di chuy\u1ec3n t\u1eeb th\u00e0nh ph\u1ed1 n\u00e0y \u0111\u1ebfn m\u1ed9t th\u00e0nh ph\u1ed1 kh\u00e1c qua c\u00e1c con \u0111\u01b0\u1eddng n\u1ed1i gi\u1eefa c\u00e1c th\u00e0nh ph\u1ed1.  - M\u1ed9t \u0111\u01b0\u1eddng \u0111i \u0111\u01a1n l\u00e0 \u0111\u01b0\u1eddng \u0111i m\u00e0 m\u1ed7i con \u0111\u01b0\u1eddng tr\u00ean \u0111\u01b0\u1eddng \u0111i \u0111\u00f3 ch\u1ec9 \u0111\u01b0\u1ee3c \u0111i qua \u0111\u00fang m\u1ed9t l\u1ea7n.  - M\u1ed9t chu tr\u00ecnh \u0111\u01a1n l\u00e0 m\u1ed9t \u0111\u01b0\u1eddng \u0111i \u0111\u01a1n k\u1ebft th\u00fac v\u00e0 b\u1eaft \u0111\u1ea7u \u1edf c\u00f9ng m\u1ed9t th\u00e0nh ph\u1ed1.</p> <p>M\u1ea1ng l\u01b0\u1edbi giao th\u00f4ng \u0111\u1ea3m b\u1ea3o lu\u00f4n c\u00f3 \u00edt nh\u1ea5t m\u1ed9t \u0111\u01b0\u1eddng \u0111i gi\u1eefa m\u1ecdi c\u1eb7p th\u00e0nh ph\u1ed1. Ngo\u00e0i ra, m\u1ed7i con \u0111\u01b0\u1eddng trong m\u1ea1ng l\u01b0\u1edbi \u0111\u1ec1u n\u1eb1m trong t\u1ed1i \u0111a m\u1ed9t chu tr\u00ecnh \u0111\u01a1n.</p> <p>Nhi\u1ec7n v\u1ee5 c\u1ee7a b\u1ea1n l\u00e0 t\u00ecm ra \u0111\u01b0\u1eddng \u0111i d\u00e0i nh\u1ea5t th\u1ecfa m\u00e3n:  - \u0110\u01b0\u1eddng \u0111i k\u1ebft th\u00fac \u1edf th\u00e0nh ph\u1ed1 \\(1\\).  - M\u1ed7i con \u0111\u01b0\u1eddng thu\u1ed9c \u0111\u01b0\u1eddng \u0111i ch\u1ec9 \u0111\u01b0\u1ee3c \u0111i qua \u0111\u00fang \\(1\\) l\u1ea7n.</p>"},{"location":"Problem/coci0708/Contest1/P6_STAZA/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a \\(2\\) s\u1ed1 nguy\u00ean \\(N\\) v\u00e0 \\(M\\) \\((2 \\leq N \\leq 10000, 1 \\leq M \\leq 2N-2)\\) - S\u1ed1 th\u00e0nh ph\u1ed1 v\u00e0 \u0111\u01b0\u1eddng \u0111i trong m\u1ea1ng l\u01b0\u1edbi.</li> <li>\\(M\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a \\(2\\) s\u1ed1 nguy\u00ean ph\u00e2n bi\u1ec7t \\(A\\) v\u00e0 \\(B\\) \\((1 \\leq A,B \\leq N)\\), th\u1ec3 hi\u1ec7n m\u1ed9t \u0111\u01b0\u1eddng \u0111i n\u1ed1i \\(2\\) th\u00e0nh ph\u1ed1 \\(A\\) v\u00e0 \\(B\\). Gi\u1eefa hai th\u00e0nh ph\u1ed1 c\u00f3 t\u1ed1i \u0111a \\(1\\) con \u0111\u01b0\u1eddng.</li> </ul>"},{"location":"Problem/coci0708/Contest1/P6_STAZA/#output","title":"Output","text":"<ul> <li>In ra \u0111\u1ed9 d\u00e0i \u0111\u01b0\u1eddng \u0111i d\u00e0i nh\u1ea5t th\u1ecfa m\u00e3n y\u00eau c\u1ea7u c\u1ee7a \u0111\u1ec1 b\u00e0i.</li> </ul> <p>Test 1</p> Input <pre><code>4 3\n1 2\n1 3\n2 4 \n</code></pre> Output <pre><code>2\n</code></pre> <p>Test 2</p> Input <pre><code>6 6\n1 2\n1 3\n2 4\n3 4\n3 5\n5 6 \n</code></pre> Output <pre><code>5\n</code></pre> <p>Test 3</p> Input <pre><code>5 6\n1 2\n2 3\n3 4\n4 5\n5 3\n3 1 \n</code></pre> Output <pre><code>6        \n</code></pre>"},{"location":"Problem/coci0708/Contest4/P1_CIRCLE/","title":"CIRCLE","text":""},{"location":"Problem/coci0708/Contest4/P1_CIRCLE/#circle","title":"CIRCLE","text":"<p>\u1ede m\u1ed9t ng\u00f4i l\u00e0ng g\u1ea7n \u0111\u00f3, ng\u01b0\u1eddi \u0111\u01b0a th\u01b0, ng\u01b0\u1eddi giao s\u1eefa v\u00e0 nh\u00e2n vi\u00ean thu gom r\u00e1c ph\u1ea3i \u0111\u1ed1i m\u1eb7t v\u1edbi c\u00f9ng m\u1ed9t v\u1ea5n \u0111\u1ec1 m\u1ed7i s\u00e1ng: ng\u00f4i nh\u00e0 s\u1ed1 18.  </p> <p>Ng\u00f4i nh\u00e0 n\u00e0y \u0111\u01b0\u1ee3c canh gi\u1eef b\u1edfi hai con ch\u00f3 th\u01b0\u1eddng xuy\u00ean g\u00e2y r\u1eafc r\u1ed1i. Tuy nhi\u00ean, \u0111i\u1ec1u m\u00e0 h\u1ecd kh\u00f4ng bi\u1ebft l\u00e0 h\u00e0nh vi c\u1ee7a nh\u1eefng con ch\u00f3 n\u00e0y ho\u00e0n to\u00e0n c\u00f3 th\u1ec3 d\u1ef1 \u0111o\u00e1n \u0111\u01b0\u1ee3c.  </p> <ul> <li>Khi m\u1ed9t ng\u00e0y b\u1eaft \u0111\u1ea7u, ch\u00f3 th\u1ee9 nh\u1ea5t s\u1ebd hung d\u1eef trong \\(A\\) ph\u00fat, sau \u0111\u00f3 b\u00ecnh t\u0129nh trong \\(B\\) ph\u00fat.  </li> <li>Ch\u00f3 th\u1ee9 hai s\u1ebd hung d\u1eef trong \\(C\\) ph\u00fat, r\u1ed3i b\u00ecnh t\u0129nh trong \\(D\\) ph\u00fat.  </li> <li>C\u1ea3 hai con ch\u00f3 l\u1eb7p l\u1ea1i h\u00e0nh vi n\u00e0y v\u00f4 h\u1ea1n l\u1ea7n, b\u1eaft \u0111\u1ea7u m\u1ed9t chu k\u1ef3 m\u1edbi ngay sau khi chu k\u1ef3 c\u0169 k\u1ebft th\u00fac.  </li> </ul> <p>V\u1edbi th\u1eddi \u0111i\u1ec3m m\u00e0 ng\u01b0\u1eddi \u0111\u01b0a th\u01b0, ng\u01b0\u1eddi giao s\u1eefa v\u00e0 nh\u00e2n vi\u00ean thu gom r\u00e1c \u0111\u1ebfn ng\u00f4i nh\u00e0 s\u1ed1 18, h\u00e3y x\u00e1c \u0111\u1ecbnh c\u00f3 bao nhi\u00eau con ch\u00f3 (kh\u00f4ng c\u00f3 con n\u00e0o, m\u1ed9t con ho\u1eb7c c\u1ea3 hai con) t\u1ea5n c\u00f4ng m\u1ed7i ng\u01b0\u1eddi.  </p>"},{"location":"Problem/coci0708/Contest4/P1_CIRCLE/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a b\u1ed1n s\u1ed1 nguy\u00ean \\(A, B, C, D\\) (\\(1 \\leq A, B, C, D \\leq 999\\)), m\u00f4 t\u1ea3 h\u00e0nh vi c\u1ee7a hai con ch\u00f3.  </li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a ba s\u1ed1 nguy\u00ean \\(P, M, G\\) (\\(1 \\leq P, M, G \\leq 999\\)), t\u01b0\u01a1ng \u1ee9ng l\u00e0 th\u1eddi \u0111i\u1ec3m ng\u01b0\u1eddi \u0111\u01b0a th\u01b0, ng\u01b0\u1eddi giao s\u1eefa v\u00e0 nh\u00e2n vi\u00ean thu gom r\u00e1c \u0111\u1ebfn ng\u00f4i nh\u00e0 s\u1ed1 18.  </li> </ul>"},{"location":"Problem/coci0708/Contest4/P1_CIRCLE/#output","title":"Output","text":"<ul> <li>In ra ba d\u00f2ng, m\u1ed7i d\u00f2ng ch\u1ee9a m\u1ed9t trong ba t\u1eeb:  </li> <li>\"both\" n\u1ebfu c\u1ea3 hai con ch\u00f3 t\u1ea5n c\u00f4ng.  </li> <li>\"one\" n\u1ebfu ch\u1ec9 c\u00f3 m\u1ed9t con ch\u00f3 t\u1ea5n c\u00f4ng.  </li> <li>\"none\" n\u1ebfu kh\u00f4ng c\u00f3 con ch\u00f3 n\u00e0o t\u1ea5n c\u00f4ng.  </li> </ul>"},{"location":"Problem/coci0708/Contest4/P1_CIRCLE/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>2 2 3 3\n1 3 4\n</code></pre> Output <pre><code>both\none\nnone\n</code></pre> <p>Test 2</p> Input <pre><code>2 3 4 5\n4 9 5\n</code></pre> Output <pre><code>one\nnone\nnone\n</code></pre>"},{"location":"Problem/coci0708/Contest4/P2_VECI/","title":"VECI","text":""},{"location":"Problem/coci0708/Contest4/P2_VECI/#veci","title":"VECI","text":"<p>Ch\u01b0\u01a1ng tr\u00ecnh c\u1ee7a b\u1ea1n s\u1ebd nh\u1eadn m\u1ed9t s\u1ed1 nguy\u00ean \\(X\\). H\u00e3y t\u00ecm s\u1ed1 nh\u1ecf nh\u1ea5t l\u1edbn h\u01a1n \\(X\\) c\u00f3 c\u00f9ng c\u00e1c ch\u1eef s\u1ed1 v\u1edbi \\(X\\).  </p>"},{"location":"Problem/coci0708/Contest4/P2_VECI/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a s\u1ed1 nguy\u00ean \\(X\\) \\((1 \\leq X \\leq 999999)\\).  </li> <li>Ch\u1eef s\u1ed1 \u0111\u1ea7u ti\u00ean c\u1ee7a \\(X\\) kh\u00f4ng ph\u1ea3i l\u00e0 s\u1ed1 \\(0\\).  </li> </ul>"},{"location":"Problem/coci0708/Contest4/P2_VECI/#output","title":"Output","text":"<ul> <li>In ra k\u1ebft qu\u1ea3 tr\u00ean m\u1ed9t d\u00f2ng. N\u1ebfu kh\u00f4ng c\u00f3 s\u1ed1 n\u00e0o th\u1ecfa m\u00e3n, in ra \\(0\\).  </li> </ul>"},{"location":"Problem/coci0708/Contest4/P2_VECI/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>156\n</code></pre> Output <pre><code>165\n</code></pre> <p>Test 2</p> Input <pre><code>330\n</code></pre> Output <pre><code>0\n</code></pre> <p>Test 3</p> Input <pre><code>27711\n</code></pre> Output <pre><code>71127\n</code></pre>"},{"location":"Problem/coci0708/Contest4/P3_LEKTIRA/","title":"LEKTIRA","text":""},{"location":"Problem/coci0708/Contest4/P3_LEKTIRA/#lektira","title":"LEKTIRA","text":"<p>Mario l\u1ea1i b\u00e0y ra nh\u1eefng tr\u00f2 ch\u01a1i ng\u1edb ng\u1ea9n thay v\u00ec \u0111\u1ecdc Dostoevsky cho b\u00e0i t\u1eadp \u1edf tr\u01b0\u1eddng. Lu\u1eadt ch\u01a1i m\u1edbi c\u1ee7a c\u1eadu nh\u01b0 sau:  </p> <ul> <li>\u0110\u1ea7u ti\u00ean, c\u1eadu ch\u1ecdn m\u1ed9t t\u1eeb ng\u1eabu nhi\u00ean trong cu\u1ed1n s\u00e1ch.  </li> <li>Sau \u0111\u00f3, c\u1eadu chia t\u1eeb \u0111\u00f3 t\u1ea1i hai v\u1ecb tr\u00ed b\u1ea5t k\u1ef3 \u0111\u1ec3 c\u00f3 ba t\u1eeb ri\u00eang bi\u1ec7t.  </li> <li>Ti\u1ebfp theo, c\u1eadu \u0111\u1ea3o ng\u01b0\u1ee3c th\u1ee9 t\u1ef1 c\u00e1c ch\u1eef c\u00e1i trong t\u1eebng t\u1eeb (\u0111\u1ed5i v\u1ecb tr\u00ed ch\u1eef c\u00e1i \u0111\u1ea7u v\u00e0 cu\u1ed1i, ch\u1eef th\u1ee9 hai v\u00e0 th\u1ee9 \u00e1p ch\u00f3t, v.v.).  </li> <li>Cu\u1ed1i c\u00f9ng, c\u1eadu gh\u00e9p ba t\u1eeb l\u1ea1i theo \u0111\u00fang th\u1ee9 t\u1ef1 ban \u0111\u1ea7u.  </li> </ul> <p>M\u1ee5c ti\u00eau c\u1ee7a tr\u00f2 ch\u01a1i l\u00e0 thu \u0111\u01b0\u1ee3c t\u1eeb c\u00f3 th\u1ee9 t\u1ef1 t\u1eeb \u0111i\u1ec3n nh\u1ecf nh\u1ea5t c\u00f3 th\u1ec3. N\u00f3i c\u00e1ch kh\u00e1c, trong t\u1ea5t c\u1ea3 c\u00e1c t\u1eeb c\u00f3 th\u1ec3 thu \u0111\u01b0\u1ee3c t\u1eeb quy tr\u00ecnh tr\u00ean, h\u00e3y t\u00ecm t\u1eeb xu\u1ea5t hi\u1ec7n s\u1edbm nh\u1ea5t trong t\u1eeb \u0111i\u1ec3n.  </p> <p>Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh gi\u00fap Mario ch\u01a1i tr\u00f2 ch\u01a1i n\u00e0y m\u1ed9t c\u00e1ch ho\u00e0n h\u1ea3o.  </p>"},{"location":"Problem/coci0708/Contest4/P3_LEKTIRA/#input","title":"Input","text":"<ul> <li>D\u00f2ng duy nh\u1ea5t ch\u1ee9a m\u1ed9t t\u1eeb do Mario ch\u1ecdn, bao g\u1ed3m c\u00e1c ch\u1eef c\u00e1i th\u01b0\u1eddng trong b\u1ea3ng ch\u1eef c\u00e1i ti\u1ebfng Anh, kh\u00f4ng c\u00f3 d\u1ea5u c\u00e1ch.  </li> <li>\u0110\u1ed9 d\u00e0i c\u1ee7a t\u1eeb n\u1eb1m trong kho\u1ea3ng t\u1eeb \\(3\\) \u0111\u1ebfn \\(50\\) k\u00fd t\u1ef1 (bao g\u1ed3m c\u1ea3 hai \u0111\u1ea7u).  </li> </ul>"},{"location":"Problem/coci0708/Contest4/P3_LEKTIRA/#output","title":"Output","text":"<ul> <li>In ra t\u1eeb t\u1ed1t nh\u1ea5t tr\u00ean m\u1ed9t d\u00f2ng.  </li> </ul>"},{"location":"Problem/coci0708/Contest4/P3_LEKTIRA/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>dcbagfekjih\n</code></pre> Output <pre><code>abcdefghijk\n</code></pre> <p>Test 2</p> Input <pre><code>mobitel\n</code></pre> Output <pre><code>bometil\n</code></pre> <p>Test 3</p> Input <pre><code>anakonda\n</code></pre> Output <pre><code>aanadnok\n</code></pre>"},{"location":"Problem/coci0708/Contest4/P4_MUZICARI/","title":"MUZICARI","text":""},{"location":"Problem/coci0708/Contest4/P4_MUZICARI/#muzicari","title":"MUZICARI","text":"<p>Nh\u00f3m nh\u1ea1c d\u00e2n gian n\u1ed5i ti\u1ebfng \"The Drinking Musicians\" s\u1eafp \u0111\u1ebfn th\u1ecb tr\u1ea5n c\u1ee7a b\u1ea1n. H\u1ecd kh\u00f4ng ch\u1ec9 n\u1ed5i ti\u1ebfng v\u1edbi k\u1ef9 n\u0103ng ch\u01a1i nh\u1ea1c m\u00e0 c\u00f2n v\u1edbi t\u00ednh c\u00e1ch ngang t\u00e0ng. H\u1ecd lu\u00f4n \u0111\u1ebfn tr\u1ec5, kh\u00f4ng bi\u1ebft m\u00ecnh \u0111ang \u1edf \u0111\u00e2u v\u00e0 th\u01b0\u1eddng xuy\u00ean g\u1eb7p kh\u00f3 kh\u0103n trong vi\u1ec7c t\u00ecm s\u00e2n kh\u1ea5u.  </p> <p>H\u01a1n n\u1eefa, trong su\u1ed1t bu\u1ed5i h\u00f2a nh\u1ea1c, m\u1ed7i nh\u1ea1c c\u00f4ng s\u1ebd c\u00f3 m\u1ed9t kho\u1ea3ng th\u1eddi gian ngh\u1ec9. N\u1ebfu c\u00f3 t\u1eeb ba ng\u01b0\u1eddi tr\u1edf l\u00ean ngh\u1ec9 c\u00f9ng l\u00fac, h\u1ecd s\u1ebd g\u00e2y r\u1eafc r\u1ed1i trong th\u1ecb tr\u1ea5n, khi\u1ebfn nh\u1eefng ng\u01b0\u1eddi c\u00f2n l\u1ea1i ho\u1ea3ng lo\u1ea1n v\u00e0 ch\u01a1i sai h\u1ee3p \u00e2m.  </p> <p>Bu\u1ed5i h\u00f2a nh\u1ea1c k\u00e9o d\u00e0i \\(T\\) ph\u00fat, trong \u0111\u00f3 m\u1ed7i nh\u1ea1c c\u00f4ng \\(N\\) s\u1ebd c\u00f3 m\u1ed9t kho\u1ea3ng ngh\u1ec9 \u0111\u00e3 bi\u1ebft tr\u01b0\u1edbc.  </p> <p>H\u00e3y gi\u00fap ban t\u1ed5 ch\u1ee9c l\u1eadp l\u1ecbch ngh\u1ec9 sao cho t\u1ea1i b\u1ea5t k\u1ef3 th\u1eddi \u0111i\u1ec3m n\u00e0o, t\u1ed1i \u0111a ch\u1ec9 c\u00f3 hai ng\u01b0\u1eddi ngh\u1ec9 c\u00f9ng l\u00fac. M\u1ecdi kho\u1ea3ng ngh\u1ec9 ph\u1ea3i ho\u00e0n to\u00e0n n\u1eb1m trong th\u1eddi gian di\u1ec5n ra bu\u1ed5i h\u00f2a nh\u1ea1c.  </p>"},{"location":"Problem/coci0708/Contest4/P4_MUZICARI/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(T\\) v\u00e0 \\(N\\) \\((1 \\leq T \\leq 5000, 1 \\leq N \\leq 500)\\), l\u00e0 th\u1eddi l\u01b0\u1ee3ng c\u1ee7a bu\u1ed5i h\u00f2a nh\u1ea1c t\u00ednh theo ph\u00fat v\u00e0 s\u1ed1 l\u01b0\u1ee3ng nh\u1ea1c c\u00f4ng trong nh\u00f3m.  </li> <li>D\u00f2ng ti\u1ebfp theo ch\u1ee9a \\(N\\) s\u1ed1 nguy\u00ean, m\u1ed7i s\u1ed1 bi\u1ec3u th\u1ecb th\u1eddi l\u01b0\u1ee3ng ngh\u1ec9 (t\u00ednh theo ph\u00fat) c\u1ee7a t\u1eebng nh\u1ea1c c\u00f4ng, c\u00e1ch nhau b\u1edfi m\u1ed9t d\u1ea5u c\u00e1ch.  </li> <li>D\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o \u0111\u1ea3m b\u1ea3o lu\u00f4n t\u1ed3n t\u1ea1i \u00edt nh\u1ea5t m\u1ed9t c\u00e1ch x\u1ebfp l\u1ecbch h\u1ee3p l\u1ec7.  </li> </ul>"},{"location":"Problem/coci0708/Contest4/P4_MUZICARI/#output","title":"Output","text":"<ul> <li>In ra \\(N\\) s\u1ed1 nguy\u00ean, m\u1ed7i s\u1ed1 bi\u1ec3u th\u1ecb s\u1ed1 ph\u00fat nh\u1ea1c c\u00f4ng \u0111\u00f3 s\u1ebd bi\u1ec3u di\u1ec5n tr\u01b0\u1edbc khi ngh\u1ec9.  </li> <li>C\u00e1c s\u1ed1 \u0111\u01b0\u1ee3c in theo \u0111\u00fang th\u1ee9 t\u1ef1 c\u1ee7a c\u00e1c nh\u1ea1c c\u00f4ng trong \u0111\u1ea7u v\u00e0o.  </li> </ul>"},{"location":"Problem/coci0708/Contest4/P4_MUZICARI/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>8 3\n4 4 4\n</code></pre> Output <pre><code>0 2 4\n</code></pre> <p>Test 2</p> Input <pre><code>10 5\n7 5 1 2 3\n</code></pre> Output <pre><code>3 3 9 0 0\n</code></pre>"},{"location":"Problem/coci0708/Contest4/P5_POKLON/","title":"POKLON","text":""},{"location":"Problem/coci0708/Contest4/P5_POKLON/#poklon","title":"POKLON","text":"<p>Mirko nh\u1eadn \u0111\u01b0\u1ee3c m\u1ed9t t\u1eadp h\u1ee3p c\u00e1c kho\u1ea3ng l\u00e0m qu\u00e0 sinh nh\u1eadt. C\u00f3 r\u1ea5t nhi\u1ec1u tr\u00f2 ch\u01a1i m\u00e0 c\u1eadu c\u00f3 th\u1ec3 ch\u01a1i v\u1edbi ch\u00fang. Trong m\u1ed9t tr\u00f2 ch\u01a1i, Mirko ph\u1ea3i t\u00ecm d\u00e3y d\u00e0i nh\u1ea5t g\u1ed3m c\u00e1c kho\u1ea3ng ph\u00e2n bi\u1ec7t sao cho:  </p> <ol> <li>M\u1ed7i kho\u1ea3ng trong d\u00e3y thu\u1ed9c t\u1eadp \u0111\u00e3 cho.  </li> <li>M\u1ed7i kho\u1ea3ng ch\u1ee9a ho\u00e0n to\u00e0n kho\u1ea3ng \u0111\u1ee9ng sau n\u00f3 trong d\u00e3y.  </li> </ol> <p>Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh t\u00ecm m\u1ed9t d\u00e3y th\u1ecfa m\u00e3n \u0111i\u1ec1u ki\u1ec7n tr\u00ean c\u00f3 \u0111\u1ed9 d\u00e0i l\u1edbn nh\u1ea5t.  </p>"},{"location":"Problem/coci0708/Contest4/P5_POKLON/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 100000)\\), s\u1ed1 l\u01b0\u1ee3ng kho\u1ea3ng trong t\u1eadp h\u1ee3p.  </li> <li>M\u1ed7i d\u00f2ng trong \\(N\\) d\u00f2ng ti\u1ebfp theo ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(A\\) v\u00e0 \\(B\\) \\((1 \\leq A &lt; B \\leq 1000000)\\) m\u00f4 t\u1ea3 m\u1ed9t kho\u1ea3ng.  </li> </ul>"},{"location":"Problem/coci0708/Contest4/P5_POKLON/#output","title":"Output","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean in ra s\u1ed1 nguy\u00ean \\(K\\), \u0111\u1ed9 d\u00e0i c\u1ee7a d\u00e3y d\u00e0i nh\u1ea5t.  </li> <li>\\(K\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a m\u1ed9t kho\u1ea3ng thu\u1ed9c d\u00e3y theo \u0111\u00fang \u0111\u1ecbnh d\u1ea1ng \u0111\u1ea7u v\u00e0o.  </li> </ul>"},{"location":"Problem/coci0708/Contest4/P5_POKLON/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>3\n3 4\n2 5\n1 6\n</code></pre> Output <pre><code>3\n1 6\n2 5\n3 4\n</code></pre> <p>Test 2</p> Input <pre><code>5\n10 30\n20 40\n30 50\n10 60\n30 40\n</code></pre> Output <pre><code>3\n10 60\n30 50\n30 40\n</code></pre> <p>Test 3</p> Input <pre><code>6\n1 4\n1 5\n1 6\n1 7\n2 5\n3 5\n</code></pre> Output <pre><code>5\n1 7\n1 6\n1 5\n2 5\n3 5\n</code></pre>"},{"location":"Problem/coci0708/Contest4/P6_KOCKE/","title":"KOCKE","text":""},{"location":"Problem/coci0708/Contest4/P6_KOCKE/#kocke","title":"KOCKE","text":"<p>M\u1ed9t con robot v\u00e0 n\u0103m kh\u1ed1i l\u1eadp ph\u01b0\u01a1ng \u0111\u01b0\u1ee3c \u0111\u1eb7t tr\u00ean m\u1ed9t b\u00e0n c\u1edd v\u00f4 h\u1ea1n g\u1ed3m c\u00e1c \u00f4 vu\u00f4ng \u0111\u01a1n v\u1ecb. Robot v\u00e0 c\u00e1c kh\u1ed1i l\u1eadp ph\u01b0\u01a1ng m\u1ed7i c\u00e1i chi\u1ebfm m\u1ed9t \u00f4.</p> <p>Robot c\u00f3 th\u1ec3 di chuy\u1ec3n theo b\u1ed1n h\u01b0\u1edbng: l\u00ean, xu\u1ed1ng, tr\u00e1i v\u00e0 ph\u1ea3i. N\u1ebfu \u00f4 m\u00e0 robot s\u1ebd di chuy\u1ec3n v\u00e0o ch\u1ee9a m\u1ed9t kh\u1ed1i l\u1eadp ph\u01b0\u01a1ng, robot s\u1ebd \u0111\u1ea9y n\u00f3 theo h\u01b0\u1edbng di chuy\u1ec3n.</p> <p>C\u00e1c kh\u1ed1i l\u1eadp ph\u01b0\u01a1ng c\u00f3 t\u00ednh ch\u1ea5t t\u1eeb t\u00ednh \u0111\u1eb7c bi\u1ec7t. Khi hai kh\u1ed1i l\u1eadp ph\u01b0\u01a1ng \u1edf \u00f4 li\u1ec1n k\u1ec1 (c\u00e1c \u00f4 c\u00f3 chung m\u1ed9t c\u1ea1nh), ch\u00fang s\u1ebd k\u1ebft h\u1ee3p v\u00e0 tr\u1edf th\u00e0nh m\u1ed9t \u0111\u1ed1i t\u01b0\u1ee3ng duy nh\u1ea5t. N\u1ebfu robot \u0111\u1ea9y m\u1ed9t kh\u1ed1i l\u1eadp ph\u01b0\u01a1ng \u0111\u00e3 k\u1ebft h\u1ee3p v\u1edbi m\u1ed9t ho\u1eb7c nhi\u1ec1u kh\u1ed1i kh\u00e1c, t\u1ea5t c\u1ea3 c\u00e1c kh\u1ed1i trong nh\u00f3m s\u1ebd di chuy\u1ec3n c\u00f9ng nhau.</p> <p>Vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh \u0111i\u1ec1u khi\u1ec3n robot sao cho n\u00f3 gh\u00e9p t\u1ea5t c\u1ea3 c\u00e1c kh\u1ed1i l\u1eadp ph\u01b0\u01a1ng th\u00e0nh m\u1ed9t nh\u00f3m duy nh\u1ea5t c\u00f3 h\u00ecnh ch\u1eef T \u1edf v\u1ecb tr\u00ed th\u1eb3ng \u0111\u1ee9ng (kh\u00f4ng \u0111\u01b0\u1ee3c xoay).</p>"},{"location":"Problem/coci0708/Contest4/P6_KOCKE/#input","title":"Input","text":"<ul> <li>\u0110\u1ea7u v\u00e0o g\u1ed3m n\u0103m d\u00f2ng. M\u1ed7i d\u00f2ng ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(X\\) v\u00e0 \\(Y\\) \\((5 \\leq X, Y \\leq 5)\\), t\u1ecda \u0111\u1ed9 ban \u0111\u1ea7u c\u1ee7a m\u1ed9t kh\u1ed1i l\u1eadp ph\u01b0\u01a1ng.</li> <li>Robot ban \u0111\u1ea7u \u1edf \u00f4 \\((0, 0)\\). Kh\u00f4ng c\u00f3 kh\u1ed1i l\u1eadp ph\u01b0\u01a1ng n\u00e0o \u1edf t\u1ecda \u0111\u1ed9 \u0111\u00f3.</li> <li>Kh\u00f4ng c\u00f3 c\u1eb7p kh\u1ed1i l\u1eadp ph\u01b0\u01a1ng n\u00e0o ban \u0111\u1ea7u \u1edf c\u00f9ng m\u1ed9t t\u1ecda \u0111\u1ed9 ho\u1eb7c \u1edf c\u00e1c \u00f4 li\u1ec1n k\u1ec1 (c\u00f3 th\u1ec3 ch\u1ea1m g\u00f3c).</li> </ul>"},{"location":"Problem/coci0708/Contest4/P6_KOCKE/#output","title":"Output","text":"<ul> <li>In ra m\u1ed9t chu\u1ed7i k\u00fd t\u1ef1 th\u1ec3 hi\u1ec7n c\u00e1c b\u01b0\u1edbc di chuy\u1ec3n c\u1ee7a robot tr\u00ean m\u1ed9t d\u00f2ng duy nh\u1ea5t.</li> <li>M\u1ed7i k\u00fd t\u1ef1 trong chu\u1ed7i ph\u1ea3i l\u00e0 m\u1ed9t trong c\u00e1c k\u00fd t\u1ef1 sau: <code>'U'</code> (l\u00ean), <code>'D'</code> (xu\u1ed1ng), <code>'L'</code> (tr\u00e1i), <code>'R'</code> (ph\u1ea3i).</li> <li>D\u00e3y l\u1ec7nh di chuy\u1ec3n kh\u00f4ng \u0111\u01b0\u1ee3c d\u00e0i qu\u00e1 9999 k\u00fd t\u1ef1.</li> </ul>"},{"location":"Problem/coci0708/Contest4/P6_KOCKE/#example","title":"Example","text":"<p>Test 1</p> Input <pre><code>0 1\n-1 0\n1 0\n0 -1\n0 -3\n</code></pre> Output <pre><code>DRRUUULLDD\n</code></pre> <p>Test 2</p> Input <pre><code>-2 0\n-1 -1\n0 -2\n1 0\n0 1\n</code></pre> Output <pre><code>URRDLLURUULDDLLLDR\n</code></pre>"},{"location":"Problem/coci0708/Contest6/P1_PARKING/","title":"PARKING","text":""},{"location":"Problem/coci0708/Contest6/P1_PARKING/#parking","title":"PARKING","text":"<p>Sau khi b\u1ecb \u0111u\u1ed5i h\u1ecdc v\u00ec t\u1ea1ch m\u00f4n H\u00f3a, Luka \u0111i l\u00e0m t\u00e0i x\u1ebf xe t\u1ea3i. M\u1ed9t bu\u1ed5i t\u1ed1i, Luka \u0111\u1eadu \\(3\\) chi\u1ebfc xe t\u1ea1i m\u1ed9t b\u00e3i \u0111\u1ed7 xe. B\u00e3i \u0111\u1ed7 xe n\u00e0y t\u00ednh ti\u1ec1n theo m\u1ed9t c\u00e1ch k\u00ec l\u00e0 - h\u1ecd gi\u1ea3m gi\u00e1 theo s\u1ed1 l\u01b0\u1ee3ng xe.</p> <p>Khi ch\u1ec9 c\u00f3 m\u1ed9t chi\u1ebfc xe \u0111\u1eadu \u1edf \u0111\u00f3, t\u00e0i x\u1ebf s\u1ebd ph\u1ea3i tr\u1ea3 \\(A\\) \u0111\u1ed3ng cho \\(1\\) ph\u00fat. Khi c\u00f3 hai chi\u1ebfc xe, m\u1ed7i t\u00e0i x\u1ebf s\u1ebd ph\u1ea3i tr\u1ea3 \\(B\\) \u0111\u1ed3ng m\u1ed7i ph\u00fat v\u00e0 khi c\u00f3 \\(3\\) chi\u1ebfc xe, m\u1ed7i t\u00e0i x\u1ebf s\u1ebd ph\u1ea3i tr\u1ea3 \\(C\\) \u0111\u1ed3ng m\u1ed7i ph\u00fat.</p> <p>Cho bi\u1ebft ba s\u1ed1 \\(A, B, C\\) v\u00e0 kho\u1ea3ng th\u1eddi gian m\u00e0 ba chi\u1ebfc xe Luka \u0111\u00e3 \u0111\u1eadu, h\u00e3y t\u00ednh s\u1ed1 ti\u1ec1n c\u1eadu \u1ea5y ph\u1ea3i tr\u1ea3.</p>"},{"location":"Problem/coci0708/Contest6/P1_PARKING/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a ba s\u1ed1 nguy\u00ean \\(A, B, C\\) (\\(1 \\leq C \\leq B \\leq A \\leq 100)\\), s\u1ed1 ti\u1ec1n ph\u1ea3i tr\u1ea3 nh\u01b0 \u0111\u00e3 \u0111\u1ecbnh ngh\u0129a \u1edf tr\u00ean.</li> <li>Ba d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a \\(2\\) s\u1ed1 nguy\u00ean trong kho\u1ea3ng t\u1eeb \\(1\\) \u0111\u1ebfn \\(100\\), th\u1ec3 hi\u1ec7n th\u1eddi gian \u0111\u1ebfn v\u00e0 \u0111i c\u1ee7a nh\u1eefng chi\u1ebfc xe c\u1ee7a Luka t\u00ednh theo ph\u00fat. D\u1eef li\u1ec7u \u0111\u1ea3m b\u1ea3o th\u1eddi gian \u0111\u1ebfn lu\u00f4n s\u1edbm h\u01a1n th\u1eddi gian \u0111i.</li> </ul>"},{"location":"Problem/coci0708/Contest6/P1_PARKING/#output","title":"Output","text":"<ul> <li>In ra s\u1ed1 ti\u1ec1n m\u00e0 Luka ph\u1ea3i tr\u1ea3.</li> </ul> <p>Test 1</p> Input <pre><code>5 3 1\n1 6\n3 5\n2 8 \n</code></pre> Output <pre><code>33\n</code></pre> <p>Test 2</p> Input <pre><code>10 8 6\n15 30\n25 50\n70 80 \n</code></pre> Output <pre><code>480\n</code></pre>"},{"location":"Problem/coci0708/Contest6/P2_SEMAFORI/","title":"SEMAFORI","text":""},{"location":"Problem/coci0708/Contest6/P2_SEMAFORI/#semafori","title":"SEMAFORI","text":"<p>Luka \u0111ang l\u00e1i xe tr\u00ean m\u1ed9t con \u0111\u01b0\u1eddng v\u1edbi r\u1ea5t nhi\u1ec1u \u0111\u00e8n giao th\u00f4ng. V\u1edbi m\u1ed7i c\u1ed9t \u0111\u00e8n, c\u1eadu \u1ea5y bi\u1ebft \u0111\u00e8n xanh v\u00e0 \u0111\u00e8n \u0111\u1ecf s\u1ebd \u0111\u01b0\u1ee3c b\u1eadt bao l\u00e2u.</p> <p>Khi Luka b\u1eaft \u0111\u1ea7u \u0111i, t\u1ea5t c\u1ea3 \u0111\u00e8n giao th\u00f4ng \u0111\u1ec1u chuy\u1ec3n sang m\u00e0u \u0111\u1ecf v\u00e0 b\u1eaft \u0111\u1ea7u ch\u1ea1y. Luka \u0111i \u0111\u01b0\u1ee3c \\(1\\) \u0111\u01a1n v\u1ecb m\u1ed7i gi\u00e2y. Khi g\u1eb7p \u0111\u00e8n \u0111\u1ecf, c\u1eadu \u1ea5y s\u1ebd d\u1eebng l\u1ea1i v\u00e0 ch\u1edd \u0111\u00e8n xanh \u0111\u1ec3 \u0111i ti\u1ebfp.</p> <p>Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh t\u00ednh th\u1eddi gian \u0111\u1ec3 Luka \u0111i \u0111\u01b0\u1ee3c \u0111\u1ebfn cu\u1ed1i con \u0111\u01b0\u1eddng. Con \u0111\u01b0\u1eddng b\u1eaft \u0111\u1ea7u \u1edf \\(0\\) v\u00e0 k\u1ebft th\u00fac \u1edf \\(L\\) \u0111\u01a1n v\u1ecb.</p>"},{"location":"Problem/coci0708/Contest6/P2_SEMAFORI/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(N\\) v\u00e0 \\(L\\) \\((1 \\leq N \\leq 100, 1 \\leq L \\leq 100)\\), s\u1ed1 l\u01b0\u1ee3ng \u0111\u00e8n giao th\u00f4ng v\u00e0 \u0111\u1ed9 d\u00e0i c\u1ee7a con \u0111\u01b0\u1eddng.</li> <li>\\(N\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a \\(3\\) s\u1ed1 nguy\u00ean  \\(D, R, G\\) \\((1 \\leq D &lt; L, 1 \\leq R \\leq 100, 1\\leq G \\leq 100)\\), bi\u1ec3u di\u1ec5n m\u1ed9t c\u1ed9t \u0111\u00e8n giao th\u00f4ng. \\(D\\) l\u00e0 kho\u1ea3ng c\u00e1ch c\u1ee7a c\u1ed9t \u0111\u00e8n t\u00ednh t\u1eeb \u0111\u1ea7u con \u0111\u01b0\u1eddng, \\(R\\) v\u00e0 \\(G\\) l\u1ea7n l\u01b0\u1ee3t l\u00e0 th\u1eddi gian s\u00e1ng \u0111\u00e8n c\u1ee7a \u0111\u00e8n \u0111\u1ecf v\u00e0 \u0111\u00e8n xanh.</li> <li>C\u00e1c c\u1ed9t \u0111\u00e8n s\u1ebd \u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp theo th\u1ee9 t\u1ef1 \\(D\\) t\u0103ng d\u1ea7n. Kh\u00f4ng c\u00f3 \\(2\\) c\u1ed9t \u0111\u00e8n n\u00e0o \u1edf chung v\u1ecb tr\u00ed.</li> </ul>"},{"location":"Problem/coci0708/Contest6/P2_SEMAFORI/#output","title":"Output","text":"<ul> <li>In ra th\u1eddi gian c\u1ea7n \u0111\u1ec3 Luka \u0111i h\u1ebft con \u0111\u01b0\u1eddng.</li> </ul> <p>Test 1</p> Input <pre><code>2 10\n3 5 5\n5 2 2\n</code></pre> Output <pre><code>12\n</code></pre> <p>Test 2</p> Input <pre><code>4 30\n7 13 5\n14 4 4\n15 3 10\n25 1 1\n</code></pre> Output <pre><code>36\n</code></pre> Note <p>Trong test \u0111\u1ea7u ti\u00ean, Luka s\u1ebd d\u1eebng \u0111\u00e8n \u0111\u1ecf \\(2\\) gi\u00e2y \u1edf \u0111\u00e8n giao th\u00f4ng \u0111\u1ea7u ti\u00ean, g\u1eb7p \u0111\u00e8n xanh \u1edf c\u1ed9t \u0111\u00e8n th\u1ee9 \\(2\\) v\u00e0 \u0111i th\u1eb3ng t\u1edbi h\u1ebft \u0111\u01b0\u1eddng.</p>"},{"location":"Problem/coci0708/Contest6/P3_GRANICA/","title":"GRANICA","text":""},{"location":"Problem/coci0708/Contest6/P3_GRANICA/#granica","title":"GRANICA","text":"<p>Luka b\u1eaft \u0111\u1ea7u h\u00e0nh tr\u00ecnh xung quanh th\u1ebf gi\u1edbi v\u1edbi chi\u1ebfc xe t\u1ea3i c\u1ee7a m\u00ecnh. Tuy nhi\u00ean, anh g\u1eb7p v\u1ea5n \u0111\u1ec1 v\u1edbi bi\u00ean gi\u1edbi c\u1ee7a Slovenia. N\u00f3 l\u00e0 \u0111i\u1ec3m d\u1eabn v\u00e0o EU n\u00ean m\u1ecdi chi\u1ebfc xe t\u1ea3i \u0111\u1ec1u \u0111\u01b0\u1ee3c ki\u1ec3m tra k\u0129 l\u01b0\u1ee1ng. Vi\u1ec7c n\u00e0y t\u1ed1n r\u1ea5t nhi\u1ec1u th\u1eddi gian v\u00e0 Luka ph\u1ea3i ch\u1edd \u0111\u1ee3i nhi\u1ec1u gi\u1edd li\u1ec1n. \u0110\u1ec3 gi\u1ebft th\u1eddi gian, Luka ngh\u0129 ra nh\u1eefng tr\u00f2 ch\u01a1i v\u1ec1 logic v\u00e0 to\u00e1n. </p> <p>Trong s\u1ed1 nh\u1eefng tr\u00f2 ch\u01a1i c\u1eadu ngh\u0129 ra, c\u00f3 m\u1ed9t tr\u00f2 ch\u01a1i nh\u01b0 sau: \u0110\u1ea7u ti\u00ean, Luka \u0111\u1ecdc \\(N\\) bi\u1ec3n s\u1ed1 xe v\u00e0 vi\u1ebft ch\u00fang l\u00ean m\u1ed9t m\u1ea3nh gi\u1ea5y. Sau \u0111\u00f3, c\u1eadu t\u00ecm ra m\u1ed9t s\u1ed1 nguy\u00ean \\(M\\) l\u1edbn h\u01a1n \\(1\\) sao cho m\u1ecdi s\u1ed1 nguy\u00ean tr\u00ean t\u1edd gi\u1ea5y khi chia l\u1ea5y d\u01b0 cho \\(M\\) \u0111\u1ec1u ra c\u00f9ng m\u1ed9t k\u1ebft qu\u1ea3. Luka mu\u1ed1n t\u00ecm \u0111\u01b0\u1ee3c nhi\u1ec1u s\u1ed1 \\(M\\) th\u1ecfa m\u00e3n nh\u1ea5t c\u00f3 th\u1ec3.</p> <p>Cho bi\u1ebft \\(N\\) s\u1ed1 nguy\u00ean Luka \u0111\u00e3 vi\u1ebft, h\u00e3y t\u00ecm ra t\u1ea5t c\u1ea3 s\u1ed1 nguy\u00ean \\(M\\) th\u1ecfa m\u00e3n \u0111i\u1ec1u ki\u1ec7n \u0111\u1ec1 b\u00e0i.</p>"},{"location":"Problem/coci0708/Contest6/P3_GRANICA/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) \\((2 \\leq N \\leq 100)\\), s\u1ed1 l\u01b0\u1ee3ng s\u1ed1 nguy\u00ean c\u1ee7a Luka.</li> <li>\\(N\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean trong kho\u1ea3ng \\([1, 10^{9}]\\). D\u1eef li\u1ec7u \u0111\u1ea3m b\u1ea3o \\(N\\) s\u1ed1 nguy\u00ean n\u00e0y l\u00e0 ph\u00e2n bi\u1ec7t v\u00e0 lu\u00f4n t\u1ed3n t\u1ea1i \u00edt nh\u1ea5t m\u1ed9t s\u1ed1 \\(M\\) th\u1ecfa m\u00e3n.</li> </ul>"},{"location":"Problem/coci0708/Contest6/P3_GRANICA/#output","title":"Output","text":"<ul> <li>In ra t\u1ea5t c\u1ea3 s\u1ed1 nguy\u00ean \\(M\\) th\u1ecfa m\u00e3n theo th\u1ee9 t\u1ef1 b\u1ea5t k\u00ec.</li> </ul>"},{"location":"Problem/coci0708/Contest6/P3_GRANICA/#scoring","title":"Scoring","text":"<ul> <li>Subtask 1: \\(N \\leq 10000\\) \\((60\\%)\\).</li> <li>Subtask 2: Kh\u00f4ng c\u00f3 r\u00e0ng bu\u1ed9c g\u00ec th\u00eam.</li> </ul> <p>Test 1</p> Input <pre><code>3\n6\n34\n38\n</code></pre> Output <pre><code>2 4\n</code></pre> <p>Test 2</p> Input <pre><code>5\n5\n17\n23\n14\n83\n</code></pre> Output <pre><code>3\n</code></pre> Note <p>Trong test \u0111\u1ea7u ti\u00ean, m\u1ecdi s\u1ed1 trong d\u00e3y khi chia \\(2\\) \u0111\u1ec1u d\u01b0 \\(0\\) v\u00e0 khi chia \\(4\\) \u0111\u1ec1u d\u01b0 \\(2\\).</p>"},{"location":"Problem/coci0708/Contest6/P4_GEORGE/","title":"GEORGE","text":""},{"location":"Problem/coci0708/Contest6/P4_GEORGE/#george","title":"GEORGE","text":"<p>Tu\u1ea7n tr\u01b0\u1edbc, ng\u00e0i George \u0111\u00e3 \u0111\u1ebfn th\u0103m Croatia. V\u00ec ng\u00e0i George l\u00e0 m\u1ed9t ng\u01b0\u1eddi r\u1ea5t quan tr\u1ecdng, n\u00ean khi \u00f4ng \u1ea5y c\u00f3 m\u1eb7t tr\u00ean m\u1ed9t con ph\u1ed1, c\u1ea3nh s\u00e1t \u0111\u00e3 c\u1ea5m c\u00e1c ph\u01b0\u01a1ng ti\u1ec7n \u0111i v\u00e0o con ph\u1ed1 \u0111\u00f3. Tuy nhi\u00ean, c\u00e1c ph\u01b0\u01a1ng ti\u1ec7n \u0111\u00e3 v\u00e0o ph\u1ed1 tr\u01b0\u1edbc khi ng\u00e0i George \u0111\u1ebfn th\u00ec v\u1eabn c\u00f3 th\u1ec3 ti\u1ebfp t\u1ee5c di chuy\u1ec3n.</p> <p>Trong th\u1eddi gian ng\u00e0i George th\u0103m th\u00e0nh ph\u1ed1, Luka l\u00e1i xe t\u1ea3i \u0111i giao h\u00e0ng. Nh\u01b0ng do m\u1ed9t s\u1ed1 con ph\u1ed1 b\u1ecb ch\u1eb7n, anh \u1ea5y kh\u00f4ng th\u1ec3 giao h\u00e0ng \u0111\u00fang h\u1ea1n v\u00e0 su\u00fdt m\u1ea5t vi\u1ec7c. M\u1eb7c d\u00f9 m\u1ecdi chuy\u1ec7n \u0111\u00e3 qua, nh\u01b0ng b\u00e2y gi\u1edd Luka mu\u1ed1n bi\u1ebft th\u1eddi gian \u00edt nh\u1ea5t c\u1ea7n thi\u1ebft \u0111\u1ec3 ho\u00e0n th\u00e0nh vi\u1ec7c giao h\u00e0ng trong khi ng\u00e0i George \u0111ang c\u00f3 m\u1eb7t trong th\u00e0nh ph\u1ed1. Anh \u1ea5y bi\u1ebft tr\u01b0\u1edbc l\u1ed9 tr\u00ecnh m\u00e0 ng\u00e0i George \u0111\u00e3 \u0111i.</p> <p>Th\u00e0nh ph\u1ed1 c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1eb1ng c\u00e1c giao l\u1ed9 v\u00e0 c\u00e1c con \u0111\u01b0\u1eddng k\u1ebft n\u1ed1i ch\u00fang v\u1edbi nhau. V\u1edbi m\u1ed7i con \u0111\u01b0\u1eddng, Luka bi\u1ebft th\u1eddi gian c\u1ea7n \u0111\u1ec3 \u0111i qua con \u0111\u01b0\u1eddng \u0111\u00f3 (\u00d4ng George c\u0169ng c\u1ea7n m\u1ed9t kho\u1ea3ng th\u1eddi gian t\u01b0\u01a1ng t\u1ef1).</p> <p>V\u00ed d\u1ee5, n\u1ebfu \u00f4ng George b\u1eaft \u0111\u1ea7u \u0111i v\u00e0o m\u1ed9t con \u0111\u01b0\u1eddng \u1edf ph\u00fat th\u1ee9 \\(10\\) v\u00e0 c\u1ea7n \\(5\\) ph\u00fat \u0111\u1ec3 \u0111i qua, con \u0111\u01b0\u1eddng \u0111\u00f3 s\u1ebd b\u1ecb ch\u1eb7n trong ph\u00fat th\u1ee9 \\(10, 11, 12, 13\\) v\u00e0 \\(14\\). Luka c\u00f3 th\u1ec3 \u0111i v\u00e0o con \u0111\u01b0\u1eddng tr\u01b0\u1edbc ph\u00fat th\u1ee9 \\(10\\) ho\u1eb7c sau ph\u00fat \\(15\\).</p> <p>Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh t\u00ednh th\u1eddi gian \u00edt nh\u1ea5t Luka c\u1ea7n \u0111\u1ec3 giao h\u00e0ng, n\u1ebfu anh \u1ea5y b\u1eaft \u0111\u1ea7u l\u00e1i xe \\(K\\) ph\u00fat sau khi \u00f4ng George \u0111\u1ebfn th\u0103m.</p>"},{"location":"Problem/coci0708/Contest6/P4_GEORGE/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a \\(2\\) s\u1ed1 nguy\u00ean \\(N\\) v\u00e0 \\(M\\) \\((2 \\leq N \\leq 1000, 2 \\leq M \\leq 10000)\\), s\u1ed1 giao l\u1ed9 v\u00e0 con \u0111\u01b0\u1eddng trong th\u00e0nh ph\u1ed1, c\u00e1c giao l\u1ed9 \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(N\\).</li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a \\(4\\) s\u1ed1 nguy\u00ean \\(A, B, K\\) v\u00e0 \\(G\\) \\((1 \\leq A,B \\leq N,\\ 0 \\leq K \\leq 1000,\\ 0 \\leq G \\leq 1000)\\), l\u1ea7n l\u01b0\u1ee3t l\u00e0:<ul> <li>Giao l\u1ed9 Luka b\u1eaft \u0111\u1ea7u \u0111i.</li> <li>Giao l\u1ed9 Luka c\u1ea7n \u0111i t\u1edbi.</li> <li>Th\u1eddi gian Luka b\u1eaft \u0111\u1ea7u \u0111i sau khi \u00f4ng George \u0111\u1ebfn. (Luka b\u1eaft \u0111\u1ea7u \u1edf giao l\u1ed9 \\(A\\) sau khi \u00f4ng George \u0111\u00e3 \u0111\u1ebfn \\(K\\) ph\u00fat)</li> <li>S\u1ed1 giao l\u1ed9 tr\u00ean \u0111\u01b0\u1eddng \u0111i c\u1ee7a \u00f4ng George.</li> </ul> </li> <li>D\u00f2ng th\u1ee9 ba ch\u1ee9a \\(G\\) s\u1ed1 nguy\u00ean l\u00e0 c\u00e1c giao l\u1ed9 m\u00e0 \u00f4ng George s\u1ebd \u0111i qua. M\u1ed7i c\u1eb7p giao l\u1ed9 k\u1ec1 nhau th\u1ec3 hi\u1ec7n con \u0111\u01b0\u1eddng m\u00e0 \u00f4ng \u1ea5y s\u1ebd \u0111i qua. D\u1eef li\u1ec7u \u0111\u1ea3m b\u1ea3o con \u0111\u01b0\u1eddng \u0111\u00f3 lu\u00f4n t\u1ed3n t\u1ea1i v\u00e0 v\u1edbi m\u1ed7i con \u0111\u01b0\u1eddng, \u00f4ng George ch\u1ec9 \u0111i qua \u0111\u00fang m\u1ed9t l\u1ea7n.</li> <li>\\(M\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a \\(3\\) s\u1ed1 nguy\u00ean \\(A, B\\) v\u00e0 \\(L\\), th\u1ec3 hi\u1ec7n m\u1ed9t con \u0111\u01b0\u1eddng n\u1ed1i gi\u1eefa \\(A\\) v\u00e0 \\(B\\) v\u00e0 c\u1ea7n \\(L\\ (1 \\leq L \\leq 1000)\\) ph\u00fat \u0111\u1ec3 \u0111i qua.</li> </ul>"},{"location":"Problem/coci0708/Contest6/P4_GEORGE/#output","title":"Output","text":"<ul> <li>In ra th\u1eddi gian ng\u1eafn nh\u1ea5t (t\u00ednh theo ph\u00fat) \u0111\u1ec3 Luka giao h\u00e0ng.</li> </ul> <p>Test 1</p> Input <pre><code>6 5\n1 6 20 4\n5 3 2 4\n1 2 2\n2 3 8\n2 4 3\n3 6 10\n3 5 15 \n</code></pre> Output <pre><code>21\n</code></pre> <p>Test 2</p> Input <pre><code>8 9\n1 5 5 5\n1 2 3 4 5\n1 2 8\n2 7 4\n2 3 10\n6 7 40\n3 6 5\n6 8 3\n4 8 4\n4 5 5\n3 4 23 \n</code></pre> Output <pre><code>40\n</code></pre>"},{"location":"Problem/coci0708/Contest6/P5_PRINCEZA/","title":"PRINCEZA","text":""},{"location":"Problem/coci0708/Contest6/P5_PRINCEZA/#princeza","title":"PRINCEZA","text":"<p>Luka \u0111\u1ed7 xe t\u1ea3i c\u1ee7a m\u00ecnh g\u1ea7n m\u1ed9t c\u00e1i h\u1ed3. H\u1ed3 n\u00e0y c\u00f3 m\u1ed9t con \u1ebfch t\u00ean l\u00e0 Barica sinh s\u1ed1ng, n\u00f3 nh\u1ea3y qua \\(N\\) c\u00e1i c\u00e2y tr\u00f4i n\u1ed5i tr\u00ean m\u1eb7t h\u1ed3. Bi\u1ebft \u0111\u01b0\u1ee3c kh\u00e1 nhi\u1ec1u c\u00e2u chuy\u1ec7n c\u1ed5 t\u00edch, Luka hi\u1ec3u r\u1eb1ng n\u1ebfu anh ta h\u00f4n Barica, n\u00f3 s\u1ebd bi\u1ebfn th\u00e0nh m\u1ed9t n\u00e0ng c\u00f4ng ch\u00faa xinh \u0111\u1eb9p. Tuy nhi\u00ean, tr\u01b0\u1edbc ti\u00ean anh ta ph\u1ea3i b\u1eaft \u0111\u01b0\u1ee3c n\u00f3!</p> <p>Gi\u1ea3 s\u1eed nh\u00ecn t\u1eeb tr\u00ean cao xu\u1ed1ng, v\u1ecb tr\u00ed c\u1ee7a m\u1ed9t c\u00e2y tr\u00ean m\u1eb7t h\u1ed3 c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c x\u00e1c \u0111\u1ecbnh b\u1eb1ng m\u1ed9t c\u1eb7p t\u1ecda \u0111\u1ed9. T\u1eeb c\u00e2y \u1edf v\u1ecb tr\u00ed \\((x, y)\\), Barica c\u00f3 th\u1ec3 nh\u1ea3y:</p> <ul> <li> <p>\u0110\u1ebfn c\u00e2y \\((x+P, y+P)\\), v\u1edbi \\(P\\) l\u00e0 s\u1ed1 nguy\u00ean d\u01b0\u01a1ng b\u1ea5t k\u1ef3. H\u01b0\u1edbng n\u00e0y g\u1ecdi l\u00e0 h\u01b0\u1edbng \\(A\\).</p> </li> <li> <p>\u0110\u1ebfn c\u00e2y \\((x+P, y\u2212P)\\), v\u1edbi \\(P\\) l\u00e0 s\u1ed1 nguy\u00ean d\u01b0\u01a1ng b\u1ea5t k\u1ef3. H\u01b0\u1edbng n\u00e0y g\u1ecdi l\u00e0 h\u01b0\u1edbng \\(B\\).</p> </li> <li> <p>\u0110\u1ebfn c\u00e2y \\((x\u2212P, y+P)\\), v\u1edbi $P l\u00e0 s\u1ed1 nguy\u00ean d\u01b0\u01a1ng b\u1ea5t k\u1ef3. H\u01b0\u1edbng n\u00e0y g\u1ecdi l\u00e0 h\u01b0\u1edbng \\(C\\).</p> </li> <li> <p>\u0110\u1ebfn c\u00e2y \\((x\u2212P, y\u2212P)\\), v\u1edbi \\(P\\) l\u00e0 s\u1ed1 nguy\u00ean d\u01b0\u01a1ng b\u1ea5t k\u1ef3. H\u01b0\u1edbng n\u00e0y g\u1ecdi l\u00e0 h\u01b0\u1edbng \\(D\\).</p> </li> </ul> <p>Barica ch\u1ecdn m\u1ed9t trong b\u1ed1n h\u01b0\u1edbng tr\u00ean v\u00e0 nh\u1ea3y \u0111\u1ebfn c\u00e2y \u0111\u1ea7u ti\u00ean theo h\u01b0\u1edbng \u0111\u00f3. N\u1ebfu kh\u00f4ng c\u00f3 c\u00e2y n\u00e0o theo h\u01b0\u1edbng \u0111\u00e3 ch\u1ecdn, Barica s\u1ebd \u0111\u1ee9ng y\u00ean. Sau khi Barica nh\u1ea3y, c\u00e2y m\u00e0 n\u00f3 v\u1eeba nh\u1ea3y \u0111i s\u1ebd ch\u00ecm xu\u1ed1ng v\u00e0 bi\u1ebfn m\u1ea5t.</p> <p>Bi\u1ebft \u0111\u01b0\u1ee3c v\u1ecb tr\u00ed c\u1ee7a c\u00e1c c\u00e2y v\u00e0 tr\u00ecnh t\u1ef1 c\u00e1c h\u01b0\u1edbng m\u00e0 Barica ch\u1ecdn, Luka mu\u1ed1n x\u00e1c \u0111\u1ecbnh t\u1ecda \u0111\u1ed9 c\u1ee7a c\u00e2y m\u00e0 Barica s\u1ebd d\u1eebng l\u1ea1i. Anh ta s\u1ebd \u0111\u1ee3i \u1edf \u0111\u00f3, ph\u1ee5c k\u00edch Barica v\u00e0 h\u00f4n n\u00f3.</p> <p>H\u00e3y vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh gi\u1ea3i b\u00e0i to\u00e1n c\u1ee7a Luka v\u00e0 gi\u00fap anh ta bi\u1ebfn Barica th\u00e0nh m\u1ed9t n\u00e0ng c\u00f4ng ch\u00faa xinh \u0111\u1eb9p.</p>"},{"location":"Problem/coci0708/Contest6/P5_PRINCEZA/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(N\\) v\u00e0 \\(K\\) \\((1 \\leq N, K \\leq 100000)\\), s\u1ed1 l\u01b0\u1ee3ng c\u00e2y v\u00e0 s\u1ed1 l\u1ea7n nh\u1ea3y c\u1ee7a Barica.</li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a \\(K\\) k\u00ed t\u1ef1 'A', 'B', 'C' ho\u1eb7c 'D'. K\u00ed t\u1ef1 th\u1ee9 \\(i\\) th\u1ec3 hi\u1ec7n h\u01b0\u1edbng c\u1ee7a l\u1ea7n nh\u1ea3y th\u1ee9 \\(i\\) c\u1ee7a Barica.</li> <li>\\(N\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a hai s\u1ed1 nguy\u00ean \\(X\\) v\u00e0 \\(Y\\) \\((0 \\leq X,Y \\leq 1000000000)\\), t\u1ecda \u0111\u1ed9 c\u1ee7a m\u1ed9t c\u00e1i c\u00e2y. Ban \u0111\u1ea7u Barica \u1edf c\u00e2y \u0111\u1ea7u ti\u00ean.</li> </ul>"},{"location":"Problem/coci0708/Contest6/P5_PRINCEZA/#output","title":"Output","text":"<ul> <li>In ra t\u1ecda \u0111\u1ed9 cu\u1ed1i c\u00f9ng c\u1ee7a Barica.</li> </ul> <p>Test 1</p> Input <pre><code>7 5\nACDBB\n5 6\n8 9\n4 13\n1 10\n7 4\n10 9\n3 7\n</code></pre> Output <pre><code>7 4\n</code></pre> <p>Test 2</p> Input <pre><code>6 12\nAAAAAABCCCDD\n1 1\n2 2\n3 3\n4 4\n5 3\n6 2  \n</code></pre> Output <pre><code>5 3\n</code></pre>"},{"location":"Problem/coci0708/Contest6/P6_CESTARINE/","title":"CESTARINE","text":""},{"location":"Problem/coci0708/Contest6/P6_CESTARINE/#cestarine","title":"CESTARINE","text":"<p>Trong m\u1ed9t ng\u00e0y, c\u00f3 \\(N\\) chi\u1ebfc xe t\u1ea3i c\u1ee7a Luka di chuy\u1ec3n tr\u00ean m\u1ed9t con \u0111\u01b0\u1eddng cao t\u1ed1c. Con \u0111\u01b0\u1eddng cao t\u1ed1c n\u00e0y c\u00f3 nhi\u1ec1u l\u1ed1i ra v\u00e0 l\u1ed1i v\u00e0o. M\u1ed9t l\u1ed1i ra c\u00f3 m\u1ed9t s\u1ed1 nh\u1ea5t \u0111\u1ecbnh s\u1ebd n\u1eb1m c\u00f9ng v\u1ecb tr\u00ed v\u1edbi l\u1ed1i v\u00e0o c\u00f3 c\u00f9ng s\u1ed1 \u0111\u00f3.</p> <p>Khi v\u00e0o \u0111\u01b0\u1eddng cao t\u1ed1c, t\u00e0i x\u1ebf xe t\u1ea3i s\u1ebd nh\u1eadn \u0111\u01b0\u1ee3c m\u1ed9t v\u00e9 ghi r\u00f5 l\u1ed1i v\u00e0o m\u00e0 anh ta \u0111\u00e3 s\u1eed d\u1ee5ng. Khi ra kh\u1ecfi \u0111\u01b0\u1eddng cao t\u1ed1c, t\u00e0i x\u1ebf ph\u1ea3i tr\u1ea3 m\u1ed9t kho\u1ea3n ph\u00ed b\u1eb1ng v\u1edbi gi\u00e1 tr\u1ecb tuy\u1ec7t \u0111\u1ed1i c\u1ee7a hi\u1ec7u s\u1ed1 gi\u1eefa s\u1ed1 l\u1ed1i v\u00e0o v\u00e0 s\u1ed1 l\u1ed1i ra. V\u00ed d\u1ee5, n\u1ebfu v\u00e9 ghi r\u1eb1ng anh ta \u0111\u00e3 v\u00e0o t\u1ea1i l\u1ed1i v\u00e0o s\u1ed1 \\(30\\), th\u00ec khi ra t\u1ea1i l\u1ed1i ra s\u1ed1 \\(12\\), anh ta s\u1ebd ph\u1ea3i tr\u1ea3 \\(18\\) \u0111\u01a1n v\u1ecb ph\u00ed.</p> <p>Luka \u0111\u00e3 t\u00ecm ra m\u1ed9t c\u00e1ch \u0111\u1ec3 ti\u1ebft ki\u1ec7m ti\u1ec1n ph\u00ed m\u00e0 c\u00f4ng ty anh ta ph\u1ea3i chi tr\u1ea3 h\u00e0ng ng\u00e0y. Hai t\u00e0i x\u1ebf b\u1ea5t k\u1ef3 c\u00f3 th\u1ec3 g\u1eb7p nhau tr\u00ean \u0111\u01b0\u1eddng cao t\u1ed1c v\u00e0 trao \u0111\u1ed5i v\u00e9, ngay c\u1ea3 khi l\u1ed9 tr\u00ecnh c\u1ee7a h\u1ecd kh\u00f4ng tr\u00f9ng nhau. C\u00e1c v\u00e9 c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c trao \u0111\u1ed5i m\u1ed9t s\u1ed1 l\u1ea7n t\u00f9y \u00fd.</p> <p>Tuy nhi\u00ean, m\u1ed9t t\u00e0i x\u1ebf kh\u00f4ng th\u1ec3 s\u1eed d\u1ee5ng m\u1ed9t l\u1ed1i ra n\u1ebfu v\u00e9 c\u1ee7a anh ta ghi r\u1eb1ng anh ta \u0111\u00e3 v\u00e0o t\u1eeb ch\u00ednh l\u1ed1i v\u00e0o \u0111\u00f3, v\u00ec \u0111i\u1ec1u \u0111\u00f3 s\u1ebd g\u00e2y nghi ng\u1edd.</p> <p>H\u00e3y vi\u1ebft m\u1ed9t ch\u01b0\u01a1ng tr\u00ecnh t\u00ednh to\u00e1n t\u1ed5ng ph\u00ed t\u1ed1i thi\u1ec3u m\u00e0 c\u00e1c t\u00e0i x\u1ebf c\u00f3 th\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c b\u1eb1ng c\u00e1ch trao \u0111\u1ed5i v\u00e9.</p>"},{"location":"Problem/coci0708/Contest6/P6_CESTARINE/#input","title":"Input","text":"<ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a m\u1ed9t s\u1ed1 nguy\u00ean \\(N\\) \\((1 \\leq N \\leq 100000)\\), s\u1ed1 l\u01b0\u1ee3ng xe t\u1ea3i.</li> <li>\\(N\\) d\u00f2ng ti\u1ebfp theo, m\u1ed7i d\u00f2ng ch\u1ee9a hai s\u1ed1 nguy\u00ean ph\u00e2n bi\u1ec7t trong kho\u1ea3ng t\u1eeb \\(1\\) \u0111\u1ebfn \\(1000000\\), l\u1ea7n l\u01b0\u1ee3t th\u1ec3 hi\u1ec7n s\u1ed1 c\u1ee7a l\u1ed1i v\u00e0o v\u00e0 l\u1ed1i ra c\u1ee7a nh\u1eefng chi\u1ebfc xe.</li> <li>L\u1ed1i v\u00e0o v\u00e0 l\u1ed1i ra c\u1ee7a m\u1ed7i xe l\u00e0 duy nh\u1ea5t, t\u1ee9c l\u00e0 kh\u00f4ng c\u00f3 hai xe n\u00e0o c\u00f3 c\u00f9ng l\u1ed1i v\u00e0o ho\u1eb7c l\u1ed1i ra.</li> </ul>"},{"location":"Problem/coci0708/Contest6/P6_CESTARINE/#output","title":"Output","text":"<ul> <li>In ra s\u1ed1 ti\u1ec1n t\u1ed1i thi\u1ec3u m\u00e0 c\u00f4ng ty c\u1ee7a Luka ph\u1ea3i chi tr\u1ea3.</li> </ul> <p>Note: S\u1eed d\u1ee5ng ki\u1ec3u d\u1eef li\u1ec7u 64-bit (long long trong C/C++, int64 trong Pascal)</p> <p>Test 1</p> Input <pre><code>3\n3 65\n45 10\n60 25 \n</code></pre> Output <pre><code>32\n</code></pre> <p>Test 2</p> Input <pre><code>3\n5 5\n6 7\n8 8\n</code></pre> Output <pre><code>5\n</code></pre> Note <p>Trong test \u0111\u1ea7u ti\u00ean, t\u00e0i x\u1ebf s\u1ed1 \\(1\\) v\u00e0 t\u00e0i x\u1ebf s\u1ed1 \\(3\\) s\u1ebd \u0111\u1ed5i v\u00e9 cho nhau. Sau \u0111\u00f3, t\u00e0i x\u1ebf s\u1ed1 \\(2\\) l\u1ea1i \u0111\u1ed5i v\u00e9 v\u1edbi t\u00e0i x\u1ebf s\u1ed1 \\(3\\). L\u00fac n\u00e0y, s\u1ed1 v\u00e9 c\u1ee7a \\(3\\) t\u00e0i x\u1ebf l\u1ea7n l\u01b0\u1ee3t l\u00e0 \\(60\\), \\(3\\) v\u00e0 \\(45\\). T\u1ed5ng s\u1ed1 ti\u1ec1n m\u00e0 c\u00f4ng ty ph\u1ea3i chi tr\u1ea3 s\u1ebd l\u00e0 \\(|65-60|+|10-3|+|25-45| = 32\\)</p>"},{"location":"Subject/XSTK/Exercise2/b4/","title":"B4","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np <p>np.pi np.e</p> In\u00a0[\u00a0]: Copied! <pre>def Laplace(x):\n    res = 0\n    numrep = 10**7\n    dt = x / numrep\n    for i in range(numrep):\n        t = i * dt\n        f = np.e ** (-t * t / 2)\n        res += f * dt\n        \n    res /= np.sqrt(2 * np.pi)\n    return res\n</pre> def Laplace(x):     res = 0     numrep = 10**7     dt = x / numrep     for i in range(numrep):         t = i * dt         f = np.e ** (-t * t / 2)         res += f * dt              res /= np.sqrt(2 * np.pi)     return res In\u00a0[\u00a0]: Copied! <pre>print(Laplace(1.96))\n</pre> print(Laplace(1.96))"},{"location":"Subject/XSTK/Exercise2/homework/","title":"B\u00e0i t\u1eadp v\u1ec1 nh\u00e0 m\u00f4n X\u00e1c su\u1ea5t th\u1ed1ng k\u00ea:","text":"<ul> <li>Ng\u01b0\u1eddi th\u1ef1c hi\u1ec7n: Nguy\u1ec5n Tr\u01b0\u1eddng Giang.</li> <li>MSSV: 24520011</li> </ul>"},{"location":"Subject/XSTK/Exercise2/homework/#bai-1","title":"B\u00e0i 1.","text":"<p>B\u1eafn 3 vi\u00ean \u0111\u1ea1n m\u1ed9t c\u00e1ch \u0111\u1ed9c l\u1eadp v\u00e0o m\u1ed9t bia. X\u00e1c su\u1ea5t tr\u00fang bia c\u1ee7a m\u1ed7i vi\u00ean l\u1ea7n l\u01b0\u1ee3t l\u00e0 0.6; 0.9; 0.7. T\u00ecm x\u00e1c su\u1ea5t: </p> <ol> <li> <p>C\u00f3 \u0111\u00fang m\u1ed9t vi\u00ean tr\u00fang \u0111\u00edch.</p> </li> <li> <p>C\u00f3 \u00edt nh\u1ea5t m\u1ed9t vi\u00ean tr\u00fang \u0111\u00edch.</p> </li> </ol> <p></p>"},{"location":"Subject/XSTK/Exercise2/homework/#bai-2","title":"B\u00e0i 2.","text":"<p>C\u00f3 3 h\u1ed9p ph\u1ea5n. </p> <ul> <li>H\u1ed9p I c\u00f3 15 vi\u00ean t\u1ed1t v\u00e0 5 vi\u00ean x\u1ea5u; </li> <li>H\u1ed9p II c\u00f3 10 vi\u00ean t\u1ed1t v\u00e0 4 vi\u00ean x\u1ea5u; </li> <li>H\u1ed9p III c\u00f3 20 vi\u00ean t\u1ed1t v\u00e0 10 vi\u00ean x\u1ea5u. </li> <li>L\u1ea5y ng\u1eabu nhi\u00ean m\u1ed7i h\u1ed9p 1 vi\u00ean ph\u1ea5n. T\u00ecm x\u00e1c su\u1ea5t \u0111\u01b0\u1ee3c \u00edt nh\u1ea5t m\u1ed9t vi\u00ean ph\u1ea5n t\u1ed1t.</li> </ul> <p></p>"},{"location":"Subject/XSTK/Exercise2/homework/#bai-3","title":"B\u00e0i 3.","text":"<ul> <li>Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh t\u00ednh g\u1ea7n \u0111\u00fang s\u1ed1 Pi</li> </ul> <p>Code: <pre><code>#include&lt;bits/stdc++.h&gt;\nusing namespace std;\nusing ll = long long;\n\nmt19937_64 rngll(chrono :: steady_clock :: now().time_since_epoch().count());\nll random(ll l, ll r) {\n    return uniform_int_distribution&lt;ll&gt;(l, r)(rngll);\n}\n\ndouble Randfloat() {\n    double num = random(-1e18, 1e18);\n    num /= 1e18;\n    return num;\n}\n\nint main() {\n    int cntin(0), cntall(0);\n\n    for (int i = 1; i &lt;= 1e9; i ++) {\n        double u = Randfloat();\n        double v = Randfloat();\n\n        double d = sqrt(abs(u * u) + abs(v * v));\n\n        cntall ++;\n        if (d &lt;= 1.0) cntin ++;\n    }\n    cout &lt;&lt; setprecision(10) &lt;&lt; fixed &lt;&lt; double(cntin) / double(cntall) * 4.0 &lt;&lt; '\\n';\n}\n</code></pre></p> <p>Output: <pre><code>3.1415237600\n</code></pre></p>"},{"location":"Subject/XSTK/Exercise2/homework/#bai-4","title":"B\u00e0i 4.","text":"<p>Vi\u1ebft ch\u01b0\u01a1ng tr\u00ecnh t\u00ednh g\u1ea7n \u0111\u00fang h\u00e0m Laplace  (\u0110\u00e1p s\u1ed1 g\u1ea7n \u0111\u00fang: Ch\u1eb3ng h\u1ea1n khi \\(x= 1.96\\) th\u00ec gi\u00e1 tr\u1ecb h\u00e0m Laplace l\u00e0 \\(0.475\\))</p> <p>Code:</p> <pre><code>import numpy as np\n\n# np.pi np.e\n\ndef Laplace(x):\n    res = 0\n    numrep = 10**7\n    dt = x / numrep\n    for i in range(numrep):\n        t = i * dt\n        f = np.e ** (-t * t / 2)\n        res += f * dt\n\n    res /= np.sqrt(2 * np.pi)\n    return res\n\nprint(Laplace(1.96))\n</code></pre> <p>Output:</p> <pre><code>0.4750021382210453\n</code></pre>"},{"location":"Subject/XSTK/Exercise3/homework/","title":"B\u00e0i t\u1eadp v\u1ec1 nh\u00e0 bu\u1ed5i 3","text":""},{"location":"Subject/XSTK/Exercise3/homework/#cau-1-hai-nguoi-a-va-b-cung-choi-tro-choi-nhu-sau","title":"C\u00e2u 1. Hai ng\u01b0\u1eddi A v\u00e0 B c\u00f9ng ch\u01a1i tr\u00f2 ch\u01a1i nh\u01b0 sau:","text":"<p>C\u1ea3 hai lu\u00e2n phi\u00ean l\u1ea5y m\u1ed7i l\u1ea7n 1 vi\u00ean bi t\u1eeb m\u1ed9t h\u1ed9p \u0111\u1ef1ng 2 bi tr\u1eafng v\u00e0 5 bi \u0111en (bi \u0111\u01b0\u1ee3c l\u1ea5y ra kh\u00f4ng tr\u1ea3 l\u1ea1i h\u1ed9p). Ng\u01b0\u1eddi n\u00e0o l\u1ea5y \u0111\u01b0\u1ee3c bi tr\u1eafng tr\u01b0\u1edbc th\u00ec th\u1eafng cu\u1ed9c. Gi\u1ea3 s\u1eed A l\u1ea5y tr\u01b0\u1edbc, t\u00ednh x\u00e1c su\u1ea5t A th\u1eafng cu\u1ed9c?</p>"},{"location":"Subject/XSTK/Exercise3/homework/#cau-2-cho-du-lieu-iem-thi-hoc-phan-xstk-trong-link-tai-lieu","title":"C\u00e2u 2. Cho d\u1eef li\u1ec7u \u0111i\u1ec3m thi h\u1ecdc ph\u1ea7n XSTK trong link t\u00e0i li\u1ec7u.","text":"<p>a. T\u00ednh th\u00eam c\u1ed9t \u0111i\u1ec3m h\u1ecdc ph\u1ea7n v\u1edbi c\u00e1c tr\u1ecdng s\u1ed1 tp l\u00e0 20, 20, 60%.</p> <p>b. Quan s\u00e1t ng\u1eabu nhi\u00ean 1 SV, t\u00ednh x\u00e1c su\u1ea5t SV \u0111\u00f3 qua h\u1ecdc ph\u1ea7n</p> <p>c. Quan s\u00e1t n/n 1 sv th\u1ea5y sv \u0111\u00f3 thi ch\u01b0a \u0111\u1ea1t k\u1ef3 thi gi\u1eefa k\u1ef3, t\u00ednh x\u00e1c su\u1ea5t ng\u01b0\u1eddi \u0111\u00f3 th\u00ec \u0111\u1ea1t k\u1ef3 thi cu\u1ed1i k\u1ef3. </p> <p>d. Quan s\u00e1t n/n 1 SV th\u00ec th\u1ea5y SV \u0111\u00f3 thi \u0111\u1ea1t. T\u00ednh x\u00e1c su\u1ea5t SV n\u00e0y thi ch\u01b0a \u0111\u1ea1t \u1edf k\u1ef3 thi gi\u0169a k\u1ef3</p>"},{"location":"home/","title":"Gi\u1edbi Thi\u1ec7u","text":""},{"location":"home/#xin-chao","title":"Xin ch\u00e0o!","text":"<ul> <li>Ch\u00e0o m\u1eebng b\u1ea1n \u0111\u1ebfn v\u1edbi trang t\u00e0i li\u1ec7u c\u1ee7a t\u00f4i! T\u00f4i l\u00e0 m\u1ed9t l\u1eadp tr\u00ecnh vi\u00ean \u0111am m\u00ea v\u1ec1 l\u1eadp tr\u00ecnh thi \u0111\u1ea5u, thu\u1eadt to\u00e1n v\u00e0 ph\u00e1t tri\u1ec3n ph\u1ea7n m\u1ec1m. V\u1edbi ni\u1ec1m y\u00eau th\u00edch v\u1ec1 c\u00f4ng ngh\u1ec7 v\u00e0 gi\u1ea3i quy\u1ebft c\u00e1c b\u00e0i to\u00e1n ph\u1ee9c t\u1ea1p, t\u00f4i lu\u00f4n t\u00ecm ki\u1ebfm nh\u1eefng th\u1eed th\u00e1ch m\u1edbi \u0111\u1ec3 n\u00e2ng cao k\u1ef9 n\u0103ng v\u00e0 m\u1edf r\u1ed9ng ki\u1ebfn th\u1ee9c c\u1ee7a m\u00ecnh.</li> </ul>"},{"location":"home/#thong-tin-ban-than","title":"Th\u00f4ng tin b\u1ea3n th\u00e2n","text":"<ul> <li>Sinh vi\u00ean \u0110\u1ea1i h\u1ecdc C\u00f4ng Ngh\u1ec7 Th\u00f4ng Tin (UIT) </li> <li>\u0110am m\u00ea l\u1eadp tr\u00ecnh thi \u0111\u1ea5u, ph\u00e1t tri\u1ec3n web v\u00e0 c\u00f4ng ngh\u1ec7 ph\u1ea7n m\u1ec1m  </li> <li>Th\u00e0nh th\u1ea1o c\u00e1c ng\u00f4n ng\u1eef: C++, Python, JavaScript, HTML/CSS </li> <li>Th\u00edch tham gia c\u00e1c cu\u1ed9c thi l\u1eadp tr\u00ecnh nh\u01b0 Codeforces, AtCoder, LeetCode </li> </ul>"},{"location":"home/#ket-noi-voi-toi","title":"\ud83c\udf10 K\u1ebft N\u1ed1i V\u1edbi T\u00f4i","text":"<p>\ud83d\udccc Facebook: Nguy\u1ec5n Tr\u01b0\u1eddng Giang \ud83d\udccc GitHub: HiGiangcoder \ud83d\udccc Zalo &amp; \u0110i\u1ec7n tho\u1ea1i: 0385630306  </p>"},{"location":"home/#tai-lieu-du-an","title":"\ud83d\udcd6 T\u00e0i Li\u1ec7u &amp; D\u1ef1 \u00c1n","text":"<ul> <li>Trang n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 l\u01b0u tr\u1eef c\u00e1c t\u00e0i li\u1ec7u h\u1ecdc t\u1eadp, h\u01b0\u1edbng d\u1eabn l\u1eadp tr\u00ecnh, c\u0169ng nh\u01b0 c\u00e1c d\u1ef1 \u00e1n c\u00e1 nh\u00e2n m\u00e0 t\u00f4i \u0111ang th\u1ef1c hi\u1ec7n. H\u00e3y theo d\u00f5i \u0111\u1ec3 c\u1eadp nh\u1eadt nh\u1eefng b\u00e0i vi\u1ebft v\u00e0 chia s\u1ebb h\u1eefu \u00edch!  </li> </ul> <ul> <li>C\u1ea3m \u01a1n b\u1ea1n \u0111\u00e3 gh\u00e9 th\u0103m! N\u1ebfu b\u1ea1n c\u00f3 c\u00e2u h\u1ecfi ho\u1eb7c mu\u1ed1n h\u1ee3p t\u00e1c, \u0111\u1eebng ng\u1ea7n ng\u1ea1i li\u00ean h\u1ec7 v\u1edbi t\u00f4i nh\u00e9!</li> </ul>"},{"location":"home/about/","title":"About","text":"<p>Ch\u01b0a c\u00f3 th\u00f4ng tin g\u00ec...</p>"},{"location":"other/other/","title":"Other","text":"<p>Ch\u01b0a c\u00f3 th\u00f4ng tin g\u00ec...</p>"}]}