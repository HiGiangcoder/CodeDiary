<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=../04_transfer_learning_in_tensorflow_part_1_feature_extraction/ rel=prev><link href=../../OOP_Practice/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.7"><title>10 Time series forecasting - My Documentation</title><link rel=stylesheet href=../../../assets/stylesheets/main.8608ea7d.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Merriweather";--md-code-font:"JetBrains Mono"}</style><link rel=stylesheet href=../../../assets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script> <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#10-milestone-project-3-time-series-forecasting-in-tensorflow-bitpredict class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="My Documentation" class="md-header__button md-logo" aria-label="My Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> My Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 10 Time series forecasting </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../home/ class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../AI_Model/lenet/lenet5/ class=md-tabs__link> Kiến thức lập trình </a> </li> <li class=md-tabs__item> <a href=../../../Subject/XSTK/Exercise2/homework/ class=md-tabs__link> Môn học </a> </li> <li class=md-tabs__item> <a href=../../../Problem/general/ class=md-tabs__link> CP-Problem </a> </li> <li class=md-tabs__item> <a href=../../../other/other/ class=md-tabs__link> Other </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="My Documentation" class="md-nav__button md-logo" aria-label="My Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> My Documentation </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../../../home/ class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../home/about/ class=md-nav__link> <span class=md-ellipsis> About </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Kiến thức lập trình </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Kiến thức lập trình </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1> <label class=md-nav__link for=__nav_2_1 id=__nav_2_1_label tabindex=0> <span class=md-ellipsis> Các kiến trúc, mô hình AI và các kĩ thuật </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Các kiến trúc, mô hình AI và các kĩ thuật </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_1> <label class=md-nav__link for=__nav_2_1_1 id=__nav_2_1_1_label tabindex=0> <span class=md-ellipsis> Lenet-5 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_1> <span class="md-nav__icon md-icon"></span> Lenet-5 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenet5/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenetpart1/ class=md-nav__link> <span class=md-ellipsis> Tutorial part 1 </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenetpart2/ class=md-nav__link> <span class=md-ellipsis> Tutorial part 2 </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/Lenet5_MNIST/ class=md-nav__link> <span class=md-ellipsis> Lenet-5 (Final Project 1) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_2> <label class=md-nav__link for=__nav_2_1_2 id=__nav_2_1_2_label tabindex=0> <span class=md-ellipsis> UNet </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_2> <span class="md-nav__icon md-icon"></span> UNet </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../AI_Model/unet/unet/ class=md-nav__link> <span class=md-ellipsis> Unet tutorial </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/unet/unet_model/ class=md-nav__link> <span class=md-ellipsis> UNet (Final Project 2) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../AI_Model/essential/essential/ class=md-nav__link> <span class=md-ellipsis> Các kiến thức cần thiết </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> Python </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Python/Numpy_1/ class=md-nav__link> <span class=md-ellipsis> Numpy 1 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Numpy_2/ class=md-nav__link> <span class=md-ellipsis> Numpy 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Python_Pandas_exercise1/ class=md-nav__link> <span class=md-ellipsis> Pandas exercise 1 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas2/ class=md-nav__link> <span class=md-ellipsis> Pandas 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Python_Pandas_exercise2/ class=md-nav__link> <span class=md-ellipsis> Pandas exercise 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas_Join_Combine/ class=md-nav__link> <span class=md-ellipsis> Pandas Join Combine </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> PyTorch </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> PyTorch </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Pytorch/00_pytorch_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 0 Pytorch Fundamentals (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/00_pytorch_and_deep_learning_fundamentals.pdf class=md-nav__link> <span class=md-ellipsis> 0 Deep learning fundamentals (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/01_pytorch_workflow/ class=md-nav__link> <span class=md-ellipsis> 1 Pytorch workflow (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/01_pytorch_workflow.pdf class=md-nav__link> <span class=md-ellipsis> 1 Pytorch workflow (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/02_pytorch_classification/ class=md-nav__link> <span class=md-ellipsis> 2 classification (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/03_pytorch_computer_vision/ class=md-nav__link> <span class=md-ellipsis> 3 Computer vision (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/03_pytorch_computer_vision.pdf class=md-nav__link> <span class=md-ellipsis> 3 Compupter vision (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/04_pytorch_custom_datasets/ class=md-nav__link> <span class=md-ellipsis> 4 Custom datasets (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/05_pytorch_going_modular.pdf class=md-nav__link> <span class=md-ellipsis> 5 Going moduler (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/06_pytorch_transfer_learning/ class=md-nav__link> <span class=md-ellipsis> 6 Transfer learning (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/06_pytorch_transfer_learning.pdf class=md-nav__link> <span class=md-ellipsis> 6 Transfer learning (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/07_pytorch_experiment_tracking/ class=md-nav__link> <span class=md-ellipsis> 7 Experiment tracking (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/07_pytorch_experiment_tracking.pdf class=md-nav__link> <span class=md-ellipsis> 7 Experiment tracking (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/08_pytorch_paper_replicating/ class=md-nav__link> <span class=md-ellipsis> 8 Paper replicating (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/08_pytorch_paper_replicating.pdf class=md-nav__link> <span class=md-ellipsis> 8 Paper replicating (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/09_pytorch_model_deployment.pdf class=md-nav__link> <span class=md-ellipsis> 9 Model deployment(pdf) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4 checked> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class=md-ellipsis> Tensorflow </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=true> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Tensorflow </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../00_tensorflow_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 0 Tensorflow fundamentals </span> </a> </li> <li class=md-nav__item> <a href=../01_neural_network_regression_in_tensorflow/ class=md-nav__link> <span class=md-ellipsis> 1 Neural network regression </span> </a> </li> <li class=md-nav__item> <a href=../02_neural_network_classification_in_tensorflow/ class=md-nav__link> <span class=md-ellipsis> 2 Neural network classification </span> </a> </li> <li class=md-nav__item> <a href=../04_transfer_learning_in_tensorflow_part_1_feature_extraction/ class=md-nav__link> <span class=md-ellipsis> 4 Transfer learning </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 10 Time series forecasting </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 10 Time series forecasting </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-a-time-series-problem class=md-nav__link> <span class=md-ellipsis> What is a time series problem? </span> </a> </li> <li class=md-nav__item> <a href=#what-were-going-to-cover class=md-nav__link> <span class=md-ellipsis> What we're going to cover </span> </a> </li> <li class=md-nav__item> <a href=#how-you-can-use-this-notebook class=md-nav__link> <span class=md-ellipsis> How you can use this notebook </span> </a> </li> <li class=md-nav__item> <a href=#check-for-gpu class=md-nav__link> <span class=md-ellipsis> Check for GPU </span> </a> </li> <li class=md-nav__item> <a href=#get-data class=md-nav__link> <span class=md-ellipsis> Get data </span> </a> <nav class=md-nav aria-label="Get data"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#importing-time-series-data-with-pandas class=md-nav__link> <span class=md-ellipsis> Importing time series data with pandas </span> </a> </li> <li class=md-nav__item> <a href=#importing-time-series-data-with-pythons-csv-module class=md-nav__link> <span class=md-ellipsis> Importing time series data with Python's CSV module </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#format-data-part-1-creatining-train-and-test-sets-for-time-series-data class=md-nav__link> <span class=md-ellipsis> Format Data Part 1: Creatining train and test sets for time series data </span> </a> <nav class=md-nav aria-label="Format Data Part 1: Creatining train and test sets for time series data"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#create-train-test-sets-for-time-series-the-wrong-way class=md-nav__link> <span class=md-ellipsis> Create train &amp; test sets for time series (the wrong way) </span> </a> </li> <li class=md-nav__item> <a href=#create-train-test-sets-for-time-series-the-right-way class=md-nav__link> <span class=md-ellipsis> Create train &amp; test sets for time series (the right way) </span> </a> </li> <li class=md-nav__item> <a href=#create-a-plotting-function class=md-nav__link> <span class=md-ellipsis> Create a plotting function </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#modelling-experiments class=md-nav__link> <span class=md-ellipsis> Modelling Experiments </span> </a> </li> <li class=md-nav__item> <a href=#model-0-naive-forecast-baseline class=md-nav__link> <span class=md-ellipsis> Model 0: Naïve forecast (baseline) </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-a-time-series-model class=md-nav__link> <span class=md-ellipsis> Evaluating a time series model </span> </a> <nav class=md-nav aria-label="Evaluating a time series model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#scale-dependent-errors class=md-nav__link> <span class=md-ellipsis> Scale-dependent errors </span> </a> </li> <li class=md-nav__item> <a href=#percentage-errors class=md-nav__link> <span class=md-ellipsis> Percentage errors </span> </a> </li> <li class=md-nav__item> <a href=#scaled-errors class=md-nav__link> <span class=md-ellipsis> Scaled errors </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#other-kinds-of-time-series-forecasting-models-which-can-be-used-for-baselines-and-actual-forecasts class=md-nav__link> <span class=md-ellipsis> Other kinds of time series forecasting models which can be used for baselines and actual forecasts </span> </a> </li> <li class=md-nav__item> <a href=#format-data-part-2-windowing-dataset class=md-nav__link> <span class=md-ellipsis> Format Data Part 2: Windowing dataset </span> </a> </li> <li class=md-nav__item> <a href=#turning-windows-into-training-and-test-sets class=md-nav__link> <span class=md-ellipsis> Turning windows into training and test sets </span> </a> </li> <li class=md-nav__item> <a href=#make-a-modelling-checkpoint class=md-nav__link> <span class=md-ellipsis> Make a modelling checkpoint </span> </a> </li> <li class=md-nav__item> <a href=#model-1-dense-model-window-7-horizon-1 class=md-nav__link> <span class=md-ellipsis> Model 1: Dense model (window = 7, horizon = 1) </span> </a> </li> <li class=md-nav__item> <a href=#making-forecasts-with-a-model-on-the-test-dataset class=md-nav__link> <span class=md-ellipsis> Making forecasts with a model (on the test dataset) </span> </a> </li> <li class=md-nav__item> <a href=#model-2-dense-window-30-horizon-1 class=md-nav__link> <span class=md-ellipsis> Model 2: Dense (window = 30, horizon = 1) </span> </a> </li> <li class=md-nav__item> <a href=#model-3-dense-window-30-horizon-7 class=md-nav__link> <span class=md-ellipsis> Model 3: Dense (window = 30, horizon = 7) </span> </a> </li> <li class=md-nav__item> <a href=#make-our-evaluation-function-work-for-larger-horizons class=md-nav__link> <span class=md-ellipsis> Make our evaluation function work for larger horizons </span> </a> </li> <li class=md-nav__item> <a href=#which-of-our-models-is-performing-best-so-far class=md-nav__link> <span class=md-ellipsis> Which of our models is performing best so far? </span> </a> </li> <li class=md-nav__item> <a href=#model-4-conv1d class=md-nav__link> <span class=md-ellipsis> Model 4: Conv1D </span> </a> </li> <li class=md-nav__item> <a href=#model-5-rnn-lstm class=md-nav__link> <span class=md-ellipsis> Model 5: RNN (LSTM) </span> </a> </li> <li class=md-nav__item> <a href=#make-a-multivariate-time-series class=md-nav__link> <span class=md-ellipsis> Make a multivariate time series </span> </a> </li> <li class=md-nav__item> <a href=#making-a-windowed-dataset-with-pandas class=md-nav__link> <span class=md-ellipsis> Making a windowed dataset with pandas </span> </a> </li> <li class=md-nav__item> <a href=#model-6-dense-multivariate-time-series class=md-nav__link> <span class=md-ellipsis> Model 6: Dense (multivariate time series) </span> </a> </li> <li class=md-nav__item> <a href=#model-7-n-beats-algorithm class=md-nav__link> <span class=md-ellipsis> Model 7: N-BEATS algorithm </span> </a> <nav class=md-nav aria-label="Model 7: N-BEATS algorithm"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#building-and-testing-an-n-beats-block-layer class=md-nav__link> <span class=md-ellipsis> Building and testing an N-BEATS block layer </span> </a> </li> <li class=md-nav__item> <a href=#preparing-data-for-the-n-beats-algorithm-using-tfdata class=md-nav__link> <span class=md-ellipsis> Preparing data for the N-BEATS algorithm using tf.data </span> </a> </li> <li class=md-nav__item> <a href=#setting-up-hyperparameters-for-n-beats-algorithm class=md-nav__link> <span class=md-ellipsis> Setting up hyperparameters for N-BEATS algorithm </span> </a> </li> <li class=md-nav__item> <a href=#getting-ready-for-residual-connections class=md-nav__link> <span class=md-ellipsis> Getting ready for residual connections </span> </a> </li> <li class=md-nav__item> <a href=#building-compiling-and-fitting-the-n-beats-algorithm class=md-nav__link> <span class=md-ellipsis> Building, compiling and fitting the N-BEATS algorithm </span> </a> </li> <li class=md-nav__item> <a href=#plotting-the-n-beats-architecture-weve-created class=md-nav__link> <span class=md-ellipsis> Plotting the N-BEATS architecture we've created </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#model-8-creating-an-ensemble-stacking-different-models-together class=md-nav__link> <span class=md-ellipsis> Model 8: Creating an ensemble (stacking different models together) </span> </a> <nav class=md-nav aria-label="Model 8: Creating an ensemble (stacking different models together)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#constructing-and-fitting-an-ensemble-of-models-using-different-loss-functions class=md-nav__link> <span class=md-ellipsis> Constructing and fitting an ensemble of models (using different loss functions) </span> </a> </li> <li class=md-nav__item> <a href=#making-predictions-with-an-ensemble-model class=md-nav__link> <span class=md-ellipsis> Making predictions with an ensemble model </span> </a> </li> <li class=md-nav__item> <a href=#plotting-the-prediction-intervals-uncertainty-estimates-of-our-ensemble class=md-nav__link> <span class=md-ellipsis> Plotting the prediction intervals (uncertainty estimates) of our ensemble </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#aside-two-types-of-uncertainty-coconut-and-subway class=md-nav__link> <span class=md-ellipsis> Aside: two types of uncertainty (coconut and subway) </span> </a> <nav class=md-nav aria-label="Aside: two types of uncertainty (coconut and subway)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#uncertainty-in-dating class=md-nav__link> <span class=md-ellipsis> Uncertainty in dating </span> </a> </li> <li class=md-nav__item> <a href=#learning-more-on-uncertainty class=md-nav__link> <span class=md-ellipsis> Learning more on uncertainty </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#model-9-train-a-model-on-the-full-historical-data-to-make-predictions-into-future class=md-nav__link> <span class=md-ellipsis> Model 9: Train a model on the full historical data to make predictions into future </span> </a> <nav class=md-nav aria-label="Model 9: Train a model on the full historical data to make predictions into future"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#make-predictions-on-the-future class=md-nav__link> <span class=md-ellipsis> Make predictions on the future </span> </a> </li> <li class=md-nav__item> <a href=#plot-future-forecasts class=md-nav__link> <span class=md-ellipsis> Plot future forecasts </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#model-10-why-forecasting-is-bs-the-turkey-problem class=md-nav__link> <span class=md-ellipsis> Model 10: Why forecasting is BS (the turkey problem 🦃) </span> </a> <nav class=md-nav aria-label="Model 10: Why forecasting is BS (the turkey problem 🦃)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#building-a-turkey-model-model-to-predict-on-turkey-data class=md-nav__link> <span class=md-ellipsis> Building a turkey model (model to predict on turkey data) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#compare-models class=md-nav__link> <span class=md-ellipsis> Compare Models </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> 🛠 Exercises </span> </a> </li> <li class=md-nav__item> <a href=#extra-curriculum class=md-nav__link> <span class=md-ellipsis> 📖 Extra-curriculum </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../OOP_Practice/ class=md-nav__link> <span class=md-ellipsis> OOP Practice </span> </a> </li> <li class=md-nav__item> <a href=../../Visualization/ class=md-nav__link> <span class=md-ellipsis> Visualization </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Môn học </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Môn học </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> Xác suất thống kê </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> Xác suất thống kê </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Subject/XSTK/Exercise2/homework/ class=md-nav__link> <span class=md-ellipsis> Bài tập về nhà buổi 2 </span> </a> </li> <li class=md-nav__item> <a href=../../../Subject/XSTK/Exercise3/homework/ class=md-nav__link> <span class=md-ellipsis> Bài tập về nhà buổi 3 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> CP-Problem </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> CP-Problem </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/general/ class=md-nav__link> <span class=md-ellipsis> General </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> COCI 2006 2007 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> COCI 2006 2007 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_2> <label class=md-nav__link for=__nav_4_2_2 id=__nav_4_2_2_label tabindex=0> <span class=md-ellipsis> Contest 1 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_2> <span class="md-nav__icon md-icon"></span> Contest 1 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P1_MODULO/ class=md-nav__link> <span class=md-ellipsis> MODULO </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P2_HERMAN/ class=md-nav__link> <span class=md-ellipsis> HERMAN </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P3_OKVIRI/ class=md-nav__link> <span class=md-ellipsis> OKVIRI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P4_SLIKAR/ class=md-nav__link> <span class=md-ellipsis> SLIKAR </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P5_BOND/ class=md-nav__link> <span class=md-ellipsis> BOND </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P6_DEBUG/ class=md-nav__link> <span class=md-ellipsis> DEBUG </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_3> <label class=md-nav__link for=__nav_4_2_3 id=__nav_4_2_3_label tabindex=0> <span class=md-ellipsis> Contest 2 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_3> <span class="md-nav__icon md-icon"></span> Contest 2 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P1_R2/ class=md-nav__link> <span class=md-ellipsis> R2 </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P2_ABC/ class=md-nav__link> <span class=md-ellipsis> ABC </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P3_KOLONE/ class=md-nav__link> <span class=md-ellipsis> KOLONE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P4_SJECISTA/ class=md-nav__link> <span class=md-ellipsis> SJECISTA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P5_STOL/ class=md-nav__link> <span class=md-ellipsis> STOL </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P6_STRAZA/ class=md-nav__link> <span class=md-ellipsis> STRAZA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_4> <label class=md-nav__link for=__nav_4_2_4 id=__nav_4_2_4_label tabindex=0> <span class=md-ellipsis> Contest 3 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_4> <span class="md-nav__icon md-icon"></span> Contest 3 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P1_PATULJCI/ class=md-nav__link> <span class=md-ellipsis> PATULJCI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P2_NPUZZLE/ class=md-nav__link> <span class=md-ellipsis> NPUZZLE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P3_TROJKE/ class=md-nav__link> <span class=md-ellipsis> TROJKE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P4_TENKICI/ class=md-nav__link> <span class=md-ellipsis> TENKICI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P5_BICIKLI/ class=md-nav__link> <span class=md-ellipsis> BICIKLI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P6_LISTA/ class=md-nav__link> <span class=md-ellipsis> LISTA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_5> <label class=md-nav__link for=__nav_4_2_5 id=__nav_4_2_5_label tabindex=0> <span class=md-ellipsis> Contest 4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_5> <span class="md-nav__icon md-icon"></span> Contest 4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P1_Sibice/ class=md-nav__link> <span class=md-ellipsis> SIBICE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P2_Skener/ class=md-nav__link> <span class=md-ellipsis> SKENER </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P3_Prsteni/ class=md-nav__link> <span class=md-ellipsis> PRSTENI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P4_Zbrka/ class=md-nav__link> <span class=md-ellipsis> ZBRKA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P5_Jogurt/ class=md-nav__link> <span class=md-ellipsis> JOGURT </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P6_Ispiti/ class=md-nav__link> <span class=md-ellipsis> ISPITI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_6> <label class=md-nav__link for=__nav_4_2_6 id=__nav_4_2_6_label tabindex=0> <span class=md-ellipsis> Contest 5 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_6_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_6> <span class="md-nav__icon md-icon"></span> Contest 5 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P1_Trik/ class=md-nav__link> <span class=md-ellipsis> TRIK </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P2_Natrij/ class=md-nav__link> <span class=md-ellipsis> NATRIJ </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P3_Tenis/ class=md-nav__link> <span class=md-ellipsis> TENIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P4_Liga/ class=md-nav__link> <span class=md-ellipsis> LIGA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P5_Ivana/ class=md-nav__link> <span class=md-ellipsis> IVANA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P6_Dvaput/ class=md-nav__link> <span class=md-ellipsis> DVAPUT </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_7> <label class=md-nav__link for=__nav_4_2_7 id=__nav_4_2_7_label tabindex=0> <span class=md-ellipsis> Contest 6 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_7_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_7> <span class="md-nav__icon md-icon"></span> Contest 6 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P1_PRASE/ class=md-nav__link> <span class=md-ellipsis> PRASE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P2_MAGIJA/ class=md-nav__link> <span class=md-ellipsis> MAGIJA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P3_MARATON/ class=md-nav__link> <span class=md-ellipsis> MARATON </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P4_KAMEN/ class=md-nav__link> <span class=md-ellipsis> KAMEN </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P5_V/ class=md-nav__link> <span class=md-ellipsis> V </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P6_PROSTOR/ class=md-nav__link> <span class=md-ellipsis> PROSTOR </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_8> <label class=md-nav__link for=__nav_4_2_8 id=__nav_4_2_8_label tabindex=0> <span class=md-ellipsis> Regional </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_8_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_8> <span class="md-nav__icon md-icon"></span> Regional </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P1_BARD/ class=md-nav__link> <span class=md-ellipsis> BARD </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P2_TETRIS/ class=md-nav__link> <span class=md-ellipsis> TETRIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P4_CIRCLE/ class=md-nav__link> <span class=md-ellipsis> CIRCLE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_9> <label class=md-nav__link for=__nav_4_2_9 id=__nav_4_2_9_label tabindex=0> <span class=md-ellipsis> Croatian Olympiad </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_9_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_9> <span class="md-nav__icon md-icon"></span> Croatian Olympiad </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P1_PATRIK/ class=md-nav__link> <span class=md-ellipsis> PATRIK </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P2_POLICIJA/ class=md-nav__link> <span class=md-ellipsis> POLICIJA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P3_SABOR/ class=md-nav__link> <span class=md-ellipsis> SABOR </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class=md-ellipsis> COCI 2007 2008 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> COCI 2007 2008 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_2> <label class=md-nav__link for=__nav_4_3_2 id=__nav_4_3_2_label tabindex=0> <span class=md-ellipsis> Contest 1 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_2> <span class="md-nav__icon md-icon"></span> Contest 1 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P1_CETVRTA/ class=md-nav__link> <span class=md-ellipsis> CETVRTA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P2_PEG/ class=md-nav__link> <span class=md-ellipsis> PEG </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P3_PRINOVA/ class=md-nav__link> <span class=md-ellipsis> PRINOVA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P4_ZAPIS/ class=md-nav__link> <span class=md-ellipsis> ZAPIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P5_SREDNJI/ class=md-nav__link> <span class=md-ellipsis> SREDNJI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P6_STAZA/ class=md-nav__link> <span class=md-ellipsis> STAZA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_3> <label class=md-nav__link for=__nav_4_3_3 id=__nav_4_3_3_label tabindex=0> <span class=md-ellipsis> Contest 4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_3> <span class="md-nav__icon md-icon"></span> Contest 4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P1_CIRCLE/ class=md-nav__link> <span class=md-ellipsis> CIRCLE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P2_VECI/ class=md-nav__link> <span class=md-ellipsis> VECI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P3_LEKTIRA/ class=md-nav__link> <span class=md-ellipsis> LEKTIRA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P4_MUZICARI/ class=md-nav__link> <span class=md-ellipsis> MUZICARI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P5_POKLON/ class=md-nav__link> <span class=md-ellipsis> POKLON </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P6_KOCKE/ class=md-nav__link> <span class=md-ellipsis> KOCKE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_4> <label class=md-nav__link for=__nav_4_3_4 id=__nav_4_3_4_label tabindex=0> <span class=md-ellipsis> Contest 6 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_4> <span class="md-nav__icon md-icon"></span> Contest 6 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P1_PARKING/ class=md-nav__link> <span class=md-ellipsis> PARKING </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P2_SEMAFORI/ class=md-nav__link> <span class=md-ellipsis> SEMAFORI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P3_GRANICA/ class=md-nav__link> <span class=md-ellipsis> GRANICA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P4_GEORGE/ class=md-nav__link> <span class=md-ellipsis> GEORGE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P5_PRINCEZA/ class=md-nav__link> <span class=md-ellipsis> PRINCEZA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P6_CESTARINE/ class=md-nav__link> <span class=md-ellipsis> CESTARINE </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../other/other/ class=md-nav__link> <span class=md-ellipsis> Other </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-a-time-series-problem class=md-nav__link> <span class=md-ellipsis> What is a time series problem? </span> </a> </li> <li class=md-nav__item> <a href=#what-were-going-to-cover class=md-nav__link> <span class=md-ellipsis> What we're going to cover </span> </a> </li> <li class=md-nav__item> <a href=#how-you-can-use-this-notebook class=md-nav__link> <span class=md-ellipsis> How you can use this notebook </span> </a> </li> <li class=md-nav__item> <a href=#check-for-gpu class=md-nav__link> <span class=md-ellipsis> Check for GPU </span> </a> </li> <li class=md-nav__item> <a href=#get-data class=md-nav__link> <span class=md-ellipsis> Get data </span> </a> <nav class=md-nav aria-label="Get data"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#importing-time-series-data-with-pandas class=md-nav__link> <span class=md-ellipsis> Importing time series data with pandas </span> </a> </li> <li class=md-nav__item> <a href=#importing-time-series-data-with-pythons-csv-module class=md-nav__link> <span class=md-ellipsis> Importing time series data with Python's CSV module </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#format-data-part-1-creatining-train-and-test-sets-for-time-series-data class=md-nav__link> <span class=md-ellipsis> Format Data Part 1: Creatining train and test sets for time series data </span> </a> <nav class=md-nav aria-label="Format Data Part 1: Creatining train and test sets for time series data"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#create-train-test-sets-for-time-series-the-wrong-way class=md-nav__link> <span class=md-ellipsis> Create train &amp; test sets for time series (the wrong way) </span> </a> </li> <li class=md-nav__item> <a href=#create-train-test-sets-for-time-series-the-right-way class=md-nav__link> <span class=md-ellipsis> Create train &amp; test sets for time series (the right way) </span> </a> </li> <li class=md-nav__item> <a href=#create-a-plotting-function class=md-nav__link> <span class=md-ellipsis> Create a plotting function </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#modelling-experiments class=md-nav__link> <span class=md-ellipsis> Modelling Experiments </span> </a> </li> <li class=md-nav__item> <a href=#model-0-naive-forecast-baseline class=md-nav__link> <span class=md-ellipsis> Model 0: Naïve forecast (baseline) </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-a-time-series-model class=md-nav__link> <span class=md-ellipsis> Evaluating a time series model </span> </a> <nav class=md-nav aria-label="Evaluating a time series model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#scale-dependent-errors class=md-nav__link> <span class=md-ellipsis> Scale-dependent errors </span> </a> </li> <li class=md-nav__item> <a href=#percentage-errors class=md-nav__link> <span class=md-ellipsis> Percentage errors </span> </a> </li> <li class=md-nav__item> <a href=#scaled-errors class=md-nav__link> <span class=md-ellipsis> Scaled errors </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#other-kinds-of-time-series-forecasting-models-which-can-be-used-for-baselines-and-actual-forecasts class=md-nav__link> <span class=md-ellipsis> Other kinds of time series forecasting models which can be used for baselines and actual forecasts </span> </a> </li> <li class=md-nav__item> <a href=#format-data-part-2-windowing-dataset class=md-nav__link> <span class=md-ellipsis> Format Data Part 2: Windowing dataset </span> </a> </li> <li class=md-nav__item> <a href=#turning-windows-into-training-and-test-sets class=md-nav__link> <span class=md-ellipsis> Turning windows into training and test sets </span> </a> </li> <li class=md-nav__item> <a href=#make-a-modelling-checkpoint class=md-nav__link> <span class=md-ellipsis> Make a modelling checkpoint </span> </a> </li> <li class=md-nav__item> <a href=#model-1-dense-model-window-7-horizon-1 class=md-nav__link> <span class=md-ellipsis> Model 1: Dense model (window = 7, horizon = 1) </span> </a> </li> <li class=md-nav__item> <a href=#making-forecasts-with-a-model-on-the-test-dataset class=md-nav__link> <span class=md-ellipsis> Making forecasts with a model (on the test dataset) </span> </a> </li> <li class=md-nav__item> <a href=#model-2-dense-window-30-horizon-1 class=md-nav__link> <span class=md-ellipsis> Model 2: Dense (window = 30, horizon = 1) </span> </a> </li> <li class=md-nav__item> <a href=#model-3-dense-window-30-horizon-7 class=md-nav__link> <span class=md-ellipsis> Model 3: Dense (window = 30, horizon = 7) </span> </a> </li> <li class=md-nav__item> <a href=#make-our-evaluation-function-work-for-larger-horizons class=md-nav__link> <span class=md-ellipsis> Make our evaluation function work for larger horizons </span> </a> </li> <li class=md-nav__item> <a href=#which-of-our-models-is-performing-best-so-far class=md-nav__link> <span class=md-ellipsis> Which of our models is performing best so far? </span> </a> </li> <li class=md-nav__item> <a href=#model-4-conv1d class=md-nav__link> <span class=md-ellipsis> Model 4: Conv1D </span> </a> </li> <li class=md-nav__item> <a href=#model-5-rnn-lstm class=md-nav__link> <span class=md-ellipsis> Model 5: RNN (LSTM) </span> </a> </li> <li class=md-nav__item> <a href=#make-a-multivariate-time-series class=md-nav__link> <span class=md-ellipsis> Make a multivariate time series </span> </a> </li> <li class=md-nav__item> <a href=#making-a-windowed-dataset-with-pandas class=md-nav__link> <span class=md-ellipsis> Making a windowed dataset with pandas </span> </a> </li> <li class=md-nav__item> <a href=#model-6-dense-multivariate-time-series class=md-nav__link> <span class=md-ellipsis> Model 6: Dense (multivariate time series) </span> </a> </li> <li class=md-nav__item> <a href=#model-7-n-beats-algorithm class=md-nav__link> <span class=md-ellipsis> Model 7: N-BEATS algorithm </span> </a> <nav class=md-nav aria-label="Model 7: N-BEATS algorithm"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#building-and-testing-an-n-beats-block-layer class=md-nav__link> <span class=md-ellipsis> Building and testing an N-BEATS block layer </span> </a> </li> <li class=md-nav__item> <a href=#preparing-data-for-the-n-beats-algorithm-using-tfdata class=md-nav__link> <span class=md-ellipsis> Preparing data for the N-BEATS algorithm using tf.data </span> </a> </li> <li class=md-nav__item> <a href=#setting-up-hyperparameters-for-n-beats-algorithm class=md-nav__link> <span class=md-ellipsis> Setting up hyperparameters for N-BEATS algorithm </span> </a> </li> <li class=md-nav__item> <a href=#getting-ready-for-residual-connections class=md-nav__link> <span class=md-ellipsis> Getting ready for residual connections </span> </a> </li> <li class=md-nav__item> <a href=#building-compiling-and-fitting-the-n-beats-algorithm class=md-nav__link> <span class=md-ellipsis> Building, compiling and fitting the N-BEATS algorithm </span> </a> </li> <li class=md-nav__item> <a href=#plotting-the-n-beats-architecture-weve-created class=md-nav__link> <span class=md-ellipsis> Plotting the N-BEATS architecture we've created </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#model-8-creating-an-ensemble-stacking-different-models-together class=md-nav__link> <span class=md-ellipsis> Model 8: Creating an ensemble (stacking different models together) </span> </a> <nav class=md-nav aria-label="Model 8: Creating an ensemble (stacking different models together)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#constructing-and-fitting-an-ensemble-of-models-using-different-loss-functions class=md-nav__link> <span class=md-ellipsis> Constructing and fitting an ensemble of models (using different loss functions) </span> </a> </li> <li class=md-nav__item> <a href=#making-predictions-with-an-ensemble-model class=md-nav__link> <span class=md-ellipsis> Making predictions with an ensemble model </span> </a> </li> <li class=md-nav__item> <a href=#plotting-the-prediction-intervals-uncertainty-estimates-of-our-ensemble class=md-nav__link> <span class=md-ellipsis> Plotting the prediction intervals (uncertainty estimates) of our ensemble </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#aside-two-types-of-uncertainty-coconut-and-subway class=md-nav__link> <span class=md-ellipsis> Aside: two types of uncertainty (coconut and subway) </span> </a> <nav class=md-nav aria-label="Aside: two types of uncertainty (coconut and subway)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#uncertainty-in-dating class=md-nav__link> <span class=md-ellipsis> Uncertainty in dating </span> </a> </li> <li class=md-nav__item> <a href=#learning-more-on-uncertainty class=md-nav__link> <span class=md-ellipsis> Learning more on uncertainty </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#model-9-train-a-model-on-the-full-historical-data-to-make-predictions-into-future class=md-nav__link> <span class=md-ellipsis> Model 9: Train a model on the full historical data to make predictions into future </span> </a> <nav class=md-nav aria-label="Model 9: Train a model on the full historical data to make predictions into future"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#make-predictions-on-the-future class=md-nav__link> <span class=md-ellipsis> Make predictions on the future </span> </a> </li> <li class=md-nav__item> <a href=#plot-future-forecasts class=md-nav__link> <span class=md-ellipsis> Plot future forecasts </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#model-10-why-forecasting-is-bs-the-turkey-problem class=md-nav__link> <span class=md-ellipsis> Model 10: Why forecasting is BS (the turkey problem 🦃) </span> </a> <nav class=md-nav aria-label="Model 10: Why forecasting is BS (the turkey problem 🦃)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#building-a-turkey-model-model-to-predict-on-turkey-data class=md-nav__link> <span class=md-ellipsis> Building a turkey model (model to predict on turkey data) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#compare-models class=md-nav__link> <span class=md-ellipsis> Compare Models </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> 🛠 Exercises </span> </a> </li> <li class=md-nav__item> <a href=#extra-curriculum class=md-nav__link> <span class=md-ellipsis> 📖 Extra-curriculum </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <p><a href=https://colab.research.google.com/github/mrdbourke/tensorflow-deep-learning/blob/main/10_time_series_forecasting_in_tensorflow.ipynb target=_parent><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p> <h1 id=10-milestone-project-3-time-series-forecasting-in-tensorflow-bitpredict>10. Milestone Project 3: Time series forecasting in TensorFlow (BitPredict 💰📈)</h1> <p>The goal of this notebook is to get you familiar with working with time series data.</p> <p>We're going to be building a series of models in an attempt to predict the price of Bitcoin. </p> <p>Welcome to Milestone Project 3, BitPredict 💰📈!</p> <blockquote> <p>🔑 <strong>Note:</strong> ⚠️ This is not financial advice, as you'll see time series forecasting for stock market prices is actually quite terrible.</p> </blockquote> <h2 id=what-is-a-time-series-problem>What is a time series problem?</h2> <p>Time series problems deal with data over time.</p> <p>Such as, the number of staff members in a company over 10-years, sales of computers for the past 5-years, electricity usage for the past 50-years.</p> <p>The timeline can be short (seconds/minutes) or long (years/decades). And the problems you might investigate using can usually be broken down into two categories.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-example-time-series-problems.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="example time series problems" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-example-time-series-problems.png></a></p> <table> <thead> <tr> <th>Problem Type</th> <th>Examples</th> <th>Output</th> </tr> </thead> <tbody> <tr> <td><strong>Classification</strong></td> <td>Anomaly detection, time series identification (where did this time series come from?)</td> <td>Discrete (a label)</td> </tr> <tr> <td><strong>Forecasting</strong></td> <td>Predicting stock market prices, forecasting future demand for a product, stocking inventory requirements</td> <td>Continuous (a number)</td> </tr> </tbody> </table> <p>In both cases above, a supervised learning approach is often used. Meaning, you'd have some example data and a label assosciated with that data.</p> <p>For example, in forecasting the price of Bitcoin, your data could be the historical price of Bitcoin for the past month and the label could be today's price (the label can't be tomorrow's price because that's what we'd want to predict).</p> <p>Can you guess what kind of problem BitPredict 💰📈 is?</p> <h2 id=what-were-going-to-cover>What we're going to cover</h2> <p>Are you ready?</p> <p>We've got a lot to go through. </p> <ul> <li>Get time series data (the historical price of Bitcoin)</li> <li>Load in time series data using pandas/Python's CSV module</li> <li>Format data for a time series problem</li> <li>Creating training and test sets (the wrong way)</li> <li>Creating training and test sets (the right way)</li> <li>Visualizing time series data</li> <li>Turning time series data into a supervised learning problem (windowing)</li> <li>Preparing univariate and multivariate (more than one variable) data</li> <li>Evaluating a time series forecasting model</li> <li>Setting up a series of deep learning modelling experiments</li> <li>Dense (fully-connected) networks</li> <li>Sequence models (LSTM and 1D CNN)</li> <li>Ensembling (combining multiple models together)</li> <li>Multivariate models</li> <li>Replicating the N-BEATS algorithm using TensorFlow layer subclassing</li> <li>Creating a modelling checkpoint to save the best performing model during training</li> <li>Making predictions (forecasts) with a time series model</li> <li>Creating prediction intervals for time series model forecasts</li> <li>Discussing two different types of uncertainty in machine learning (data uncertainty and model uncertainty)</li> <li>Demonstrating why forecasting in an open system is BS (the turkey problem) </li> </ul> <h2 id=how-you-can-use-this-notebook>How you can use this notebook</h2> <p>You can read through the descriptions and the code (it should all run), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to <strong>write more code</strong>.</p> <blockquote> <p>📖 <strong>Resource:</strong> Get all of the materials you need for this notebook on the <a href=https://github.com/mrdbourke/tensorflow-deep-learning>course GitHub</a>.</p> </blockquote> <h2 id=check-for-gpu>Check for GPU</h2> <p>In order for our deep learning models to run as fast as possible, we'll need access to a GPU.</p> <p>In Google Colab, you can set this up by going to Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>After selecting GPU, you may have to restart the runtime.</p> <div class=highlight><pre><span></span><code><span class=c1># Check for GPU</span>
<span class=err>!</span><span class=n>nvidia</span><span class=o>-</span><span class=n>smi</span> <span class=o>-</span><span class=n>L</span>
</code></pre></div> <div class=highlight><pre><span></span><code>GPU 0: Tesla K80 (UUID: GPU-c7456639-4229-1150-8316-e4197bf2c93e)
</code></pre></div> <h2 id=get-data>Get data</h2> <p>To build a time series forecasting model, the first thing we're going to need is data.</p> <p>And since we're trying to predict the price of Bitcoin, we'll need Bitcoin data.</p> <p>Specifically, we're going to get the prices of Bitcoin from 01 October 2013 to 18 May 2021.</p> <p>Why these dates?</p> <p>Because 01 October 2013 is when our data source (<a href=https://www.coindesk.com/price/bitcoin>Coindesk</a>) started recording the price of Bitcoin and 18 May 2021 is when this notebook was created.</p> <p>If you're going through this notebook at a later date, you'll be able to use what you learn to predict on later dates of Bitcoin, you'll just have to adjust the data source.</p> <blockquote> <p>📖 <strong>Resource:</strong> To get the Bitcoin historical data, I went to the <a href=https://www.coindesk.com/price/bitcoin>Coindesk page for Bitcoin prices</a>, clicked on "all" and then clicked on "Export data" and selected "CSV". </p> </blockquote> <p>You can find the data we're going to use on <a href=https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv>GitHub</a>.</p> <div class=highlight><pre><span></span><code><span class=c1># Download Bitcoin historical data from GitHub </span>
<span class=c1># Note: you&#39;ll need to select &quot;Raw&quot; to download the data in the correct format</span>
<span class=err>!</span><span class=n>wget</span> <span class=n>https</span><span class=p>:</span><span class=o>//</span><span class=n>raw</span><span class=o>.</span><span class=n>githubusercontent</span><span class=o>.</span><span class=n>com</span><span class=o>/</span><span class=n>mrdbourke</span><span class=o>/</span><span class=n>tensorflow</span><span class=o>-</span><span class=n>deep</span><span class=o>-</span><span class=n>learning</span><span class=o>/</span><span class=n>main</span><span class=o>/</span><span class=n>extras</span><span class=o>/</span><span class=n>BTC_USD_2013</span><span class=o>-</span><span class=mi>10</span><span class=o>-</span><span class=mi>01_2021</span><span class=o>-</span><span class=mi>05</span><span class=o>-</span><span class=mi>18</span><span class=o>-</span><span class=n>CoinDesk</span><span class=o>.</span><span class=n>csv</span> 
</code></pre></div> <div class=highlight><pre><span></span><code>--2021-09-27 03:40:22--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 178509 (174K) [text/plain]
Saving to: ‘BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv’

BTC_USD_2013-10-01_ 100%[===================&gt;] 174.33K  --.-KB/s    in 0.01s

2021-09-27 03:40:22 (16.5 MB/s) - ‘BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv’ saved [178509/178509]
</code></pre></div> <h3 id=importing-time-series-data-with-pandas>Importing time series data with pandas</h3> <p>Now we've got some data to work with, let's import it using pandas so we can visualize it.</p> <p>Because our data is in <strong>CSV (comma separated values)</strong> format (a very common data format for time series), we'll use the pandas <a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html><code>read_csv()</code></a> function.</p> <p>And because our data has a date component, we'll tell pandas to parse the dates using the <code>parse_dates</code> parameter passing it the name our of the date column ("Date").</p> <div class=highlight><pre><span></span><code><span class=c1># Import with pandas </span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=c1># Parse dates and set date column to index</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&quot;/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv&quot;</span><span class=p>,</span> 
                 <span class=n>parse_dates</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;Date&quot;</span><span class=p>],</span> 
                 <span class=n>index_col</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;Date&quot;</span><span class=p>])</span> <span class=c1># parse the date column (tell pandas column 1 is a datetime)</span>
<span class=n>df</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>Currency</th> <th>Closing Price (USD)</th> <th>24h Open (USD)</th> <th>24h High (USD)</th> <th>24h Low (USD)</th> </tr> <tr> <th>Date</th> <th></th> <th></th> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <th>2013-10-01</th> <td>BTC</td> <td>123.65499</td> <td>124.30466</td> <td>124.75166</td> <td>122.56349</td> </tr> <tr> <th>2013-10-02</th> <td>BTC</td> <td>125.45500</td> <td>123.65499</td> <td>125.75850</td> <td>123.63383</td> </tr> <tr> <th>2013-10-03</th> <td>BTC</td> <td>108.58483</td> <td>125.45500</td> <td>125.66566</td> <td>83.32833</td> </tr> <tr> <th>2013-10-04</th> <td>BTC</td> <td>118.67466</td> <td>108.58483</td> <td>118.67500</td> <td>107.05816</td> </tr> <tr> <th>2013-10-05</th> <td>BTC</td> <td>121.33866</td> <td>118.67466</td> <td>121.93633</td> <td>118.00566</td> </tr> </tbody> </table> </div> <p>Looking good! Let's get some more info.</p> <div class=highlight><pre><span></span><code><span class=n>df</span><span class=o>.</span><span class=n>info</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
DatetimeIndex: 2787 entries, 2013-10-01 to 2021-05-18
Data columns (total 5 columns):
 #   Column               Non-Null Count  Dtype  
---  ------               --------------  -----  
 0   Currency             2787 non-null   object 
 1   Closing Price (USD)  2787 non-null   float64
 2   24h Open (USD)       2787 non-null   float64
 3   24h High (USD)       2787 non-null   float64
 4   24h Low (USD)        2787 non-null   float64
dtypes: float64(4), object(1)
memory usage: 130.6+ KB
</code></pre></div> <p>Because we told pandas to parse the date column and set it as the index, its not in the list of columns.</p> <p>You can also see there isn't many samples.</p> <div class=highlight><pre><span></span><code><span class=c1># How many samples do we have?</span>
<span class=nb>len</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>2787
</code></pre></div> <p>We've collected the historical price of Bitcoin for the past ~8 years but there's only 2787 total samples.</p> <p>This is something you'll run into with time series data problems. Often, the number of samples isn't as large as other kinds of data.</p> <p>For example, collecting one sample at different time frames results in:</p> <table> <thead> <tr> <th>1 sample per timeframe</th> <th>Number of samples per year</th> </tr> </thead> <tbody> <tr> <td>Second</td> <td>31,536,000</td> </tr> <tr> <td>Hour</td> <td>8,760</td> </tr> <tr> <td>Day</td> <td>365</td> </tr> <tr> <td>Week</td> <td>52</td> </tr> <tr> <td>Month</td> <td>12</td> </tr> </tbody> </table> <blockquote> <p>🔑 <strong>Note:</strong> The frequency at which a time series value is collected is often referred to as <strong>seasonality</strong>. This is usually mesaured in number of samples per year. For example, collecting the price of Bitcoin once per day would result in a time series with a seasonality of 365. Time series data collected with different seasonality values often exhibit seasonal patterns (e.g. electricity demand behing higher in Summer months for air conditioning than Winter months). For more on different time series patterns, see <a href=https://otexts.com/fpp3/tspatterns.html>Forecasting: Principles and Practice Chapter 2.3</a>.</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-types-of-time-series-patterns.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="different types of time series patterns" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-types-of-time-series-patterns.png></a> <em>Example of different kinds of patterns you'll see in time series data. Notice the bottom right time series (Google stock price changes) has little to no patterns, making it difficult to predict. See <a href=https://otexts.com/fpp3/tspatterns.html>Forecasting: Principles and Practice Chapter 2.3</a> for full graphic.</em></p> <p>Deep learning algorithms usually flourish with lots of data, in the range of thousands to millions of samples.</p> <p>In our case, we've got the daily prices of Bitcoin, a max of 365 samples per year.</p> <p>But that doesn't we can't try them with our data.</p> <p>To simplify, let's remove some of the columns from our data so we're only left with a date index and the closing price.</p> <div class=highlight><pre><span></span><code><span class=c1># Only want closing price for each day </span>
<span class=n>bitcoin_prices</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s2>&quot;Closing Price (USD)&quot;</span><span class=p>])</span><span class=o>.</span><span class=n>rename</span><span class=p>(</span><span class=n>columns</span><span class=o>=</span><span class=p>{</span><span class=s2>&quot;Closing Price (USD)&quot;</span><span class=p>:</span> <span class=s2>&quot;Price&quot;</span><span class=p>})</span>
<span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>Price</th> </tr> <tr> <th>Date</th> <th></th> </tr> </thead> <tbody> <tr> <th>2013-10-01</th> <td>123.65499</td> </tr> <tr> <th>2013-10-02</th> <td>125.45500</td> </tr> <tr> <th>2013-10-03</th> <td>108.58483</td> </tr> <tr> <th>2013-10-04</th> <td>118.67466</td> </tr> <tr> <th>2013-10-05</th> <td>121.33866</td> </tr> </tbody> </table> </div> <p>Much better!</p> <p>But that's only five days worth of Bitcoin prices, let's plot everything we've got.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;BTC Price&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Price of Bitcoin from 1 Oct 2013 to 18 May 2021&quot;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>fontsize</span><span class=o>=</span><span class=mi>14</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_15_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_15_0.png></a></p> <p>Woah, looks like it would've been a good idea to buy Bitcoin back in 2014.</p> <h3 id=importing-time-series-data-with-pythons-csv-module>Importing time series data with Python's CSV module</h3> <p>If your time series data comes in CSV form you don't necessarily have to use pandas.</p> <p>You can use Python's <a href=https://docs.python.org/3/library/csv.html>in-built <code>csv</code> module</a>. And if you're working with dates, you might also want to use Python's <a href=https://docs.python.org/3/library/datetime.html><code>datetime</code></a>.</p> <p>Let's see how we can replicate the plot we created before except this time using Python's <code>csv</code> and <code>datetime</code> modules.</p> <blockquote> <p>📖 <strong>Resource:</strong> For a great guide on using Python's <code>csv</code> module, check out Real Python's tutorial on <a href=https://realpython.com/python-csv/ >Reading and Writing CSV files in Python</a>.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Importing and formatting historical Bitcoin data with Python</span>
<span class=kn>import</span><span class=w> </span><span class=nn>csv</span>
<span class=kn>from</span><span class=w> </span><span class=nn>datetime</span><span class=w> </span><span class=kn>import</span> <span class=n>datetime</span>

<span class=n>timesteps</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>btc_price</span> <span class=o>=</span> <span class=p>[]</span>
<span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&quot;/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv&quot;</span><span class=p>,</span> <span class=s2>&quot;r&quot;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
  <span class=n>csv_reader</span> <span class=o>=</span> <span class=n>csv</span><span class=o>.</span><span class=n>reader</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>delimiter</span><span class=o>=</span><span class=s2>&quot;,&quot;</span><span class=p>)</span> <span class=c1># read in the target CSV</span>
  <span class=nb>next</span><span class=p>(</span><span class=n>csv_reader</span><span class=p>)</span> <span class=c1># skip first line (this gets rid of the column titles)</span>
  <span class=k>for</span> <span class=n>line</span> <span class=ow>in</span> <span class=n>csv_reader</span><span class=p>:</span>
    <span class=n>timesteps</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>datetime</span><span class=o>.</span><span class=n>strptime</span><span class=p>(</span><span class=n>line</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=s2>&quot;%Y-%m-</span><span class=si>%d</span><span class=s2>&quot;</span><span class=p>))</span> <span class=c1># get the dates as dates (not strings), strptime = string parse time</span>
    <span class=n>btc_price</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=nb>float</span><span class=p>(</span><span class=n>line</span><span class=p>[</span><span class=mi>2</span><span class=p>]))</span> <span class=c1># get the closing price as float</span>

<span class=c1># View first 10 of each</span>
<span class=n>timesteps</span><span class=p>[:</span><span class=mi>10</span><span class=p>],</span> <span class=n>btc_price</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>([datetime.datetime(2013, 10, 1, 0, 0),
  datetime.datetime(2013, 10, 2, 0, 0),
  datetime.datetime(2013, 10, 3, 0, 0),
  datetime.datetime(2013, 10, 4, 0, 0),
  datetime.datetime(2013, 10, 5, 0, 0),
  datetime.datetime(2013, 10, 6, 0, 0),
  datetime.datetime(2013, 10, 7, 0, 0),
  datetime.datetime(2013, 10, 8, 0, 0),
  datetime.datetime(2013, 10, 9, 0, 0),
  datetime.datetime(2013, 10, 10, 0, 0)],
 [123.65499,
  125.455,
  108.58483,
  118.67466,
  121.33866,
  120.65533,
  121.795,
  123.033,
  124.049,
  125.96116])
</code></pre></div> <p>Beautiful! Now, let's see how things look.</p> <div class=highlight><pre><span></span><code><span class=c1># Plot from CSV</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>timesteps</span><span class=p>,</span> <span class=n>btc_price</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Price of Bitcoin from 1 Oct 2013 to 18 May 2021&quot;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Date&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;BTC Price&quot;</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_20_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_20_0.png></a></p> <p>Ho ho! Would you look at that! Just like the pandas plot. And because we formatted the <code>timesteps</code> to be <code>datetime</code> objects, <code>matplotlib</code> displays a fantastic looking date axis.</p> <h2 id=format-data-part-1-creatining-train-and-test-sets-for-time-series-data>Format Data Part 1: Creatining train and test sets for time series data</h2> <p>Alrighty. What's next?</p> <p>If you guessed preparing our data for a model, you'd be right.</p> <p>What's the most important first step for preparing any machine learning dataset?</p> <p>Scaling?</p> <p>No...</p> <p>Removing outliers?</p> <p>No...</p> <p>How about creating train and test splits?</p> <p>Yes! </p> <p>Usually, you could create a train and test split using a function like Scikit-Learn's outstanding <a href=https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html><code>train_test_split()</code></a> but as we'll see in a moment, this doesn't really cut it for time series data.</p> <p>But before we do create splits, it's worth talking about what <em>kind</em> of data we have.</p> <p>In time series problems, you'll either have <strong>univariate</strong> or <strong>multivariate</strong> data.</p> <p>Can you guess what our data is?</p> <ul> <li><strong>Univariate</strong> time series data deals with <em>one</em> variable, for example, using the price of Bitcoin to predict the price of Bitcoin.</li> <li><strong>Multivariate</strong> time series data deals with <em>more than one</em> variable, for example, predicting electricity demand using the day of week, time of year and number of houses in a region.</li> </ul> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-univariate-and-multivariate-time-series-data.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="comparison of univariate and multivariate time series data" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-univariate-and-multivariate-time-series-data.png></a> <em>Example of univariate and multivariate time series data. Univariate involves using the target to predict the target. Multivariate inolves using the target as well as another time series to predict the target.</em></p> <h3 id=create-train-test-sets-for-time-series-the-wrong-way>Create train &amp; test sets for time series (the wrong way)</h3> <p>Okay, we've figured out we're dealing with a univariate time series, so we only have to make a split on one variable (for multivariate time series, you will have to split multiple variables). </p> <p>How about we first see the <em>wrong way</em> for splitting time series data?</p> <p>Let's turn our DataFrame index and column into NumPy arrays.</p> <div class=highlight><pre><span></span><code><span class=c1># Get bitcoin date array</span>
<span class=n>timesteps</span> <span class=o>=</span> <span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>index</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>()</span>
<span class=n>prices</span> <span class=o>=</span> <span class=n>bitcoin_prices</span><span class=p>[</span><span class=s2>&quot;Price&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>()</span>

<span class=n>timesteps</span><span class=p>[:</span><span class=mi>10</span><span class=p>],</span> <span class=n>prices</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([&#39;2013-10-01T00:00:00.000000000&#39;, &#39;2013-10-02T00:00:00.000000000&#39;,
        &#39;2013-10-03T00:00:00.000000000&#39;, &#39;2013-10-04T00:00:00.000000000&#39;,
        &#39;2013-10-05T00:00:00.000000000&#39;, &#39;2013-10-06T00:00:00.000000000&#39;,
        &#39;2013-10-07T00:00:00.000000000&#39;, &#39;2013-10-08T00:00:00.000000000&#39;,
        &#39;2013-10-09T00:00:00.000000000&#39;, &#39;2013-10-10T00:00:00.000000000&#39;],
       dtype=&#39;datetime64[ns]&#39;),
 array([123.65499, 125.455  , 108.58483, 118.67466, 121.33866, 120.65533,
        121.795  , 123.033  , 124.049  , 125.96116]))
</code></pre></div> <p>And now we'll use the ever faithful <code>train_test_split</code> from Scikit-Learn to create our train and test sets.</p> <div class=highlight><pre><span></span><code><span class=c1># Wrong way to make train/test sets for time series</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span> 

<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>timesteps</span><span class=p>,</span> <span class=c1># dates</span>
                                                    <span class=n>prices</span><span class=p>,</span> <span class=c1># prices</span>
                                                    <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span>
                                                    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_train</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>X_test</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y_train</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y_test</span><span class=o>.</span><span class=n>shape</span> 
</code></pre></div> <div class=highlight><pre><span></span><code>((2229,), (558,), (2229,), (558,))
</code></pre></div> <p>Looks like the splits worked well, but let's not trust numbers on a page, let's visualize, visualize, visualize!</p> <div class=highlight><pre><span></span><code><span class=c1># Let&#39;s plot wrong train and test splits</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Train data&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Test data&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Date&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;BTC Price&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>fontsize</span><span class=o>=</span><span class=mi>14</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>();</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_28_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_28_0.png></a></p> <p>Hmmm... what's wrong with this plot?</p> <p>Well, let's remind ourselves of what we're trying to do.</p> <p>We're trying to use the historical price of Bitcoin to predict future prices of Bitcoin.</p> <p>With this in mind, our seen data (training set) is what?</p> <p>Prices of Bitcoin in the past.</p> <p>And our unseen data (test set) is?</p> <p>Prices of Bitcoin in the future. </p> <p>Does the plot above reflect this?</p> <p>No. </p> <p>Our test data is scattered all throughout the training data.</p> <p>This kind of random split is okay for datasets without a time component (such as images or passages of text for classification problems) but for time series, we've got to take the time factor into account.</p> <p>To fix this, we've got to split our data in a way that reflects what we're actually trying to do.</p> <p>We need to split our historical Bitcoin data to have a dataset that reflects the past (train set) and a dataset that reflects the future (test set).</p> <h3 id=create-train-test-sets-for-time-series-the-right-way>Create train &amp; test sets for time series (the right way)</h3> <p>Of course, there's no way we can actually access data from the future.</p> <p>But we can engineer our test set to be in the future with respect to the training set.</p> <p>To do this, we can create an abitrary point in time to split our data.</p> <p>Everything before the point in time can be considered the training set and everything after the point in time can be considered the test set.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-time-series-train-test-split.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="time series train test split" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-time-series-train-test-split.png></a> <em>Demonstration of time series split. Rather than a traditionaly random train/test split, it's best to split the time series data sequentially. Meaning, the test data should be data from the future when compared to the training data.</em></p> <div class=highlight><pre><span></span><code><span class=c1># Create train and test splits the right way for time series data</span>
<span class=n>split_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=mf>0.8</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>prices</span><span class=p>))</span> <span class=c1># 80% train, 20% test</span>

<span class=c1># Create train data splits (everything before the split)</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span> <span class=o>=</span> <span class=n>timesteps</span><span class=p>[:</span><span class=n>split_size</span><span class=p>],</span> <span class=n>prices</span><span class=p>[:</span><span class=n>split_size</span><span class=p>]</span>

<span class=c1># Create test data splits (everything after the split)</span>
<span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>timesteps</span><span class=p>[</span><span class=n>split_size</span><span class=p>:],</span> <span class=n>prices</span><span class=p>[</span><span class=n>split_size</span><span class=p>:]</span>

<span class=nb>len</span><span class=p>(</span><span class=n>X_train</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>X_test</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_train</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2229, 558, 2229, 558)
</code></pre></div> <p>Okay, looks like our custom made splits are the same lengths as the splits we made with <code>train_test_split</code>.</p> <p>But again, these are numbers on a page.</p> <p>And you know how the saying goes, trust one eye more than two ears.</p> <p>Let's visualize.</p> <div class=highlight><pre><span></span><code><span class=c1># Plot correctly made splits</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Train data&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Test data&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Date&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;BTC Price&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>fontsize</span><span class=o>=</span><span class=mi>14</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>();</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_33_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_33_0.png></a></p> <p>That looks much better!</p> <p>Do you see what's happened here?</p> <p>We're going to be using the training set (past) to train a model to try and predict values on the test set (future).</p> <p>Because the test set is an <em>artificial</em> future, we can guage how our model might perform on <em>actual</em> future data.</p> <blockquote> <p>🔑 <strong>Note:</strong> The amount of data you reserve for your test set not set in stone. You could have 80/20, 90/10, 95/5 splits or in some cases, you might not even have enough data to split into train and test sets (see the resource below). The point is to remember the test set is a pseudofuture and not the actual future, it is only meant to give you an indication of how the models you're building are performing. </p> <p>📖 <strong>Resource:</strong> Working with time series data can be tricky compared to other kinds of data. And there are a few pitfalls to watch out for, such as how much data to use for a test set. The article <a href=https://towardsdatascience.com/3-facts-about-time-series-forecasting-that-surprise-experienced-machine-learning-practitioners-69c18ee89387><em>3 facts about time series forecasting that surprise experienced machine learning practitioners</em></a> talks about different things to watch out for when working with time series data, I'd recommend reading it.</p> </blockquote> <h3 id=create-a-plotting-function>Create a plotting function</h3> <p>Rather than retyping <code>matplotlib</code> commands to continuously plot data, let's make a plotting function we can reuse later.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a function to plot time series data</span>
<span class=k>def</span><span class=w> </span><span class=nf>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=p>,</span> <span class=n>values</span><span class=p>,</span> <span class=nb>format</span><span class=o>=</span><span class=s1>&#39;.&#39;</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Plots a timesteps (a series of points in time) against values (a series of values across timesteps).</span>

<span class=sd>  Parameters</span>
<span class=sd>  ---------</span>
<span class=sd>  timesteps : array of timesteps</span>
<span class=sd>  values : array of values across time</span>
<span class=sd>  format : style of plot, default &quot;.&quot;</span>
<span class=sd>  start : where to start the plot (setting a value will index from start of timesteps &amp; values)</span>
<span class=sd>  end : where to end the plot (setting a value will index from end of timesteps &amp; values)</span>
<span class=sd>  label : label to show on plot of values</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=c1># Plot the series</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>timesteps</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>end</span><span class=p>],</span> <span class=n>values</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>end</span><span class=p>],</span> <span class=nb>format</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=n>label</span><span class=p>)</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Time&quot;</span><span class=p>)</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;BTC Price&quot;</span><span class=p>)</span>
  <span class=k>if</span> <span class=n>label</span><span class=p>:</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>fontsize</span><span class=o>=</span><span class=mi>14</span><span class=p>)</span> <span class=c1># make label bigger</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Try out our plotting function</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_train</span><span class=p>,</span> <span class=n>values</span><span class=o>=</span><span class=n>y_train</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Train data&quot;</span><span class=p>)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>,</span> <span class=n>values</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Test data&quot;</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_37_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_37_0.png></a></p> <p>Looking good!</p> <p>Time for some modelling experiments.</p> <h2 id=modelling-experiments>Modelling Experiments</h2> <p>We can build almost any kind of model for our problem as long as the data inputs and outputs are formatted correctly.</p> <p>However, just because we <em>can</em> build <em>almost any</em> kind of model, doesn't mean it'll perform well/should be used in a production setting.</p> <p>We'll see what this means as we build and evaluate models throughout.</p> <p>Before we discuss what modelling experiments we're going to run, there are two terms you should be familiar with, <strong>horizon</strong> and <strong>window</strong>. * <strong>horizon</strong> = number of timesteps to predict into future * <strong>window</strong> = number of timesteps from past used to predict <strong>horizon</strong></p> <p>For example, if we wanted to predict the price of Bitcoin for tomorrow (1 day in the future) using the previous week's worth of Bitcoin prices (7 days in the past), the horizon would be 1 and the window would be 7.</p> <p>Now, how about those modelling experiments?</p> <table> <thead> <tr> <th>Model Number</th> <th>Model Type</th> <th>Horizon size</th> <th>Window size</th> <th>Extra data</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>Naïve model (baseline)</td> <td>NA</td> <td>NA</td> <td>NA</td> </tr> <tr> <td>1</td> <td>Dense model</td> <td>1</td> <td>7</td> <td>NA</td> </tr> <tr> <td>2</td> <td>Same as 1</td> <td>1</td> <td>30</td> <td>NA</td> </tr> <tr> <td>3</td> <td>Same as 1</td> <td>7</td> <td>30</td> <td>NA</td> </tr> <tr> <td>4</td> <td>Conv1D</td> <td>1</td> <td>7</td> <td>NA</td> </tr> <tr> <td>5</td> <td>LSTM</td> <td>1</td> <td>7</td> <td>NA</td> </tr> <tr> <td>6</td> <td>Same as 1 (but with multivariate data)</td> <td>1</td> <td>7</td> <td>Block reward size</td> </tr> <tr> <td>7</td> <td><a href=https://arxiv.org/pdf/1905.10437.pdf>N-BEATs Algorithm</a></td> <td>1</td> <td>7</td> <td>NA</td> </tr> <tr> <td>8</td> <td>Ensemble (multiple models optimized on different loss functions)</td> <td>1</td> <td>7</td> <td>NA</td> </tr> <tr> <td>9</td> <td>Future prediction model (model to predict future values)</td> <td>1</td> <td>7</td> <td>NA</td> </tr> <tr> <td>10</td> <td>Same as 1 (but with turkey 🦃 data introduced)</td> <td>1</td> <td>7</td> <td>NA</td> </tr> </tbody> </table> <blockquote> <p>🔑 <strong>Note:</strong> To reiterate, as you can see, we can build many types of models for the data we're working with. But that doesn't mean that they'll perform well. Deep learning is a powerful technique but it doesn't always work. And as always, start with a simple model first and then add complexity as needed. </p> </blockquote> <h2 id=model-0-naive-forecast-baseline>Model 0: Naïve forecast (baseline)</h2> <p>As usual, let's start with a baseline.</p> <p>One of the most common baseline models for time series forecasting, the naïve model (also called the <a href=https://otexts.com/fpp3/simple-methods.html#na%C3%AFve-method>naïve forecast</a>), requires no training at all.</p> <p>That's because all the naïve model does is use the previous timestep value to predict the next timestep value.</p> <p>The formula looks like this:</p> <div class=arithmatex>\[\hat{y}_{t} = y_{t-1}\]</div> <p>In English: </p> <blockquote> <p>The prediction at timestep <code>t</code> (y-hat) is equal to the value at timestep <code>t-1</code> (the previous timestep).</p> </blockquote> <p>Sound simple?</p> <p>Maybe not.</p> <p>In an open system (like a stock market or crypto market), you'll often find beating the naïve forecast with <em>any</em> kind of model is quite hard.</p> <blockquote> <p>🔑 <strong>Note:</strong> For the sake of this notebook, an <strong>open system</strong> is a system where inputs and outputs can freely flow, such as a market (stock or crypto). Where as, a <strong>closed system</strong> the inputs and outputs are contained within the system (like a poker game with your buddies, you know the buy in and you know how much the winner can get). Time series forecasting in <strong>open systems</strong> is generally quite poor.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Create a naïve forecast</span>
<span class=n>naive_forecast</span> <span class=o>=</span> <span class=n>y_test</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=c1># Naïve forecast equals every value excluding the last value</span>
<span class=n>naive_forecast</span><span class=p>[:</span><span class=mi>10</span><span class=p>],</span> <span class=n>naive_forecast</span><span class=p>[</span><span class=o>-</span><span class=mi>10</span><span class=p>:]</span> <span class=c1># View frist 10 and last 10 </span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([9226.48582088, 8794.35864452, 8798.04205463, 9081.18687849,
        8711.53433917, 8760.89271814, 8749.52059102, 8656.97092235,
        8500.64355816, 8469.2608989 ]),
 array([57107.12067189, 58788.20967893, 58102.19142623, 55715.54665129,
        56573.5554719 , 52147.82118698, 49764.1320816 , 50032.69313676,
        47885.62525472, 45604.61575361]))
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Plot naive forecast</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_train</span><span class=p>,</span> <span class=n>values</span><span class=o>=</span><span class=n>y_train</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Train data&quot;</span><span class=p>)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>,</span> <span class=n>values</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Test data&quot;</span><span class=p>)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>[</span><span class=mi>1</span><span class=p>:],</span> <span class=n>values</span><span class=o>=</span><span class=n>naive_forecast</span><span class=p>,</span> <span class=nb>format</span><span class=o>=</span><span class=s2>&quot;-&quot;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Naive forecast&quot;</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_42_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_42_0.png></a></p> <p>The naive forecast looks like it's following the data well. </p> <p>Let's zoom in to take a better look.</p> <p>We can do so by creating an offset value and passing it to the <code>start</code> parameter of our <code>plot_time_series()</code> function.</p> <div class=highlight><pre><span></span><code><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>offset</span> <span class=o>=</span> <span class=mi>300</span> <span class=c1># offset the values by 300 timesteps </span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>,</span> <span class=n>values</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Test data&quot;</span><span class=p>)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>[</span><span class=mi>1</span><span class=p>:],</span> <span class=n>values</span><span class=o>=</span><span class=n>naive_forecast</span><span class=p>,</span> <span class=nb>format</span><span class=o>=</span><span class=s2>&quot;-&quot;</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Naive forecast&quot;</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_44_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_44_0.png></a></p> <p>When we zoom in we see the naïve forecast comes slightly after the test data. This makes sense because the naive forecast uses the previous timestep value to predict the next timestep value.</p> <p>Forecast made. Time to evaluate it.</p> <h2 id=evaluating-a-time-series-model>Evaluating a time series model</h2> <p>Time series forecasting often involves predicting a number (in our case, the price of Bitcoin).</p> <p>And what kind of problem is predicting a number?</p> <p>Ten points if you said regression.</p> <p>With this known, we can use regression evaluation metrics to evaluate our time series forecasts.</p> <p>The main thing we will be evaluating is: <strong>how do our model's predictions (<code>y_pred</code>) compare against the actual values (<code>y_true</code> or <em>ground truth values</em>)</strong>? </p> <blockquote> <p>📖 <strong>Resource:</strong> We're going to be using several metrics to evaluate our different model's time series forecast accuracy. Many of them are sourced and explained mathematically and conceptually in <a href=https://otexts.com/fpp3/accuracy.html>Forecasting: Principles and Practice chapter 5.8</a>, I'd recommend reading through here for a more in-depth overview of what we're going to practice.</p> </blockquote> <p>For all of the following metrics, <strong>lower is better</strong> (for example an MAE of 0 is better than an MAE 100).</p> <h3 id=scale-dependent-errors>Scale-dependent errors</h3> <p>These are metrics which can be used to compare time series values and forecasts that are on the same scale.</p> <p>For example, Bitcoin historical prices in USD veresus Bitcoin forecast values in USD.</p> <table> <thead> <tr> <th>Metric</th> <th>Details</th> <th>Code</th> </tr> </thead> <tbody> <tr> <td><strong>MAE</strong> (mean absolute error)</td> <td>Easy to interpret (a forecast is X amount different from actual amount). Forecast methods which minimises the MAE will lead to forecasts of the median.</td> <td><a href=https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError><code>tf.keras.metrics.mean_absolute_error()</code></a></td> </tr> <tr> <td><strong>RMSE</strong> (root mean square error)</td> <td>Forecasts which minimise the RMSE lead to forecasts of the mean.</td> <td><code>tf.sqrt(</code><a href=https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError><code>tf.keras.metrics.mean_square_error()</code></a><code>)</code></td> </tr> </tbody> </table> <h3 id=percentage-errors>Percentage errors</h3> <p>Percentage errors do not have units, this means they can be used to compare forecasts across different datasets.</p> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Details</strong></th> <th><strong>Code</strong></th> </tr> </thead> <tbody> <tr> <td><strong>MAPE</strong> (mean absolute percentage error)</td> <td>Most commonly used percentage error. May explode (not work) if <code>y=0</code>.</td> <td><a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAPE><code>tf.keras.metrics.mean_absolute_percentage_error()</code></a></td> </tr> <tr> <td><strong>sMAPE</strong> (symmetric mean absolute percentage error)</td> <td>Recommended not to be used by <a href=https://otexts.com/fpp3/accuracy.html#percentage-errors>Forecasting: Principles and Practice</a>, though it is used in forecasting competitions.</td> <td>Custom implementation</td> </tr> </tbody> </table> <h3 id=scaled-errors>Scaled errors</h3> <p>Scaled errors are an alternative to percentage errors when comparing forecast performance across different time series.</p> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Details</strong></th> <th><strong>Code</strong></th> </tr> </thead> <tbody> <tr> <td><strong>MASE</strong> (mean absolute scaled error).</td> <td>MASE equals one for the naive forecast (or very close to one). A forecast which performs better than the naïve should get &lt;1 MASE.</td> <td>See sktime's <a href=https://github.com/alan-turing-institute/sktime/blob/ee7a06843a44f4aaec7582d847e36073a9ab0566/sktime/performance_metrics/forecasting/_functions.py#L16><code>mase_loss()</code></a></td> </tr> </tbody> </table> <blockquote> <p>🤔 <strong>Question:</strong> There are so many metrics... which one should I pay most attention to? It's going to depend on your problem. However, since its ease of interpretation (you can explain it in a sentence to your grandma), MAE is often a very good place to start.</p> </blockquote> <p>Since we're going to be evaluing a lot of models, let's write a function to help us calculate evaluation metrics on their forecasts.</p> <p>First we'll need TensorFlow.</p> <div class=highlight><pre><span></span><code><span class=c1># Let&#39;s get TensorFlow! </span>
<span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>
</code></pre></div> <p>And since TensorFlow doesn't have a ready made version of MASE (mean aboslute scaled error), how about we create our own?</p> <p>We'll take inspiration from <a href=https://github.com/alan-turing-institute/sktime>sktime</a>'s (Scikit-Learn for time series) <a href=https://www.sktime.org/en/stable/api_reference/auto_generated/sktime.performance_metrics.forecasting.MeanAbsoluteScaledError.html#sktime.performance_metrics.forecasting.MeanAbsoluteScaledError><code>MeanAbsoluteScaledError</code></a> class which calculates the MASE.</p> <div class=highlight><pre><span></span><code><span class=c1># MASE implemented courtesy of sktime - https://github.com/alan-turing-institute/sktime/blob/ee7a06843a44f4aaec7582d847e36073a9ab0566/sktime/performance_metrics/forecasting/_functions.py#L16</span>
<span class=k>def</span><span class=w> </span><span class=nf>mean_absolute_scaled_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Implement MASE (assuming no seasonality of data).</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=n>mae</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>))</span>

  <span class=c1># Find MAE of naive forecast (no seasonality)</span>
  <span class=n>mae_naive_no_season</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span> <span class=o>-</span> <span class=n>y_true</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]))</span> <span class=c1># our seasonality is 1 day (hence the shifting of 1 day)</span>

  <span class=k>return</span> <span class=n>mae</span> <span class=o>/</span> <span class=n>mae_naive_no_season</span>
</code></pre></div> <p>You'll notice the version of MASE above doesn't take in the training values like sktime's <code>mae_loss()</code>. In our case, we're comparing the MAE of our predictions on the test to the MAE of the naïve forecast on the test set.</p> <p>In practice, if we've created the function correctly, the naïve model should achieve an MASE of 1 (or very close to 1). Any model worse than the naïve forecast will achieve an MASE of &gt;1 and any model better than the naïve forecast will achieve an MASE of &lt;1.</p> <p>Let's put each of our different evaluation metrics together into a function.</p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>):</span>
  <span class=c1># Make sure float32 (for metric calculations)</span>
  <span class=n>y_true</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
  <span class=n>y_pred</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>

  <span class=c1># Calculate various metrics</span>
  <span class=n>mae</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
  <span class=n>mse</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span> <span class=c1># puts and emphasis on outliers (all errors get squared)</span>
  <span class=n>rmse</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>mse</span><span class=p>)</span>
  <span class=n>mape</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_percentage_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
  <span class=n>mase</span> <span class=o>=</span> <span class=n>mean_absolute_scaled_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

  <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;mae&quot;</span><span class=p>:</span> <span class=n>mae</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span>
          <span class=s2>&quot;mse&quot;</span><span class=p>:</span> <span class=n>mse</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span>
          <span class=s2>&quot;rmse&quot;</span><span class=p>:</span> <span class=n>rmse</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span>
          <span class=s2>&quot;mape&quot;</span><span class=p>:</span> <span class=n>mape</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span>
          <span class=s2>&quot;mase&quot;</span><span class=p>:</span> <span class=n>mase</span><span class=o>.</span><span class=n>numpy</span><span class=p>()}</span>
</code></pre></div> <p>Looking good! How about we test our function on the naive forecast?</p> <div class=highlight><pre><span></span><code><span class=n>naive_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>y_test</span><span class=p>[</span><span class=mi>1</span><span class=p>:],</span>
                               <span class=n>y_pred</span><span class=o>=</span><span class=n>naive_forecast</span><span class=p>)</span>
<span class=n>naive_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 567.9802,
 &#39;mape&#39;: 2.516525,
 &#39;mase&#39;: 0.99957,
 &#39;mse&#39;: 1147547.0,
 &#39;rmse&#39;: 1071.2362}
</code></pre></div> <p>Alright, looks like we've got some baselines to beat.</p> <p>Taking a look at the naïve forecast's MAE, it seems on average each forecast is ~$567 different than the actual Bitcoin price.</p> <p>How does this compare to the average price of Bitcoin in the test dataset?</p> <div class=highlight><pre><span></span><code><span class=c1># Find average price of Bitcoin in test dataset</span>
<span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>y_test</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>20056.632963737226
</code></pre></div> <p>Okay, looking at these two values is starting to give us an idea of how our model is performing: * The average price of Bitcoin in the test dataset is: $20,056 (note: average may not be the best measure here, since the highest price is over 3x this value and the lowest price is over 4x lower)</p> <ul> <li>Each prediction in naive forecast is on average off by: $567</li> </ul> <p>Is this enough to say it's a good model?</p> <p>That's up your own interpretation. Personally, I'd prefer a model which was closer to the mark.</p> <p>How about we try and build one?</p> <h2 id=other-kinds-of-time-series-forecasting-models-which-can-be-used-for-baselines-and-actual-forecasts>Other kinds of time series forecasting models which can be used for baselines and actual forecasts</h2> <p>Since we've got a naïve forecast baseline to work with, it's time we start building models to try and beat it.</p> <p>And because this course is focused on TensorFlow and deep learning, we're going to be using TensorFlow to build deep learning models to try and improve on our naïve forecasting results.</p> <p>That being said, there are many other kinds of models you may want to look into for building baselines/performing forecasts.</p> <p>Some of them may even beat our best performing models in this notebook, however, I'll leave trying them out for extra-curriculum.</p> <table> <thead> <tr> <th><strong>Model/Library Name</strong></th> <th><strong>Resource</strong></th> </tr> </thead> <tbody> <tr> <td>Moving average</td> <td>https://machinelearningmastery.com/moving-average-smoothing-for-time-series-forecasting-python/</td> </tr> <tr> <td>ARIMA (Autoregression Integrated Moving Average)</td> <td>https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/</td> </tr> <tr> <td>sktime (Scikit-Learn for time series)</td> <td>https://github.com/alan-turing-institute/sktime</td> </tr> <tr> <td>TensorFlow Decision Forests (random forest, gradient boosting trees)</td> <td>https://www.tensorflow.org/decision_forests</td> </tr> <tr> <td>Facebook Kats (purpose-built forecasting and time series analysis library by Facebook)</td> <td>https://github.com/facebookresearch/Kats</td> </tr> <tr> <td>LinkedIn Greykite (flexible, intuitive and fast forecasts)</td> <td>https://github.com/linkedin/greykite</td> </tr> </tbody> </table> <h2 id=format-data-part-2-windowing-dataset>Format Data Part 2: Windowing dataset</h2> <p>Surely we'd be ready to start building models by now?</p> <p>We're so close! Only one more step (really two) to go.</p> <p>We've got to window our time series.</p> <p>Why do we window?</p> <p>Windowing is a method to turn a time series dataset into <strong>supervised learning problem</strong>. </p> <p>In other words, we want to use windows of the past to predict the future.</p> <p>For example for a univariate time series, windowing for one week (<code>window=7</code>) to predict the next single value (<code>horizon=1</code>) might look like:</p> <div class=highlight><pre><span></span><code>Window for one week (univariate time series)

[0, 1, 2, 3, 4, 5, 6] -&gt; [7]
[1, 2, 3, 4, 5, 6, 7] -&gt; [8]
[2, 3, 4, 5, 6, 7, 8] -&gt; [9]
</code></pre></div> <p>Or for the price of Bitcoin, it'd look like:</p> <div class=highlight><pre><span></span><code>Window for one week with the target of predicting the next day (Bitcoin prices)

[123.654, 125.455, 108.584, 118.674, 121.338, 120.655, 121.795] -&gt; [123.033]
[125.455, 108.584, 118.674, 121.338, 120.655, 121.795, 123.033] -&gt; [124.049]
[108.584, 118.674, 121.338, 120.655, 121.795, 123.033, 124.049] -&gt; [125.961]
</code></pre></div> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-time-series-windows-and-horizons.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="windows and horizons for turning time series data into a supervised learning problem" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-time-series-windows-and-horizons.png></a> <em>Example of windows and horizons for Bitcoin data. Windowing can be used to turn time series data into a supervised learning problem.</em></p> <p>Let's build some functions which take in a univariate time series and turn it into windows and horizons of specified sizes.</p> <p>We'll start with the default horizon size of 1 and a window size of 7 (these aren't necessarily the best values to use, I've just picked them).</p> <div class=highlight><pre><span></span><code><span class=n>HORIZON</span> <span class=o>=</span> <span class=mi>1</span> <span class=c1># predict 1 step at a time</span>
<span class=n>WINDOW_SIZE</span> <span class=o>=</span> <span class=mi>7</span> <span class=c1># use a week worth of timesteps to predict the horizon</span>
</code></pre></div> <p>Now we'll write a function to take in an array and turn it into a window and horizon.</p> <div class=highlight><pre><span></span><code><span class=c1># Create function to label windowed data</span>
<span class=k>def</span><span class=w> </span><span class=nf>get_labelled_windows</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>horizon</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Creates labels for windowed dataset.</span>

<span class=sd>  E.g. if horizon=1 (default)</span>
<span class=sd>  Input: [1, 2, 3, 4, 5, 6] -&gt; Output: ([1, 2, 3, 4, 5], [6])</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=k>return</span> <span class=n>x</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=n>horizon</span><span class=p>],</span> <span class=n>x</span><span class=p>[:,</span> <span class=o>-</span><span class=n>horizon</span><span class=p>:]</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Test out the window labelling function</span>
<span class=n>test_window</span><span class=p>,</span> <span class=n>test_label</span> <span class=o>=</span> <span class=n>get_labelled_windows</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>8</span><span class=p>)</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span> <span class=n>horizon</span><span class=o>=</span><span class=n>HORIZON</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Window: </span><span class=si>{</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>test_window</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=s2> -&gt; Label: </span><span class=si>{</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>test_label</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Window: [1 2 3 4 5 6 7] -&gt; Label: 8
</code></pre></div> <p>Oh yeah, that's what I'm talking about!</p> <p>Now we need a way to make windows for an entire time series.</p> <p>We could do this with Python for loops, however, for large time series, that'd be quite slow.</p> <p>To speed things up, we'll leverage <a href=https://numpy.org/doc/stable/reference/arrays.indexing.html>NumPy's array indexing</a>.</p> <p>Let's write a function which: 1. Creates a window step of specific window size, for example: <code>[[0, 1, 2, 3, 4, 5, 6, 7]]</code> 2. Uses NumPy indexing to create a 2D of multiple window steps, for example: <div class=highlight><pre><span></span><code>[[0, 1, 2, 3, 4, 5, 6, 7],
 [1, 2, 3, 4, 5, 6, 7, 8],
 [2, 3, 4, 5, 6, 7, 8, 9]]
</code></pre></div> 3. Uses the 2D array of multuple window steps to index on a target series 4. Uses the <code>get_labelled_windows()</code> function we created above to turn the window steps into windows with a specified horizon</p> <blockquote> <p>📖 <strong>Resource:</strong> The function created below has been adapted from Syafiq Kamarul Azman's article <a href=https://towardsdatascience.com/fast-and-robust-sliding-window-vectorization-with-numpy-3ad950ed62f5><em>Fast and Robust Sliding Window Vectorization with NumPy</em></a>.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Create function to view NumPy arrays as windows </span>
<span class=k>def</span><span class=w> </span><span class=nf>make_windows</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=mi>7</span><span class=p>,</span> <span class=n>horizon</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Turns a 1D array into a 2D array of sequential windows of window_size.</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=c1># 1. Create a window of specific window_size (add the horizon on the end for later labelling)</span>
  <span class=n>window_step</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>window_size</span><span class=o>+</span><span class=n>horizon</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
  <span class=c1># print(f&quot;Window step:\n {window_step}&quot;)</span>

  <span class=c1># 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)</span>
  <span class=n>window_indexes</span> <span class=o>=</span> <span class=n>window_step</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>-</span><span class=p>(</span><span class=n>window_size</span><span class=o>+</span><span class=n>horizon</span><span class=o>-</span><span class=mi>1</span><span class=p>)),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>T</span> <span class=c1># create 2D array of windows of size window_size</span>
  <span class=c1># print(f&quot;Window indexes:\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}&quot;)</span>

  <span class=c1># 3. Index on the target array (time series) with 2D array of multiple window steps</span>
  <span class=n>windowed_array</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>window_indexes</span><span class=p>]</span>

  <span class=c1># 4. Get the labelled windows</span>
  <span class=n>windows</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>get_labelled_windows</span><span class=p>(</span><span class=n>windowed_array</span><span class=p>,</span> <span class=n>horizon</span><span class=o>=</span><span class=n>horizon</span><span class=p>)</span>

  <span class=k>return</span> <span class=n>windows</span><span class=p>,</span> <span class=n>labels</span>
</code></pre></div> <p>Phew! A few steps there... let's see how it goes.</p> <div class=highlight><pre><span></span><code><span class=n>full_windows</span><span class=p>,</span> <span class=n>full_labels</span> <span class=o>=</span> <span class=n>make_windows</span><span class=p>(</span><span class=n>prices</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=n>WINDOW_SIZE</span><span class=p>,</span> <span class=n>horizon</span><span class=o>=</span><span class=n>HORIZON</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>full_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>full_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2780, 2780)
</code></pre></div> <p>Of course we have to visualize, visualize, visualize!</p> <div class=highlight><pre><span></span><code><span class=c1># View the first 3 windows/labels</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>):</span>
  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Window: </span><span class=si>{</span><span class=n>full_windows</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>}</span><span class=s2> -&gt; Label: </span><span class=si>{</span><span class=n>full_labels</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Window: [123.65499 125.455   108.58483 118.67466 121.33866 120.65533 121.795  ] -&gt; Label: [123.033]
Window: [125.455   108.58483 118.67466 121.33866 120.65533 121.795   123.033  ] -&gt; Label: [124.049]
Window: [108.58483 118.67466 121.33866 120.65533 121.795   123.033   124.049  ] -&gt; Label: [125.96116]
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># View the last 3 windows/labels</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>):</span>
  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Window: </span><span class=si>{</span><span class=n>full_windows</span><span class=p>[</span><span class=n>i</span><span class=o>-</span><span class=mi>3</span><span class=p>]</span><span class=si>}</span><span class=s2> -&gt; Label: </span><span class=si>{</span><span class=n>full_labels</span><span class=p>[</span><span class=n>i</span><span class=o>-</span><span class=mi>3</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Window: [58788.20967893 58102.19142623 55715.54665129 56573.5554719
 52147.82118698 49764.1320816  50032.69313676] -&gt; Label: [47885.62525472]
Window: [58102.19142623 55715.54665129 56573.5554719  52147.82118698
 49764.1320816  50032.69313676 47885.62525472] -&gt; Label: [45604.61575361]
Window: [55715.54665129 56573.5554719  52147.82118698 49764.1320816
 50032.69313676 47885.62525472 45604.61575361] -&gt; Label: [43144.47129086]
</code></pre></div> <blockquote> <p>🔑 <strong>Note:</strong> You can find a function which achieves similar results to the ones we implemented above at <a href=https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array><code>tf.keras.preprocessing.timeseries_dataset_from_array()</code></a>. Just like ours, it takes in an array and returns a windowed dataset. It has the benefit of returning data in the form of a tf.data.Dataset instance (we'll see how to do this with our own data later).</p> </blockquote> <h2 id=turning-windows-into-training-and-test-sets>Turning windows into training and test sets</h2> <p>Look how good those windows look! Almost like the stain glass windows on the Sistine Chapel, well, maybe not that good but still.</p> <p>Time to turn our windows into training and test splits.</p> <p>We could've windowed our existing training and test splits, however, with the nature of windowing (windowing often requires an offset at some point in the data), it usually works better to window the data first, then split it into training and test sets.</p> <p>Let's write a function which takes in full sets of windows and their labels and splits them into train and test splits.</p> <div class=highlight><pre><span></span><code><span class=c1># Make the train/test splits</span>
<span class=k>def</span><span class=w> </span><span class=nf>make_train_test_splits</span><span class=p>(</span><span class=n>windows</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>test_split</span><span class=o>=</span><span class=mf>0.2</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Splits matching pairs of windows and labels into train and test splits.</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=n>split_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>windows</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>test_split</span><span class=p>))</span> <span class=c1># this will default to 80% train/20% test</span>
  <span class=n>train_windows</span> <span class=o>=</span> <span class=n>windows</span><span class=p>[:</span><span class=n>split_size</span><span class=p>]</span>
  <span class=n>train_labels</span> <span class=o>=</span> <span class=n>labels</span><span class=p>[:</span><span class=n>split_size</span><span class=p>]</span>
  <span class=n>test_windows</span> <span class=o>=</span> <span class=n>windows</span><span class=p>[</span><span class=n>split_size</span><span class=p>:]</span>
  <span class=n>test_labels</span> <span class=o>=</span> <span class=n>labels</span><span class=p>[</span><span class=n>split_size</span><span class=p>:]</span>
  <span class=k>return</span> <span class=n>train_windows</span><span class=p>,</span> <span class=n>test_windows</span><span class=p>,</span> <span class=n>train_labels</span><span class=p>,</span> <span class=n>test_labels</span>
</code></pre></div> <p>Look at that amazing function, lets test it.</p> <div class=highlight><pre><span></span><code><span class=n>train_windows</span><span class=p>,</span> <span class=n>test_windows</span><span class=p>,</span> <span class=n>train_labels</span><span class=p>,</span> <span class=n>test_labels</span> <span class=o>=</span> <span class=n>make_train_test_splits</span><span class=p>(</span><span class=n>full_windows</span><span class=p>,</span> <span class=n>full_labels</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>train_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_labels</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2224, 556, 2224, 556)
</code></pre></div> <p>Notice the default split of 80% training data and 20% testing data (this split can be adjusted if needed).</p> <p>How do the first 5 samples of the training windows and labels looks?</p> <div class=highlight><pre><span></span><code><span class=n>train_windows</span><span class=p>[:</span><span class=mi>5</span><span class=p>],</span> <span class=n>train_labels</span><span class=p>[:</span><span class=mi>5</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([[123.65499, 125.455  , 108.58483, 118.67466, 121.33866, 120.65533,
         121.795  ],
        [125.455  , 108.58483, 118.67466, 121.33866, 120.65533, 121.795  ,
         123.033  ],
        [108.58483, 118.67466, 121.33866, 120.65533, 121.795  , 123.033  ,
         124.049  ],
        [118.67466, 121.33866, 120.65533, 121.795  , 123.033  , 124.049  ,
         125.96116],
        [121.33866, 120.65533, 121.795  , 123.033  , 124.049  , 125.96116,
         125.27966]]), array([[123.033  ],
        [124.049  ],
        [125.96116],
        [125.27966],
        [125.9275 ]]))
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check to see if same (accounting for horizon and window size)</span>
<span class=n>np</span><span class=o>.</span><span class=n>array_equal</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>train_labels</span><span class=p>[:</span><span class=o>-</span><span class=n>HORIZON</span><span class=o>-</span><span class=mi>1</span><span class=p>]),</span> <span class=n>y_train</span><span class=p>[</span><span class=n>WINDOW_SIZE</span><span class=p>:])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>True
</code></pre></div> <h2 id=make-a-modelling-checkpoint>Make a modelling checkpoint</h2> <p>We're so close to building models. So so so close.</p> <p>Because our model's performance will fluctuate from experiment to experiment, we'll want to make sure we're comparing apples to apples.</p> <p>What I mean by this is in order for a fair comparison, we want to compare each model's best performance against each model's best performance.</p> <p>For example, if <code>model_1</code> performed incredibly well on epoch 55 but its performance fell off toward epoch 100, we want the version of the model from epoch 55 to compare to other models rather than the version of the model from epoch 100.</p> <p>And the same goes for each of our other models: compare the best against the best.</p> <p>To take of this, we'll implement a <a href=https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint><code>ModelCheckpoint</code></a> callback.</p> <p>The <code>ModelCheckpoint</code> callback will monitor our model's performance during training and save the best model to file by setting <code>save_best_only=True</code>.</p> <p>That way when evaluating our model we could restore its best performing configuration from file.</p> <blockquote> <p>🔑 <strong>Note:</strong> Because of the size of the dataset (smaller than usual), you'll notice our modelling experiment results fluctuate quite a bit during training (hence the implementation of the <code>ModelCheckpoint</code> callback to save the best model).</p> </blockquote> <p>Because we're going to be running multiple experiments, it makes sense to keep track of them by saving models to file under different names.</p> <p>To do this, we'll write a small function to create a <code>ModelCheckpoint</code> callback which saves a model to specified filename.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>os</span>

<span class=c1># Create a function to implement a ModelCheckpoint callback with a specific filename </span>
<span class=k>def</span><span class=w> </span><span class=nf>create_model_checkpoint</span><span class=p>(</span><span class=n>model_name</span><span class=p>,</span> <span class=n>save_path</span><span class=o>=</span><span class=s2>&quot;model_experiments&quot;</span><span class=p>):</span>
  <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>ModelCheckpoint</span><span class=p>(</span><span class=n>filepath</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>save_path</span><span class=p>,</span> <span class=n>model_name</span><span class=p>),</span> <span class=c1># create filepath to save model</span>
                                            <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=c1># only output a limited amount of text</span>
                                            <span class=n>save_best_only</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># save only the best model to file</span>
</code></pre></div> <h2 id=model-1-dense-model-window-7-horizon-1>Model 1: Dense model (window = 7, horizon = 1)</h2> <p>Finally!</p> <p>Time to build one of our models.</p> <p>If you think we've been through a fair bit of preprocessing before getting here, you're right.</p> <p>Often, preparing data for a model is one of the largest parts of any machine learning project.</p> <p>And once you've got a good model in place, you'll probably notice far more improvements from manipulating the data (e.g. collecting more, improving the quality) than manipulating the model.</p> <p>We're going to start by keeping it simple, <code>model_1</code> will have: * A single dense layer with 128 hidden units and ReLU (rectified linear unit) activation * An output layer with linear activation (or no activation) * Adam optimizer and MAE loss function * Batch size of 128 * 100 epochs</p> <p>Why these values?</p> <p>I picked them out of experimentation. </p> <p>A batch size of 32 works pretty well too and we could always train for less epochs but since the model runs so fast (you'll see in a second, it's because the number of samples we have isn't massive) we might as well train for more.</p> <blockquote> <p>🔑 <strong>Note:</strong> As always, many of the values for machine learning problems are experimental. A reminder that the values you can set yourself in a machine learning algorithm (the hidden units, the batch size, horizon size, window size) are called [<strong>hyperparameters</strong>](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning). And experimenting to find the best values for hyperparameters is called <strong>hyperparameter tuning</strong>. Where as parameters learned by a model itself (patterns in the data, formally called weights &amp; biases) are referred to as <strong>parameters</strong>.</p> </blockquote> <p>Let's import TensorFlow and build our first deep learning model for time series.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>
<span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras</span><span class=w> </span><span class=kn>import</span> <span class=n>layers</span>

<span class=c1># Set random seed for as reproducible results as possible</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Construct model</span>
<span class=n>model_1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>HORIZON</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;linear&quot;</span><span class=p>)</span> <span class=c1># linear activation is the same as having no activation                        </span>
<span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=s2>&quot;model_1_dense&quot;</span><span class=p>)</span> <span class=c1># give the model a name so we can save it</span>

<span class=c1># Compile model</span>
<span class=n>model_1</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>])</span> <span class=c1># we don&#39;t necessarily need this when the loss function is already MAE</span>

<span class=c1># Fit model</span>
<span class=n>model_1</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x</span><span class=o>=</span><span class=n>train_windows</span><span class=p>,</span> <span class=c1># train windows of 7 timesteps of Bitcoin prices</span>
            <span class=n>y</span><span class=o>=</span><span class=n>train_labels</span><span class=p>,</span> <span class=c1># horizon value of 1 (using the previous 7 timesteps to predict next day)</span>
            <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
            <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
            <span class=n>batch_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
            <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>),</span>
            <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>create_model_checkpoint</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=n>model_1</span><span class=o>.</span><span class=n>name</span><span class=p>)])</span> <span class=c1># create ModelCheckpoint callback to save best model</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
18/18 [==============================] - 3s 12ms/step - loss: 780.3455 - mae: 780.3455 - val_loss: 2279.6526 - val_mae: 2279.6526
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 2/100
18/18 [==============================] - 0s 4ms/step - loss: 247.6756 - mae: 247.6756 - val_loss: 1005.9991 - val_mae: 1005.9991
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 3/100
18/18 [==============================] - 0s 4ms/step - loss: 188.4116 - mae: 188.4116 - val_loss: 923.2862 - val_mae: 923.2862
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 4/100
18/18 [==============================] - 0s 4ms/step - loss: 169.4340 - mae: 169.4340 - val_loss: 900.5872 - val_mae: 900.5872
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 5/100
18/18 [==============================] - 0s 4ms/step - loss: 165.0895 - mae: 165.0895 - val_loss: 895.2238 - val_mae: 895.2238
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 6/100
18/18 [==============================] - 0s 4ms/step - loss: 158.5210 - mae: 158.5210 - val_loss: 855.1982 - val_mae: 855.1982
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 7/100
18/18 [==============================] - 0s 5ms/step - loss: 151.3566 - mae: 151.3566 - val_loss: 840.9168 - val_mae: 840.9168
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 8/100
18/18 [==============================] - 0s 5ms/step - loss: 145.2560 - mae: 145.2560 - val_loss: 803.5956 - val_mae: 803.5956
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 9/100
18/18 [==============================] - 0s 4ms/step - loss: 144.3546 - mae: 144.3546 - val_loss: 799.5455 - val_mae: 799.5455
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 10/100
18/18 [==============================] - 0s 4ms/step - loss: 141.2943 - mae: 141.2943 - val_loss: 763.5010 - val_mae: 763.5010
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 11/100
18/18 [==============================] - 0s 4ms/step - loss: 135.6595 - mae: 135.6595 - val_loss: 771.3357 - val_mae: 771.3357
Epoch 12/100
18/18 [==============================] - 0s 5ms/step - loss: 134.1700 - mae: 134.1700 - val_loss: 782.8079 - val_mae: 782.8079
Epoch 13/100
18/18 [==============================] - 0s 4ms/step - loss: 134.6015 - mae: 134.6015 - val_loss: 784.4449 - val_mae: 784.4449
Epoch 14/100
18/18 [==============================] - 0s 4ms/step - loss: 130.6127 - mae: 130.6127 - val_loss: 751.3234 - val_mae: 751.3234
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 15/100
18/18 [==============================] - 0s 5ms/step - loss: 128.8347 - mae: 128.8347 - val_loss: 696.5757 - val_mae: 696.5757
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 16/100
18/18 [==============================] - 0s 5ms/step - loss: 124.7739 - mae: 124.7739 - val_loss: 702.4698 - val_mae: 702.4698
Epoch 17/100
18/18 [==============================] - 0s 4ms/step - loss: 123.4474 - mae: 123.4474 - val_loss: 704.9241 - val_mae: 704.9241
Epoch 18/100
18/18 [==============================] - 0s 5ms/step - loss: 122.2105 - mae: 122.2105 - val_loss: 667.9725 - val_mae: 667.9725
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 19/100
18/18 [==============================] - 0s 4ms/step - loss: 121.7263 - mae: 121.7263 - val_loss: 718.8797 - val_mae: 718.8797
Epoch 20/100
18/18 [==============================] - 0s 4ms/step - loss: 119.2420 - mae: 119.2420 - val_loss: 657.0667 - val_mae: 657.0667
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 21/100
18/18 [==============================] - 0s 4ms/step - loss: 121.2275 - mae: 121.2275 - val_loss: 637.0330 - val_mae: 637.0330
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 22/100
18/18 [==============================] - 0s 4ms/step - loss: 119.9544 - mae: 119.9544 - val_loss: 671.2490 - val_mae: 671.2490
Epoch 23/100
18/18 [==============================] - 0s 5ms/step - loss: 121.9248 - mae: 121.9248 - val_loss: 633.3593 - val_mae: 633.3593
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 24/100
18/18 [==============================] - 0s 5ms/step - loss: 116.3666 - mae: 116.3666 - val_loss: 624.4852 - val_mae: 624.4852
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 25/100
18/18 [==============================] - 0s 4ms/step - loss: 114.6816 - mae: 114.6816 - val_loss: 619.7571 - val_mae: 619.7571
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 26/100
18/18 [==============================] - 0s 4ms/step - loss: 116.4455 - mae: 116.4455 - val_loss: 615.6364 - val_mae: 615.6364
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 27/100
18/18 [==============================] - 0s 5ms/step - loss: 116.5868 - mae: 116.5868 - val_loss: 615.9631 - val_mae: 615.9631
Epoch 28/100
18/18 [==============================] - 0s 4ms/step - loss: 113.4691 - mae: 113.4691 - val_loss: 608.0920 - val_mae: 608.0920
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 29/100
18/18 [==============================] - 0s 5ms/step - loss: 113.7598 - mae: 113.7598 - val_loss: 621.9306 - val_mae: 621.9306
Epoch 30/100
18/18 [==============================] - 0s 4ms/step - loss: 116.8613 - mae: 116.8613 - val_loss: 604.4056 - val_mae: 604.4056
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 31/100
18/18 [==============================] - 0s 4ms/step - loss: 111.9375 - mae: 111.9375 - val_loss: 609.3882 - val_mae: 609.3882
Epoch 32/100
18/18 [==============================] - 0s 4ms/step - loss: 112.4175 - mae: 112.4175 - val_loss: 603.0588 - val_mae: 603.0588
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 33/100
18/18 [==============================] - 0s 4ms/step - loss: 112.6697 - mae: 112.6697 - val_loss: 645.6975 - val_mae: 645.6975
Epoch 34/100
18/18 [==============================] - 0s 4ms/step - loss: 111.9867 - mae: 111.9867 - val_loss: 604.7632 - val_mae: 604.7632
Epoch 35/100
18/18 [==============================] - 0s 4ms/step - loss: 110.9451 - mae: 110.9451 - val_loss: 593.4648 - val_mae: 593.4648
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 36/100
18/18 [==============================] - 0s 5ms/step - loss: 114.4816 - mae: 114.4816 - val_loss: 608.0073 - val_mae: 608.0073
Epoch 37/100
18/18 [==============================] - 0s 4ms/step - loss: 110.2017 - mae: 110.2017 - val_loss: 597.2309 - val_mae: 597.2309
Epoch 38/100
18/18 [==============================] - 0s 5ms/step - loss: 112.2372 - mae: 112.2372 - val_loss: 637.9797 - val_mae: 637.9797
Epoch 39/100
18/18 [==============================] - 0s 4ms/step - loss: 115.1289 - mae: 115.1289 - val_loss: 587.4679 - val_mae: 587.4679
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 40/100
18/18 [==============================] - 0s 5ms/step - loss: 110.0854 - mae: 110.0854 - val_loss: 592.7117 - val_mae: 592.7117
Epoch 41/100
18/18 [==============================] - 0s 4ms/step - loss: 110.6343 - mae: 110.6343 - val_loss: 593.8997 - val_mae: 593.8997
Epoch 42/100
18/18 [==============================] - 0s 4ms/step - loss: 113.5762 - mae: 113.5762 - val_loss: 636.3674 - val_mae: 636.3674
Epoch 43/100
18/18 [==============================] - 0s 5ms/step - loss: 116.2286 - mae: 116.2286 - val_loss: 662.9264 - val_mae: 662.9264
Epoch 44/100
18/18 [==============================] - 0s 4ms/step - loss: 120.0192 - mae: 120.0192 - val_loss: 635.6360 - val_mae: 635.6360
Epoch 45/100
18/18 [==============================] - 0s 4ms/step - loss: 110.9675 - mae: 110.9675 - val_loss: 601.9926 - val_mae: 601.9926
Epoch 46/100
18/18 [==============================] - 0s 4ms/step - loss: 111.6012 - mae: 111.6012 - val_loss: 593.3531 - val_mae: 593.3531
Epoch 47/100
18/18 [==============================] - 0s 6ms/step - loss: 109.6161 - mae: 109.6161 - val_loss: 637.0014 - val_mae: 637.0014
Epoch 48/100
18/18 [==============================] - 0s 5ms/step - loss: 109.1368 - mae: 109.1368 - val_loss: 598.4199 - val_mae: 598.4199
Epoch 49/100
18/18 [==============================] - 0s 5ms/step - loss: 112.4355 - mae: 112.4355 - val_loss: 579.7040 - val_mae: 579.7040
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 50/100
18/18 [==============================] - 0s 4ms/step - loss: 110.2108 - mae: 110.2108 - val_loss: 639.2326 - val_mae: 639.2326
Epoch 51/100
18/18 [==============================] - 0s 5ms/step - loss: 111.0958 - mae: 111.0958 - val_loss: 597.3575 - val_mae: 597.3575
Epoch 52/100
18/18 [==============================] - 0s 4ms/step - loss: 110.7351 - mae: 110.7351 - val_loss: 580.7227 - val_mae: 580.7227
Epoch 53/100
18/18 [==============================] - 0s 5ms/step - loss: 111.1785 - mae: 111.1785 - val_loss: 648.3588 - val_mae: 648.3588
Epoch 54/100
18/18 [==============================] - 0s 4ms/step - loss: 114.0832 - mae: 114.0832 - val_loss: 593.2007 - val_mae: 593.2007
Epoch 55/100
18/18 [==============================] - 0s 4ms/step - loss: 110.4910 - mae: 110.4910 - val_loss: 579.5065 - val_mae: 579.5065
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 56/100
18/18 [==============================] - 0s 5ms/step - loss: 108.0489 - mae: 108.0489 - val_loss: 807.3851 - val_mae: 807.3851
Epoch 57/100
18/18 [==============================] - 0s 4ms/step - loss: 125.0614 - mae: 125.0614 - val_loss: 674.1654 - val_mae: 674.1654
Epoch 58/100
18/18 [==============================] - 0s 4ms/step - loss: 115.4340 - mae: 115.4340 - val_loss: 582.2698 - val_mae: 582.2698
Epoch 59/100
18/18 [==============================] - 0s 5ms/step - loss: 110.0881 - mae: 110.0881 - val_loss: 606.7637 - val_mae: 606.7637
Epoch 60/100
18/18 [==============================] - 0s 4ms/step - loss: 108.7156 - mae: 108.7156 - val_loss: 602.3102 - val_mae: 602.3102
Epoch 61/100
18/18 [==============================] - 0s 4ms/step - loss: 108.1525 - mae: 108.1525 - val_loss: 573.9990 - val_mae: 573.9990
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 62/100
18/18 [==============================] - 0s 4ms/step - loss: 107.3727 - mae: 107.3727 - val_loss: 581.7012 - val_mae: 581.7012
Epoch 63/100
18/18 [==============================] - 0s 4ms/step - loss: 110.7667 - mae: 110.7667 - val_loss: 637.5252 - val_mae: 637.5252
Epoch 64/100
18/18 [==============================] - 0s 4ms/step - loss: 110.1539 - mae: 110.1539 - val_loss: 586.6601 - val_mae: 586.6601
Epoch 65/100
18/18 [==============================] - 0s 5ms/step - loss: 108.2325 - mae: 108.2325 - val_loss: 573.5620 - val_mae: 573.5620
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 66/100
18/18 [==============================] - 0s 5ms/step - loss: 108.6825 - mae: 108.6825 - val_loss: 572.2206 - val_mae: 572.2206
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 67/100
18/18 [==============================] - 0s 4ms/step - loss: 106.6371 - mae: 106.6371 - val_loss: 646.6349 - val_mae: 646.6349
Epoch 68/100
18/18 [==============================] - 0s 4ms/step - loss: 114.1603 - mae: 114.1603 - val_loss: 681.8561 - val_mae: 681.8561
Epoch 69/100
18/18 [==============================] - 0s 5ms/step - loss: 124.5514 - mae: 124.5514 - val_loss: 655.9885 - val_mae: 655.9885
Epoch 70/100
18/18 [==============================] - 0s 4ms/step - loss: 125.0235 - mae: 125.0235 - val_loss: 601.0032 - val_mae: 601.0032
Epoch 71/100
18/18 [==============================] - 0s 6ms/step - loss: 110.3652 - mae: 110.3652 - val_loss: 595.3962 - val_mae: 595.3962
Epoch 72/100
18/18 [==============================] - 0s 5ms/step - loss: 107.9285 - mae: 107.9285 - val_loss: 573.7085 - val_mae: 573.7085
Epoch 73/100
18/18 [==============================] - 0s 4ms/step - loss: 109.5085 - mae: 109.5085 - val_loss: 580.4180 - val_mae: 580.4180
Epoch 74/100
18/18 [==============================] - 0s 4ms/step - loss: 108.7380 - mae: 108.7380 - val_loss: 576.1211 - val_mae: 576.1211
Epoch 75/100
18/18 [==============================] - 0s 5ms/step - loss: 107.9404 - mae: 107.9404 - val_loss: 591.1477 - val_mae: 591.1477
Epoch 76/100
18/18 [==============================] - 0s 5ms/step - loss: 109.4232 - mae: 109.4232 - val_loss: 597.8605 - val_mae: 597.8605
Epoch 77/100
18/18 [==============================] - 0s 4ms/step - loss: 107.5879 - mae: 107.5879 - val_loss: 571.9299 - val_mae: 571.9299
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 78/100
18/18 [==============================] - 0s 4ms/step - loss: 108.1598 - mae: 108.1598 - val_loss: 575.2383 - val_mae: 575.2383
Epoch 79/100
18/18 [==============================] - 0s 4ms/step - loss: 107.9175 - mae: 107.9175 - val_loss: 617.3071 - val_mae: 617.3071
Epoch 80/100
18/18 [==============================] - 0s 4ms/step - loss: 108.9510 - mae: 108.9510 - val_loss: 583.4847 - val_mae: 583.4847
Epoch 81/100
18/18 [==============================] - 0s 5ms/step - loss: 106.0505 - mae: 106.0505 - val_loss: 570.0802 - val_mae: 570.0802
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 82/100
18/18 [==============================] - 0s 4ms/step - loss: 115.6827 - mae: 115.6827 - val_loss: 575.7382 - val_mae: 575.7382
Epoch 83/100
18/18 [==============================] - 0s 5ms/step - loss: 110.9379 - mae: 110.9379 - val_loss: 659.6570 - val_mae: 659.6570
Epoch 84/100
18/18 [==============================] - 0s 5ms/step - loss: 111.4836 - mae: 111.4836 - val_loss: 570.1959 - val_mae: 570.1959
Epoch 85/100
18/18 [==============================] - 0s 4ms/step - loss: 107.5948 - mae: 107.5948 - val_loss: 601.5945 - val_mae: 601.5945
Epoch 86/100
18/18 [==============================] - 0s 4ms/step - loss: 108.9426 - mae: 108.9426 - val_loss: 592.8107 - val_mae: 592.8107
Epoch 87/100
18/18 [==============================] - 0s 5ms/step - loss: 105.7717 - mae: 105.7717 - val_loss: 603.6169 - val_mae: 603.6169
Epoch 88/100
18/18 [==============================] - 0s 5ms/step - loss: 107.9217 - mae: 107.9217 - val_loss: 569.0500 - val_mae: 569.0500
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 89/100
18/18 [==============================] - 0s 5ms/step - loss: 106.0344 - mae: 106.0344 - val_loss: 568.9512 - val_mae: 568.9512
INFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets
Epoch 90/100
18/18 [==============================] - 0s 5ms/step - loss: 105.4977 - mae: 105.4977 - val_loss: 581.7681 - val_mae: 581.7681
Epoch 91/100
18/18 [==============================] - 0s 5ms/step - loss: 108.8468 - mae: 108.8468 - val_loss: 573.6023 - val_mae: 573.6023
Epoch 92/100
18/18 [==============================] - 0s 5ms/step - loss: 110.8884 - mae: 110.8884 - val_loss: 576.8247 - val_mae: 576.8247
Epoch 93/100
18/18 [==============================] - 0s 5ms/step - loss: 113.8781 - mae: 113.8781 - val_loss: 608.3018 - val_mae: 608.3018
Epoch 94/100
18/18 [==============================] - 0s 5ms/step - loss: 110.5763 - mae: 110.5763 - val_loss: 601.6047 - val_mae: 601.6047
Epoch 95/100
18/18 [==============================] - 0s 4ms/step - loss: 106.5906 - mae: 106.5906 - val_loss: 570.3652 - val_mae: 570.3652
Epoch 96/100
18/18 [==============================] - 0s 4ms/step - loss: 116.9515 - mae: 116.9515 - val_loss: 615.2581 - val_mae: 615.2581
Epoch 97/100
18/18 [==============================] - 0s 5ms/step - loss: 108.0739 - mae: 108.0739 - val_loss: 580.3073 - val_mae: 580.3073
Epoch 98/100
18/18 [==============================] - 0s 4ms/step - loss: 108.7102 - mae: 108.7102 - val_loss: 586.6512 - val_mae: 586.6512
Epoch 99/100
18/18 [==============================] - 0s 5ms/step - loss: 109.0488 - mae: 109.0488 - val_loss: 570.0629 - val_mae: 570.0629
Epoch 100/100
18/18 [==============================] - 0s 4ms/step - loss: 106.1845 - mae: 106.1845 - val_loss: 585.9763 - val_mae: 585.9763





&lt;keras.callbacks.History at 0x7fdcf48bd110&gt;
</code></pre></div> <p>Because of the small size of our data (less than 3000 total samples), the model trains very fast.</p> <p>Let's evaluate it.</p> <div class=highlight><pre><span></span><code><span class=c1># Evaluate model on test data</span>
<span class=n>model_1</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>18/18 [==============================] - 0s 2ms/step - loss: 585.9762 - mae: 585.9762





[585.9761962890625, 585.9761962890625]
</code></pre></div> <p>You'll notice the model achieves the same <code>val_loss</code> (in this case, this is MAE) as the last epoch.</p> <p>But if we load in the version of <code>model_1</code> which was saved to file using the <code>ModelCheckpoint</code> callback, we should see an improvement in results.</p> <div class=highlight><pre><span></span><code><span class=c1># Load in saved best performing model_1 and evaluate on test data</span>
<span class=n>model_1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;model_experiments/model_1_dense&quot;</span><span class=p>)</span>
<span class=n>model_1</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>18/18 [==============================] - 0s 3ms/step - loss: 568.9512 - mae: 568.9512





[568.951171875, 568.951171875]
</code></pre></div> <p>Much better! Due to the fluctuating performance of the model during training, loading back in the best performing model see's a sizeable improvement in MAE.</p> <h2 id=making-forecasts-with-a-model-on-the-test-dataset>Making forecasts with a model (on the test dataset)</h2> <p>We've trained a model and evaluated the it on the test data, but the project we're working on is called BitPredict 💰📈 so how do you think we could use our model to make predictions?</p> <p>Since we're going to be running more modelling experiments, let's write a function which: 1. Takes in a trained model (just like <code>model_1</code>) 2. Takes in some input data (just like the data the model was trained on) 3. Passes the input data to the model's <code>predict()</code> method 4. Returns the predictions</p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>make_preds</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>input_data</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Uses model to make predictions on input_data.</span>

<span class=sd>  Parameters</span>
<span class=sd>  ----------</span>
<span class=sd>  model: trained model </span>
<span class=sd>  input_data: windowed input data (same kind of data model was trained on)</span>

<span class=sd>  Returns model predictions on input_data.</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=n>forecast</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>input_data</span><span class=p>)</span>
  <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>forecast</span><span class=p>)</span> <span class=c1># return 1D array of predictions</span>
</code></pre></div> <p>Nice! </p> <p>Now let's use our <code>make_preds()</code> and see how it goes.</p> <div class=highlight><pre><span></span><code><span class=c1># Make predictions using model_1 on the test dataset and view the results</span>
<span class=n>model_1_preds</span> <span class=o>=</span> <span class=n>make_preds</span><span class=p>(</span><span class=n>model_1</span><span class=p>,</span> <span class=n>test_windows</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>model_1_preds</span><span class=p>),</span> <span class=n>model_1_preds</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(556, &lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
 array([8861.711, 8769.886, 9015.71 , 8795.517, 8723.809, 8730.11 ,
        8691.95 , 8502.054, 8460.961, 8516.547], dtype=float32)&gt;)
</code></pre></div> <blockquote> <p>🔑 <strong>Note:</strong> With these outputs, our model isn't <em>forecasting</em> yet. It's only making predictions on the test dataset. Forecasting would involve a model making predictions into the future, however, the test dataset is only a pseudofuture.</p> </blockquote> <p>Excellent! Now we've got some prediction values, let's use the <code>evaluate_preds()</code> we created before to compare them to the ground truth.</p> <div class=highlight><pre><span></span><code><span class=c1># Evaluate preds</span>
<span class=n>model_1_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>test_labels</span><span class=p>),</span> <span class=c1># reduce to right shape</span>
                                 <span class=n>y_pred</span><span class=o>=</span><span class=n>model_1_preds</span><span class=p>)</span>
<span class=n>model_1_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 568.95123,
 &#39;mape&#39;: 2.5448983,
 &#39;mase&#39;: 0.9994897,
 &#39;mse&#39;: 1171744.0,
 &#39;rmse&#39;: 1082.4713}
</code></pre></div> <p>How did our model go? Did it beat the naïve forecast?</p> <div class=highlight><pre><span></span><code><span class=n>naive_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 567.9802,
 &#39;mape&#39;: 2.516525,
 &#39;mase&#39;: 0.99957,
 &#39;mse&#39;: 1147547.0,
 &#39;rmse&#39;: 1071.2362}
</code></pre></div> <p>It looks like our naïve model beats our first deep model on nearly every metric.</p> <p>That goes to show the power of the naïve model and the reason for having a baseline for any machine learning project.</p> <p>And of course, no evaluation would be finished without visualizing the results.</p> <p>Let's use the <code>plot_time_series()</code> function to plot <code>model_1_preds</code> against the test data.</p> <div class=highlight><pre><span></span><code><span class=n>offset</span> <span class=o>=</span> <span class=mi>300</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=c1># Account for the test_window offset and index into test_labels to ensure correct plotting</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>):],</span> <span class=n>values</span><span class=o>=</span><span class=n>test_labels</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Test_data&quot;</span><span class=p>)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>):],</span> <span class=n>values</span><span class=o>=</span><span class=n>model_1_preds</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>,</span> <span class=nb>format</span><span class=o>=</span><span class=s2>&quot;-&quot;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;model_1_preds&quot;</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_95_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_95_0.png></a></p> <p>What's wrong with these predictions?</p> <p>As mentioned before, they're on the test dataset. So they're not actual forecasts.</p> <p>With our current model setup, how do you think we'd make forecasts for the future?</p> <p>Have a think about it for now, we'll cover this later on.</p> <h2 id=model-2-dense-window-30-horizon-1>Model 2: Dense (window = 30, horizon = 1)</h2> <p>A naïve model is currently beating our handcrafted deep learning model. </p> <p>We can't let this happen.</p> <p>Let's continue our modelling experiments.</p> <p>We'll keep the previous model architecture but use a window size of 30.</p> <p>In other words, we'll use the previous 30 days of Bitcoin prices to try and predict the next day price.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-bitcoin-price-window-for-one-month.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="bitcoin prices windowed for 30 days to predict a horizon of 1 day" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-bitcoin-price-window-for-one-month.png></a> <em>Example of Bitcoin prices windowed for 30 days to predict a horizon of 1.</em></p> <blockquote> <p>🔑 <strong>Note:</strong> Recall from before, the window size (how many timesteps to use to fuel a forecast) and the horizon (how many timesteps to predict into the future) are <strong>hyperparameters</strong>. This means you can tune them to try and find values will result in better performance.</p> </blockquote> <p>We'll start our second modelling experiment by preparing datasets using the functions we created earlier.</p> <div class=highlight><pre><span></span><code><span class=n>HORIZON</span> <span class=o>=</span> <span class=mi>1</span> <span class=c1># predict one step at a time</span>
<span class=n>WINDOW_SIZE</span> <span class=o>=</span> <span class=mi>30</span> <span class=c1># use 30 timesteps in the past</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make windowed data with appropriate horizon and window sizes</span>
<span class=n>full_windows</span><span class=p>,</span> <span class=n>full_labels</span> <span class=o>=</span> <span class=n>make_windows</span><span class=p>(</span><span class=n>prices</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=n>WINDOW_SIZE</span><span class=p>,</span> <span class=n>horizon</span><span class=o>=</span><span class=n>HORIZON</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>full_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>full_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2757, 2757)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make train and testing windows</span>
<span class=n>train_windows</span><span class=p>,</span> <span class=n>test_windows</span><span class=p>,</span> <span class=n>train_labels</span><span class=p>,</span> <span class=n>test_labels</span> <span class=o>=</span> <span class=n>make_train_test_splits</span><span class=p>(</span><span class=n>windows</span><span class=o>=</span><span class=n>full_windows</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=n>full_labels</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>train_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_labels</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2205, 552, 2205, 552)
</code></pre></div> <p>Data prepared!</p> <p>Now let's construct <code>model_2</code>, a model with the same architecture as <code>model_1</code> as well as the same training routine.</p> <div class=highlight><pre><span></span><code><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create model (same model as model 1 but data input will be different)</span>
<span class=n>model_2</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>HORIZON</span><span class=p>)</span> <span class=c1># need to predict horizon number of steps into the future</span>
<span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=s2>&quot;model_2_dense&quot;</span><span class=p>)</span>

<span class=n>model_2</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>())</span>

<span class=n>model_2</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_windows</span><span class=p>,</span>
            <span class=n>train_labels</span><span class=p>,</span>
            <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
            <span class=n>batch_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
            <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
            <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>),</span>
            <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>create_model_checkpoint</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=n>model_2</span><span class=o>.</span><span class=n>name</span><span class=p>)])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets





&lt;keras.callbacks.History at 0x7fdcf1094290&gt;
</code></pre></div> <p>Once again, training goes nice and fast.</p> <p>Let's evaluate our model's performance.</p> <div class=highlight><pre><span></span><code><span class=c1># Evaluate model 2 preds</span>
<span class=n>model_2</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>18/18 [==============================] - 0s 2ms/step - loss: 608.9615





608.9614868164062
</code></pre></div> <p>Hmmm... is that the best it did?</p> <p>How about we try loading in the best performing <code>model_2</code> which was saved to file thanks to our <code>ModelCheckpoint</code> callback.</p> <div class=highlight><pre><span></span><code><span class=c1># Load in best performing model</span>
<span class=n>model_2</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;model_experiments/model_2_dense/&quot;</span><span class=p>)</span>
<span class=n>model_2</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>18/18 [==============================] - 0s 2ms/step - loss: 608.9615





608.9614868164062
</code></pre></div> <p>Excellent! Loading back in the best performing model see's a performance boost.</p> <p>But let's not stop there, let's make some predictions with <code>model_2</code> and then evaluate them just as we did before.</p> <div class=highlight><pre><span></span><code><span class=c1># Get forecast predictions</span>
<span class=n>model_2_preds</span> <span class=o>=</span> <span class=n>make_preds</span><span class=p>(</span><span class=n>model_2</span><span class=p>,</span>
                           <span class=n>input_data</span><span class=o>=</span><span class=n>test_windows</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate results for model 2 predictions</span>
<span class=n>model_2_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>test_labels</span><span class=p>),</span> <span class=c1># remove 1 dimension of test labels</span>
                                 <span class=n>y_pred</span><span class=o>=</span><span class=n>model_2_preds</span><span class=p>)</span>
<span class=n>model_2_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 608.9615,
 &#39;mape&#39;: 2.7693386,
 &#39;mase&#39;: 1.0644706,
 &#39;mse&#39;: 1281438.8,
 &#39;rmse&#39;: 1132.0065}
</code></pre></div> <p>It looks like <code>model_2</code> performs worse than the naïve model as well as <code>model_1</code>!</p> <p>Does this mean a smaller window size is better? (I'll leave this as a challenge you can experiment with)</p> <p>How do the predictions look?</p> <div class=highlight><pre><span></span><code><span class=n>offset</span> <span class=o>=</span> <span class=mi>300</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=c1># Account for the test_window offset</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>):],</span> <span class=n>values</span><span class=o>=</span><span class=n>test_labels</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;test_data&quot;</span><span class=p>)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>):],</span> <span class=n>values</span><span class=o>=</span><span class=n>model_2_preds</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>,</span> <span class=nb>format</span><span class=o>=</span><span class=s2>&quot;-&quot;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;model_2_preds&quot;</span><span class=p>)</span> 
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_111_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_111_0.png></a></p> <h2 id=model-3-dense-window-30-horizon-7>Model 3: Dense (window = 30, horizon = 7)</h2> <p>Let's try and predict 7 days ahead given the previous 30 days.</p> <p>First, we'll update the <code>HORIZON</code> and <code>WINDOW_SIZE</code> variables and create windowed data.</p> <div class=highlight><pre><span></span><code><span class=n>HORIZON</span> <span class=o>=</span> <span class=mi>7</span>
<span class=n>WINDOW_SIZE</span> <span class=o>=</span> <span class=mi>30</span>

<span class=n>full_windows</span><span class=p>,</span> <span class=n>full_labels</span> <span class=o>=</span> <span class=n>make_windows</span><span class=p>(</span><span class=n>prices</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=n>WINDOW_SIZE</span><span class=p>,</span> <span class=n>horizon</span><span class=o>=</span><span class=n>HORIZON</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>full_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>full_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2751, 2751)
</code></pre></div> <p>And we'll split the full dataset windows into training and test sets.</p> <div class=highlight><pre><span></span><code><span class=n>train_windows</span><span class=p>,</span> <span class=n>test_windows</span><span class=p>,</span> <span class=n>train_labels</span><span class=p>,</span> <span class=n>test_labels</span> <span class=o>=</span> <span class=n>make_train_test_splits</span><span class=p>(</span><span class=n>windows</span><span class=o>=</span><span class=n>full_windows</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=n>full_labels</span><span class=p>,</span> <span class=n>test_split</span><span class=o>=</span><span class=mf>0.2</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>train_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_labels</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2200, 551, 2200, 551)
</code></pre></div> <p>Now let's build, compile, fit and evaluate a model.</p> <div class=highlight><pre><span></span><code><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create model (same as model_1 except with different data input size)</span>
<span class=n>model_3</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>HORIZON</span><span class=p>)</span>
<span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=s2>&quot;model_3_dense&quot;</span><span class=p>)</span>

<span class=n>model_3</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>())</span>

<span class=n>model_3</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_windows</span><span class=p>,</span>
            <span class=n>train_labels</span><span class=p>,</span>
            <span class=n>batch_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
            <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
            <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
            <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>),</span>
            <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>create_model_checkpoint</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=n>model_3</span><span class=o>.</span><span class=n>name</span><span class=p>)])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets
INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets





&lt;keras.callbacks.History at 0x7fdcf1d36350&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># How did our model with a larger window size and horizon go?</span>
<span class=n>model_3</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>18/18 [==============================] - 0s 2ms/step - loss: 1321.5201





1321.5201416015625
</code></pre></div> <p>To compare apples to apples (best performing model to best performing model), we've got to load in the best version of <code>model_3</code>. </p> <div class=highlight><pre><span></span><code><span class=c1># Load in best version of model_3 and evaluate</span>
<span class=n>model_3</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;model_experiments/model_3_dense/&quot;</span><span class=p>)</span>
<span class=n>model_3</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>18/18 [==============================] - 0s 2ms/step - loss: 1237.5063





1237.50634765625
</code></pre></div> <p>In this case, the error will be higher because we're predicting 7 steps at a time. </p> <p>This makes sense though because the further you try and predict, the larger your error will be (think of trying to predict the weather 7 days in advance).</p> <p>Let's make predictions with our model using the <code>make_preds()</code> function and evaluate them using the <code>evaluate_preds()</code> function.</p> <div class=highlight><pre><span></span><code><span class=c1># The predictions are going to be 7 steps at a time (this is the HORIZON size)</span>
<span class=n>model_3_preds</span> <span class=o>=</span> <span class=n>make_preds</span><span class=p>(</span><span class=n>model_3</span><span class=p>,</span>
                           <span class=n>input_data</span><span class=o>=</span><span class=n>test_windows</span><span class=p>)</span>
<span class=n>model_3_preds</span><span class=p>[:</span><span class=mi>5</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(5, 7), dtype=float32, numpy=
array([[9004.693 , 9048.1   , 9425.088 , 9258.258 , 9495.798 , 9558.451 ,
        9357.354 ],
       [8735.507 , 8840.304 , 9247.793 , 8885.6   , 9097.188 , 9174.328 ,
        9156.819 ],
       [8672.508 , 8782.388 , 9123.8545, 8770.37  , 9007.13  , 9003.87  ,
        9042.724 ],
       [8874.398 , 8784.737 , 9043.901 , 8943.051 , 9033.479 , 9176.488 ,
        9039.676 ],
       [8825.891 , 8777.4375, 8926.779 , 8870.178 , 9213.232 , 9268.156 ,
        8942.485 ]], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Calculate model_3 results - these are going to be multi-dimensional because</span>
<span class=c1># we&#39;re trying to predict more than one step at a time.</span>
<span class=n>model_3_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>test_labels</span><span class=p>),</span>
                                 <span class=n>y_pred</span><span class=o>=</span><span class=n>model_3_preds</span><span class=p>)</span>
<span class=n>model_3_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: array([ 513.60516 ,  355.08356 ,  327.17035 ,  358.50977 ,  420.53207 ,
         537.8539  ,  545.6606  ,  485.92307 ,  584.4969  ,  687.3814  ,
         836.22675 ,  755.1571  ,  731.4959  ,  775.3398  ,  567.9548  ,
         266.80865 ,  188.80217 ,  188.1077  ,  253.09521 ,  301.4336  ,
         151.10742 ,  196.81424 ,  191.46184 ,  231.65067 ,  143.6114  ,
         122.59033 ,  132.78844 ,  190.8116  ,  179.1598  ,  228.25949 ,
         314.44016 ,  379.093   ,  278.3254  ,  295.34604 ,  299.38525 ,
         248.64963 ,  299.75635 ,  259.6937  ,  180.30559 ,  206.72887 ,
         374.62906 ,  144.85129 ,  142.33607 ,  131.1158  ,   93.94057 ,
          54.825542,   73.7943  ,  103.59989 ,  121.3337  ,  168.67209 ,
         183.90945 ,  152.25314 ,  186.57137 ,  146.91309 ,  240.42955 ,
         351.00668 ,  540.9516  ,  549.15674 ,  521.2422  ,  526.8553  ,
         453.36313 ,  257.98166 ,  277.29492 ,  301.82465 ,  455.71756 ,
         458.96002 ,  503.44427 ,  522.3119  ,  223.07687 ,  250.09473 ,
         297.14468 ,  400.56976 ,  495.79785 ,  364.33664 ,  283.3654  ,
         325.59457 ,  238.21178 ,  318.9777  ,  460.77246 ,  651.0755  ,
         835.88074 ,  669.9654  ,  319.5452  ,  261.99496 ,  142.39217 ,
         136.62834 ,  154.75252 ,  221.32631 ,  290.50446 ,  503.8846  ,
         414.2602  ,  434.35727 ,  377.98926 ,  251.7899  ,  204.28418 ,
         388.22684 ,  360.65945 ,  493.80902 ,  614.86035 ,  754.7017  ,
         533.7708  ,  378.98898 ,  280.49484 ,  339.48062 ,  413.12875 ,
         452.87933 ,  550.53906 ,  634.57214 ,  935.57227 ,  931.6003  ,
         881.2804  ,  426.40094 ,  179.45885 ,  121.66225 ,  160.43806 ,
         372.07037 ,  341.85776 ,  476.52475 ,  618.3239  , 1038.8976  ,
        1569.5022  , 2157.1196  , 1987.6074  , 2158.6108  , 2303.5603  ,
        2662.9421  , 1405.5017  ,  728.30145 ,  351.70047 ,  322.03168 ,
         493.94742 ,  435.48492 ,  565.55615 ,  350.1165  ,  289.0203  ,
         251.21645 ,  409.05566 ,  342.2103  ,  320.83594 ,  330.88596 ,
         357.8457  ,  335.9481  ,  206.1031  ,  544.3837  ,  700.0093  ,
         468.91965 ,  404.4963  ,  172.80176 ,  308.33664 ,  210.47566 ,
         318.95444 ,  486.1319  ,  428.87982 ,  533.91095 ,  433.7813  ,
         396.89447 ,  138.40346 ,  189.96617 ,  170.39133 ,  181.54387 ,
         282.8902  ,  264.3889  ,  250.62172 ,  240.33713 ,  276.9541  ,
         326.0306  ,  489.61572 ,  686.2451  ,  526.9798  ,  603.0119  ,
         825.38275 ,  871.04694 ,  990.06903 , 1090.0593  ,  560.2842  ,
         310.5219  ,  371.39035 ,  348.45996 ,  355.73465 ,  429.56473 ,
         581.2839  ,  550.5218  ,  635.4312  ,  913.40375 ,  840.71844 ,
         305.03488 ,  493.93414 ,  751.3177  ,  410.63434 ,  220.62459 ,
         282.25473 ,  291.85352 ,  422.50293 ,  458.65375 ,  637.2345  ,
         647.82367 ,  417.24442 ,  220.23717 ,  246.72168 ,  200.22935 ,
         455.14926 ,  719.6058  ,  696.3037  ,  485.09836 ,  294.9534  ,
         170.52371 ,  211.82463 ,  270.56488 ,  189.89355 ,  171.21721 ,
         366.4471  ,  231.96303 ,  318.78726 ,  273.79352 ,  358.55582 ,
         412.22797 ,  512.31573 ,  185.43848 ,  196.38113 ,  200.00586 ,
         224.58678 ,  213.41965 ,  186.23926 ,  113.37096 ,  172.23117 ,
         168.89885 ,  236.09584 ,  307.59683 ,  328.93903 ,  566.5961  ,
         285.04282 ,  300.4495  ,  125.45201 ,  168.94322 ,  137.25754 ,
         143.50404 ,  145.27776 ,  107.340126,   77.16044 ,  131.87096 ,
         134.01953 ,  167.45871 ,  137.92188 ,  148.91281 ,  204.95467 ,
         157.89732 ,  196.95201 ,  167.93861 ,  156.45578 ,  188.6808  ,
         161.44113 ,   90.997765,  136.84096 ,  198.51799 ,  230.13881 ,
         294.7839  ,  594.24286 ,  699.64856 ,  815.137   ,  905.33887 ,
        1127.2999  , 1342.6952  , 1317.0686  ,  590.1699  ,  296.34543 ,
         243.4368  ,  256.3987  ,  222.28488 ,  323.8387  ,   82.51339 ,
         120.14453 ,  249.14857 ,  205.73674 ,  243.45451 ,  250.64857 ,
         287.27567 ,  224.3803  ,  266.55972 ,  221.50935 ,  218.49539 ,
         272.42242 ,  279.5964  ,  252.58775 ,  381.56473 ,  417.97028 ,
         624.5562  ,  368.0364  ,  327.25473 ,  263.4396  ,  349.95242 ,
         398.62485 ,  297.07465 ,  147.04924 ,  164.54367 ,  313.36246 ,
         477.7486  ,  675.042   ,  897.269   , 1094.7098  , 1460.177   ,
        1398.5858  ,  952.72375 ,  645.9594  ,  166.17912 ,  144.66867 ,
         189.1822  ,  304.81375 ,  435.10742 ,  449.64426 ,  425.244   ,
         441.93637 ,  407.29605 ,  252.4036  ,  248.36928 ,  336.17062 ,
         482.8111  ,  437.53043 ,  533.0855  ,  346.98047 ,  127.68722 ,
         110.208565,  301.75223 ,  195.50697 ,  174.65248 ,  238.4707  ,
         302.3711  ,  313.51703 ,  310.1289  ,  200.29701 ,  172.47209 ,
         140.2302  ,  252.48479 ,  289.32407 ,  343.62222 ,  504.11816 ,
         635.1339  ,  602.131   ,  519.0612  ,  214.90291 ,  195.91197 ,
         265.03683 ,  198.9008  ,  345.51227 ,  517.22235 ,  631.02997 ,
         988.4249  , 1174.2136  , 1196.2849  , 1253.93    ,  526.06573 ,
         210.31306 ,  215.27205 ,  169.86008 ,  283.78543 ,  269.76117 ,
         228.63477 ,  186.64719 ,  410.5434  ,  601.2659  ,  618.12164 ,
         768.7538  , 1158.5449  , 1232.8798  , 1254.429   ,  423.02524 ,
         390.03613 ,  367.23926 ,  209.55803 ,  530.2231  ,  821.38293 ,
         812.37195 ,  741.19464 ,  953.48285 , 1258.9928  , 1844.9332  ,
        1605.2852  , 1112.7673  ,  594.18945 ,  549.7676  ,  632.5438  ,
         829.2656  , 1103.2213  , 1130.1024  , 1033.3527  ,  878.5851  ,
         595.4096  , 1115.6515  , 1371.1725  , 1385.25    ,  387.52484 ,
         303.74945 ,  495.12305 ,  719.5804  ,  648.9032  ,  766.1624  ,
         683.23157 ,  596.4121  ,  523.09235 ,  577.34015 , 1337.6241  ,
        2454.597   , 2759.2363  , 3135.8757  , 3407.3806  , 3602.6362  ,
        2527.1584  , 1158.1016  ,  679.84375 ,  748.5586  , 1073.4896  ,
        1217.7305  , 1459.5664  , 2502.7336  , 3075.8264  , 3090.2869  ,
        2564.758   , 2660.5317  , 2928.9348  , 3690.5566  , 3525.1433  ,
        4183.6704  , 5144.0693  , 4384.6533  , 4164.9614  , 4431.0967  ,
        3335.121   , 3017.6858  , 2589.5403  , 4103.6313  , 5582.089   ,
        5108.456   , 1382.9648  ,  953.5435  , 1081.6842  , 2483.1992  ,
        2992.056   , 3127.0942  , 2972.2717  , 3054.6477  , 3283.7107  ,
        3617.3652  , 1170.2556  , 1074.1886  , 1059.3181  , 1044.5385  ,
        1060.6202  , 2115.7234  , 3569.8901  , 3474.3647  , 2448.0173  ,
        2641.4202  , 3188.6016  , 4899.913   , 5516.293   , 4635.4883  ,
        4677.036   , 5732.8804  , 5866.7344  , 8083.341   , 5049.8237  ,
        1476.2316  , 1873.3314  , 1219.1033  , 2324.0698  , 2467.0044  ,
        3005.0864  , 2805.0486  , 3688.2556  , 2962.7131  , 3664.236   ,
        5618.6836  , 6925.0913  , 9751.091   , 8790.822   , 5057.624   ,
        2971.6863  , 1715.495   , 1125.7689  , 2201.432   , 3969.5005  ,
        2988.5112  , 2911.9336  , 2796.869   , 3937.8755  , 5449.5913  ,
        6395.02    , 6148.2524  , 5437.706   , 4291.8267  , 2362.3962  ,
        1657.3444  , 1426.63    , 1647.7142  , 2730.3962  , 1813.716   ,
        2061.149   , 3095.72    , 4312.8096  , 5500.7227  , 5258.029   ,
        5152.029   , 1331.4045  , 1634.4017  , 2493.4336  , 3957.9548  ,
        4499.2153  , 2215.8533  , 2587.4136  , 1622.4392  ,  591.8778  ,
        1297.4688  , 1284.5931  ,  915.22656 ,  940.4565  , 1084.6562  ,
         875.89954 , 1041.3527  , 2299.2512  , 3152.6038  , 3518.3923  ,
        2531.5212  , 2682.5737  , 3094.2947  , 3829.3286  , 5550.609   ,
        7159.343   , 8820.526   , 8587.338   , 6777.814   , 5193.2866  ,
        4793.332   , 2605.2517  , 1668.3544  , 2510.7244  , 4105.6865  ,
        6278.3906  , 4109.759   , 1711.1211  , 2261.9878  , 2384.754   ,
        1246.6602  , 1672.8494  , 1103.1322  , 1743.3594  , 1608.452   ,
        1933.8995  , 2742.3455  , 3945.3633  , 5765.9414  , 6955.8384  ,
        8107.707   ], dtype=float32),
 &#39;mape&#39;: array([ 5.8601456 ,  4.087344  ,  3.7758112 ,  4.187648  ,  4.9781313 ,
         6.4483633 ,  6.614449  ,  6.0616755 ,  7.547588  ,  9.095916  ,
        11.300213  , 10.396466  , 10.109091  , 10.697323  ,  7.8628383 ,
         3.6872823 ,  2.585636  ,  2.505     ,  3.3504546 ,  4.013595  ,
         2.0036254 ,  2.6397336 ,  2.6087892 ,  3.160931  ,  1.9612147 ,
         1.6679796 ,  1.7952247 ,  2.5901198 ,  2.436256  ,  3.1327312 ,
         4.3595414 ,  5.2856445 ,  3.9403565 ,  4.3142104 ,  4.3733163 ,
         3.6376827 ,  4.362974  ,  3.7757344 ,  2.6192985 ,  2.9007592 ,
         5.1501136 ,  2.014742  ,  1.9706616 ,  1.8104095 ,  1.3007166 ,
         0.75351447,  1.0192169 ,  1.4282547 ,  1.6741827 ,  2.3589606 ,
         2.563099  ,  2.1327207 ,  2.5974777 ,  2.017332  ,  3.1387668 ,
         4.502132  ,  6.959625  ,  6.9602156 ,  6.5331526 ,  6.5757833 ,
         5.610295  ,  3.0642097 ,  3.2474015 ,  3.4847279 ,  5.222716  ,
         5.229475  ,  5.727943  ,  5.9273834 ,  2.5284538 ,  2.8663971 ,
         3.4309018 ,  4.6874056 ,  5.841506  ,  4.30981   ,  3.3552744 ,
         3.8162014 ,  2.7363582 ,  3.4623704 ,  4.971694  ,  7.0260944 ,
         8.974986  ,  7.167823  ,  3.401335  ,  2.7761257 ,  1.522323  ,
         1.4535459 ,  1.6354691 ,  2.252781  ,  2.9475648 ,  5.073901  ,
         4.1032834 ,  4.295298  ,  3.7052956 ,  2.4620814 ,  2.013232  ,
         3.8428285 ,  3.6306674 ,  5.0162754 ,  6.2794676 ,  7.757465  ,
         5.495671  ,  3.8907118 ,  2.8546908 ,  3.531113  ,  4.464891  ,
         5.0162473 ,  6.1257253 ,  7.1294317 , 10.711446  , 10.66408   ,
        10.087247  ,  4.893642  ,  2.071729  ,  1.362841  ,  1.8011061 ,
         4.219873  ,  4.055696  ,  5.744903  ,  7.492872  , 15.032968  ,
        24.38846   , 35.42812   , 34.702423  , 39.86109   , 42.83657   ,
        49.6046    , 26.081123  , 13.669959  ,  6.360867  ,  5.4986367 ,
         7.9925113 ,  6.8575478 ,  8.82061   ,  5.311197  ,  4.5279207 ,
         3.870413  ,  6.2283797 ,  5.2470355 ,  5.1045485 ,  5.329424  ,
         5.7789664 ,  5.358487  ,  3.2211263 ,  8.118441  , 10.231978  ,
         6.726861  ,  5.757309  ,  2.4041245 ,  4.2864537 ,  2.977692  ,
         4.454852  ,  6.9261975 ,  6.1590724 ,  7.7638254 ,  6.308201  ,
         5.7371497 ,  1.9719449 ,  2.7191157 ,  2.3998728 ,  2.568591  ,
         3.953004  ,  3.6584775 ,  3.453558  ,  3.301869  ,  3.7942226 ,
         4.310324  ,  6.4319835 ,  8.567106  ,  6.293662  ,  7.0238814 ,
         9.469167  ,  9.915066  , 11.217036  , 12.28066   ,  6.209284  ,
         3.2802734 ,  3.8508573 ,  3.6124747 ,  3.7172396 ,  4.6127787 ,
         6.2338514 ,  6.037696  ,  7.1359625 , 10.132258  ,  9.337389  ,
         3.379447  ,  5.190343  ,  7.822919  ,  4.2644997 ,  2.328503  ,
         3.0320923 ,  3.122556  ,  4.5721846 ,  5.013454  ,  7.0593185 ,
         7.180876  ,  4.6288185 ,  2.4294462 ,  2.6893413 ,  2.1346936 ,
         4.6703887 ,  7.429678  ,  7.177754  ,  4.9274387 ,  2.969763  ,
         1.7036035 ,  2.1020818 ,  2.7900844 ,  1.9541025 ,  1.7562655 ,
         3.8170404 ,  2.4557195 ,  3.335657  ,  2.8912652 ,  3.8006802 ,
         4.3685193 ,  5.4371476 ,  1.96905   ,  2.086854  ,  2.1329875 ,
         2.4014482 ,  2.2829742 ,  1.9878384 ,  1.2127163 ,  1.8306142 ,
         1.8156711 ,  2.5207567 ,  3.2923858 ,  3.5837193 ,  6.1877694 ,
         3.1183949 ,  3.2884538 ,  1.3740205 ,  1.8472785 ,  1.5017135 ,
         1.5671413 ,  1.5889224 ,  1.1630745 ,  0.83710796,  1.4223018 ,
         1.4407477 ,  1.8030001 ,  1.4797007 ,  1.599079  ,  2.2175813 ,
         1.7130904 ,  2.1422603 ,  1.8232663 ,  1.7015574 ,  2.0563016 ,
         1.7596645 ,  0.98229814,  1.4560648 ,  2.1070251 ,  2.4114208 ,
         3.0499995 ,  5.7738442 ,  6.618569  ,  7.495159  ,  8.227665  ,
        10.186895  , 11.928129  , 11.676125  ,  5.199727  ,  2.5929737 ,
         2.1249924 ,  2.2335236 ,  1.9233094 ,  2.846623  ,  0.7076666 ,
         1.0350616 ,  2.132228  ,  1.778991  ,  2.1161108 ,  2.1635604 ,
         2.4792838 ,  1.9270526 ,  2.2551262 ,  1.8457135 ,  1.8112589 ,
         2.262672  ,  2.3515143 ,  2.1173332 ,  3.2111585 ,  3.5736024 ,
         5.3715696 ,  3.18694   ,  2.858538  ,  2.295805  ,  3.0580947 ,
         3.4806054 ,  2.5900245 ,  1.2625037 ,  1.4050376 ,  2.7622259 ,
         4.3049536 ,  6.2830725 ,  8.513568  , 10.517086  , 14.161078  ,
        13.640608  ,  9.320237  ,  6.3080745 ,  1.6300542 ,  1.3983166 ,
         1.8191801 ,  2.8875985 ,  4.068832  ,  4.1736484 ,  3.92286   ,
         4.0472727 ,  3.7387483 ,  2.3069727 ,  2.3063238 ,  3.1941137 ,
         4.5895944 ,  4.157942  ,  5.0558047 ,  3.3005743 ,  1.2102847 ,
         1.037109  ,  2.8088598 ,  1.8151615 ,  1.6353209 ,  2.2353542 ,
         2.8412042 ,  2.952003  ,  2.9187465 ,  1.8913074 ,  1.6178632 ,
         1.2982845 ,  2.3039308 ,  2.6034784 ,  3.017652  ,  4.444261  ,
         5.5989385 ,  5.26695   ,  4.535601  ,  1.8671715 ,  1.7055075 ,
         2.3196962 ,  1.7318225 ,  2.7897227 ,  4.062603  ,  4.8944497 ,
         7.6816416 ,  9.107798  ,  9.207181  ,  9.502263  ,  3.9754786 ,
         1.5744586 ,  1.5997065 ,  1.242373  ,  2.0770247 ,  1.974714  ,
         1.6764196 ,  1.3502706 ,  2.7858412 ,  3.9915662 ,  4.0835853 ,
         5.016299  ,  7.6388373 ,  8.078356  ,  8.1091385 ,  2.6612654 ,
         2.4994845 ,  2.2963924 ,  1.304031  ,  3.2585588 ,  4.9145164 ,
         4.7377896 ,  4.253874  ,  5.353144  ,  6.9800787 , 10.170477  ,
         8.772201  ,  6.0037975 ,  3.184015  ,  3.0090022 ,  3.637018  ,
         4.7740936 ,  6.289234  ,  6.4191875 ,  5.904088  ,  4.9963098 ,
         3.1906037 ,  5.8562336 ,  7.18103   ,  7.211652  ,  2.0219958 ,
         1.5912417 ,  2.624319  ,  3.8534112 ,  3.5140653 ,  4.1405296 ,
         3.6843975 ,  3.215591  ,  2.8228219 ,  2.8898468 ,  6.2849083 ,
        11.458009  , 12.342688  , 13.764669  , 14.824157  , 15.566204  ,
        10.829556  ,  4.9278235 ,  2.8345613 ,  3.0226748 ,  4.311801  ,
         4.7200484 ,  5.516797  ,  9.303045  , 11.212711  , 11.037065  ,
         8.631595  ,  8.57773   ,  9.389048  , 11.497772  , 10.463535  ,
        11.683612  , 13.823776  , 11.273857  , 10.805961  , 11.400717  ,
         8.4966135 ,  7.8893795 ,  7.169477  , 11.423848  , 15.540573  ,
        14.24638   ,  3.8430922 ,  2.545029  ,  2.926236  ,  7.288314  ,
         8.999365  ,  9.586316  ,  9.271836  ,  9.423251  , 10.242945  ,
        11.396603  ,  3.6557918 ,  3.3454742 ,  3.2290876 ,  3.159098  ,
         3.1852577 ,  6.1412444 , 10.163153  ,  9.673946  ,  6.657339  ,
         6.975828  ,  8.377453  , 12.244584  , 13.00238   , 10.547294  ,
        10.269914  , 12.4054    , 12.598526  , 17.187769  , 10.61679   ,
         3.0717697 ,  3.7662785 ,  2.3915377 ,  4.4263554 ,  4.6217527 ,
         5.4785805 ,  5.1152287 ,  6.826434  ,  5.61515   ,  7.0909567 ,
        11.599258  , 14.51015   , 20.678543  , 18.670784  , 10.813841  ,
         6.395925  ,  3.68131   ,  2.323965  ,  4.4593787 ,  8.030951  ,
         5.9532943 ,  5.6428704 ,  5.1379447 ,  7.1625338 ,  9.831415  ,
        11.209798  , 10.556711  ,  9.350725  ,  7.3507624 ,  3.9944    ,
         2.8233469 ,  2.3929296 ,  2.8540497 ,  4.729086  ,  3.236382  ,
         3.6810744 ,  5.6685753 ,  8.005048  , 10.247115  ,  9.747856  ,
         9.5460415 ,  2.4771707 ,  2.9854116 ,  4.3493814 ,  6.8806467 ,
         7.758685  ,  3.8011405 ,  4.4251776 ,  2.7610898 ,  1.0093751 ,
         2.2422    ,  2.2289147 ,  1.58688   ,  1.6239213 ,  1.8814766 ,
         1.5162641 ,  1.7166423 ,  3.7594385 ,  5.0978    ,  5.664717  ,
         4.0754285 ,  4.3486743 ,  5.1419683 ,  6.618967  ,  9.823991  ,
        12.98137   , 16.388018  , 16.260675  , 13.249782  , 10.180592  ,
         9.383685  ,  5.1218452 ,  3.2893803 ,  4.634893  ,  7.4068375 ,
        11.284324  ,  7.287351  ,  3.0196326 ,  4.006139  ,  4.172643  ,
         2.233759  ,  3.0054173 ,  1.9702865 ,  3.1210938 ,  2.7774746 ,
         3.4924886 ,  5.1419373 ,  7.6572022 , 11.470654  , 14.311543  ,
        17.28017   ], dtype=float32),
 &#39;mase&#39;: 2.2020736,
 &#39;mse&#39;: array([3.15562156e+05, 1.69165547e+05, 1.44131812e+05, 1.76002922e+05,
        2.63519750e+05, 3.91517188e+05, 3.99524688e+05, 3.44422312e+05,
        4.93892156e+05, 6.88785625e+05, 9.18703562e+05, 8.00988125e+05,
        6.35508625e+05, 6.45535125e+05, 3.57740000e+05, 1.03007117e+05,
        5.33132578e+04, 4.65829805e+04, 1.10349188e+05, 1.11647695e+05,
        4.35359297e+04, 5.45387773e+04, 4.55849375e+04, 7.64886406e+04,
        4.03074453e+04, 2.71072188e+04, 1.90268887e+04, 4.55625000e+04,
        4.25345820e+04, 6.63246172e+04, 1.16741711e+05, 1.69440984e+05,
        1.10486977e+05, 1.59707406e+05, 1.61631141e+05, 1.23861828e+05,
        1.50757328e+05, 1.31196297e+05, 5.50281289e+04, 6.28155391e+04,
        1.77474391e+05, 2.68843926e+04, 2.53033750e+04, 2.58044258e+04,
        1.34592480e+04, 3.88203076e+03, 9.22772559e+03, 1.22399551e+04,
        2.20868379e+04, 3.60825352e+04, 4.87962109e+04, 3.74652812e+04,
        4.10560898e+04, 4.17418750e+04, 1.10490836e+05, 2.00762031e+05,
        3.48149000e+05, 3.58060812e+05, 3.28375281e+05, 2.96505844e+05,
        2.33354141e+05, 1.54047203e+05, 1.43171328e+05, 1.62586469e+05,
        2.96452719e+05, 2.66880719e+05, 2.94189062e+05, 3.10067656e+05,
        6.27178750e+04, 8.63868672e+04, 9.69545391e+04, 2.01814359e+05,
        2.97053594e+05, 2.00674891e+05, 1.27305148e+05, 1.32509578e+05,
        8.46020391e+04, 1.74817453e+05, 2.95115844e+05, 4.75888719e+05,
        7.21894500e+05, 4.62711281e+05, 1.33154750e+05, 8.13640156e+04,
        3.43108672e+04, 3.45204180e+04, 5.36851641e+04, 7.42413281e+04,
        1.09339375e+05, 2.79682844e+05, 2.19119422e+05, 1.99934359e+05,
        1.65903391e+05, 8.16738047e+04, 4.82395195e+04, 1.65301000e+05,
        1.81699391e+05, 3.30645000e+05, 4.63625750e+05, 6.75044000e+05,
        3.34911500e+05, 1.69343750e+05, 1.03219234e+05, 1.48347609e+05,
        2.98684844e+05, 3.71952656e+05, 4.43306688e+05, 5.27904688e+05,
        1.12945275e+06, 1.00652938e+06, 8.03246875e+05, 2.21277969e+05,
        6.64886406e+04, 2.66182090e+04, 3.29688867e+04, 1.68663250e+05,
        1.76257219e+05, 2.99365562e+05, 4.31793438e+05, 1.91346712e+06,
        3.90353825e+06, 6.29502500e+06, 5.49042400e+06, 6.25676850e+06,
        6.28618000e+06, 7.23876400e+06, 2.08143525e+06, 6.66786500e+05,
        1.44639844e+05, 1.83413594e+05, 3.41026562e+05, 3.02069719e+05,
        3.71339750e+05, 2.02418219e+05, 1.36151141e+05, 8.24758203e+04,
        1.96006719e+05, 1.61595656e+05, 1.71401391e+05, 2.00606359e+05,
        2.37198891e+05, 1.58720172e+05, 6.36030352e+04, 3.38904031e+05,
        5.23524156e+05, 2.86212188e+05, 1.92326125e+05, 3.73765547e+04,
        1.31962000e+05, 9.33163047e+04, 1.18443672e+05, 3.00598719e+05,
        2.28996984e+05, 3.57954281e+05, 2.36219078e+05, 1.71530359e+05,
        2.68482539e+04, 4.79163125e+04, 4.00419297e+04, 5.41031875e+04,
        1.07641391e+05, 9.80020000e+04, 7.08350078e+04, 6.55985312e+04,
        8.53740000e+04, 1.33837578e+05, 2.72516312e+05, 6.54133500e+05,
        4.97212750e+05, 6.11190062e+05, 9.92907688e+05, 9.56692312e+05,
        1.09541412e+06, 1.19283125e+06, 3.75002219e+05, 2.22053797e+05,
        2.60620031e+05, 1.88109625e+05, 1.87927500e+05, 2.49058609e+05,
        3.79192656e+05, 4.07706781e+05, 6.24783062e+05, 9.62079375e+05,
        7.95344562e+05, 1.40557531e+05, 2.65227406e+05, 6.26269500e+05,
        2.05804234e+05, 6.53089570e+04, 1.33562969e+05, 1.14692070e+05,
        2.26913016e+05, 2.61226500e+05, 5.08516250e+05, 4.90744656e+05,
        2.38721828e+05, 6.85555703e+04, 7.30576641e+04, 5.36113750e+04,
        3.28223562e+05, 6.26447375e+05, 5.17735438e+05, 3.32536062e+05,
        1.72657266e+05, 8.28138125e+04, 1.43392656e+05, 8.83169844e+04,
        5.15846680e+04, 4.20798125e+04, 1.78049141e+05, 1.17351570e+05,
        1.16870422e+05, 1.02416836e+05, 1.67488781e+05, 2.05509953e+05,
        2.73221469e+05, 4.73881641e+04, 4.60577500e+04, 5.33725703e+04,
        7.87071094e+04, 7.06486719e+04, 5.23131758e+04, 2.50280723e+04,
        4.52024609e+04, 4.14732461e+04, 7.00107344e+04, 1.24132148e+05,
        1.34168141e+05, 3.38401562e+05, 9.75599844e+04, 1.10023000e+05,
        1.99194824e+04, 3.32738164e+04, 2.43711914e+04, 2.76617012e+04,
        2.70386660e+04, 1.58115801e+04, 1.16166348e+04, 2.74852148e+04,
        2.95266426e+04, 3.95874922e+04, 2.81846035e+04, 3.19155312e+04,
        5.60368477e+04, 3.98338398e+04, 5.02201602e+04, 3.19759512e+04,
        2.96142285e+04, 4.48333711e+04, 4.32265664e+04, 1.52425576e+04,
        2.86077324e+04, 4.23488125e+04, 7.25032578e+04, 1.21924688e+05,
        6.41490688e+05, 8.50461562e+05, 1.11135362e+06, 1.22003100e+06,
        1.54110162e+06, 2.03452138e+06, 1.76440575e+06, 4.22100781e+05,
        1.30513797e+05, 7.30581250e+04, 8.56110391e+04, 7.42401094e+04,
        1.49466766e+05, 1.25623496e+04, 2.13100293e+04, 9.81795703e+04,
        7.28018203e+04, 1.26064742e+05, 1.01337961e+05, 1.24553164e+05,
        7.13400000e+04, 8.41950469e+04, 6.25289375e+04, 8.18789453e+04,
        1.27303195e+05, 8.94661250e+04, 1.04100523e+05, 1.66147031e+05,
        2.32076422e+05, 4.41965062e+05, 1.75838281e+05, 1.76445328e+05,
        9.12169375e+04, 1.67975469e+05, 1.87876422e+05, 1.07636930e+05,
        3.26578105e+04, 5.35412461e+04, 1.38143531e+05, 3.28638219e+05,
        6.38893688e+05, 1.08279325e+06, 1.55802525e+06, 2.42136925e+06,
        2.08980088e+06, 9.83019312e+05, 4.27435500e+05, 5.84014961e+04,
        4.02855312e+04, 5.27582578e+04, 1.33694297e+05, 2.41687594e+05,
        2.54570500e+05, 2.18475578e+05, 2.39844719e+05, 1.72920359e+05,
        9.46134531e+04, 8.14810703e+04, 1.83791891e+05, 3.41631531e+05,
        2.67090906e+05, 3.54819906e+05, 1.69607641e+05, 2.63053145e+04,
        2.11339043e+04, 1.11686047e+05, 6.08852578e+04, 4.64014297e+04,
        6.97661953e+04, 1.16936102e+05, 1.44631031e+05, 1.14626898e+05,
        7.92795234e+04, 5.69158320e+04, 3.03999824e+04, 9.47752891e+04,
        1.19674977e+05, 2.15400000e+05, 3.15528031e+05, 4.17611781e+05,
        3.91754000e+05, 2.83129906e+05, 8.15816328e+04, 5.00839805e+04,
        1.01595930e+05, 6.17696953e+04, 3.16726438e+05, 5.76748000e+05,
        7.41103312e+05, 1.33218000e+06, 1.56516188e+06, 1.55553338e+06,
        1.68517900e+06, 3.21850031e+05, 6.30693242e+04, 7.78374766e+04,
        5.12543047e+04, 1.14444250e+05, 1.13520102e+05, 7.03014531e+04,
        4.71108398e+04, 4.52435781e+05, 7.79108188e+05, 7.32536312e+05,
        1.03909088e+06, 1.56208738e+06, 1.63956762e+06, 1.72789975e+06,
        4.00525438e+05, 2.34608641e+05, 1.76724859e+05, 6.78014609e+04,
        3.90382062e+05, 9.80141562e+05, 1.03537131e+06, 8.44669812e+05,
        1.38893975e+06, 1.98341138e+06, 3.62835425e+06, 2.66920750e+06,
        1.42117000e+06, 4.54602219e+05, 3.65967062e+05, 9.46589188e+05,
        1.45184738e+06, 1.85900112e+06, 1.89930475e+06, 1.82373112e+06,
        1.21286088e+06, 4.94641594e+05, 1.49650312e+06, 2.03091800e+06,
        2.25657350e+06, 2.11182750e+05, 1.21396250e+05, 3.04308844e+05,
        7.11903438e+05, 6.20312000e+05, 8.05386562e+05, 6.26902875e+05,
        5.66389875e+05, 4.27517438e+05, 7.83491562e+05, 3.63025775e+06,
        8.18037650e+06, 1.07676250e+07, 1.26563580e+07, 1.33500370e+07,
        1.33957390e+07, 6.52511100e+06, 1.60696775e+06, 8.56758062e+05,
        1.15616712e+06, 1.69269000e+06, 2.17122950e+06, 3.34218200e+06,
        7.84994850e+06, 1.08378930e+07, 1.05726990e+07, 1.01602540e+07,
        1.14205900e+07, 1.16179750e+07, 1.73510000e+07, 1.79301800e+07,
        2.70626460e+07, 3.72665640e+07, 2.97687340e+07, 2.14580120e+07,
        2.44066140e+07, 1.56006580e+07, 1.03771360e+07, 9.93532800e+06,
        2.18889460e+07, 3.45001800e+07, 2.72495080e+07, 2.51545225e+06,
        2.11410550e+06, 1.65101288e+06, 8.78065000e+06, 1.42263970e+07,
        1.50277600e+07, 1.42628750e+07, 1.11520980e+07, 1.31385740e+07,
        1.44270080e+07, 2.10807200e+06, 2.37278650e+06, 2.06521262e+06,
        1.51271400e+06, 1.63591925e+06, 6.01038950e+06, 1.48181850e+07,
        1.51462730e+07, 8.88826900e+06, 1.03656340e+07, 1.24236510e+07,
        3.12300180e+07, 4.44122480e+07, 3.35616920e+07, 3.37026000e+07,
        4.33024720e+07, 4.04647440e+07, 6.68120760e+07, 2.68142400e+07,
        3.37228800e+06, 6.18199650e+06, 3.33604150e+06, 9.76003100e+06,
        9.50538200e+06, 1.48300340e+07, 1.08088990e+07, 1.52812380e+07,
        1.21298770e+07, 1.47110990e+07, 4.46196920e+07, 6.19225800e+07,
        1.05739048e+08, 8.06071760e+07, 3.00356620e+07, 1.30779130e+07,
        4.55988750e+06, 1.67929975e+06, 6.99432300e+06, 1.63487150e+07,
        1.21498980e+07, 1.21798540e+07, 1.68979740e+07, 2.39387020e+07,
        3.73065680e+07, 5.09610480e+07, 4.59523240e+07, 3.20003840e+07,
        2.06371540e+07, 8.14507050e+06, 4.14795750e+06, 4.23499100e+06,
        3.09784650e+06, 8.37917500e+06, 7.36108300e+06, 7.06094350e+06,
        1.57310800e+07, 2.78767160e+07, 4.07339880e+07, 3.28521600e+07,
        2.89016500e+07, 3.40228600e+06, 3.84223250e+06, 8.10806000e+06,
        1.74778820e+07, 2.13824600e+07, 5.77506950e+06, 7.30700800e+06,
        3.70581275e+06, 6.14524500e+05, 1.92765088e+06, 2.29573075e+06,
        1.09031975e+06, 1.19423488e+06, 2.01535425e+06, 1.38583700e+06,
        2.60915325e+06, 7.48127550e+06, 1.25728690e+07, 1.49188240e+07,
        7.64712100e+06, 9.50861100e+06, 1.16537410e+07, 2.15388360e+07,
        4.32744960e+07, 6.52254160e+07, 9.30125040e+07, 8.49970400e+07,
        5.87007080e+07, 3.43532560e+07, 2.82685780e+07, 9.30442400e+06,
        4.66932300e+06, 8.00189950e+06, 2.05975580e+07, 4.07234120e+07,
        1.93118840e+07, 4.04353375e+06, 5.92508700e+06, 7.37346950e+06,
        2.68488000e+06, 5.27761500e+06, 1.84823362e+06, 4.87232050e+06,
        4.87888400e+06, 5.77534400e+06, 1.21702240e+07, 2.70199400e+07,
        4.94704840e+07, 6.90300960e+07, 8.50554880e+07], dtype=float32),
 &#39;rmse&#39;: array([  561.7492  ,   411.2974  ,   379.64694 ,   419.52707 ,
          513.34174 ,   625.7134  ,   632.07965 ,   586.87506 ,
          702.7746  ,   829.9311  ,   958.4903  ,   894.97943 ,
          797.1879  ,   803.452   ,   598.1137  ,   320.94724 ,
          230.89664 ,   215.8309  ,   332.18848 ,   334.13724 ,
          208.65266 ,   233.53539 ,   213.50632 ,   276.5658  ,
          200.76715 ,   164.6427  ,   137.93799 ,   213.45375 ,
          206.23912 ,   257.53568 ,   341.67487 ,   411.6321  ,
          332.3958  ,   399.6341  ,   402.03375 ,   351.9401  ,
          388.27478 ,   362.2103  ,   234.58073 ,   250.63026 ,
          421.2771  ,   163.96461 ,   159.07036 ,   160.63757 ,
          116.014   ,    62.305946,    96.06105 ,   110.63433 ,
          148.61641 ,   189.95403 ,   220.89862 ,   193.55951 ,
          202.62302 ,   204.30829 ,   332.4016  ,   448.06476 ,
          590.04156 ,   598.38184 ,   573.0404  ,   544.5235  ,
          483.0674  ,   392.4885  ,   378.37988 ,   403.22012 ,
          544.47473 ,   516.605   ,   542.39197 ,   556.8372  ,
          250.43535 ,   293.91644 ,   311.3752  ,   449.23752 ,
          545.02625 ,   447.9675  ,   356.79846 ,   364.01865 ,
          290.8643  ,   418.11176 ,   543.24567 ,   689.84686 ,
          849.64374 ,   680.2288  ,   364.9038  ,   285.2438  ,
          185.23193 ,   185.7967  ,   231.7006  ,   272.47263 ,
          330.66504 ,   528.85046 ,   468.10193 ,   447.1402  ,
          407.3124  ,   285.7863  ,   219.63496 ,   406.57227 ,
          426.26212 ,   575.0174  ,   680.9007  ,   821.6106  ,
          578.7154  ,   411.51398 ,   321.27753 ,   385.1592  ,
          546.5207  ,   609.8793  ,   665.8128  ,   726.57056 ,
         1062.7572  ,  1003.25934 ,   896.2404  ,   470.40195 ,
          257.8539  ,   163.15088 ,   181.57336 ,   410.6863  ,
          419.83    ,   547.14307 ,   657.1099  ,  1383.2812  ,
         1975.7374  ,  2508.9888  ,  2343.1653  ,  2501.3535  ,
         2507.2256  ,  2690.495   ,  1442.718   ,   816.56995 ,
          380.3155  ,   428.26813 ,   583.9748  ,   549.6087  ,
          609.3766  ,   449.90912 ,   368.98663 ,   287.18607 ,
          442.72644 ,   401.98962 ,   414.00653 ,   447.891   ,
          487.03067 ,   398.39697 ,   252.1964  ,   582.15466 ,
          723.54974 ,   534.98804 ,   438.55002 ,   193.33018 ,
          363.26575 ,   305.47717 ,   344.15646 ,   548.26886 ,
          478.53625 ,   598.29285 ,   486.0237  ,   414.16226 ,
          163.85437 ,   218.89795 ,   200.1048  ,   232.60092 ,
          328.0875  ,   313.0527  ,   266.14847 ,   256.1221  ,
          292.1883  ,   365.8382  ,   522.03094 ,   808.7852  ,
          705.1331  ,   781.7865  ,   996.4475  ,   978.1065  ,
         1046.6204  ,  1092.1682  ,   612.37427 ,   471.22586 ,
          510.50952 ,   433.7161  ,   433.50607 ,   499.05774 ,
          615.78625 ,   638.5192  ,   790.4321  ,   980.85645 ,
          891.8209  ,   374.91006 ,   515.0024  ,   791.3719  ,
          453.65652 ,   255.55615 ,   365.4627  ,   338.66217 ,
          476.35388 ,   511.10318 ,   713.1032  ,   700.5317  ,
          488.5917  ,   261.8312  ,   270.2918  ,   231.54132 ,
          572.908   ,   791.48425 ,   719.5384  ,   576.6594  ,
          415.52045 ,   287.7739  ,   378.6722  ,   297.18173 ,
          227.1226  ,   205.13364 ,   421.95868 ,   342.56616 ,
          341.86316 ,   320.02628 ,   409.25394 ,   453.33206 ,
          522.70593 ,   217.68823 ,   214.61069 ,   231.02502 ,
          280.54785 ,   265.7982  ,   228.72073 ,   158.20264 ,
          212.60869 ,   203.64981 ,   264.5954  ,   352.3239  ,
          366.2897  ,   581.72296 ,   312.34595 ,   331.69714 ,
          141.1364  ,   182.4111  ,   156.11276 ,   166.31807 ,
          164.43439 ,   125.7441  ,   107.780495,   165.78665 ,
          171.83319 ,   198.96605 ,   167.8827  ,   178.64919 ,
          236.72102 ,   199.58418 ,   224.09854 ,   178.8182  ,
          172.08786 ,   211.73892 ,   207.91    ,   123.46075 ,
          169.1382  ,   205.78828 ,   269.2643  ,   349.1772  ,
          800.93115 ,   922.2048  ,  1054.2076  ,  1104.5502  ,
         1241.4113  ,  1426.3665  ,  1328.3093  ,   649.6928  ,
          361.26694 ,   270.29266 ,   292.59366 ,   272.4704  ,
          386.6093  ,   112.081894,   145.97955 ,   313.3362  ,
          269.8181  ,   355.05597 ,   318.33624 ,   352.9209  ,
          267.0955  ,   290.16382 ,   250.05786 ,   286.145   ,
          356.79572 ,   299.1089  ,   322.64612 ,   407.6114  ,
          481.7431  ,   664.8045  ,   419.33078 ,   420.05396 ,
          302.0214  ,   409.84808 ,   433.44717 ,   328.0807  ,
          180.71472 ,   231.3898  ,   371.67667 ,   573.2698  ,
          799.3082  ,  1040.5736  ,  1248.2089  ,  1556.075   ,
         1445.6144  ,   991.4733  ,   653.7855  ,   241.664   ,
          200.71255 ,   229.69164 ,   365.6423  ,   491.61737 ,
          504.54974 ,   467.4137  ,   489.7394  ,   415.83694 ,
          307.593   ,   285.44888 ,   428.70956 ,   584.49255 ,
          516.80835 ,   595.6676  ,   411.8345  ,   162.18913 ,
          145.37505 ,   334.1946  ,   246.74936 ,   215.40991 ,
          264.1329  ,   341.95923 ,   380.3039  ,   338.56595 ,
          281.5662  ,   238.5704  ,   174.35591 ,   307.85596 ,
          345.94073 ,   464.11203 ,   561.7188  ,   646.22894 ,
          625.9026  ,   532.09955 ,   285.625   ,   223.7945  ,
          318.74115 ,   248.5351  ,   562.78455 ,   759.4393  ,
          860.8736  ,  1154.201   ,  1251.0643  ,  1247.2102  ,
         1298.1444  ,   567.3183  ,   251.13608 ,   278.9937  ,
          226.39412 ,   338.29608 ,   336.92746 ,   265.14423 ,
          217.05034 ,   672.6335  ,   882.671   ,   855.88336 ,
         1019.35803 ,  1249.835   ,  1280.4559  ,  1314.4961  ,
          632.8708  ,   484.36414 ,   420.38657 ,   260.38715 ,
          624.8056  ,   990.02106 ,  1017.5319  ,   919.0592  ,
         1178.533   ,  1408.3364  ,  1904.8239  ,  1633.771   ,
         1192.1284  ,   674.242   ,   604.9521  ,   972.9281  ,
         1204.9263  ,  1363.4519  ,  1378.1527  ,  1350.4559  ,
         1101.2997  ,   703.3076  ,  1223.3164  ,  1425.1029  ,
         1502.1896  ,   459.54623 ,   348.41965 ,   551.64197 ,
          843.7437  ,   787.5989  ,   897.4333  ,   791.77203 ,
          752.5888  ,   653.84814 ,   885.1505  ,  1905.3235  ,
         2860.1357  ,  3281.406   ,  3557.5776  ,  3653.7703  ,
         3660.019   ,  2554.4297  ,  1267.6624  ,   925.6123  ,
         1075.2522  ,  1301.0342  ,  1473.5092  ,  1828.1636  ,
         2801.7761  ,  3292.0957  ,  3251.5688  ,  3187.5154  ,
         3379.4363  ,  3408.5151  ,  4165.453   ,  4234.4043  ,
         5202.1772  ,  6104.635   ,  5456.073   ,  4632.28    ,
         4940.305   ,  3949.767   ,  3221.3562  ,  3152.0356  ,
         4678.5625  ,  5873.6855  ,  5220.106   ,  1586.0177  ,
         1453.9965  ,  1284.9175  ,  2963.2163  ,  3771.7898  ,
         3876.5652  ,  3776.622   ,  3339.4758  ,  3624.7173  ,
         3798.29    ,  1451.92    ,  1540.3851  ,  1437.0847  ,
         1229.9243  ,  1279.0306  ,  2451.6096  ,  3849.44    ,
         3891.8213  ,  2981.3203  ,  3219.5703  ,  3524.72    ,
         5588.383   ,  6664.252   ,  5793.246   ,  5805.394   ,
         6580.4614  ,  6361.1904  ,  8173.865   ,  5178.2466  ,
         1836.379   ,  2486.362   ,  1826.4835  ,  3124.1047  ,
         3083.0798  ,  3850.9783  ,  3287.689   ,  3909.1228  ,
         3482.797   ,  3835.5054  ,  6679.7974  ,  7869.091   ,
        10282.95    ,  8978.149   ,  5480.48    ,  3616.3398  ,
         2135.3894  ,  1295.878   ,  2644.6782  ,  4043.3542  ,
         3485.6704  ,  3489.9646  ,  4110.7144  ,  4892.719   ,
         6107.91    ,  7138.7007  ,  6778.814   ,  5656.8877  ,
         4542.8135  ,  2853.957   ,  2036.6534  ,  2057.9094  ,
         1760.0701  ,  2894.6804  ,  2713.1316  ,  2657.2437  ,
         3966.2427  ,  5279.841   ,  6382.3184  ,  5731.68    ,
         5376.0254  ,  1844.5288  ,  1960.1613  ,  2847.4656  ,
         4180.656   ,  4624.117   ,  2403.1375  ,  2703.1477  ,
         1925.0488  ,   783.9162  ,  1388.3987  ,  1515.1669  ,
         1044.1838  ,  1092.8105  ,  1419.6317  ,  1177.2158  ,
         1615.2874  ,  2735.192   ,  3545.8242  ,  3862.4895  ,
         2765.3428  ,  3083.6038  ,  3413.7578  ,  4640.995   ,
         6578.3354  ,  8076.225   ,  9644.3     ,  9219.384   ,
         7661.638   ,  5861.165   ,  5316.8203  ,  3050.3152  ,
         2160.8616  ,  2828.763   ,  4538.453   ,  6381.4897  ,
         4394.529   ,  2010.854   ,  2434.1501  ,  2715.4133  ,
         1638.5604  ,  2297.3062  ,  1359.4976  ,  2207.3335  ,
         2208.8198  ,  2403.1946  ,  3488.5847  ,  5198.071   ,
         7033.5254  ,  8308.436   ,  9222.554   ], dtype=float32)}
</code></pre></div> <h2 id=make-our-evaluation-function-work-for-larger-horizons>Make our evaluation function work for larger horizons</h2> <p>You'll notice the outputs for <code>model_3_results</code> are multi-dimensional. </p> <p>This is because the predictions are getting evaluated across the <code>HORIZON</code> timesteps (7 predictions at a time).</p> <p>To fix this, let's adjust our <code>evaluate_preds()</code> function to work with multiple shapes of data.</p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>):</span>
  <span class=c1># Make sure float32 (for metric calculations)</span>
  <span class=n>y_true</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
  <span class=n>y_pred</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>

  <span class=c1># Calculate various metrics</span>
  <span class=n>mae</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
  <span class=n>mse</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
  <span class=n>rmse</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>mse</span><span class=p>)</span>
  <span class=n>mape</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_percentage_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
  <span class=n>mase</span> <span class=o>=</span> <span class=n>mean_absolute_scaled_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

  <span class=c1># Account for different sized metrics (for longer horizons, reduce to single number)</span>
  <span class=k>if</span> <span class=n>mae</span><span class=o>.</span><span class=n>ndim</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span> <span class=c1># if mae isn&#39;t already a scalar, reduce it to one by aggregating tensors to mean</span>
    <span class=n>mae</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>mae</span><span class=p>)</span>
    <span class=n>mse</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>mse</span><span class=p>)</span>
    <span class=n>rmse</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>rmse</span><span class=p>)</span>
    <span class=n>mape</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>mape</span><span class=p>)</span>
    <span class=n>mase</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>mase</span><span class=p>)</span>

  <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;mae&quot;</span><span class=p>:</span> <span class=n>mae</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span>
          <span class=s2>&quot;mse&quot;</span><span class=p>:</span> <span class=n>mse</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span>
          <span class=s2>&quot;rmse&quot;</span><span class=p>:</span> <span class=n>rmse</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span>
          <span class=s2>&quot;mape&quot;</span><span class=p>:</span> <span class=n>mape</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span>
          <span class=s2>&quot;mase&quot;</span><span class=p>:</span> <span class=n>mase</span><span class=o>.</span><span class=n>numpy</span><span class=p>()}</span>
</code></pre></div> <p>Now we've updated <code>evaluate_preds()</code> to work with multiple shapes, how does it look?</p> <div class=highlight><pre><span></span><code><span class=c1># Get model_3 results aggregated to single values</span>
<span class=n>model_3_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>test_labels</span><span class=p>),</span>
                                 <span class=n>y_pred</span><span class=o>=</span><span class=n>model_3_preds</span><span class=p>)</span>
<span class=n>model_3_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 1237.5063,
 &#39;mape&#39;: 5.5588784,
 &#39;mase&#39;: 2.2020736,
 &#39;mse&#39;: 5405198.5,
 &#39;rmse&#39;: 1425.7477}
</code></pre></div> <p>Time to visualize.</p> <p>If our prediction evaluation metrics were mutli-dimensional, how do you think the predictions will look like if we plot them?</p> <div class=highlight><pre><span></span><code><span class=n>offset</span> <span class=o>=</span> <span class=mi>300</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>):],</span> <span class=n>values</span><span class=o>=</span><span class=n>test_labels</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Test_data&quot;</span><span class=p>)</span>
<span class=c1># Checking the shape of model_3_preds results in [n_test_samples, HORIZON] (this will screw up the plot)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>):],</span> <span class=n>values</span><span class=o>=</span><span class=n>model_3_preds</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;model_3_preds&quot;</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_129_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_129_0.png></a></p> <p>When we try to plot our multi-horizon predicts, we get a funky looking plot.</p> <p>Again, we can fix this by aggregating our model's predictions.</p> <blockquote> <p>🔑 <strong>Note:</strong> Aggregating the predictions (e.g. reducing a 7-day horizon to one value such as the mean) loses information from the original prediction. As in, the model predictions were trained to be made for 7-days but by reducing them to one, we gain the ability to plot them visually but we lose the extra information contained across multiple days.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=n>offset</span> <span class=o>=</span> <span class=mi>300</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=c1># Plot model_3_preds by aggregating them (note: this condenses information so the preds will look fruther ahead than the test data)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>):],</span> 
                 <span class=n>values</span><span class=o>=</span><span class=n>test_labels</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> 
                 <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>,</span> 
                 <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Test_data&quot;</span><span class=p>)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>X_test</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>):],</span> 
                 <span class=n>values</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>model_3_preds</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> 
                 <span class=nb>format</span><span class=o>=</span><span class=s2>&quot;-&quot;</span><span class=p>,</span>
                 <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>,</span> 
                 <span class=n>label</span><span class=o>=</span><span class=s2>&quot;model_3_preds&quot;</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_131_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_131_0.png></a></p> <h2 id=which-of-our-models-is-performing-best-so-far>Which of our models is performing best so far?</h2> <p>So far, we've trained 3 models which use the same architecture but use different data inputs.</p> <p>Let's compare them with the naïve model to see which model is performing the best so far.</p> <div class=highlight><pre><span></span><code><span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span><span class=s2>&quot;naive&quot;</span><span class=p>:</span> <span class=n>naive_results</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>],</span> 
              <span class=s2>&quot;horizon_1_window_7&quot;</span><span class=p>:</span> <span class=n>model_1_results</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>],</span> 
              <span class=s2>&quot;horizon_1_window_30&quot;</span><span class=p>:</span> <span class=n>model_2_results</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>],</span> 
              <span class=s2>&quot;horizon_7_window_30&quot;</span><span class=p>:</span> <span class=n>model_3_results</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>]},</span> <span class=n>index</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>])</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>),</span> <span class=n>kind</span><span class=o>=</span><span class=s2>&quot;bar&quot;</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_133_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_133_0.png></a></p> <p>Woah, our naïve model is performing best (it's very hard to beat a naïve model in open systems) but the dense model with a horizon of 1 and a window size of 7 looks to be performing cloest.</p> <p>Because of this, let's use <code>HORIZON=1</code> and <code>WINDOW_SIZE=7</code> for our next series of modelling experiments (in other words, we'll use the previous week of Bitcoin prices to try and predict the next day).</p> <blockquote> <p>🔑 <strong>Note:</strong> You might be wondering, why are the naïve results so good? One of the reasons could be due the presence of <strong>autocorrelation</strong> in the data. If a time series has <strong>autocorrelation</strong> it means the value at <code>t+1</code> (the next timestep) is typically close to the value at <code>t</code> (the current timestep). In other words, today's value is probably pretty close to yesterday's value. Of course, this isn't always the case but when it is, a naïve model will often get fairly good results.</p> <p>📖 <strong>Resource:</strong> For more on how autocorrelation influences a model's predictions, see the article <a href=https://towardsdatascience.com/how-not-to-use-machine-learning-for-time-series-forecasting-avoiding-the-pitfalls-19f9d7adf424><em>How (not) to use Machine Learning for time series forecasting: Avoiding the pitfalls</em></a> by Vegard Flovik</p> </blockquote> <h2 id=model-4-conv1d>Model 4: Conv1D</h2> <p>Onto the next modelling experiment!</p> <p>This time, we'll be using a Conv1D model. Because as we saw in the sequence modelling notebook, Conv1D models can be used for seq2seq (sequence to sequence) problems.</p> <p>In our case, the input sequence is the previous 7 days of Bitcoin price data and the output is the next day (in seq2seq terms this is called a many to one problem).</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-bitcoin-forecast-in-seq2seq-terms.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="bitcoin prediction in seq2seq terms" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-bitcoin-forecast-in-seq2seq-terms.png></a> <em>Framing Bitcoin forecasting in seq2seq (sequence to sequence) terms. Using a window size of 7 and a horizon of one results in a many to one problem. Using a window size of &gt;1 and a horizon of &gt;1 results in a many to many problem. The diagram comes from Andrei Karpathy's <a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/ >The Unreasonable Effectiveness of Recurrent Neural Networks</a></em>.</p> <p>Before we build a Conv1D model, let's recreate our datasets.</p> <div class=highlight><pre><span></span><code><span class=n>HORIZON</span> <span class=o>=</span> <span class=mi>1</span> <span class=c1># predict next day</span>
<span class=n>WINDOW_SIZE</span> <span class=o>=</span> <span class=mi>7</span> <span class=c1># use previous week worth of data</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Create windowed dataset</span>
<span class=n>full_windows</span><span class=p>,</span> <span class=n>full_labels</span> <span class=o>=</span> <span class=n>make_windows</span><span class=p>(</span><span class=n>prices</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=n>WINDOW_SIZE</span><span class=p>,</span> <span class=n>horizon</span><span class=o>=</span><span class=n>HORIZON</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>full_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>full_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2780, 2780)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Create train/test splits</span>
<span class=n>train_windows</span><span class=p>,</span> <span class=n>test_windows</span><span class=p>,</span> <span class=n>train_labels</span><span class=p>,</span> <span class=n>test_labels</span> <span class=o>=</span> <span class=n>make_train_test_splits</span><span class=p>(</span><span class=n>full_windows</span><span class=p>,</span> <span class=n>full_labels</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>train_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_labels</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2224, 556, 2224, 556)
</code></pre></div> <p>Data windowed!</p> <p>Now, since we're going to be using <a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D>Conv1D layers</a>, we need to make sure our input shapes are correct.</p> <p>The Conv1D layer in TensorFlow takes an input of: <code>(batch_size, timesteps, input_dim)</code>.</p> <p>In our case, the <code>batch_size</code> (by default this is 32 but we can change it) is handled for us but the other values will be: * <code>timesteps = WINDOW_SIZE</code> - the <code>timesteps</code> is also often referred to as <code>features</code>, our features are the previous <code>WINDOW_SIZE</code> values of Bitcoin * <code>input_dim = HORIZON</code> - our model views <code>WINDOW_SIZE</code> (one week) worth of data at a time to predict <code>HORIZON</code> (one day)</p> <p>Right now, our data has the <code>timesteps</code> dimension ready but we'll have to adjust it to have the <code>input_dim</code> dimension. </p> <div class=highlight><pre><span></span><code><span class=c1># Check data sample shapes</span>
<span class=n>train_windows</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span> <span class=c1># returns (WINDOW_SIZE, )</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(7,)
</code></pre></div> <p>To fix this, we could adjust the shape of all of our <code>train_windows</code> or we could use a <a href=https://keras.io/api/layers/core_layers/lambda/ ><code>tf.keras.layers.Lamdba</code></a> (called a Lambda layer) to do this for us in our model.</p> <p>The Lambda layer wraps a function into a layer which can be used with a model.</p> <p>Let's try it out.</p> <div class=highlight><pre><span></span><code><span class=c1># Before we pass our data to the Conv1D layer, we have to reshape it in order to make sure it works</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>train_windows</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
<span class=n>expand_dims_layer</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Lambda</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span> <span class=c1># add an extra dimension for timesteps</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Original shape: </span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span> <span class=c1># (WINDOW_SIZE)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Expanded shape: </span><span class=si>{</span><span class=n>expand_dims_layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span> <span class=c1># (WINDOW_SIZE, input_dim) </span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Original values with expanded shape:</span><span class=se>\n</span><span class=s2> </span><span class=si>{</span><span class=n>expand_dims_layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Original shape: (7,)
Expanded shape: (7, 1)
Original values with expanded shape:
 [[123.65499]
 [125.455  ]
 [108.58483]
 [118.67466]
 [121.33866]
 [120.65533]
 [121.795  ]]
</code></pre></div> <p>Looking good!</p> <p>Now we've got a Lambda layer, let's build, compile, fit and evaluate a Conv1D model on our data.</p> <blockquote> <p>🔑 <strong>Note:</strong> If you run the model below without the Lambda layer, you'll get an input shape error (one of the most common errors when building neural networks).</p> </blockquote> <div class=highlight><pre><span></span><code><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create model</span>
<span class=n>model_4</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=c1># Create Lambda layer to reshape inputs, without this layer, the model will error</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Lambda</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)),</span> <span class=c1># resize the inputs to adjust for window size / Conv1D 3D input requirements</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Conv1D</span><span class=p>(</span><span class=n>filters</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s2>&quot;causal&quot;</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>HORIZON</span><span class=p>)</span>
<span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=s2>&quot;model_4_conv1D&quot;</span><span class=p>)</span>

<span class=c1># Compile model</span>
<span class=n>model_4</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>())</span>

<span class=c1># Fit model</span>
<span class=n>model_4</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_windows</span><span class=p>,</span>
            <span class=n>train_labels</span><span class=p>,</span>
            <span class=n>batch_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> 
            <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
            <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
            <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>),</span>
            <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>create_model_checkpoint</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=n>model_4</span><span class=o>.</span><span class=n>name</span><span class=p>)])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets
INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets





&lt;keras.callbacks.History at 0x7fdcf1dfba50&gt;
</code></pre></div> <p>What does the Lambda layer look like in a summary?</p> <div class=highlight><pre><span></span><code><span class=n>model_4</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Model: &quot;model_4_conv1D&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lambda_1 (Lambda)            (None, 1, 7)              0         
_________________________________________________________________
conv1d (Conv1D)              (None, 1, 128)            4608      
_________________________________________________________________
dense_6 (Dense)              (None, 1, 1)              129       
=================================================================
Total params: 4,737
Trainable params: 4,737
Non-trainable params: 0
_________________________________________________________________
</code></pre></div> <p>The Lambda layer appears the same as any other regular layer. </p> <p>Time to evaluate the Conv1D model.</p> <div class=highlight><pre><span></span><code><span class=c1># Load in best performing Conv1D model and evaluate it on the test data</span>
<span class=n>model_4</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;model_experiments/model_4_conv1D&quot;</span><span class=p>)</span>
<span class=n>model_4</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>18/18 [==============================] - 0s 3ms/step - loss: 570.8283





570.8283081054688
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make predictions</span>
<span class=n>model_4_preds</span> <span class=o>=</span> <span class=n>make_preds</span><span class=p>(</span><span class=n>model_4</span><span class=p>,</span> <span class=n>test_windows</span><span class=p>)</span>
<span class=n>model_4_preds</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
array([8851.464, 8754.471, 8983.928, 8759.672, 8703.627, 8708.295,
       8661.667, 8494.839, 8435.317, 8492.115], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate predictions</span>
<span class=n>model_4_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>test_labels</span><span class=p>),</span>
                                 <span class=n>y_pred</span><span class=o>=</span><span class=n>model_4_preds</span><span class=p>)</span>
<span class=n>model_4_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 570.8283,
 &#39;mape&#39;: 2.5593357,
 &#39;mase&#39;: 1.0027872,
 &#39;mse&#39;: 1176671.1,
 &#39;rmse&#39;: 1084.7448}
</code></pre></div> <h2 id=model-5-rnn-lstm>Model 5: RNN (LSTM)</h2> <p>As you might've guessed, we can also use a recurrent neural network to model our sequential time series data.</p> <blockquote> <p>📖 <strong>Resource:</strong> For more on the different types of recurrent neural networks you can use for sequence problems, see the <a href=https://dev.mrdbourke.com/tensorflow-deep-learning/08_introduction_to_nlp_in_tensorflow/#recurrent-neural-networks-rnns>Recurrent Neural Networks section of notebook 08</a>.</p> </blockquote> <p>Let's reuse the same data we used for the Conv1D model, except this time we'll create an <a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM>LSTM-cell</a> powered RNN to model our Bitcoin data. </p> <p>Once again, one of the most important steps for the LSTM model will be getting our data into the right shape.</p> <p>The <a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM><code>tf.keras.layers.LSTM()</code></a> layer takes a tensor with <code>[batch, timesteps, feature]</code> dimensions.</p> <p>As mentioned earlier, the <code>batch</code> dimension gets taken care of for us but our data is currently only has the <code>feature</code> dimension (<code>WINDOW_SIZE</code>).</p> <p>To fix this, just like we did with the <code>Conv1D</code> model, we can use a <code>tf.keras.layers.Lambda()</code> layer to adjust the shape of our input tensors to the LSTM layer.</p> <div class=highlight><pre><span></span><code><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Let&#39;s build an LSTM model with the Functional API</span>
<span class=n>inputs</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>WINDOW_SIZE</span><span class=p>))</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Lambda</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>))(</span><span class=n>inputs</span><span class=p>)</span> <span class=c1># expand input dimension to be compatible with LSTM</span>
<span class=c1># print(x.shape)</span>
<span class=c1># x = layers.LSTM(128, activation=&quot;relu&quot;, return_sequences=True)(x) # this layer will error if the inputs are not the right shape</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span> <span class=c1># using the tanh loss function results in a massive error</span>
<span class=c1># print(x.shape)</span>
<span class=c1># Add another optional dense layer (you could add more of these to see if they improve model performance)</span>
<span class=c1># x = layers.Dense(32, activation=&quot;relu&quot;)(x)</span>
<span class=n>output</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>HORIZON</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span>
<span class=n>model_5</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=n>inputs</span><span class=p>,</span> <span class=n>outputs</span><span class=o>=</span><span class=n>output</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s2>&quot;model_5_lstm&quot;</span><span class=p>)</span>

<span class=c1># Compile model</span>
<span class=n>model_5</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>())</span>

<span class=c1># Seems when saving the model several warnings are appearing: https://github.com/tensorflow/tensorflow/issues/47554 </span>
<span class=n>model_5</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_windows</span><span class=p>,</span>
            <span class=n>train_labels</span><span class=p>,</span>
            <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
            <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
            <span class=n>batch_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
            <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>),</span>
            <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>create_model_checkpoint</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=n>model_5</span><span class=o>.</span><span class=n>name</span><span class=p>)])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets
INFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets





&lt;keras.callbacks.History at 0x7fdcf17b5190&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Load in best version of model 5 and evaluate on the test data</span>
<span class=n>model_5</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;model_experiments/model_5_lstm/&quot;</span><span class=p>)</span>
<span class=n>model_5</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>test_windows</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
18/18 [==============================] - 0s 2ms/step - loss: 596.6447





596.6446533203125
</code></pre></div> <p>Now we've got the best performing LSTM model loaded in, let's make predictions with it and evaluate them.</p> <div class=highlight><pre><span></span><code><span class=c1># Make predictions with our LSTM model</span>
<span class=n>model_5_preds</span> <span class=o>=</span> <span class=n>make_preds</span><span class=p>(</span><span class=n>model_5</span><span class=p>,</span> <span class=n>test_windows</span><span class=p>)</span>
<span class=n>model_5_preds</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
array([8991.225, 8823.2  , 9009.359, 8847.859, 8742.254, 8788.655,
       8744.746, 8552.568, 8514.823, 8542.873], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate model 5 preds</span>
<span class=n>model_5_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>test_labels</span><span class=p>),</span>
                                 <span class=n>y_pred</span><span class=o>=</span><span class=n>model_5_preds</span><span class=p>)</span>
<span class=n>model_5_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 596.64465,
 &#39;mape&#39;: 2.6838453,
 &#39;mase&#39;: 1.0481395,
 &#39;mse&#39;: 1273486.9,
 &#39;rmse&#39;: 1128.4888}
</code></pre></div> <p>Hmmm... it seems even with an LSTM-powered RNN we weren't able to beat our naïve models results.</p> <p>Perhaps adding another variable will help?</p> <blockquote> <p>🔑 <strong>Note:</strong> I'm putting this here again as a reminder that because neural networks are such powerful algorithms, they can be used for almost any problem, however, that doesn't mean they'll achieve performant or usable results. You're probably starting to clue onto this now.</p> </blockquote> <h2 id=make-a-multivariate-time-series>Make a multivariate time series</h2> <p>So far all of our models have barely kept up with the naïve forecast.</p> <p>And so far all of them have been trained on a single variable (also called univariate time series): the historical price of Bitcoin.</p> <p>If predicting the price of Bitcoin using the price of Bitcoin hasn't worked out very well, maybe giving our model more information may help.</p> <p>More information is a vague term because we could actually feed almost anything to our model(s) and they would still try to find patterns. </p> <p>For example, we could use the historical price of Bitcoin as well as anyone with the name <a href=https://twitter.com/mrdbourke>Daniel Bourke Tweeted</a> on that day to predict the future price of Bitcoin.</p> <p>But would this help?</p> <p>Porbably not.</p> <p>What would be better is if we passed our model something related to Bitcoin (again, this is quite vauge, since in an open system like a market, you could argue everything is related).</p> <p>This will be different for almost every time series you work on but in our case, we could try to see if the <a href=https://www.investopedia.com/terms/b/block-reward.asp>Bitcoin block reward size</a> adds any predictive power to our model(s).</p> <p>What is the Bitcoin block reward size?</p> <p>The Bitcoin block reward size is the number of Bitcoin someone receives from mining a Bitcoin block.</p> <p>At its inception, the Bitcoin block reward size was 50.</p> <p>But every four years or so, the Bitcoin block reward halves.</p> <p>For example, the block reward size went from 50 (starting January 2009) to 25 on November 28 2012.</p> <p>Let's encode this information into our time series data and see if it helps a model's performance.</p> <blockquote> <p>🔑 <strong>Note:</strong> Adding an extra feature to our dataset such as the Bitcoin block reward size will take our data from <strong>univariate</strong> (only the historical price of Bitcoin) to <strong>multivariate</strong> (the price of Bitcoin as well as the block reward size).</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Let&#39;s make a multivariate time series</span>
<span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>Price</th> </tr> <tr> <th>Date</th> <th></th> </tr> </thead> <tbody> <tr> <th>2013-10-01</th> <td>123.65499</td> </tr> <tr> <th>2013-10-02</th> <td>125.45500</td> </tr> <tr> <th>2013-10-03</th> <td>108.58483</td> </tr> <tr> <th>2013-10-04</th> <td>118.67466</td> </tr> <tr> <th>2013-10-05</th> <td>121.33866</td> </tr> </tbody> </table> </div> <p>Alright, time to add another feature column, the block reward size.</p> <p>First, we'll need to create variables for the different block reward sizes as well as the dates they came into play.</p> <p>The following block rewards and dates were sourced from <a href=https://www.cmcmarkets.com/en/learn-cryptocurrencies/bitcoin-halving>cmcmarkets.com</a>.</p> <table> <thead> <tr> <th>Block Reward</th> <th>Start Date</th> </tr> </thead> <tbody> <tr> <td>50</td> <td>3 January 2009 (2009-01-03)</td> </tr> <tr> <td>25</td> <td>28 November 2012</td> </tr> <tr> <td>12.5</td> <td>9 July 2016</td> </tr> <tr> <td>6.25</td> <td>11 May 2020</td> </tr> <tr> <td>3.125</td> <td>TBA (expected 2024)</td> </tr> <tr> <td>1.5625</td> <td>TBA (expected 2028)</td> </tr> </tbody> </table> <blockquote> <p>🔑 <strong>Note:</strong> Since our Bitcoin historical data starts from 01 October 2013, none of the timesteps in our multivariate time series will have a block reward of 50.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Block reward values</span>
<span class=n>block_reward_1</span> <span class=o>=</span> <span class=mi>50</span> <span class=c1># 3 January 2009 (2009-01-03) - this block reward isn&#39;t in our dataset (it starts from 01 October 2013)</span>
<span class=n>block_reward_2</span> <span class=o>=</span> <span class=mi>25</span> <span class=c1># 28 November 2012 </span>
<span class=n>block_reward_3</span> <span class=o>=</span> <span class=mf>12.5</span> <span class=c1># 9 July 2016</span>
<span class=n>block_reward_4</span> <span class=o>=</span> <span class=mf>6.25</span> <span class=c1># 11 May 2020</span>

<span class=c1># Block reward dates (datetime form of the above date stamps)</span>
<span class=n>block_reward_2_datetime</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>datetime64</span><span class=p>(</span><span class=s2>&quot;2012-11-28&quot;</span><span class=p>)</span>
<span class=n>block_reward_3_datetime</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>datetime64</span><span class=p>(</span><span class=s2>&quot;2016-07-09&quot;</span><span class=p>)</span>
<span class=n>block_reward_4_datetime</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>datetime64</span><span class=p>(</span><span class=s2>&quot;2020-05-11&quot;</span><span class=p>)</span>
</code></pre></div> <p>We're going to get the days (indexes) for different block reward values. </p> <p>This is important because if we're going to use multiple variables for our time series, they have to the same frequency as our original variable. For example, if our Bitcoin prices are daily, we need the block reward values to be daily as well.</p> <blockquote> <p>🔑 <strong>Note:</strong> For using multiple variables, make sure they're the same frequency as each other. If your variables aren't at the same frequency (e.g. Bitcoin prices are daily but block rewards are weekly), you may need to transform them in a way that they can be used with your model.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Get date indexes for when to add in different block dates</span>
<span class=n>block_reward_2_days</span> <span class=o>=</span> <span class=p>(</span><span class=n>block_reward_3_datetime</span> <span class=o>-</span> <span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>index</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span><span class=o>.</span><span class=n>days</span>
<span class=n>block_reward_3_days</span> <span class=o>=</span> <span class=p>(</span><span class=n>block_reward_4_datetime</span> <span class=o>-</span> <span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>index</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span><span class=o>.</span><span class=n>days</span>
<span class=n>block_reward_2_days</span><span class=p>,</span> <span class=n>block_reward_3_days</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(1012, 2414)
</code></pre></div> <p>Now we can add another feature to our dataset <code>block_reward</code> (this gets lower over time so it may lead to increasing prices of Bitcoin).</p> <div class=highlight><pre><span></span><code><span class=c1># Add block_reward column</span>
<span class=n>bitcoin_prices_block</span> <span class=o>=</span> <span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
<span class=n>bitcoin_prices_block</span><span class=p>[</span><span class=s2>&quot;block_reward&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>

<span class=c1># Set values of block_reward column (it&#39;s the last column hence -1 indexing on iloc)</span>
<span class=n>bitcoin_prices_block</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:</span><span class=n>block_reward_2_days</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>block_reward_2</span>
<span class=n>bitcoin_prices_block</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>block_reward_2_days</span><span class=p>:</span><span class=n>block_reward_3_days</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>block_reward_3</span>
<span class=n>bitcoin_prices_block</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>block_reward_3_days</span><span class=p>:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>block_reward_4</span>
<span class=n>bitcoin_prices_block</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>Price</th> <th>block_reward</th> </tr> <tr> <th>Date</th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <th>2013-10-01</th> <td>123.65499</td> <td>25</td> </tr> <tr> <th>2013-10-02</th> <td>125.45500</td> <td>25</td> </tr> <tr> <th>2013-10-03</th> <td>108.58483</td> <td>25</td> </tr> <tr> <th>2013-10-04</th> <td>118.67466</td> <td>25</td> </tr> <tr> <th>2013-10-05</th> <td>121.33866</td> <td>25</td> </tr> </tbody> </table> </div> <p>Woohoo! We've officially added another variable to our time series data.</p> <p>Let's see what it looks like.</p> <div class=highlight><pre><span></span><code><span class=c1># Plot the block reward/price over time</span>
<span class=c1># Note: Because of the different scales of our values we&#39;ll scale them to be between 0 and 1.</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>minmax_scale</span>
<span class=n>scaled_price_block_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>minmax_scale</span><span class=p>(</span><span class=n>bitcoin_prices_block</span><span class=p>[[</span><span class=s2>&quot;Price&quot;</span><span class=p>,</span> <span class=s2>&quot;block_reward&quot;</span><span class=p>]]),</span> <span class=c1># we need to scale the data first</span>
                                     <span class=n>columns</span><span class=o>=</span><span class=n>bitcoin_prices_block</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
                                     <span class=n>index</span><span class=o>=</span><span class=n>bitcoin_prices_block</span><span class=o>.</span><span class=n>index</span><span class=p>)</span>
<span class=n>scaled_price_block_df</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>));</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_167_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_167_0.png></a></p> <p>When we scale the block reward and the Bitcoin price, we can see the price goes up as the block reward goes down, perhaps this information will be helpful to our model's performance.</p> <h2 id=making-a-windowed-dataset-with-pandas>Making a windowed dataset with pandas</h2> <p>Previously, we used some custom made functions to window our <strong>univariate</strong> time series.</p> <p>However, since we've just added another variable to our dataset, these functions won't work.</p> <p>Not to worry though. Since our data is in a pandas DataFrame, we can leverage the <a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html><code>pandas.DataFrame.shift()</code></a> method to create a windowed <strong>multivariate</strong> time series.</p> <p>The <code>shift()</code> method offsets an index by a specified number of periods.</p> <p>Let's see it in action.</p> <div class=highlight><pre><span></span><code><span class=c1># Setup dataset hyperparameters</span>
<span class=n>HORIZON</span> <span class=o>=</span> <span class=mi>1</span>
<span class=n>WINDOW_SIZE</span> <span class=o>=</span> <span class=mi>7</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make a copy of the Bitcoin historical data with block reward feature</span>
<span class=n>bitcoin_prices_windowed</span> <span class=o>=</span> <span class=n>bitcoin_prices_block</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>

<span class=c1># Add windowed columns</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>WINDOW_SIZE</span><span class=p>):</span> <span class=c1># Shift values for each step in WINDOW_SIZE</span>
  <span class=n>bitcoin_prices_windowed</span><span class=p>[</span><span class=sa>f</span><span class=s2>&quot;Price+</span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>bitcoin_prices_windowed</span><span class=p>[</span><span class=s2>&quot;Price&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>shift</span><span class=p>(</span><span class=n>periods</span><span class=o>=</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span>
<span class=n>bitcoin_prices_windowed</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>Price</th> <th>block_reward</th> <th>Price+1</th> <th>Price+2</th> <th>Price+3</th> <th>Price+4</th> <th>Price+5</th> <th>Price+6</th> <th>Price+7</th> </tr> <tr> <th>Date</th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <th>2013-10-01</th> <td>123.65499</td> <td>25</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> </tr> <tr> <th>2013-10-02</th> <td>125.45500</td> <td>25</td> <td>123.65499</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> </tr> <tr> <th>2013-10-03</th> <td>108.58483</td> <td>25</td> <td>125.45500</td> <td>123.65499</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> </tr> <tr> <th>2013-10-04</th> <td>118.67466</td> <td>25</td> <td>108.58483</td> <td>125.45500</td> <td>123.65499</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> </tr> <tr> <th>2013-10-05</th> <td>121.33866</td> <td>25</td> <td>118.67466</td> <td>108.58483</td> <td>125.45500</td> <td>123.65499</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> </tr> <tr> <th>2013-10-06</th> <td>120.65533</td> <td>25</td> <td>121.33866</td> <td>118.67466</td> <td>108.58483</td> <td>125.45500</td> <td>123.65499</td> <td>NaN</td> <td>NaN</td> </tr> <tr> <th>2013-10-07</th> <td>121.79500</td> <td>25</td> <td>120.65533</td> <td>121.33866</td> <td>118.67466</td> <td>108.58483</td> <td>125.45500</td> <td>123.65499</td> <td>NaN</td> </tr> <tr> <th>2013-10-08</th> <td>123.03300</td> <td>25</td> <td>121.79500</td> <td>120.65533</td> <td>121.33866</td> <td>118.67466</td> <td>108.58483</td> <td>125.45500</td> <td>123.65499</td> </tr> <tr> <th>2013-10-09</th> <td>124.04900</td> <td>25</td> <td>123.03300</td> <td>121.79500</td> <td>120.65533</td> <td>121.33866</td> <td>118.67466</td> <td>108.58483</td> <td>125.45500</td> </tr> <tr> <th>2013-10-10</th> <td>125.96116</td> <td>25</td> <td>124.04900</td> <td>123.03300</td> <td>121.79500</td> <td>120.65533</td> <td>121.33866</td> <td>118.67466</td> <td>108.58483</td> </tr> </tbody> </table> </div> <p>Now that we've got a windowed dataset, let's separate features (<code>X</code>) from labels (<code>y</code>).</p> <p>Remember in our windowed dataset, we're trying to use the previous <code>WINDOW_SIZE</code> steps to predict <code>HORIZON</code> steps.</p> <div class=highlight><pre><span></span><code>Window for a week (7) to predict a horizon of 1 (multivariate time series)
WINDOW_SIZE &amp; block_reward -&gt; HORIZON

[0, 1, 2, 3, 4, 5, 6, block_reward] -&gt; [7]
[1, 2, 3, 4, 5, 6, 7, block_reward] -&gt; [8]
[2, 3, 4, 5, 6, 7, 8, block_reward] -&gt; [9]
</code></pre></div> <p>We'll also remove the <code>NaN</code> values using pandas <a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html><code>dropna()</code></a> method, this equivalent to starting our windowing function at <code>sample 0 (the first sample) + WINDOW_SIZE</code>.</p> <div class=highlight><pre><span></span><code><span class=c1># Let&#39;s create X &amp; y, remove the NaN&#39;s and convert to float32 to prevent TensorFlow errors </span>
<span class=n>X</span> <span class=o>=</span> <span class=n>bitcoin_prices_windowed</span><span class=o>.</span><span class=n>dropna</span><span class=p>()</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s2>&quot;Price&quot;</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span> 
<span class=n>y</span> <span class=o>=</span> <span class=n>bitcoin_prices_windowed</span><span class=o>.</span><span class=n>dropna</span><span class=p>()[</span><span class=s2>&quot;Price&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
<span class=n>X</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>block_reward</th> <th>Price+1</th> <th>Price+2</th> <th>Price+3</th> <th>Price+4</th> <th>Price+5</th> <th>Price+6</th> <th>Price+7</th> </tr> <tr> <th>Date</th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <th>2013-10-08</th> <td>25.0</td> <td>121.794998</td> <td>120.655327</td> <td>121.338661</td> <td>118.674660</td> <td>108.584831</td> <td>125.455002</td> <td>123.654991</td> </tr> <tr> <th>2013-10-09</th> <td>25.0</td> <td>123.032997</td> <td>121.794998</td> <td>120.655327</td> <td>121.338661</td> <td>118.674660</td> <td>108.584831</td> <td>125.455002</td> </tr> <tr> <th>2013-10-10</th> <td>25.0</td> <td>124.049004</td> <td>123.032997</td> <td>121.794998</td> <td>120.655327</td> <td>121.338661</td> <td>118.674660</td> <td>108.584831</td> </tr> <tr> <th>2013-10-11</th> <td>25.0</td> <td>125.961159</td> <td>124.049004</td> <td>123.032997</td> <td>121.794998</td> <td>120.655327</td> <td>121.338661</td> <td>118.674660</td> </tr> <tr> <th>2013-10-12</th> <td>25.0</td> <td>125.279663</td> <td>125.961159</td> <td>124.049004</td> <td>123.032997</td> <td>121.794998</td> <td>120.655327</td> <td>121.338661</td> </tr> </tbody> </table> </div> <div class=highlight><pre><span></span><code><span class=c1># View labels</span>
<span class=n>y</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Date
2013-10-08    123.032997
2013-10-09    124.049004
2013-10-10    125.961159
2013-10-11    125.279663
2013-10-12    125.927498
Name: Price, dtype: float32
</code></pre></div> <p>What a good looking dataset, let's split it into train and test sets using an 80/20 split just as we've done before.</p> <div class=highlight><pre><span></span><code><span class=c1># Make train and test sets</span>
<span class=n>split_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.8</span><span class=p>)</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:</span><span class=n>split_size</span><span class=p>],</span> <span class=n>y</span><span class=p>[:</span><span class=n>split_size</span><span class=p>]</span>
<span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>split_size</span><span class=p>:],</span> <span class=n>y</span><span class=p>[</span><span class=n>split_size</span><span class=p>:]</span>
<span class=nb>len</span><span class=p>(</span><span class=n>X_train</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_train</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>X_test</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2224, 2224, 556, 556)
</code></pre></div> <p>Training and test multivariate time series datasets made! Time to build a model.</p> <h2 id=model-6-dense-multivariate-time-series>Model 6: Dense (multivariate time series)</h2> <p>To keep things simple, let's the <code>model_1</code> architecture and use it to train and make predictions on our multivariate time series data.</p> <p>By replicating the <code>model_1</code> architecture we'll be able to see whether or not adding the block reward feature improves or detracts from model performance.</p> <div class=highlight><pre><span></span><code><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Make multivariate time series model</span>
<span class=n>model_6</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=c1># layers.Dense(128, activation=&quot;relu&quot;), # adding an extra layer here should lead to beating the naive model</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>HORIZON</span><span class=p>)</span>
<span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=s2>&quot;model_6_dense_multivariate&quot;</span><span class=p>)</span>

<span class=c1># Compile</span>
<span class=n>model_6</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>())</span>

<span class=c1># Fit</span>
<span class=n>model_6</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span>
            <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
            <span class=n>batch_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
            <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=c1># only print 1 line per epoch</span>
            <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>),</span>
            <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>create_model_checkpoint</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=n>model_6</span><span class=o>.</span><span class=n>name</span><span class=p>)])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets
INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets





&lt;keras.callbacks.History at 0x7fdceed05590&gt;
</code></pre></div> <p>Multivariate model fit!</p> <p>You might've noticed that the model inferred the input shape of our data automatically (the data now has an extra feature). Often this will be the case, however, if you're running into shape issues, you can always explicitly define the input shape using <code>input_shape</code> parameter of the first layer in a model.</p> <p>Time to evaluate our multivariate model.</p> <div class=highlight><pre><span></span><code><span class=c1># Make sure best model is loaded and evaluate</span>
<span class=n>model_6</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;model_experiments/model_6_dense_multivariate&quot;</span><span class=p>)</span>
<span class=n>model_6</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>18/18 [==============================] - 0s 2ms/step - loss: 567.5873





567.5873413085938
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make predictions on multivariate data</span>
<span class=n>model_6_preds</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>model_6</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>))</span>
<span class=n>model_6_preds</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
array([8836.276, 8763.8  , 9040.486, 8741.225, 8719.326, 8765.071,
       8661.102, 8496.891, 8463.231, 8521.585], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate preds</span>
<span class=n>model_6_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span>
                                 <span class=n>y_pred</span><span class=o>=</span><span class=n>model_6_preds</span><span class=p>)</span>
<span class=n>model_6_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 567.5874,
 &#39;mape&#39;: 2.541387,
 &#39;mase&#39;: 0.99709386,
 &#39;mse&#39;: 1161688.4,
 &#39;rmse&#39;: 1077.8165}
</code></pre></div> <p>Hmmm... how do these results compare to <code>model_1</code> (same window size and horizon but without the block reward feature)?</p> <div class=highlight><pre><span></span><code><span class=n>model_1_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 568.95123,
 &#39;mape&#39;: 2.5448983,
 &#39;mase&#39;: 0.9994897,
 &#39;mse&#39;: 1171744.0,
 &#39;rmse&#39;: 1082.4713}
</code></pre></div> <p>It looks like the adding in the block reward may have helped our model slightly.</p> <p>But there a few more things we could try.</p> <blockquote> <p>📖 <strong>Resource:</strong> For different ideas on how to improve a neural network model (from a model perspective), refer to the <a href=https://dev.mrdbourke.com/tensorflow-deep-learning/02_neural_network_classification_in_tensorflow/#improving-a-model><em>Improving a model</em></a> section in notebook 02.</p> <p>🛠 <strong>Exercise(s):</strong> 1. Try adding an extra <code>tf.keras.layers.Dense()</code> layer with 128 hidden units to <code>model_6</code>, how does this effect model performance? 2. Is there a better way to create this model? As in, should the <code>block_reward</code> feature be bundled in with the Bitcoin historical price feature? Perhaps you could test whether building a multi-input model (e.g. one model input for Bitcoin price history and one model input for <code>block_reward</code>) works better? See <a href=https://dev.mrdbourke.com/tensorflow-deep-learning/09_SkimLit_nlp_milestone_project_2/#model-4-combining-pretrained-token-embeddings-character-embeddings-hybrid-embedding-layer><em>Model 4: Hybrid embedding</em></a> section of notebook 09 for an idea on how to create a multi-input model.</p> </blockquote> <h2 id=model-7-n-beats-algorithm>Model 7: N-BEATS algorithm</h2> <p>Time to step things up a notch.</p> <p>So far we've tried a bunch of smaller models, models with only a couple of layers.</p> <p>But one of the best ways to improve a model's performance is to increase the number of layers in it.</p> <p>That's exactly what the <a href=https://arxiv.org/pdf/1905.10437.pdf>N-BEATS (Neural Basis Expansion Analysis for Interpretable Time Series Forecasting) algorithm</a> does.</p> <p>The N-BEATS algorithm focuses on univariate time series problems and achieved state-of-the-art performance in the winner of the <a href=https://www.sciencedirect.com/science/article/pii/S0169207019301128>M4 competition</a> (a forecasting competition).</p> <p>For our next modelling experiment we're going to be replicating the <strong>generic architecture</strong> of the N-BEATS algorithm (see <a href=https://arxiv.org/pdf/1905.10437.pdf>section 3.3 of the N-BEATS paper</a>).</p> <p>We're not going to go through all of the details in the paper, instead we're going to focus on: 1. Replicating the model architecture in <a href=https://arxiv.org/pdf/1905.10437.pdf>Figure 1 of the N-BEATS paper</a> </p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-figure-1-nbeats-paper-annotated.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="figure 1 from N-BBEATS paper, the algorithm we're going to build" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-figure-1-nbeats-paper-annotated.png></a> <em>N-BEATS algorithm we're going to replicate with TensorFlow with window (input) and horizon (output) annotations.</em></p> <ol> <li>Using the same hyperparameters as the paper which can be found in <a href=https://arxiv.org/pdf/1905.10437.pdf>Appendix D of the N-BEATS paper</a></li> </ol> <p>Doing this will give us an opportunity to practice: * Creating a custom layer for the <code>NBeatsBlock</code> by subclassing <a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer><code>tf.keras.layers.Layer</code></a> * Creating a custom layer is helpful for when TensorFlow doesn't already have an existing implementation of a layer or if you'd like to make a layer configuration repeat a number of times (e.g. like a stack of N-BEATS blocks) * Implementing a custom architecture using the Functional API * Finding a paper related to our problem and seeing how it goes </p> <blockquote> <p>🔑 <strong>Note:</strong> As you'll see in the paper, the authors state “N-BEATS is implemented and trained in TensorFlow”, that's what we'll be doing too!</p> </blockquote> <h3 id=building-and-testing-an-n-beats-block-layer>Building and testing an N-BEATS block layer</h3> <p>Let's start by building an N-BEATS block layer, we'll write the code first and then discuss what's going on.</p> <div class=highlight><pre><span></span><code><span class=c1># Create NBeatsBlock custom layer </span>
<span class=k>class</span><span class=w> </span><span class=nc>NBeatsBlock</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Layer</span><span class=p>):</span>
  <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=c1># the constructor takes all the hyperparameters for the layer</span>
               <span class=n>input_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
               <span class=n>theta_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
               <span class=n>horizon</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
               <span class=n>n_neurons</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
               <span class=n>n_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
               <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span> <span class=c1># the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)</span>
    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
    <span class=bp>self</span><span class=o>.</span><span class=n>input_size</span> <span class=o>=</span> <span class=n>input_size</span>
    <span class=bp>self</span><span class=o>.</span><span class=n>theta_size</span> <span class=o>=</span> <span class=n>theta_size</span>
    <span class=bp>self</span><span class=o>.</span><span class=n>horizon</span> <span class=o>=</span> <span class=n>horizon</span>
    <span class=bp>self</span><span class=o>.</span><span class=n>n_neurons</span> <span class=o>=</span> <span class=n>n_neurons</span>
    <span class=bp>self</span><span class=o>.</span><span class=n>n_layers</span> <span class=o>=</span> <span class=n>n_layers</span>

    <span class=c1># Block contains stack of 4 fully connected layers each has ReLU activation</span>
    <span class=bp>self</span><span class=o>.</span><span class=n>hidden</span> <span class=o>=</span> <span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>n_neurons</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_layers</span><span class=p>)]</span>
    <span class=c1># Output of block is a theta layer with linear activation</span>
    <span class=bp>self</span><span class=o>.</span><span class=n>theta_layer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>theta_size</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;linear&quot;</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s2>&quot;theta&quot;</span><span class=p>)</span>

  <span class=k>def</span><span class=w> </span><span class=nf>call</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>):</span> <span class=c1># the call method is what runs when the layer is called </span>
    <span class=n>x</span> <span class=o>=</span> <span class=n>inputs</span> 
    <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden</span><span class=p>:</span> <span class=c1># pass inputs through each hidden layer </span>
      <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
    <span class=n>theta</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>theta_layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> 
    <span class=c1># Output the backcast and forecast from theta</span>
    <span class=n>backcast</span><span class=p>,</span> <span class=n>forecast</span> <span class=o>=</span> <span class=n>theta</span><span class=p>[:,</span> <span class=p>:</span><span class=bp>self</span><span class=o>.</span><span class=n>input_size</span><span class=p>],</span> <span class=n>theta</span><span class=p>[:,</span> <span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>horizon</span><span class=p>:]</span>
    <span class=k>return</span> <span class=n>backcast</span><span class=p>,</span> <span class=n>forecast</span>
</code></pre></div> <p>Setting up the <code>NBeatsBlock</code> custom layer we see: * The class inherits from <code>tf.keras.layers.Layer</code> (this gives it all of the methods assosciated with <code>tf.keras.layers.Layer</code>) * The constructor (<code>def __init__(...)</code>) takes all of the layer hyperparameters as well as the <code>**kwargs</code> argument * The <code>**kwargs</code> argument takes care of all of the hyperparameters which aren't mentioned in the constructor such as, <code>input_shape</code>, <code>trainable</code> and <code>name</code> * In the constructor, the block architecture layers are created: * The hidden layers are created as a stack of fully connected with <code>n_nuerons</code> hidden units layers with ReLU activation * The theta layer uses <code>theta_size</code> hidden units as well as linear activation * The <code>call()</code> method is what is run when the layer is called: * It first passes the inputs (the historical Bitcoin data) through each of the hidden layers (a stack of fully connected layers with ReLU activation) * After the inputs have been through each of the fully connected layers, they get passed through the theta layer where the backcast (backwards predictions, shape: <code>input_size</code>) and forecast (forward predictions, shape: <code>horizon</code>) are returned</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-nbeats-basic-block-replication-with-tensorflow-layer-subclassing.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="code mapping to image of N-BEATS basic block" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-nbeats-basic-block-replication-with-tensorflow-layer-subclassing.png></a> <em>Using TensorFlow layer subclassing to replicate the N-BEATS basic block. See section 3.1 of N-BEATS paper for details.</em></p> <p>Let's see our block replica in action by together by creating a toy version of <code>NBeatsBlock</code>.</p> <blockquote> <p>📖 <strong>Resource:</strong> Much of the creation of the time series materials (the ones you're going through now), including replicating the N-BEATS algorithm were streamed live on Twitch. If you'd like to see replays of how the algorithm was replicated, check out the <a href="https://youtube.com/playlist?list=PL8IpyNZ21vUSCM7nRAuS-hW_E-sgwdmaI"><em>Time series research and TensorFlow course material creation playlist</em></a> on the Daniel Bourke arXiv YouTube channel.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Set up dummy NBeatsBlock layer to represent inputs and outputs</span>
<span class=n>dummy_nbeats_block_layer</span> <span class=o>=</span> <span class=n>NBeatsBlock</span><span class=p>(</span><span class=n>input_size</span><span class=o>=</span><span class=n>WINDOW_SIZE</span><span class=p>,</span> 
                                       <span class=n>theta_size</span><span class=o>=</span><span class=n>WINDOW_SIZE</span><span class=o>+</span><span class=n>HORIZON</span><span class=p>,</span> <span class=c1># backcast + forecast </span>
                                       <span class=n>horizon</span><span class=o>=</span><span class=n>HORIZON</span><span class=p>,</span>
                                       <span class=n>n_neurons</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
                                       <span class=n>n_layers</span><span class=o>=</span><span class=mi>4</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Create dummy inputs (have to be same size as input_size)</span>
<span class=n>dummy_inputs</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=n>WINDOW_SIZE</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=c1># input shape to the model has to reflect Dense layer input requirements (ndim=2)</span>
<span class=n>dummy_inputs</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7]], dtype=int32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Pass dummy inputs to dummy NBeatsBlock layer</span>
<span class=n>backcast</span><span class=p>,</span> <span class=n>forecast</span> <span class=o>=</span> <span class=n>dummy_nbeats_block_layer</span><span class=p>(</span><span class=n>dummy_inputs</span><span class=p>)</span>
<span class=c1># These are the activation outputs of the theta layer (they&#39;ll be random due to no training of the model)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Backcast: </span><span class=si>{</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>backcast</span><span class=o>.</span><span class=n>numpy</span><span class=p>())</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Forecast: </span><span class=si>{</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>forecast</span><span class=o>.</span><span class=n>numpy</span><span class=p>())</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Backcast: [ 0.19014978  0.83798355 -0.32870018  0.25159916 -0.47540277 -0.77836645
 -0.5299447 ]
Forecast: -0.7554212808609009
</code></pre></div> <h3 id=preparing-data-for-the-n-beats-algorithm-using-tfdata>Preparing data for the N-BEATS algorithm using <code>tf.data</code></h3> <p>We've got the basic building block for the N-BEATS architecture ready to go.</p> <p>But before we use it to replicate the entire N-BEATS generic architecture, let's create some data.</p> <p>This time, because we're going to be using a larger model architecture, to ensure our model training runs as fast as possible, we'll setup our datasets using the <code>tf.data</code> API.</p> <p>And because the N-BEATS algorithm is focused on univariate time series, we'll start by making training and test windowed datasets of Bitcoin prices (just as we've done above).</p> <div class=highlight><pre><span></span><code><span class=n>HORIZON</span> <span class=o>=</span> <span class=mi>1</span> <span class=c1># how far to predict forward</span>
<span class=n>WINDOW_SIZE</span> <span class=o>=</span> <span class=mi>7</span> <span class=c1># how far to lookback</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Create NBEATS data inputs (NBEATS works with univariate time series)</span>
<span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>Price</th> </tr> <tr> <th>Date</th> <th></th> </tr> </thead> <tbody> <tr> <th>2013-10-01</th> <td>123.65499</td> </tr> <tr> <th>2013-10-02</th> <td>125.45500</td> </tr> <tr> <th>2013-10-03</th> <td>108.58483</td> </tr> <tr> <th>2013-10-04</th> <td>118.67466</td> </tr> <tr> <th>2013-10-05</th> <td>121.33866</td> </tr> </tbody> </table> </div> <div class=highlight><pre><span></span><code><span class=c1># Add windowed columns</span>
<span class=n>bitcoin_prices_nbeats</span> <span class=o>=</span> <span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>WINDOW_SIZE</span><span class=p>):</span>
  <span class=n>bitcoin_prices_nbeats</span><span class=p>[</span><span class=sa>f</span><span class=s2>&quot;Price+</span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>bitcoin_prices_nbeats</span><span class=p>[</span><span class=s2>&quot;Price&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>shift</span><span class=p>(</span><span class=n>periods</span><span class=o>=</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span>
<span class=n>bitcoin_prices_nbeats</span><span class=o>.</span><span class=n>dropna</span><span class=p>()</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>Price</th> <th>Price+1</th> <th>Price+2</th> <th>Price+3</th> <th>Price+4</th> <th>Price+5</th> <th>Price+6</th> <th>Price+7</th> </tr> <tr> <th>Date</th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <th>2013-10-08</th> <td>123.03300</td> <td>121.79500</td> <td>120.65533</td> <td>121.33866</td> <td>118.67466</td> <td>108.58483</td> <td>125.45500</td> <td>123.65499</td> </tr> <tr> <th>2013-10-09</th> <td>124.04900</td> <td>123.03300</td> <td>121.79500</td> <td>120.65533</td> <td>121.33866</td> <td>118.67466</td> <td>108.58483</td> <td>125.45500</td> </tr> <tr> <th>2013-10-10</th> <td>125.96116</td> <td>124.04900</td> <td>123.03300</td> <td>121.79500</td> <td>120.65533</td> <td>121.33866</td> <td>118.67466</td> <td>108.58483</td> </tr> <tr> <th>2013-10-11</th> <td>125.27966</td> <td>125.96116</td> <td>124.04900</td> <td>123.03300</td> <td>121.79500</td> <td>120.65533</td> <td>121.33866</td> <td>118.67466</td> </tr> <tr> <th>2013-10-12</th> <td>125.92750</td> <td>125.27966</td> <td>125.96116</td> <td>124.04900</td> <td>123.03300</td> <td>121.79500</td> <td>120.65533</td> <td>121.33866</td> </tr> </tbody> </table> </div> <div class=highlight><pre><span></span><code><span class=c1># Make features and labels</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>bitcoin_prices_nbeats</span><span class=o>.</span><span class=n>dropna</span><span class=p>()</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s2>&quot;Price&quot;</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>bitcoin_prices_nbeats</span><span class=o>.</span><span class=n>dropna</span><span class=p>()[</span><span class=s2>&quot;Price&quot;</span><span class=p>]</span>

<span class=c1># Make train and test sets</span>
<span class=n>split_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.8</span><span class=p>)</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:</span><span class=n>split_size</span><span class=p>],</span> <span class=n>y</span><span class=p>[:</span><span class=n>split_size</span><span class=p>]</span>
<span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>split_size</span><span class=p>:],</span> <span class=n>y</span><span class=p>[</span><span class=n>split_size</span><span class=p>:]</span>
<span class=nb>len</span><span class=p>(</span><span class=n>X_train</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_train</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>X_test</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2224, 2224, 556, 556)
</code></pre></div> <p>Train and test sets ready to go!</p> <p>Now let's convert them into TensorFlow <code>tf.data.Dataset</code>'s to ensure they run as fast as possible whilst training.</p> <p>We'll do this by: 1. Turning the arrays in tensor Datasets using <a href=https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices><code>tf.data.Dataset.from_tensor_slices()</code></a> * Note: <code>from_tensor_slices()</code> works best when your data fits in memory, for extremely large datasets, you'll want to look into using the <a href=https://www.tensorflow.org/tutorials/load_data/tfrecord><code>TFRecord</code> format</a> 2. Combine the labels and features tensors into a Dataset using <a href=https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip><code>tf.data.Dataset.zip()</code></a> 3. Batch and prefetch the Datasets using <a href=https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch><code>batch()</code></a> and <a href=https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch><code>prefetch()</code></a> * Batching and prefetching ensures the loading time from CPU (preparing data) to GPU (computing on data) is as small as possible </p> <blockquote> <p>📖 <strong>Resource:</strong> For more on building highly performant TensorFlow data pipelines, I'd recommend reading through the <a href=https://www.tensorflow.org/guide/data_performance><em>Better performance with the tf.data API</em></a> guide.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># 1. Turn train and test arrays into tensor Datasets</span>
<span class=n>train_features_dataset</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>from_tensor_slices</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
<span class=n>train_labels_dataset</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>from_tensor_slices</span><span class=p>(</span><span class=n>y_train</span><span class=p>)</span>

<span class=n>test_features_dataset</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>from_tensor_slices</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=n>test_labels_dataset</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>from_tensor_slices</span><span class=p>(</span><span class=n>y_test</span><span class=p>)</span>

<span class=c1># 2. Combine features &amp; labels</span>
<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>zip</span><span class=p>((</span><span class=n>train_features_dataset</span><span class=p>,</span> <span class=n>train_labels_dataset</span><span class=p>))</span>
<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>zip</span><span class=p>((</span><span class=n>test_features_dataset</span><span class=p>,</span> <span class=n>test_labels_dataset</span><span class=p>))</span>

<span class=c1># 3. Batch and prefetch for optimal performance</span>
<span class=n>BATCH_SIZE</span> <span class=o>=</span> <span class=mi>1024</span> <span class=c1># taken from Appendix D in N-BEATS paper</span>
<span class=n>train_dataset</span> <span class=o>=</span> <span class=n>train_dataset</span><span class=o>.</span><span class=n>batch</span><span class=p>(</span><span class=n>BATCH_SIZE</span><span class=p>)</span><span class=o>.</span><span class=n>prefetch</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>AUTOTUNE</span><span class=p>)</span>
<span class=n>test_dataset</span> <span class=o>=</span> <span class=n>test_dataset</span><span class=o>.</span><span class=n>batch</span><span class=p>(</span><span class=n>BATCH_SIZE</span><span class=p>)</span><span class=o>.</span><span class=n>prefetch</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>AUTOTUNE</span><span class=p>)</span>

<span class=n>train_dataset</span><span class=p>,</span> <span class=n>test_dataset</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;PrefetchDataset shapes: ((None, 7), (None,)), types: (tf.float64, tf.float64)&gt;,
 &lt;PrefetchDataset shapes: ((None, 7), (None,)), types: (tf.float64, tf.float64)&gt;)
</code></pre></div> <p>Data prepared! Notice the input shape for the features <code>(None, 7)</code>, the <code>None</code> leaves space for the batch size where as the <code>7</code> represents the <code>WINDOW_SIZE</code>.</p> <p>Time to get create the N-BEATS architecture. </p> <h3 id=setting-up-hyperparameters-for-n-beats-algorithm>Setting up hyperparameters for N-BEATS algorithm</h3> <p>Ho ho, would you look at that! Datasets ready, model building block ready, what'd you say we put things together?</p> <p>Good idea.</p> <p>Okay.</p> <p>Let's go.</p> <p>To begin, we'll create variables for each of the hyperparameters we'll be using for our N-BEATS replica.</p> <blockquote> <p>📖 <strong>Resource:</strong> The following hyperparameters are taken from Figure 1 and Table 18/Appendix D of the <a href=https://arxiv.org/pdf/1905.10437.pdf>N-BEATS paper</a>.</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-nbeats-table-18-hyperparameters.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="table 18 from N-BEATS paper" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-nbeats-table-18-hyperparameters.png></a> <em>Table 18 from <a href=https://arxiv.org/pdf/1905.10437.pdf>N-BEATS paper</a> describing the hyperparameters used for the different variants of N-BEATS. We're using N-BEATS-G which stands for the generic version of N-BEATS.</em></p> <blockquote> <p>🔑 <strong>Note:</strong> If you see variables in a machine learning example in all caps, such as "<code>N_EPOCHS = 100</code>", these variables are often hyperparameters which are used through the example. You'll usually see them instantiated towards the start of an experiment and then used throughout.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Values from N-BEATS paper Figure 1 and Table 18/Appendix D</span>
<span class=n>N_EPOCHS</span> <span class=o>=</span> <span class=mi>5000</span> <span class=c1># called &quot;Iterations&quot; in Table 18</span>
<span class=n>N_NEURONS</span> <span class=o>=</span> <span class=mi>512</span> <span class=c1># called &quot;Width&quot; in Table 18</span>
<span class=n>N_LAYERS</span> <span class=o>=</span> <span class=mi>4</span>
<span class=n>N_STACKS</span> <span class=o>=</span> <span class=mi>30</span>

<span class=n>INPUT_SIZE</span> <span class=o>=</span> <span class=n>WINDOW_SIZE</span> <span class=o>*</span> <span class=n>HORIZON</span> <span class=c1># called &quot;Lookback&quot; in Table 18</span>
<span class=n>THETA_SIZE</span> <span class=o>=</span> <span class=n>INPUT_SIZE</span> <span class=o>+</span> <span class=n>HORIZON</span>

<span class=n>INPUT_SIZE</span><span class=p>,</span> <span class=n>THETA_SIZE</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(7, 8)
</code></pre></div> <h3 id=getting-ready-for-residual-connections>Getting ready for residual connections</h3> <p>Beautiful! Hyperparameters ready, now before we create the N-BEATS model, there are two layers to go through which play a large roll in the architecture.</p> <p>They're what make N-BEATS <strong>double residual stacking</strong> (section 3.2 of the <a href=https://arxiv.org/pdf/1905.10437.pdf>N-BEATS paper</a>) possible: * <a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/subtract><code>tf.keras.layers.subtract(inputs)</code></a> - subtracts list of input tensors from each other * <a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/add><code>tf.keras.layers.add(inputs)</code></a> - adds list of input tensors to each other</p> <p>Let's try them out.</p> <div class=highlight><pre><span></span><code><span class=c1># Make tensors</span>
<span class=n>tensor_1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span> <span class=o>+</span> <span class=mi>10</span>
<span class=n>tensor_2</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># Subtract</span>
<span class=n>subtracted</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>subtract</span><span class=p>([</span><span class=n>tensor_1</span><span class=p>,</span> <span class=n>tensor_2</span><span class=p>])</span>

<span class=c1># Add</span>
<span class=n>added</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>add</span><span class=p>([</span><span class=n>tensor_1</span><span class=p>,</span> <span class=n>tensor_2</span><span class=p>])</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Input tensors: </span><span class=si>{</span><span class=n>tensor_1</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=s2> &amp; </span><span class=si>{</span><span class=n>tensor_2</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Subtracted: </span><span class=si>{</span><span class=n>subtracted</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Added: </span><span class=si>{</span><span class=n>added</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Input tensors: [10 11 12 13 14 15 16 17 18 19] &amp; [0 1 2 3 4 5 6 7 8 9]
Subtracted: [10 10 10 10 10 10 10 10 10 10]
Added: [10 12 14 16 18 20 22 24 26 28]
</code></pre></div> <p>Both of these layer functions are straight-forward, subtract or add together their inputs.</p> <p>And as mentioned before, they're what powers N-BEATS double residual stacking.</p> <p>The power of <strong>residual stacking</strong> or <strong>residual connections</strong> was revealed in <a href=https://arxiv.org/abs/1512.03385>Deep Residual Learning for Image Recognition</a> where the authors were able to build a deeper but less complex neural network (this is what introduced the popular <a href=https://en.wikipedia.org/wiki/Residual_neural_network>ResNet architecture</a>) than previous attempts. </p> <p>This deeper neural network led to state of the art results on the ImageNet challenge in 2015 and different versions of residual connections have been present in deep learning ever since.</p> <blockquote> <p>What is a residual connection?</p> </blockquote> <p>A <strong>residual connection</strong> (also called skip connections) involves a deeper neural network layer receiving the outputs as well as the inputs of a shallower neural network layer.</p> <p>In the case of N-BEATS, the architecture uses residual connections which: * Subtract the backcast outputs from a previous block from the backcast inputs to the current block * Add the forecast outputs from all blocks together in a stack </p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-nbeats-architecture-double-residual-stacking.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="N-BEATS architecture double residual stacking" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-nbeats-architecture-double-residual-stacking.png></a> <em>Annotated version of Figure 1 from the N-BEATS paper highlighting the double residual stacking (section 3.2) of the architecture. Backcast residuals of each block are subtracted from each other and used as the input to the next block where as the forecasts of each block are added together to become the stack forecast.</em></p> <blockquote> <p>What are the benefits of residual connections?</p> </blockquote> <p>In practice, residual connections have been beneficial for training deeper models (N-BEATS reaches ~150 layers, also see "These approaches provide clear advantages in improving the trainability of deep architectures" in section 3.2 of the <a href=https://arxiv.org/pdf/1905.10437.pdf>N-BEATS paper</a>). </p> <p>It's thought that they help avoid the problem of <a href=https://en.wikipedia.org/wiki/Vanishing_gradient_problem>vanishing gradients</a> (patterns learned by a neural network not being passed through to deeper layers).</p> <h3 id=building-compiling-and-fitting-the-n-beats-algorithm>Building, compiling and fitting the N-BEATS algorithm</h3> <p>Okay, we've finally got all of the pieces of the puzzle ready for building and training the N-BEATS algorithm.</p> <p>We'll do so by going through the following:</p> <ol> <li>Setup an instance of the N-BEATS block layer using <code>NBeatsBlock</code> (this'll be the initial block used for the network, the rest will be created as part of stacks)</li> <li>Create an input layer for the N-BEATS stack (we'll be using the <a href=https://www.tensorflow.org/guide/keras/functional>Keras Functional API</a> for this)</li> <li>Make the initial backcast and forecasts for the model with the layer created in (1)</li> <li>Use a for loop to create stacks of block layers</li> <li>Use the NBeatsBlock class within the for loop created in (4) to create blocks which return backcasts and block-level forecasts</li> <li>Create the double residual stacking using subtract and add layers</li> <li>Put the model inputs and outputs together using <a href=https://www.tensorflow.org/api_docs/python/tf/keras/Model><code>tf.keras.Model()</code></a></li> <li>Compile the model with MAE loss (the paper uses multiple losses but we'll use MAE to keep it inline with our other models) and Adam optimizer with default settings as per section 5.2 of <a href=https://arxiv.org/pdf/1905.10437.pdf>N-BEATS paper</a>)</li> <li>Fit the N-BEATS model for 5000 epochs and since it's fitting for so many epochs, we'll use a couple of callbacks:</li> <li><a href=https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping><code>tf.keras.callbacks.EarlyStopping()</code></a> - stop the model from training if it doesn't improve validation loss for 200 epochs and restore the best performing weights using <code>restore_best_weights=True</code> (this'll prevent the model from training for loooongggggg period of time without improvement)</li> <li><a href=https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau><code>tf.keras.callbacks.ReduceLROnPlateau()</code></a> - if the model's validation loss doesn't improve for 100 epochs, reduce the learning rate by 10x to try and help it make incremental improvements (the smaller the learning rate, the smaller updates a model tries to make)</li> </ol> <p>Woah. A bunch of steps. But I'm sure you're up to it.</p> <p>Let's do it!</p> <div class=highlight><pre><span></span><code> <span class=o>%%</span><span class=n>time</span>

<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># 1. Setup N-BEATS Block layer</span>
<span class=n>nbeats_block_layer</span> <span class=o>=</span> <span class=n>NBeatsBlock</span><span class=p>(</span><span class=n>input_size</span><span class=o>=</span><span class=n>INPUT_SIZE</span><span class=p>,</span>
                                 <span class=n>theta_size</span><span class=o>=</span><span class=n>THETA_SIZE</span><span class=p>,</span>
                                 <span class=n>horizon</span><span class=o>=</span><span class=n>HORIZON</span><span class=p>,</span>
                                 <span class=n>n_neurons</span><span class=o>=</span><span class=n>N_NEURONS</span><span class=p>,</span>
                                 <span class=n>n_layers</span><span class=o>=</span><span class=n>N_LAYERS</span><span class=p>,</span>
                                 <span class=n>name</span><span class=o>=</span><span class=s2>&quot;InitialBlock&quot;</span><span class=p>)</span>

<span class=c1># 2. Create input to stacks</span>
<span class=n>stack_input</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>INPUT_SIZE</span><span class=p>),</span> <span class=n>name</span><span class=o>=</span><span class=s2>&quot;stack_input&quot;</span><span class=p>)</span>

<span class=c1># 3. Create initial backcast and forecast input (backwards predictions are referred to as residuals in the paper)</span>
<span class=n>backcast</span><span class=p>,</span> <span class=n>forecast</span> <span class=o>=</span> <span class=n>nbeats_block_layer</span><span class=p>(</span><span class=n>stack_input</span><span class=p>)</span>
<span class=c1># Add in subtraction residual link, thank you to: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/174 </span>
<span class=n>residuals</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>subtract</span><span class=p>([</span><span class=n>stack_input</span><span class=p>,</span> <span class=n>backcast</span><span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=sa>f</span><span class=s2>&quot;subtract_00&quot;</span><span class=p>)</span> 

<span class=c1># 4. Create stacks of blocks</span>
<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>N_STACKS</span><span class=o>-</span><span class=mi>1</span><span class=p>)):</span> <span class=c1># first stack is already creted in (3)</span>

  <span class=c1># 5. Use the NBeatsBlock to calculate the backcast as well as block forecast</span>
  <span class=n>backcast</span><span class=p>,</span> <span class=n>block_forecast</span> <span class=o>=</span> <span class=n>NBeatsBlock</span><span class=p>(</span>
      <span class=n>input_size</span><span class=o>=</span><span class=n>INPUT_SIZE</span><span class=p>,</span>
      <span class=n>theta_size</span><span class=o>=</span><span class=n>THETA_SIZE</span><span class=p>,</span>
      <span class=n>horizon</span><span class=o>=</span><span class=n>HORIZON</span><span class=p>,</span>
      <span class=n>n_neurons</span><span class=o>=</span><span class=n>N_NEURONS</span><span class=p>,</span>
      <span class=n>n_layers</span><span class=o>=</span><span class=n>N_LAYERS</span><span class=p>,</span>
      <span class=n>name</span><span class=o>=</span><span class=sa>f</span><span class=s2>&quot;NBeatsBlock_</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&quot;</span>
  <span class=p>)(</span><span class=n>residuals</span><span class=p>)</span> <span class=c1># pass it in residuals (the backcast)</span>

  <span class=c1># 6. Create the double residual stacking</span>
  <span class=n>residuals</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>subtract</span><span class=p>([</span><span class=n>residuals</span><span class=p>,</span> <span class=n>backcast</span><span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=sa>f</span><span class=s2>&quot;subtract_</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span> 
  <span class=n>forecast</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>add</span><span class=p>([</span><span class=n>forecast</span><span class=p>,</span> <span class=n>block_forecast</span><span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=sa>f</span><span class=s2>&quot;add_</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># 7. Put the stack model together</span>
<span class=n>model_7</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=n>stack_input</span><span class=p>,</span> 
                         <span class=n>outputs</span><span class=o>=</span><span class=n>forecast</span><span class=p>,</span> 
                         <span class=n>name</span><span class=o>=</span><span class=s2>&quot;model_7_N-BEATS&quot;</span><span class=p>)</span>

<span class=c1># 8. Compile with MAE loss and Adam optimizer</span>
<span class=n>model_7</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=mf>0.001</span><span class=p>),</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span> <span class=s2>&quot;mse&quot;</span><span class=p>])</span>

<span class=c1># 9. Fit the model with EarlyStopping and ReduceLROnPlateau callbacks</span>
<span class=n>model_7</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span>
            <span class=n>epochs</span><span class=o>=</span><span class=n>N_EPOCHS</span><span class=p>,</span>
            <span class=n>validation_data</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span>
            <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=c1># prevent large amounts of training outputs</span>
            <span class=c1># callbacks=[create_model_checkpoint(model_name=stack_model.name)] # saving model every epoch consumes far too much time</span>
            <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>EarlyStopping</span><span class=p>(</span><span class=n>monitor</span><span class=o>=</span><span class=s2>&quot;val_loss&quot;</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span> <span class=n>restore_best_weights</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
                      <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>ReduceLROnPlateau</span><span class=p>(</span><span class=n>monitor</span><span class=o>=</span><span class=s2>&quot;val_loss&quot;</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span><span class=p>)])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 00328: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00428: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
CPU times: user 1min 23s, sys: 4.58 s, total: 1min 28s
Wall time: 3min 44s
</code></pre></div> <p>And would you look at that! N-BEATS algorithm fit to our Bitcoin historical data.</p> <p>How did it perform?</p> <div class=highlight><pre><span></span><code><span class=c1># Evaluate N-BEATS model on the test dataset</span>
<span class=n>model_7</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>test_dataset</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>1/1 [==============================] - 0s 46ms/step - loss: 585.4998 - mae: 585.4998 - mse: 1179491.5000





[585.4998168945312, 585.4998168945312, 1179491.5]
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make predictions with N-BEATS model</span>
<span class=n>model_7_preds</span> <span class=o>=</span> <span class=n>make_preds</span><span class=p>(</span><span class=n>model_7</span><span class=p>,</span> <span class=n>test_dataset</span><span class=p>)</span>
<span class=n>model_7_preds</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
array([8908.059, 8854.672, 8990.933, 8759.821, 8819.711, 8774.012,
       8604.187, 8547.038, 8495.928, 8489.514], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate N-BEATS model predictions</span>
<span class=n>model_7_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span>
                                 <span class=n>y_pred</span><span class=o>=</span><span class=n>model_7_preds</span><span class=p>)</span>
<span class=n>model_7_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 585.4998,
 &#39;mape&#39;: 2.7445195,
 &#39;mase&#39;: 1.028561,
 &#39;mse&#39;: 1179491.5,
 &#39;rmse&#39;: 1086.044}
</code></pre></div> <p>Woah... even with all of those special layers and hand-crafted network, it looks like the N-BEATS model doesn't perform as well as <code>model_1</code> or the original naive forecast.</p> <p>This goes to show the power of smaller networks as well as the fact not all larger models are better suited for a certain type of data.</p> <h3 id=plotting-the-n-beats-architecture-weve-created>Plotting the N-BEATS architecture we've created</h3> <p>You know what would be cool?</p> <p>If we could plot the N-BEATS model we've crafted.</p> <p>Well it turns out we can using <a href=https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model><code>tensorflow.keras.utils.plot_model()</code></a>.</p> <p>Let's see what it looks like.</p> <div class=highlight><pre><span></span><code><span class=c1># Plot the N-BEATS model and inspect the architecture</span>
<span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras.utils</span><span class=w> </span><span class=kn>import</span> <span class=n>plot_model</span>
<span class=n>plot_model</span><span class=p>(</span><span class=n>model_7</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_214_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_214_0.png></a></p> <p>Now that is one good looking model! </p> <p>It even looks similar to the model shown in Figure 1 of the N-BEATS paper. </p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-nbeats-stack-model-vs-nbeats-architecture.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="N-BEATS replica model compared with actual N-BEATS architecture" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-nbeats-stack-model-vs-nbeats-architecture.png></a> <em>Comparison of <code>model_7</code> (N-BEATS replica model make with Keras Functional API) versus actual N-BEATS architecture diagram.</em></p> <p>Looks like our Functional API usage did the trick! </p> <blockquote> <p>🔑 <strong>Note:</strong> Our N-BEATS model replicates the N-BEATS <strong>generic architecture</strong>, the training setups are largely the same, except for the N-BEATS paper used an ensemble of models to make predictions (multiple different loss functions and multiple different lookback windows), see Table 18 of the <a href=https://arxiv.org/pdf/1905.10437.pdf>N-BEATS paper</a> for more. An extension could be to setup this kind of training regime and see if it improves performance.</p> </blockquote> <p>How about we try and save our version of the N-BEATS model?</p> <div class=highlight><pre><span></span><code><span class=c1># This will error out unless a &quot;get_config()&quot; method is implemented - this could be extra curriculum</span>
<span class=n>model_7</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>model_7</span><span class=o>.</span><span class=n>name</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>WARNING:absl:Found untraced functions such as theta_layer_call_and_return_conditional_losses, theta_layer_call_fn, theta_layer_call_and_return_conditional_losses, theta_layer_call_fn, theta_layer_call_and_return_conditional_losses while saving (showing 5 of 750). These functions will not be directly callable after loading.


INFO:tensorflow:Assets written to: model_7_N-BEATS/assets


INFO:tensorflow:Assets written to: model_7_N-BEATS/assets
/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
  category=CustomMaskWarning)
</code></pre></div> <p>You'll notice a warning appears telling us to fully save our model correctly we need to implement a <a href=https://www.tensorflow.org/guide/keras/save_and_serialize#how_savedmodel_handles_custom_objects><code>get_config()</code></a> method in our custom layer class.</p> <blockquote> <p>📖 <strong>Resource:</strong> If you would like to save and load the N-BEATS model or any other custom or subclassed layer/model configuration, you should overwrite the <code>get_config()</code> and optionally <code>from_config()</code> methods. See the <a href=https://www.tensorflow.org/guide/keras/save_and_serialize#custom_objects>TensorFlow Custom Objects documentation</a> for more.</p> </blockquote> <h2 id=model-8-creating-an-ensemble-stacking-different-models-together>Model 8: Creating an ensemble (stacking different models together)</h2> <p>After all that effort, the N-BEATS algorithm's performance was underwhelming.</p> <p>But again, this is part of the parcel of machine learning. Not everything will work.</p> <p>That's when we refer back to the motto: experiment, experiment, experiment.</p> <p>Our next experiment is creating an <a href=https://en.wikipedia.org/wiki/Ensemble_learning><strong>ensemble</strong> of models</a>.</p> <p>An <strong>ensemble</strong> involves training and combining multiple different models on the same problem. Ensemble models are often the types of models you'll see winning data science competitions on websites like Kaggle.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-ensemble-model-example.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="high level overview of ensemble model: combining many different models may result in better results than one single model" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-ensemble-model-example.png></a> <em>Example of the power of ensembling. One Daniel model makes a decision with a smart level of 7 but when a Daniel model teams up with multiple different people, together (ensembled) they make a decision with a smart level of 10. The key here is combining the decision power of people with different backgrounds, if you combined multiple Daniel models, you'd end up with an average smart level of 7. Note: smart level is not an actual measurement of decision making, it is for demonstration purposes only.</em></p> <p>For example, in the N-BEATS paper, they trained an ensemble of models (180 in total, see <a href=https://arxiv.org/pdf/1905.10437.pdf>section 3.4</a>) to achieve the results they did using a combination of: * Different loss functions (sMAPE, MASE and MAPE) * Different window sizes (2 x horizon, 3 x horizon, 4 x horizon...)</p> <p>The benefit of ensembling models is you get the "decision of the crowd effect". Rather than relying on a single model's predictions, you can <a href=https://en.wikipedia.org/wiki/Ensemble_forecasting>take the average or median of many different models</a>.</p> <p>The keyword being: different.</p> <p>It wouldn't make sense to train the same model 10 times on the same data and then average the predictions.</p> <p>Fortunately, due to their random initialization, even deep learning models with the same architecture can produce different results. </p> <p>What I mean by this is each time you create a deep learning model, it starts with random patterns (weights &amp; biases) and then it adjusts these random patterns to better suit the dataset it's being trained on. </p> <p>However, the process it adjusts these patterns is often a form of guided randomness as well (the SGD optimizer stands for stochastic or random gradient descent).</p> <p>To create our ensemble models we're going to be using a combination of: * Different loss functions (MAE, MSE, MAPE) * Randomly initialized models </p> <p>Essentially, we'll be creating a suite of different models all attempting to model the same data.</p> <p>And hopefully the combined predictive power of each model is better than a single model on its own.</p> <p>Let's find out!</p> <p>We'll start by creating a function to produce a list of different models trained with different loss functions. Each layer in the ensemble models will be initialized with a random normal <a href=https://en.wikipedia.org/wiki/Normal_distribution>(Gaussian) distribution</a> using <a href=https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal>He normal initialization</a>, this'll help estimating the prediction intervals later on.</p> <blockquote> <p>🔑 <strong>Note:</strong> In your machine leanring experiments, you may have already dealt with examples of ensemble models. Algorithms such as the <a href=https://towardsdatascience.com/understanding-random-forest-58381e0602d2>random forest model</a> are a form of ensemble, it uses a number of randomly created decision trees where each individual tree may perform poorly but when combined gives great results.</p> </blockquote> <h3 id=constructing-and-fitting-an-ensemble-of-models-using-different-loss-functions>Constructing and fitting an ensemble of models (using different loss functions)</h3> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>get_ensemble_models</span><span class=p>(</span><span class=n>horizon</span><span class=o>=</span><span class=n>HORIZON</span><span class=p>,</span> 
                        <span class=n>train_data</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span>
                        <span class=n>test_data</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>,</span>
                        <span class=n>num_iter</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> 
                        <span class=n>num_epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> 
                        <span class=n>loss_fns</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span> <span class=s2>&quot;mse&quot;</span><span class=p>,</span> <span class=s2>&quot;mape&quot;</span><span class=p>]):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Returns a list of num_iter models each trained on MAE, MSE and MAPE loss.</span>

<span class=sd>  For example, if num_iter=10, a list of 30 trained models will be returned:</span>
<span class=sd>  10 * len([&quot;mae&quot;, &quot;mse&quot;, &quot;mape&quot;]).</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=c1># Make empty list for trained ensemble models</span>
  <span class=n>ensemble_models</span> <span class=o>=</span> <span class=p>[]</span>

  <span class=c1># Create num_iter number of models per loss function</span>
  <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_iter</span><span class=p>):</span>
    <span class=c1># Build and fit a new model with a different loss function</span>
    <span class=k>for</span> <span class=n>loss_function</span> <span class=ow>in</span> <span class=n>loss_fns</span><span class=p>:</span>
      <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Optimizing model by reducing: </span><span class=si>{</span><span class=n>loss_function</span><span class=si>}</span><span class=s2> for </span><span class=si>{</span><span class=n>num_epochs</span><span class=si>}</span><span class=s2> epochs, model number: </span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

      <span class=c1># Construct a simple model (similar to model_1)</span>
      <span class=n>model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
        <span class=c1># Initialize layers with normal (Gaussian) distribution so we can use the models for prediction</span>
        <span class=c1># interval estimation later: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal</span>
        <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>kernel_initializer</span><span class=o>=</span><span class=s2>&quot;he_normal&quot;</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span> 
        <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>kernel_initializer</span><span class=o>=</span><span class=s2>&quot;he_normal&quot;</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
        <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>HORIZON</span><span class=p>)</span>                                 
      <span class=p>])</span>

      <span class=c1># Compile simple model with current loss function</span>
      <span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>loss_function</span><span class=p>,</span>
                    <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span>
                    <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span> <span class=s2>&quot;mse&quot;</span><span class=p>])</span>

      <span class=c1># Fit model</span>
      <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_data</span><span class=p>,</span>
                <span class=n>epochs</span><span class=o>=</span><span class=n>num_epochs</span><span class=p>,</span>
                <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
                <span class=n>validation_data</span><span class=o>=</span><span class=n>test_data</span><span class=p>,</span>
                <span class=c1># Add callbacks to prevent training from going/stalling for too long</span>
                <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>EarlyStopping</span><span class=p>(</span><span class=n>monitor</span><span class=o>=</span><span class=s2>&quot;val_loss&quot;</span><span class=p>,</span>
                                                            <span class=n>patience</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span>
                                                            <span class=n>restore_best_weights</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
                           <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>ReduceLROnPlateau</span><span class=p>(</span><span class=n>monitor</span><span class=o>=</span><span class=s2>&quot;val_loss&quot;</span><span class=p>,</span>
                                                                <span class=n>patience</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
                                                                <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span><span class=p>)])</span>

      <span class=c1># Append fitted model to list of ensemble models</span>
      <span class=n>ensemble_models</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>

  <span class=k>return</span> <span class=n>ensemble_models</span> <span class=c1># return list of trained models</span>
</code></pre></div> <p>Ensemble model creator function created!</p> <p>Let's try it out by running <code>num_iter=5</code> runs for 1000 epochs. This will result in 15 total models (5 for each different loss function).</p> <p>Of course, these numbers could be tweaked to create more models trained for longer. </p> <blockquote> <p>🔑 <strong>Note:</strong> With ensembles, you'll generally find more total models means better performance. However, this comes with the tradeoff of having to train more models (longer training time) and make predictions with more models (longer prediction time).</p> </blockquote> <div class=highlight><pre><span></span><code><span class=o>%%</span><span class=n>time</span>
<span class=c1># Get list of trained ensemble models</span>
<span class=n>ensemble_models</span> <span class=o>=</span> <span class=n>get_ensemble_models</span><span class=p>(</span><span class=n>num_iter</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
                                      <span class=n>num_epochs</span><span class=o>=</span><span class=mi>1000</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Optimizing model by reducing: mae for 1000 epochs, model number: 0

Epoch 00794: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00928: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Optimizing model by reducing: mse for 1000 epochs, model number: 0

Epoch 00591: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00707: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.

Epoch 00807: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Optimizing model by reducing: mape for 1000 epochs, model number: 0

Epoch 00165: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00282: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.

Epoch 00382: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Optimizing model by reducing: mae for 1000 epochs, model number: 1
Optimizing model by reducing: mse for 1000 epochs, model number: 1

Epoch 00409: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00509: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Optimizing model by reducing: mape for 1000 epochs, model number: 1

Epoch 00185: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00726: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.

Epoch 00826: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Optimizing model by reducing: mae for 1000 epochs, model number: 2
Optimizing model by reducing: mse for 1000 epochs, model number: 2
Optimizing model by reducing: mape for 1000 epochs, model number: 2

Epoch 00241: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00341: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Optimizing model by reducing: mae for 1000 epochs, model number: 3

Epoch 00572: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Optimizing model by reducing: mse for 1000 epochs, model number: 3

Epoch 00304: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00607: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.

Epoch 00707: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Optimizing model by reducing: mape for 1000 epochs, model number: 3

Epoch 00301: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00401: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Optimizing model by reducing: mae for 1000 epochs, model number: 4
Optimizing model by reducing: mse for 1000 epochs, model number: 4

Epoch 00640: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00740: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Optimizing model by reducing: mape for 1000 epochs, model number: 4

Epoch 00132: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00609: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.

Epoch 00709: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
CPU times: user 6min 24s, sys: 38.6 s, total: 7min 3s
Wall time: 7min 58s
</code></pre></div> <p>Look at all of those models! </p> <p>How about we now write a function to use the list of trained ensemble models to make predictions and then return a list of predictions (one set of predictions per model)?</p> <h3 id=making-predictions-with-an-ensemble-model>Making predictions with an ensemble model</h3> <div class=highlight><pre><span></span><code><span class=c1># Create a function which uses a list of trained models to make and return a list of predictions</span>
<span class=k>def</span><span class=w> </span><span class=nf>make_ensemble_preds</span><span class=p>(</span><span class=n>ensemble_models</span><span class=p>,</span> <span class=n>data</span><span class=p>):</span>
  <span class=n>ensemble_preds</span> <span class=o>=</span> <span class=p>[]</span>
  <span class=k>for</span> <span class=n>model</span> <span class=ow>in</span> <span class=n>ensemble_models</span><span class=p>:</span>
    <span class=n>preds</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>data</span><span class=p>)</span> <span class=c1># make predictions with current ensemble model</span>
    <span class=n>ensemble_preds</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>preds</span><span class=p>)</span>
  <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>ensemble_preds</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Create a list of ensemble predictions</span>
<span class=n>ensemble_preds</span> <span class=o>=</span> <span class=n>make_ensemble_preds</span><span class=p>(</span><span class=n>ensemble_models</span><span class=o>=</span><span class=n>ensemble_models</span><span class=p>,</span>
                                     <span class=n>data</span><span class=o>=</span><span class=n>test_dataset</span><span class=p>)</span>
<span class=n>ensemble_preds</span>
</code></pre></div> <div class=highlight><pre><span></span><code>WARNING:tensorflow:5 out of the last 22 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdcef255d40&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.


WARNING:tensorflow:5 out of the last 22 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdcef255d40&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.


WARNING:tensorflow:6 out of the last 23 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdc77fed9e0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.


WARNING:tensorflow:6 out of the last 23 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdc77fed9e0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.





&lt;tf.Tensor: shape=(15, 556), dtype=float32, numpy=
array([[ 8805.756,  8773.019,  9028.609, ..., 50112.656, 49132.555,
        46455.695],
       [ 8764.092,  8740.744,  9051.838, ..., 49355.098, 48502.336,
        45333.934],
       [ 8732.57 ,  8719.407,  9093.386, ..., 49921.9  , 47992.15 ,
        45316.45 ],
       ...,
       [ 8938.421,  8773.84 ,  9045.577, ..., 49488.133, 49741.4  ,
        46536.25 ],
       [ 8724.761,  8805.311,  9094.972, ..., 49553.086, 48492.86 ,
        45084.266],
       [ 8823.311,  8768.297,  9047.492, ..., 49759.902, 48090.945,
        45874.336]], dtype=float32)&gt;
</code></pre></div> <p>Now we've got a set of ensemble predictions, we can evaluate them against the ground truth values.</p> <p>However, since we've trained 15 models, there's going to be 15 sets of predictions. Rather than comparing every set of predictions to the ground truth, let's take the median (you could also take the mean too but <a href=https://www.johndcook.com/blog/2009/03/06/student-t-distribution-mean-median/ >the median is usually more robust than the mean</a>).</p> <div class=highlight><pre><span></span><code><span class=c1># Evaluate ensemble model(s) predictions</span>
<span class=n>ensemble_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span>
                                  <span class=n>y_pred</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>median</span><span class=p>(</span><span class=n>ensemble_preds</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>))</span> <span class=c1># take the median across all ensemble predictions</span>
<span class=n>ensemble_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 567.4423,
 &#39;mape&#39;: 2.5843322,
 &#39;mase&#39;: 0.996839,
 &#39;mse&#39;: 1144512.9,
 &#39;rmse&#39;: 1069.8191}
</code></pre></div> <p>Nice! Looks like the ensemble model is the best performing model on the MAE metric so far.</p> <h3 id=plotting-the-prediction-intervals-uncertainty-estimates-of-our-ensemble>Plotting the prediction intervals (uncertainty estimates) of our ensemble</h3> <p>Right now all of our model's (prior to the ensemble model) are predicting single points. </p> <p>Meaning, given a set of <code>WINDOW_SIZE=7</code> values, the model will predict <code>HORIZION=1</code>.</p> <p>But what might be more helpful than a single value?</p> <p>Perhaps a range of values?</p> <p>For example, if a model is predicting the price of Bitcoin to be 50,000USD tomorrow, would it be helpful to know it's predicting the 50,000USD because it's predicting the price to be between 48,000 and 52,000USD? (note: "$" has been omitted from the previous sentence due to formatting issues)</p> <p>Knowing the range of values a model is predicting may help you make better decisions for your forecasts.</p> <p>You'd know that although the model is predicting 50,000USD (a <strong>point prediction</strong>, or single value in time), the value could actually be within the range 48,000USD to 52,000USD (of course, the value could also be <em>outside</em> of this range as well, but we'll get to that later).</p> <p>These kind of prediction ranges are called <strong>prediction intervals</strong> or <strong>uncertainty estimates</strong>. And they're often as important as the forecast itself.</p> <p>Why?</p> <p>Because <strong>point predictions</strong> are almost always going to be wrong. So having a range of values can help with decision making.</p> <blockquote> <p>📖 <strong>Resource(s):</strong> * The steps we're about to take have been inspired by the Machine Learning Mastery blog post <a href=https://machinelearningmastery.com/prediction-intervals-for-deep-learning-neural-networks/ ><em>Prediction Intervals for Deep Learning Neural Networks</em></a>. Check out the post for more options to measure uncertainty with neural networks. * For an example of uncertainty estimates being used in the wild, I'd also refer to Uber's <a href=https://eng.uber.com/neural-networks-uncertainty-estimation/ ><em>Engineering Uncertainty Estimation in Neural Networks for Time Series Prediction at Uber</em></a> blog post.</p> </blockquote> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-prediction-intervals-from-ubers-uncertainty-measures.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="example of Uber's prediction intervals for daily completed trips in San Francisco" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-prediction-intervals-from-ubers-uncertainty-measures.png></a> <em>Example of how uncertainty estimates and predictions intervals can give an understanding of where point predictions (a single number) may not include all of useful information you'd like to know. For example, your model's point prediction for Uber trips on New Years Eve might be 100 (a made up number) but really, the prediction intervals are between 55 and 153 (both made up for the example). In this case, preparing 100 rides might end up being 53 short (it could even be more, like the point prediction, the prediction intervals are also estimates). The image comes from Uber's <a href=https://eng.uber.com/neural-networks-uncertainty-estimation/ >blog post on uncertainty estimation in neural networks</a>.</em></p> <p>One way of getting the 95% condfidnece prediction intervals for a deep learning model is the bootstrap method: 1. Take the predictions from a number of randomly initialized models (we've got this thanks to our ensemble model) 2. Measure the standard deviation of the predictions 3. Multiply standard deviation by <a href=https://en.wikipedia.org/wiki/1.96>1.96</a> (assuming the distribution is Gaussian, 95% of observations fall within 1.96 standard deviations of the mean, this is why we initialized our neural networks with a normal distribution) 4. To get the prediction interval upper and lower bounds, add and subtract the value obtained in (3) to the mean/median of the predictions made in (1)</p> <div class=highlight><pre><span></span><code><span class=c1># Find upper and lower bounds of ensemble predictions</span>
<span class=k>def</span><span class=w> </span><span class=nf>get_upper_lower</span><span class=p>(</span><span class=n>preds</span><span class=p>):</span> <span class=c1># 1. Take the predictions of multiple randomly initialized deep learning neural networks</span>

  <span class=c1># 2. Measure the standard deviation of the predictions</span>
  <span class=n>std</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>math</span><span class=o>.</span><span class=n>reduce_std</span><span class=p>(</span><span class=n>preds</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

  <span class=c1># 3. Multiply the standard deviation by 1.96</span>
  <span class=n>interval</span> <span class=o>=</span> <span class=mf>1.96</span> <span class=o>*</span> <span class=n>std</span> <span class=c1># https://en.wikipedia.org/wiki/1.96 </span>

  <span class=c1># 4. Get the prediction interval upper and lower bounds</span>
  <span class=n>preds_mean</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>preds</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
  <span class=n>lower</span><span class=p>,</span> <span class=n>upper</span> <span class=o>=</span> <span class=n>preds_mean</span> <span class=o>-</span> <span class=n>interval</span><span class=p>,</span> <span class=n>preds_mean</span> <span class=o>+</span> <span class=n>interval</span>
  <span class=k>return</span> <span class=n>lower</span><span class=p>,</span> <span class=n>upper</span>

<span class=c1># Get the upper and lower bounds of the 95% </span>
<span class=n>lower</span><span class=p>,</span> <span class=n>upper</span> <span class=o>=</span> <span class=n>get_upper_lower</span><span class=p>(</span><span class=n>preds</span><span class=o>=</span><span class=n>ensemble_preds</span><span class=p>)</span>
</code></pre></div> <p>Wonderful, now we've got the upper and lower bounds for the the 95% prediction interval, let's plot them against our ensemble model's predictions.</p> <p>To do so, we can use our plotting function as well as the <a href=https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.fill_between.html><code>matplotlib.pyplot.fill_between()</code></a> method to shade in the space between the upper and lower bounds.</p> <div class=highlight><pre><span></span><code><span class=c1># Get the median values of our ensemble preds</span>
<span class=n>ensemble_median</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>median</span><span class=p>(</span><span class=n>ensemble_preds</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Plot the median of our ensemble preds along with the prediction intervals (where the predictions fall between)</span>
<span class=n>offset</span><span class=o>=</span><span class=mi>500</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>X_test</span><span class=o>.</span><span class=n>index</span><span class=p>[</span><span class=n>offset</span><span class=p>:],</span> <span class=n>y_test</span><span class=p>[</span><span class=n>offset</span><span class=p>:],</span> <span class=s2>&quot;g&quot;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Test Data&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>X_test</span><span class=o>.</span><span class=n>index</span><span class=p>[</span><span class=n>offset</span><span class=p>:],</span> <span class=n>ensemble_median</span><span class=p>[</span><span class=n>offset</span><span class=p>:],</span> <span class=s2>&quot;k-&quot;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Ensemble Median&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Date&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;BTC Price&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>fill_between</span><span class=p>(</span><span class=n>X_test</span><span class=o>.</span><span class=n>index</span><span class=p>[</span><span class=n>offset</span><span class=p>:],</span> 
                 <span class=p>(</span><span class=n>lower</span><span class=p>)[</span><span class=n>offset</span><span class=p>:],</span> 
                 <span class=p>(</span><span class=n>upper</span><span class=p>)[</span><span class=n>offset</span><span class=p>:],</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Prediction Intervals&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>loc</span><span class=o>=</span><span class=s2>&quot;upper left&quot;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>14</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_232_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_232_0.png></a></p> <p>We've just plotted: * The test data (the ground truth Bitcoin prices) * The median of the ensemble predictions * The 95% prediction intervals (assuming the data is Gaussian/normal, the model is saying that 95% of the time, predicted value should fall between this range)</p> <p>What can you tell about the ensemble model from the plot above?</p> <p>It looks like the ensemble predictions are lagging slightly behind the actual data.</p> <p>And the prediction intervals are fairly low throughout.</p> <p>The combination of lagging predictions as well as low prediction intervals indicates that our ensemble model may be <strong>overfitting</strong> the data, meaning it's basically replicating what a naïve model would do and just predicting the previous timestep value for the next value.</p> <p>This would explain why previous attempts to beat the naïve forecast have been futile.</p> <p>We can test this hypothesis of overfitting by creating a model to make predictions into the future and seeing what they look like.</p> <blockquote> <p>🔑 <strong>Note:</strong> Our prediction intervals assume that the data we're using come from a Gaussian/normal distribution (also called a bell curve), however, open systems rarely follow the Gaussian. We'll see this later on with the turkey problem 🦃. For further reading on this topic, I'd recommend reading <a href=https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable><em>The Black Swan</em> by Nassim Nicholas Taleb</a>, especially Part 2 and Chapter 15.</p> </blockquote> <h2 id=aside-two-types-of-uncertainty-coconut-and-subway>Aside: two types of uncertainty (coconut and subway)</h2> <p>Inheritly, you know you cannot predict the future.</p> <p>That doesn't mean trying to isn't valuable.</p> <p>For many things, future predictions are helpful. Such as knowing the bus you're trying to catch to the library leaves at 10:08am. The time 10:08am is a <strong>point prediction</strong>, if the bus left at a random time every day, how helpful would it be?</p> <p>Just like saying the price of Bitcoin tomorrow will be 50,000USD is a point prediction.</p> <p>However, as we've discussed knowing a <strong>prediction interval</strong> or <strong>uncertainty estimate</strong> can be as helpful or even more helpful than a point prediction itself.</p> <p>Uncertainty estimates seek out to qualitatively and quantitatively answer the questions: * What can my model know? (with perfect data, what's possible to learn?) * What doesn't my model know? (what can a model never predict?)</p> <p>There are two types of uncertainty in machine learning you should be aware of:</p> <ul> <li><strong>Aleatoric uncertainty</strong> - this type of uncertainty cannot be reduced, it is also referred to as "data" or "subway" uncertainty.</li> <li>Let's say your train is scheduled to arrive at 10:08am but very rarely does it arrive at <em>exactly</em> 10:08am. You know it's usually a minute or two either side and perhaps up to 10-minutes late if traffic is bad. Even with all the data you could imagine, this level of uncertainty is still going to be present (much of it being noise).</li> <li> <p>When we measured prediction intervals, we were measuring a form of subway uncertainty for Bitcoin price predictions (a little either side of the point prediction).</p> </li> <li> <p><strong>Epistemic uncertainty</strong> - this type of uncertainty can be reduced, it is also referred to as "model" or "coconut" uncertainty, it is very hard to calculate.</p> </li> <li>The analogy for coconut uncertainty involves whether or not you'd get hit on the head by a coconut when going to a beach. <ul> <li>If you were at a beach with coconuts trees, as you could imagine, this would be very hard to calculate. How often does a coconut fall of a tree? Where are you standing? </li> <li>But you could reduce this uncertainty to zero by going to a beach without coconuts (collect more data about your situation).</li> </ul> </li> <li>Model uncertainty can be reduced by collecting more data samples/building a model to capture different parameters about the data you're modelling.</li> </ul> <p>The lines between these are blurred (one type of uncertainty can change forms into the other) and they can be confusing at first but are important to keep in mind for any kind of time series prediction.</p> <p>If you ignore the uncertanties, are you really going to get a reliable prediction?</p> <p>Perhaps another example might help.</p> <h3 id=uncertainty-in-dating>Uncertainty in dating</h3> <p>Let's say you're going on a First Date Feedback Radio Show to help improve your dating skills.</p> <p>Where you go on a blind first date with a girl (feel free to replace girl with your own preference) and the radio hosts record the date and then playback snippets of where you could've improved.</p> <p>And now let's add a twist. </p> <p>Last week your friend went on the same show. They told you about the girl they met and how the conversation went.</p> <p>Because you're now a machine learning engineer, you decide to build a machine learning model to help you with first date conversations.</p> <p>What levels of uncertainty do we have here? </p> <p>From an <strong>aleatory uncertainty</strong> (data) point of view, no matter how many conversations of first dates you collect, the conversation you end up having will likely be different to the rest (the best conversations have no subject and appear random).</p> <p>From an <strong>epistemic uncertainty</strong> (model) point of view, if the date is truly blind and both parties don't know who they're seeing until they meet in person, the epistemic uncertainty would be high. Because now you have no idea who the person you're going to meet is nor what you might talk about.</p> <p>However, the level of epistemic uncertainty would be reduced if your friend told about the girl they went on a date with last week on the show and it turns out you're going on a date with the same girl.</p> <p>But even though you know a little bit about the girl, your <strong>aleatory uncertainty</strong> (or subway uncertainty) is still high because you're not sure where the conversation will go.</p> <p>If you're wondering where above scenario came from, it happened to me this morning. Good timing right?</p> <h3 id=learning-more-on-uncertainty>Learning more on uncertainty</h3> <p>The field of quantifying uncertainty estimation in machine learning is a growing area of research.</p> <p>If you'd like to learn more I'd recommend the following.</p> <blockquote> <p>📖 <strong>Resources:</strong> Places to learn more about uncertainty in machine learning/forecasting: * 🎥 <a href=https://youtu.be/toTcf7tZK8c>MIT 6.S191: Evidential Deep Learning and Uncertainty</a> * <a href=https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty>Uncertainty quantification on Wikipedia</a> * <a href=https://towardsdatascience.com/why-you-should-care-about-the-nate-silver-vs-nassim-taleb-twitter-war-a581dce1f5fc><em>Why you should care about the Nate Silver vs. Nassim Taleb Twitter war</em></a> by Isaac Faber - a great insight into the role of uncertainty in the example of election prediction. * <a href=https://towardsdatascience.com/3-facts-about-time-series-forecasting-that-surprise-experienced-machine-learning-practitioners-69c18ee89387><em>3 facts about time series forecasting that surprise experienced machine learning practitioners</em></a> by Skander Hannachi - fantastic outline of some of the main mistakes people make when building forecasting models, especially forgetting about uncertainty estimates. * <a href=https://eng.uber.com/neural-networks-uncertainty-estimation/ ><em>Engineering Uncertainty Estimation in Neural Networks for Time Series Prediction at Uber</em></a> - a discussion on techniques Uber used to engineer uncertainty estimates into their time sereis neural networks.</p> </blockquote> <h2 id=model-9-train-a-model-on-the-full-historical-data-to-make-predictions-into-future>Model 9: Train a model on the full historical data to make predictions into future</h2> <p>What would a forecasting model be worth if we didn't use it to predict into the future?</p> <p>It's time we created a model which is able to make future predictions on the price of Bitcoin.</p> <p>To make predictions into the future, we'll train a model on the full dataset and then get to make predictions to some future horizon.</p> <p>Why use the full dataset?</p> <p>Previously, we split our data into training and test sets to evaluate how our model did on pseudo-future data (the test set).</p> <p>But since the goal of a forecasting model is to predict values into the actual-future, we won't be using a test set.</p> <blockquote> <p>🔑 <strong>Note:</strong> Forecasting models need to be retrained every time a forecast is made. Why? Because if Bitcoin prices are updated daily and you predict the price for tomorrow. Your model is only really valid for one day. When a new price comes out (e.g. the next day), you'll have to retrain your model to incorporate that new price to predict the next forecast.</p> </blockquote> <p>Let's get some data ready.</p> <div class=highlight><pre><span></span><code><span class=n>bitcoin_prices_windowed</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>Price</th> <th>block_reward</th> <th>Price+1</th> <th>Price+2</th> <th>Price+3</th> <th>Price+4</th> <th>Price+5</th> <th>Price+6</th> <th>Price+7</th> </tr> <tr> <th>Date</th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <th>2013-10-01</th> <td>123.65499</td> <td>25</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> </tr> <tr> <th>2013-10-02</th> <td>125.45500</td> <td>25</td> <td>123.65499</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> </tr> <tr> <th>2013-10-03</th> <td>108.58483</td> <td>25</td> <td>125.45500</td> <td>123.65499</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> </tr> <tr> <th>2013-10-04</th> <td>118.67466</td> <td>25</td> <td>108.58483</td> <td>125.45500</td> <td>123.65499</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> </tr> <tr> <th>2013-10-05</th> <td>121.33866</td> <td>25</td> <td>118.67466</td> <td>108.58483</td> <td>125.45500</td> <td>123.65499</td> <td>NaN</td> <td>NaN</td> <td>NaN</td> </tr> </tbody> </table> </div> <div class=highlight><pre><span></span><code><span class=c1># Train model on entire data to make prediction for the next day </span>
<span class=n>X_all</span> <span class=o>=</span> <span class=n>bitcoin_prices_windowed</span><span class=o>.</span><span class=n>drop</span><span class=p>([</span><span class=s2>&quot;Price&quot;</span><span class=p>,</span> <span class=s2>&quot;block_reward&quot;</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>dropna</span><span class=p>()</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>()</span> <span class=c1># only want prices, our future model can be a univariate model</span>
<span class=n>y_all</span> <span class=o>=</span> <span class=n>bitcoin_prices_windowed</span><span class=o>.</span><span class=n>dropna</span><span class=p>()[</span><span class=s2>&quot;Price&quot;</span><span class=p>]</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>()</span>
</code></pre></div> <p>Windows and labels ready! Let's turn them into performance optimized TensorFlow Datasets by: 1. Turning <code>X_all</code> and <code>y_all</code> into tensor Datasets using <a href=https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices><code>tf.data.Dataset.from_tensor_slices()</code></a> 2. Combining the features and labels into a Dataset tuple using <a href=https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip><code>tf.data.Dataset.zip()</code></a> 3. Batch and prefetch the data using <a href=https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch><code>tf.data.Dataset.batch()</code></a> and <a href=https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch><code>tf.data.Dataset.prefetch()</code></a> respectively</p> <div class=highlight><pre><span></span><code><span class=c1># 1. Turn X and y into tensor Datasets</span>
<span class=n>features_dataset_all</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>from_tensor_slices</span><span class=p>(</span><span class=n>X_all</span><span class=p>)</span>
<span class=n>labels_dataset_all</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>from_tensor_slices</span><span class=p>(</span><span class=n>y_all</span><span class=p>)</span>

<span class=c1># 2. Combine features &amp; labels</span>
<span class=n>dataset_all</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>zip</span><span class=p>((</span><span class=n>features_dataset_all</span><span class=p>,</span> <span class=n>labels_dataset_all</span><span class=p>))</span>

<span class=c1># 3. Batch and prefetch for optimal performance</span>
<span class=n>BATCH_SIZE</span> <span class=o>=</span> <span class=mi>1024</span> <span class=c1># taken from Appendix D in N-BEATS paper</span>
<span class=n>dataset_all</span> <span class=o>=</span> <span class=n>dataset_all</span><span class=o>.</span><span class=n>batch</span><span class=p>(</span><span class=n>BATCH_SIZE</span><span class=p>)</span><span class=o>.</span><span class=n>prefetch</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>AUTOTUNE</span><span class=p>)</span>

<span class=n>dataset_all</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;PrefetchDataset shapes: ((None, 7), (None,)), types: (tf.float64, tf.float64)&gt;
</code></pre></div> <p>And now let's create a model similar to <code>model_1</code> except with an extra layer, we'll also fit it to the entire dataset for 100 epochs (feel free to play around with the number of epochs or callbacks here, you've got the skills to now).</p> <div class=highlight><pre><span></span><code><span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create model (nice and simple, just to test)</span>
<span class=n>model_9</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>HORIZON</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Compile</span>
<span class=n>model_9</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>())</span>

<span class=c1># Fit model on all of the data to make future forecasts</span>
<span class=n>model_9</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>dataset_all</span><span class=p>,</span>
            <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
            <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=c1># don&#39;t print out anything, we&#39;ve seen this all before</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;keras.callbacks.History at 0x7fdc77ae0dd0&gt;
</code></pre></div> <h3 id=make-predictions-on-the-future>Make predictions on the future</h3> <p>Let's predict the future and get rich!</p> <p>Well... maybe not.</p> <p>As you've seen so far, our machine learning models have performed quite poorly at predicting the price of Bitcoin (time series forecasting in open systems is typically a game of luck), often worse than the naive forecast.</p> <p>That doesn't mean we can't use our models to <em>try</em> and predict into the future right?</p> <p>To do so, let's start by defining a variable <code>INTO_FUTURE</code> which decides how many timesteps we'd like to predict into the future.</p> <div class=highlight><pre><span></span><code><span class=c1># How many timesteps to predict into the future?</span>
<span class=n>INTO_FUTURE</span> <span class=o>=</span> <span class=mi>14</span> <span class=c1># since our Bitcoin data is daily, this is for 14 days</span>
</code></pre></div> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-prediction-loop-for-forecasts.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="flowchart representation prediction loop to make forecasts and then append forecasts to data and make more forecasts continuously until forecast horizon is exhausted" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-prediction-loop-for-forecasts.png></a> <em>Example flow chart representing the loop we're about to create for making forecasts. Not pictured: retraining a forecasting model every time a forecast is made &amp; new data is acquired. For example, if you're predicting the price of Bitcoin daily, you'd want to retrain your model every day, since each day you're going to have a new data point to work with.</em></p> <p>Alright, let's create a function which returns <code>INTO_FUTURE</code> forecasted values using a trained model.</p> <p>To do so, we'll build the following steps: 1. Function which takes as input: * a list of values (the Bitcoin historical data) * a trained model (such as <code>model_9</code>) * a window into the future to predict (our <code>INTO_FUTURE</code> variable) * the window size a model was trained on (<code>WINDOW_SIZE</code>) - the model can only predict on the same kind of data it was trained on 2. Creates an empty list for future forecasts (this will be returned at the end of the function) and extracts the last <code>WINDOW_SIZE</code> values from the input values (predictions will start from the last <code>WINDOW_SIZE</code> values of the training data) 3. Loop <code>INTO_FUTURE</code> times making a prediction on <code>WINDOW_SIZE</code> datasets which update to remove the first the value and append the latest prediction * Eventually future predictions will be made using the model's own previous predictions as input</p> <div class=highlight><pre><span></span><code><span class=c1># 1. Create function to make predictions into the future</span>
<span class=k>def</span><span class=w> </span><span class=nf>make_future_forecast</span><span class=p>(</span><span class=n>values</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>into_future</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=n>WINDOW_SIZE</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>:</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Makes future forecasts into_future steps after values ends.</span>

<span class=sd>  Returns future forecasts as list of floats.</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=c1># 2. Make an empty list for future forecasts/prepare data to forecast on</span>
  <span class=n>future_forecast</span> <span class=o>=</span> <span class=p>[]</span>
  <span class=n>last_window</span> <span class=o>=</span> <span class=n>values</span><span class=p>[</span><span class=o>-</span><span class=n>WINDOW_SIZE</span><span class=p>:]</span> <span class=c1># only want preds from the last window (this will get updated)</span>

  <span class=c1># 3. Make INTO_FUTURE number of predictions, altering the data which gets predicted on each time </span>
  <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>into_future</span><span class=p>):</span>

    <span class=c1># Predict on last window then append it again, again, again (model starts to make forecasts on its own forecasts)</span>
    <span class=n>future_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>last_window</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>))</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Predicting on: </span><span class=se>\n</span><span class=s2> </span><span class=si>{</span><span class=n>last_window</span><span class=si>}</span><span class=s2> -&gt; Prediction: </span><span class=si>{</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>future_pred</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=se>\n</span><span class=s2>&quot;</span><span class=p>)</span>

    <span class=c1># Append predictions to future_forecast</span>
    <span class=n>future_forecast</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>future_pred</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>())</span>
    <span class=c1># print(future_forecast)</span>

    <span class=c1># Update last window with new pred and get WINDOW_SIZE most recent preds (model was trained on WINDOW_SIZE windows)</span>
    <span class=n>last_window</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>last_window</span><span class=p>,</span> <span class=n>future_pred</span><span class=p>)[</span><span class=o>-</span><span class=n>WINDOW_SIZE</span><span class=p>:]</span>

  <span class=k>return</span> <span class=n>future_forecast</span>
</code></pre></div> <p>Nice! Time to bring BitPredict 💰📈 to life and make future forecasts of the price of Bitcoin.</p> <blockquote> <p>🛠 <strong>Exercise:</strong> In terms of a forecasting model, what might another approach to our <code>make_future_forecasts()</code> function? Recall, that for making forecasts, you need to retrain a model each time you want to generate a new prediction. </p> <p>So perhaps you could try to: make a prediction (one timestep into the future), retrain a model with this new prediction appended to the data, make a prediction, append the prediction, retrain a model... etc. </p> <p>As it is, the <code>make_future_forecasts()</code> function skips the retraining of a model part.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Make forecasts into future of the price of Bitcoin</span>
<span class=c1># Note: if you&#39;re reading this at a later date, you may already be in the future, so the forecasts </span>
<span class=c1># we&#39;re making may not actually be forecasts, if that&#39;s the case, readjust the training data.</span>
<span class=n>future_forecast</span> <span class=o>=</span> <span class=n>make_future_forecast</span><span class=p>(</span><span class=n>values</span><span class=o>=</span><span class=n>y_all</span><span class=p>,</span>
                                       <span class=n>model</span><span class=o>=</span><span class=n>model_9</span><span class=p>,</span>
                                       <span class=n>into_future</span><span class=o>=</span><span class=n>INTO_FUTURE</span><span class=p>,</span>
                                       <span class=n>window_size</span><span class=o>=</span><span class=n>WINDOW_SIZE</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Predicting on: 
 [56573.5554719  52147.82118698 49764.1320816  50032.69313676
 47885.62525472 45604.61575361 43144.47129086] -&gt; Prediction: 55764.46484375

Predicting on: 
 [52147.82118698 49764.1320816  50032.69313676 47885.62525472
 45604.61575361 43144.47129086 55764.46484375] -&gt; Prediction: 50985.9453125

Predicting on: 
 [49764.1320816  50032.69313676 47885.62525472 45604.61575361
 43144.47129086 55764.46484375 50985.9453125 ] -&gt; Prediction: 48522.96484375

Predicting on: 
 [50032.69313676 47885.62525472 45604.61575361 43144.47129086
 55764.46484375 50985.9453125  48522.96484375] -&gt; Prediction: 48137.203125

Predicting on: 
 [47885.62525472 45604.61575361 43144.47129086 55764.46484375
 50985.9453125  48522.96484375 48137.203125  ] -&gt; Prediction: 47880.63671875

Predicting on: 
 [45604.61575361 43144.47129086 55764.46484375 50985.9453125
 48522.96484375 48137.203125   47880.63671875] -&gt; Prediction: 46879.71875

Predicting on: 
 [43144.47129086 55764.46484375 50985.9453125  48522.96484375
 48137.203125   47880.63671875 46879.71875   ] -&gt; Prediction: 48227.6015625

Predicting on: 
 [55764.46484375 50985.9453125  48522.96484375 48137.203125
 47880.63671875 46879.71875    48227.6015625 ] -&gt; Prediction: 53963.69140625

Predicting on: 
 [50985.9453125  48522.96484375 48137.203125   47880.63671875
 46879.71875    48227.6015625  53963.69140625] -&gt; Prediction: 49685.55859375

Predicting on: 
 [48522.96484375 48137.203125   47880.63671875 46879.71875
 48227.6015625  53963.69140625 49685.55859375] -&gt; Prediction: 47596.17578125

Predicting on: 
 [48137.203125   47880.63671875 46879.71875    48227.6015625
 53963.69140625 49685.55859375 47596.17578125] -&gt; Prediction: 48114.4296875

Predicting on: 
 [47880.63671875 46879.71875    48227.6015625  53963.69140625
 49685.55859375 47596.17578125 48114.4296875 ] -&gt; Prediction: 48808.0078125

Predicting on: 
 [46879.71875    48227.6015625  53963.69140625 49685.55859375
 47596.17578125 48114.4296875  48808.0078125 ] -&gt; Prediction: 48623.85546875

Predicting on: 
 [48227.6015625  53963.69140625 49685.55859375 47596.17578125
 48114.4296875  48808.0078125  48623.85546875] -&gt; Prediction: 50178.72265625
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>future_forecast</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[55764.465,
 50985.945,
 48522.965,
 48137.203,
 47880.637,
 46879.72,
 48227.6,
 53963.69,
 49685.56,
 47596.176]
</code></pre></div> <h3 id=plot-future-forecasts>Plot future forecasts</h3> <p>This is so exciting! Forecasts made! </p> <p>But right now, they're just numbers on a page.</p> <p>Let's bring them to life by adhering to the data explorer's motto: visualize, visualize, visualize!</p> <p>To plot our model's future forecasts against the historical data of Bitcoin, we're going to need a series of future dates (future dates from the final date of where our dataset ends).</p> <p>How about we create a function to return a date range from some specified start date to a specified number of days into the future (<code>INTO_FUTURE</code>).</p> <p>To do so, we'll use a combination of NumPy's <a href=https://numpy.org/doc/stable/reference/arrays.datetime.html><code>datetime64</code> datatype</a> (our Bitcoin dates are already in this datatype) as well as NumPy's <code>timedelta64</code> method which helps to create date ranges.</p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>get_future_dates</span><span class=p>(</span><span class=n>start_date</span><span class=p>,</span> <span class=n>into_future</span><span class=p>,</span> <span class=n>offset</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Returns array of datetime values from ranging from start_date to start_date+horizon.</span>

<span class=sd>  start_date: date to start range (np.datetime64)</span>
<span class=sd>  into_future: number of days to add onto start date for range (int)</span>
<span class=sd>  offset: number of days to offset start_date by (default 1)</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=n>start_date</span> <span class=o>=</span> <span class=n>start_date</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>timedelta64</span><span class=p>(</span><span class=n>offset</span><span class=p>,</span> <span class=s2>&quot;D&quot;</span><span class=p>)</span> <span class=c1># specify start date, &quot;D&quot; stands for day</span>
  <span class=n>end_date</span> <span class=o>=</span> <span class=n>start_date</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>timedelta64</span><span class=p>(</span><span class=n>into_future</span><span class=p>,</span> <span class=s2>&quot;D&quot;</span><span class=p>)</span> <span class=c1># specify end date</span>
  <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>start_date</span><span class=p>,</span> <span class=n>end_date</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=s2>&quot;datetime64[D]&quot;</span><span class=p>)</span> <span class=c1># return a date range between start date and end date</span>
</code></pre></div> <p>The start date of our forecasted dates will be the last date of our dataset. </p> <div class=highlight><pre><span></span><code><span class=c1># Last timestep of timesteps (currently in np.datetime64 format)</span>
<span class=n>last_timestep</span> <span class=o>=</span> <span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>index</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
<span class=n>last_timestep</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Timestamp(&#39;2021-05-18 00:00:00&#39;)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Get next two weeks of timesteps</span>
<span class=n>next_time_steps</span> <span class=o>=</span> <span class=n>get_future_dates</span><span class=p>(</span><span class=n>start_date</span><span class=o>=</span><span class=n>last_timestep</span><span class=p>,</span> 
                                   <span class=n>into_future</span><span class=o>=</span><span class=n>INTO_FUTURE</span><span class=p>)</span>
<span class=n>next_time_steps</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([&#39;2021-05-19&#39;, &#39;2021-05-20&#39;, &#39;2021-05-21&#39;, &#39;2021-05-22&#39;,
       &#39;2021-05-23&#39;, &#39;2021-05-24&#39;, &#39;2021-05-25&#39;, &#39;2021-05-26&#39;,
       &#39;2021-05-27&#39;, &#39;2021-05-28&#39;, &#39;2021-05-29&#39;, &#39;2021-05-30&#39;,
       &#39;2021-05-31&#39;, &#39;2021-06-01&#39;], dtype=&#39;datetime64[D]&#39;)
</code></pre></div> <p>Look at that! We've now got a list of dates we can use to visualize our future Bitcoin predictions.</p> <p>But to make sure the lines of the plot connect (try not running the cell below and then plotting the data to see what I mean), let's insert the last timestep and Bitcoin price of our training data to the <code>next_time_steps</code> and <code>future_forecast</code> arrays.</p> <div class=highlight><pre><span></span><code><span class=c1># Insert last timestep/final price so the graph doesn&#39;t look messed</span>
<span class=n>next_time_steps</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=n>next_time_steps</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>last_timestep</span><span class=p>)</span>
<span class=n>future_forecast</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=n>future_forecast</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>btc_price</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
<span class=n>next_time_steps</span><span class=p>,</span> <span class=n>future_forecast</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([&#39;2021-05-18&#39;, &#39;2021-05-19&#39;, &#39;2021-05-20&#39;, &#39;2021-05-21&#39;,
        &#39;2021-05-22&#39;, &#39;2021-05-23&#39;, &#39;2021-05-24&#39;, &#39;2021-05-25&#39;,
        &#39;2021-05-26&#39;, &#39;2021-05-27&#39;, &#39;2021-05-28&#39;, &#39;2021-05-29&#39;,
        &#39;2021-05-30&#39;, &#39;2021-05-31&#39;, &#39;2021-06-01&#39;], dtype=&#39;datetime64[D]&#39;),
 array([43144.473, 55764.465, 50985.945, 48522.965, 48137.203, 47880.637,
        46879.72 , 48227.6  , 53963.69 , 49685.56 , 47596.176, 48114.43 ,
        48808.008, 48623.855, 50178.723], dtype=float32))
</code></pre></div> <p>Time to plot!</p> <div class=highlight><pre><span></span><code><span class=c1># Plot future price predictions of Bitcoin</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>index</span><span class=p>,</span> <span class=n>btc_price</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=mi>2500</span><span class=p>,</span> <span class=nb>format</span><span class=o>=</span><span class=s2>&quot;-&quot;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Actual BTC Price&quot;</span><span class=p>)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>next_time_steps</span><span class=p>,</span> <span class=n>future_forecast</span><span class=p>,</span> <span class=nb>format</span><span class=o>=</span><span class=s2>&quot;-&quot;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Predicted BTC Price&quot;</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_257_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_257_0.png></a></p> <p>Hmmm... how did our model go? </p> <p>It looks like our predictions are starting to form a bit of a cyclic pattern (up and down in the same way).</p> <p>Perhaps that's due to our model overfitting the training data and not generalizing well for future data. Also, as you could imagine, the further you predict into the future, the higher your chance for error (try seeing what happens when you predict 100 days into the future).</p> <p>But of course, we can't measure these predictions as they are because after all, they're predictions into the actual-future (by the time you read this, the future might have already happened, if so, how did the model go?).</p> <blockquote> <p>🔑 <strong>Note:</strong> A reminder, the predictions we've made here are not financial advice. And by now, you should be well aware of just how poor machine learning models can be at forecasting values in an open system - anyone promising you a model which can "beat the market" is likely trying to scam you, oblivious to their errors or very lucky.</p> </blockquote> <h2 id=model-10-why-forecasting-is-bs-the-turkey-problem>Model 10: Why forecasting is BS (the turkey problem 🦃)</h2> <p>When creating any kind of forecast, you must keep the <strong>turkey problem</strong> in mind.</p> <p>The <strong>turkey problem</strong> is an analogy for when your observational data (your historical data) fails to capture a future event which is catostrophic and could lead you to ruin.</p> <p>The story goes, a turkey lives a good life for 1000 days, being fed every day and taken care of by its owners until the evening before Thanksgiving.</p> <p>Based on the turkey's observational data, it has no reason to believe things shouldn't keep going the way they are.</p> <p>In other words, how could a turkey possibly predict that on day 1001, after 1000 consectutive good days, it was about to have a far from ideal day.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-the-turkey-problem.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="the turkey problem illustrated, a turkey lives 1000 good days until the eve of thanksgiving" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-the-turkey-problem.png></a> <em>Example of the turkey problem. A turkey might live 1000 good days and none of them would be a sign of what's to happen on day 1001. Similar with forecasting, your historical data may not have any indication of a change which is about to come. The graph image is from page 41 of <a href=https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable>The Black Swan</a> by Nassim Taleb (I added in the turkey graphics).</em></p> <p>How does this relate to predicting the price of Bitcoin (or the price of any stock or figure in an open market)?</p> <p>You could have the historical data of Bitcoin for its entire existence and build a model which predicts it perfectly.</p> <p>But then one day for some unknown and unpredictable reason, the price of Bitcoin plummets 100x in a single day.</p> <p>Of course, this kind of scenario is unlikely.</p> <p>But that doesn't take away from its significance.</p> <p>Think about it in your own life, how many times have the most significant events happened seemingly out of the blue?</p> <p>As in, you could go to a cafe and run into the love of your life, despite visiting the same cafe for 10-years straight and never running into this person before. </p> <p>The same thing goes for predicting the price of Bitcoin, you could make money for 10-years straight and then lose it all in a single day.</p> <p>It doesn't matter how many times you get paid, it matters the amount you get paid.</p> <blockquote> <p>📖 <strong>Resource:</strong> If you'd like to learn more about the turkey problem, I'd recommend the following: * <a href=https://youtu.be/hRwimmE2wEk>Explaining both the XIV trade and why forecasting is BS</a> by Nassim Taleb * <a href=https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable><em>The Black Swan</em></a> by Nassim Taleb (epsecially Chapter 4 which outlines and discusses the turkey problem)</p> </blockquote> <p>Let's get specific and see how the turkey problem effects us modelling the historical and future price of Bitcoin.</p> <p>To do so, we're going to manufacture a highly unlikely data point into the historical price of Bitcoin, the price falling 100x in one day.</p> <blockquote> <p>🔑 <strong>Note:</strong> A very unlikely and unpredictable event such as the price of Bitcoin falling 100x in a single day (note: the adjective "unlikely" is based on the historical price changes of Bitcoin) is also referred to a <a href=https://en.wikipedia.org/wiki/Black_swan_theory><strong>Black Swan</strong> event</a>. A Black Swan event is an unknown unknown, you have no way of predicting whether or not it will happen but these kind of events often have a large impact. </p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Let&#39;s introduce a Turkey problem to our BTC data (price BTC falls 100x in one day)</span>
<span class=n>btc_price_turkey</span> <span class=o>=</span> <span class=n>btc_price</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
<span class=n>btc_price_turkey</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>btc_price_turkey</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>/</span> <span class=mi>100</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Manufacture an extra price on the end (to showcase the Turkey problem)</span>
<span class=n>btc_price_turkey</span><span class=p>[</span><span class=o>-</span><span class=mi>10</span><span class=p>:]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[58788.2096789273,
 58102.1914262342,
 55715.5466512869,
 56573.5554719043,
 52147.8211869823,
 49764.1320815975,
 50032.6931367648,
 47885.6252547166,
 45604.6157536131,
 431.44471290860304]
</code></pre></div> <p>Notice the last value is 100x lower than what it actually was (remember, this is not a real data point, its only to illustrate the effects of the turkey problem).</p> <p>Now we've got Bitcoin prices including a turkey problem data point, let's get the timesteps. </p> <div class=highlight><pre><span></span><code><span class=c1># Get the timesteps for the turkey problem </span>
<span class=n>btc_timesteps_turkey</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>bitcoin_prices</span><span class=o>.</span><span class=n>index</span><span class=p>)</span>
<span class=n>btc_timesteps_turkey</span><span class=p>[</span><span class=o>-</span><span class=mi>10</span><span class=p>:]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([&#39;2021-05-09T00:00:00.000000000&#39;, &#39;2021-05-10T00:00:00.000000000&#39;,
       &#39;2021-05-11T00:00:00.000000000&#39;, &#39;2021-05-12T00:00:00.000000000&#39;,
       &#39;2021-05-13T00:00:00.000000000&#39;, &#39;2021-05-14T00:00:00.000000000&#39;,
       &#39;2021-05-15T00:00:00.000000000&#39;, &#39;2021-05-16T00:00:00.000000000&#39;,
       &#39;2021-05-17T00:00:00.000000000&#39;, &#39;2021-05-18T00:00:00.000000000&#39;],
      dtype=&#39;datetime64[ns]&#39;)
</code></pre></div> <p>Beautiful! Let's see our artificially created turkey problem Bitcoin data.</p> <div class=highlight><pre><span></span><code><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>btc_timesteps_turkey</span><span class=p>,</span> 
                 <span class=n>values</span><span class=o>=</span><span class=n>btc_price_turkey</span><span class=p>,</span> 
                 <span class=nb>format</span><span class=o>=</span><span class=s2>&quot;-&quot;</span><span class=p>,</span> 
                 <span class=n>label</span><span class=o>=</span><span class=s2>&quot;BTC Price + Turkey Problem&quot;</span><span class=p>,</span> 
                 <span class=n>start</span><span class=o>=</span><span class=mi>2500</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_265_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_265_0.png></a></p> <p>How do you think building a model on this data will go?</p> <p>Remember, all we've changed is a single data point out of our entire dataset.</p> <p>Before we build a model, let's create some windowed datasets with our turkey data.</p> <div class=highlight><pre><span></span><code><span class=c1># Create train and test sets for turkey problem data</span>
<span class=n>full_windows</span><span class=p>,</span> <span class=n>full_labels</span> <span class=o>=</span> <span class=n>make_windows</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>btc_price_turkey</span><span class=p>),</span> <span class=n>window_size</span><span class=o>=</span><span class=n>WINDOW_SIZE</span><span class=p>,</span> <span class=n>horizon</span><span class=o>=</span><span class=n>HORIZON</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>full_windows</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>full_labels</span><span class=p>)</span>

<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>make_train_test_splits</span><span class=p>(</span><span class=n>full_windows</span><span class=p>,</span> <span class=n>full_labels</span><span class=p>)</span>
<span class=nb>len</span><span class=p>(</span><span class=n>X_train</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>X_test</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_train</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(2224, 556, 2224, 556)
</code></pre></div> <h3 id=building-a-turkey-model-model-to-predict-on-turkey-data>Building a turkey model (model to predict on turkey data)</h3> <p>With our updated data, we only changed 1 value.</p> <p>Let's see how it effects a model.</p> <p>To keep things comparable to previous models, we'll create a <code>turkey_model</code> which is a clone of <code>model_1</code> (same architecture, but different data).</p> <p>That way, when we evaluate the <code>turkey_model</code> we can compare its results to <code>model_1_results</code> and see how much a single data point can influence a model's performance.</p> <div class=highlight><pre><span></span><code><span class=c1># Clone model 1 architecture for turkey model and fit the turkey model on the turkey data</span>
<span class=n>turkey_model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>clone_model</span><span class=p>(</span><span class=n>model_1</span><span class=p>)</span>
<span class=n>turkey_model</span><span class=o>.</span><span class=n>_name</span> <span class=o>=</span> <span class=s2>&quot;Turkey_Model&quot;</span>
<span class=n>turkey_model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s2>&quot;mae&quot;</span><span class=p>,</span>
                     <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>())</span>
<span class=n>turkey_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span>
                 <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
                 <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
                 <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>),</span>
                 <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>create_model_checkpoint</span><span class=p>(</span><span class=n>turkey_model</span><span class=o>.</span><span class=n>name</span><span class=p>)])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets


INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets





&lt;keras.callbacks.History at 0x7fdc7a4dd550&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate turkey model on test data</span>
<span class=n>turkey_model</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>18/18 [==============================] - 0s 2ms/step - loss: 696.1285





696.1284790039062
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Load best model and evaluate on test data</span>
<span class=n>turkey_model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;model_experiments/Turkey_Model/&quot;</span><span class=p>)</span>
<span class=n>turkey_model</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>18/18 [==============================] - 0s 2ms/step - loss: 638.3047





638.3046875
</code></pre></div> <p>Alright, now let's make some predictions with our model and evaluate them on the test data.</p> <div class=highlight><pre><span></span><code><span class=c1># Make predictions with Turkey model</span>
<span class=n>turkey_preds</span> <span class=o>=</span> <span class=n>make_preds</span><span class=p>(</span><span class=n>turkey_model</span><span class=p>,</span> <span class=n>X_test</span><span class=p>)</span>
<span class=n>turkey_preds</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
array([8858.391, 8803.98 , 9039.575, 8785.937, 8778.044, 8735.638,
       8684.118, 8558.659, 8461.373, 8542.206], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate turkey preds</span>
<span class=n>turkey_results</span> <span class=o>=</span> <span class=n>evaluate_preds</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span>
                                <span class=n>y_pred</span><span class=o>=</span><span class=n>turkey_preds</span><span class=p>)</span>
<span class=n>turkey_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 17144.766,
 &#39;mape&#39;: 121.58286,
 &#39;mase&#39;: 26.53158,
 &#39;mse&#39;: 615487800.0,
 &#39;rmse&#39;: 23743.305}
</code></pre></div> <p>And with just one value change, our error metrics go through the roof.</p> <p>To make sure, let's remind ourselves of how <code>model_1</code> went on unmodified Bitcoin data (no turkey problem).</p> <div class=highlight><pre><span></span><code><span class=n>model_1_results</span>
</code></pre></div> <div class=highlight><pre><span></span><code>{&#39;mae&#39;: 568.95123,
 &#39;mape&#39;: 2.5448983,
 &#39;mase&#39;: 0.9994897,
 &#39;mse&#39;: 1171744.0,
 &#39;rmse&#39;: 1082.4713}
</code></pre></div> <p>By changing just one value, the <code>turkey_model</code> MAE increases almost 30x over <code>model_1</code>.</p> <p>Finally, we'll visualize the turkey predictions over the test turkey data.</p> <div class=highlight><pre><span></span><code><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=c1># plot_time_series(timesteps=btc_timesteps_turkey[:split_size], values=btc_price_turkey[:split_size], label=&quot;Train Data&quot;)</span>
<span class=n>offset</span><span class=o>=</span><span class=mi>300</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>btc_timesteps_turkey</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>X_test</span><span class=p>):],</span> 
                 <span class=n>values</span><span class=o>=</span><span class=n>btc_price_turkey</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>y_test</span><span class=p>):],</span> 
                 <span class=nb>format</span><span class=o>=</span><span class=s2>&quot;-&quot;</span><span class=p>,</span> 
                 <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Turkey Test Data&quot;</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>)</span>
<span class=n>plot_time_series</span><span class=p>(</span><span class=n>timesteps</span><span class=o>=</span><span class=n>btc_timesteps_turkey</span><span class=p>[</span><span class=o>-</span><span class=nb>len</span><span class=p>(</span><span class=n>X_test</span><span class=p>):],</span>
                 <span class=n>values</span><span class=o>=</span><span class=n>turkey_preds</span><span class=p>,</span> 
                 <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Turkey Preds&quot;</span><span class=p>,</span> 
                 <span class=n>start</span><span class=o>=</span><span class=n>offset</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_278_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_278_0.png></a></p> <p>Why does this happen?</p> <p>Why does our model fail to capture the turkey problem data point?</p> <p>Think about it like this, just like a turkey who lives 1000 joyful days, based on observation alone has no reason to believe day 1001 won't be as joyful as the last, a model which has been trained on historical data of Bitcoin which has no single event where the price decreased by 100x in a day, has no reason to predict it will in the future.</p> <p>A model cannot predict anything in the future outside of the distribution it was trained on.</p> <p>In turn, highly unlikely price movements (based on historical movements), upward or downward will likely never be part of a forecast.</p> <p>However, as we've seen, despite their unlikeliness, these events can have huuuuuuuuge impacts to the performance of our models.</p> <blockquote> <p>📖 <strong>Resource:</strong> For a great article which discusses Black Swan events and how they often get ignored due to the assumption that historical events come from a certain distribution and that future events will come from the same distribution see <a href=https://spendmatters.com/uk/black-swans-normal-distributions-supply-chain-risk/ ><em>Black Swans, Normal Distributions and Supply Chain Risk</em></a> by Spend Matters. </p> </blockquote> <h2 id=compare-models>Compare Models</h2> <p>We've trained a bunch of models.</p> <p>And if anything, we've seen just how poorly machine learning and deep learning models are at forecasting the price of Bitcoin (or any kind of open market value).</p> <p>To highlight this, let's compare the results of all of the modelling experiments we've performed so far.</p> <div class=highlight><pre><span></span><code><span class=c1># Compare different model results (w = window, h = horizon, e.g. w=7 means a window size of 7)</span>
<span class=n>model_results</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span><span class=s2>&quot;naive_model&quot;</span><span class=p>:</span> <span class=n>naive_results</span><span class=p>,</span>
                              <span class=s2>&quot;model_1_dense_w7_h1&quot;</span><span class=p>:</span> <span class=n>model_1_results</span><span class=p>,</span>
                              <span class=s2>&quot;model_2_dense_w30_h1&quot;</span><span class=p>:</span> <span class=n>model_2_results</span><span class=p>,</span>
                              <span class=s2>&quot;model_3_dense_w30_h7&quot;</span><span class=p>:</span> <span class=n>model_3_results</span><span class=p>,</span>
                              <span class=s2>&quot;model_4_CONV1D&quot;</span><span class=p>:</span> <span class=n>model_4_results</span><span class=p>,</span>
                              <span class=s2>&quot;model_5_LSTM&quot;</span><span class=p>:</span> <span class=n>model_5_results</span><span class=p>,</span>
                              <span class=s2>&quot;model_6_multivariate&quot;</span><span class=p>:</span> <span class=n>model_6_results</span><span class=p>,</span>
                              <span class=s2>&quot;model_8_NBEATs&quot;</span><span class=p>:</span> <span class=n>model_7_results</span><span class=p>,</span>
                              <span class=s2>&quot;model_9_ensemble&quot;</span><span class=p>:</span> <span class=n>ensemble_results</span><span class=p>,</span>
                              <span class=s2>&quot;model_10_turkey&quot;</span><span class=p>:</span> <span class=n>turkey_results</span><span class=p>})</span><span class=o>.</span><span class=n>T</span>
<span class=n>model_results</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>mae</th> <th>mse</th> <th>rmse</th> <th>mape</th> <th>mase</th> </tr> </thead> <tbody> <tr> <th>naive_model</th> <td>567.980225</td> <td>1.147547e+06</td> <td>1071.236206</td> <td>2.516525</td> <td>0.999570</td> </tr> <tr> <th>model_1_dense_w7_h1</th> <td>568.951233</td> <td>1.171744e+06</td> <td>1082.471313</td> <td>2.544898</td> <td>0.999490</td> </tr> <tr> <th>model_2_dense_w30_h1</th> <td>608.961487</td> <td>1.281439e+06</td> <td>1132.006470</td> <td>2.769339</td> <td>1.064471</td> </tr> <tr> <th>model_3_dense_w30_h7</th> <td>1237.506348</td> <td>5.405198e+06</td> <td>1425.747681</td> <td>5.558878</td> <td>2.202074</td> </tr> <tr> <th>model_4_CONV1D</th> <td>570.828308</td> <td>1.176671e+06</td> <td>1084.744751</td> <td>2.559336</td> <td>1.002787</td> </tr> <tr> <th>model_5_LSTM</th> <td>596.644653</td> <td>1.273487e+06</td> <td>1128.488770</td> <td>2.683845</td> <td>1.048139</td> </tr> <tr> <th>model_6_multivariate</th> <td>567.587402</td> <td>1.161688e+06</td> <td>1077.816528</td> <td>2.541387</td> <td>0.997094</td> </tr> <tr> <th>model_8_NBEATs</th> <td>585.499817</td> <td>1.179492e+06</td> <td>1086.043945</td> <td>2.744519</td> <td>1.028561</td> </tr> <tr> <th>model_9_ensemble</th> <td>567.442322</td> <td>1.144513e+06</td> <td>1069.819092</td> <td>2.584332</td> <td>0.996839</td> </tr> <tr> <th>model_10_turkey</th> <td>17144.765625</td> <td>6.154878e+08</td> <td>23743.304688</td> <td>121.582863</td> <td>26.531580</td> </tr> </tbody> </table> </div> <div class=highlight><pre><span></span><code><span class=c1># Sort model results by MAE and plot them</span>
<span class=n>model_results</span><span class=p>[[</span><span class=s2>&quot;mae&quot;</span><span class=p>]]</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=n>by</span><span class=o>=</span><span class=s2>&quot;mae&quot;</span><span class=p>)</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>),</span> <span class=n>kind</span><span class=o>=</span><span class=s2>&quot;bar&quot;</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_282_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../10_time_series_forecasting_in_tensorflow_files/10_time_series_forecasting_in_tensorflow_282_0.png></a></p> <p>The majority of our deep learning models perform on par or only slightly better than the naive model. And for the turkey model, changing a single data point destroys its performance.</p> <blockquote> <p>🔑 <strong>Note:</strong> Just because one type of model performs better here doesn't mean it'll perform the best elsewhere (and vice versa, just because one model performs poorly here, doesn't mean it'll perform poorly elsewhere).</p> </blockquote> <p>As I said at the start, this is not financial advice.</p> <p>After what we've gone through, you'll now have some of the skills required to callout BS for any future tutorial or blog post or investment sales guide claiming to have model which is able to predict the futrue.</p> <p><a href="https://twitter.com/marksaroufim/status/1366871736604532739?s=20">Mark Saroufim's Tweet</a> sums this up nicely (stock market forecasting with a machine learning model is just as reliable as palm reading).</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-mark-saroufim-tweet-forecasting-bs.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="Mark Saroufim tweet on what forecasting with a machine learning model reminds him of: palm reading, basic heuristics, comparing calculations in the brain of different organisms" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/10-mark-saroufim-tweet-forecasting-bs.png></a> <em>Beware the tutorials or trading courses which claim to use some kind of algorithm to beat the market (an open system), they're likely a scam or the creator is very lucky and hasn't yet come across a turkey problem.</em></p> <p>Don't let these results get you down though, forecasting in a closed system (such as predicting the demand of electricity) often yields quite usable results.</p> <p>If anything, this module teaches anti-knowledge. Knowing that forecasting methods usually <em>don't</em> perform well in open systems.</p> <p>Plus, sometimes not knowing the future is a benefit. A known future is already the past.</p> <h2 id=exercises>🛠 Exercises</h2> <ol> <li>Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 &amp; 1) </li> <li>Try doing this for a univariate model (e.g. <code>model_1</code>) and a multivariate model (e.g. <code>model_6</code>) and see if it effects model training or evaluation results.</li> <li>Get the most up to date data on Bitcoin, train a model &amp; see how it goes (our data goes up to May 18 2021).</li> <li>You can download the Bitcoin historical data for free from <a href=https://www.coindesk.com/price/bitcoin>coindesk.com/price/bitcoin</a> and clicking "Export Data" -&gt; "CSV".</li> <li>For most of our models we used <code>WINDOW_SIZE=7</code>, but is there a better window size?</li> <li>Setup a series of experiments to find whether or not there's a better window size.</li> <li>For example, you might train 10 different models with <code>HORIZON=1</code> but with window sizes ranging from 2-12.</li> <li>Create a windowed dataset just like the ones we used for <code>model_1</code> using <a href=https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array><code>tf.keras.preprocessing.timeseries_dataset_from_array()</code></a> and retrain <code>model_1</code> using the recreated dataset.</li> <li>For our multivariate modelling experiment, we added the Bitcoin block reward size as an extra feature to make our time series multivariate. </li> <li>Are there any other features you think you could add? </li> <li>If so, try it out, how do these affect the model?</li> <li>Make prediction intervals for future forecasts. To do so, one way would be to train an ensemble model on all of the data, make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for <code>model_8</code>.</li> <li>For future predictions, try to make a prediction, retrain a model on the predictions, make a prediction, retrain a model, make a prediction, retrain a model, make a prediction (retrain a model each time a new prediction is made). Plot the results, how do they look compared to the future predictions where a model wasn't retrained for every forecast (<code>model_9</code>)?</li> <li>Throughout this notebook, we've only tried algorithms we've handcrafted ourselves. But it's worth seeing how a purpose built forecasting algorithm goes. </li> <li>Try out one of the extra algorithms listed in the modelling experiments part such as:<ul> <li><a href=https://github.com/facebookresearch/Kats>Facebook's Kats library</a> - there are many models in here, remember the machine learning practioner's motto: experiment, experiment, experiment.</li> <li><a href=https://github.com/linkedin/greykite>LinkedIn's Greykite library</a></li> </ul> </li> </ol> <h2 id=extra-curriculum>📖 Extra-curriculum</h2> <p>We've only really scratched the surface with time series forecasting and time series modelling in general. But the good news is, you've got plenty of hands-on coding experience with it already.</p> <p>If you'd like to dig deeper in to the world of time series, I'd recommend the following:</p> <ul> <li><a href=https://otexts.com/fpp3/ >Forecasting: Principles and Practice</a> is an outstanding online textbook which discusses at length many of the most important concepts in time series forecasting. I'd especially recommend reading at least Chapter 1 in full.</li> <li>I'd definitely recommend at least checking out chapter 1 as well as the chapter on forecasting accuracy measures.</li> <li>🎥 <a href=https://youtu.be/wqQKFu41FIw>Introduction to machine learning and time series</a> by Markus Loning goes through different time series problems and how to approach them. It focuses on using the <code>sktime</code> library (Scikit-Learn for time series), though the principles are applicable elsewhere.</li> <li><a href=https://towardsdatascience.com/why-you-should-care-about-the-nate-silver-vs-nassim-taleb-twitter-war-a581dce1f5fc><em>Why you should care about the Nate Silver vs. Nassim Taleb Twitter war</em></a> by Isaac Faber is an outstanding discussion insight into the role of uncertainty in the example of election prediction.</li> <li><a href=https://www.tensorflow.org/tutorials/structured_data/time_series>TensorFlow time series tutorial</a> - A tutorial on using TensorFlow to forecast weather time series data with TensorFlow.</li> <li>📕 <a href=https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable><em>The Black Swan</em></a> by Nassim Nicholas Taleb - Nassim Taleb was a pit trader (a trader who trades on their own behalf) for 25 years, this book compiles many of the lessons he learned from first-hand experience. It changed my whole perspective on our ability to predict. </li> <li><a href=https://towardsdatascience.com/3-facts-about-time-series-forecasting-that-surprise-experienced-machine-learning-practitioners-69c18ee89387><em>3 facts about time series forecasting that surprise experienced machine learning practitioners</em></a> by Skander Hannachi, Ph.D - time series data is different to other kinds of data, if you've worked on other kinds of machine learning problems before, getting into time series might require some adjustments, Hannachi outlines 3 of the most common.</li> <li>🎥 World-class lectures by Jordan Kern, watching these will take you from 0 to 1 with time series problems: </li> <li><a href=https://youtu.be/Prpu_U5tKkE>Time Series Analysis</a> - how to analyse time series data.</li> <li><a href="https://www.youtube.com/watch?v=s3XH7fTHMb4">Time Series Modelling</a> - different techniques for modelling time series data (many of which aren't deep learning).</li> </ul> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.instant", "navigation.path", "navigation.indexes", "navigation.top", "navigation.tracking"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>