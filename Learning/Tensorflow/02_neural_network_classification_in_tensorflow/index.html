<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=../01_neural_network_regression_in_tensorflow/ rel=prev><link href=../04_transfer_learning_in_tensorflow_part_1_feature_extraction/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.7"><title>2 Neural network classification - My Documentation</title><link rel=stylesheet href=../../../assets/stylesheets/main.8608ea7d.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Merriweather";--md-code-font:"JetBrains Mono"}</style><link rel=stylesheet href=../../../assets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script> <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#02-neural-network-classification-with-tensorflow class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="My Documentation" class="md-header__button md-logo" aria-label="My Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> My Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 2 Neural network classification </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../home/ class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../AI_Model/lenet/lenet5/ class=md-tabs__link> Kiến thức lập trình </a> </li> <li class=md-tabs__item> <a href=../../../Subject/XSTK/Exercise2/homework/ class=md-tabs__link> Môn học </a> </li> <li class=md-tabs__item> <a href=../../../Problem/general/ class=md-tabs__link> CP-Problem </a> </li> <li class=md-tabs__item> <a href=../../../other/other/ class=md-tabs__link> Other </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="My Documentation" class="md-nav__button md-logo" aria-label="My Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> My Documentation </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../../../home/ class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../home/about/ class=md-nav__link> <span class=md-ellipsis> About </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Kiến thức lập trình </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Kiến thức lập trình </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1> <label class=md-nav__link for=__nav_2_1 id=__nav_2_1_label tabindex=0> <span class=md-ellipsis> Các kiến trúc, mô hình AI và các kĩ thuật </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Các kiến trúc, mô hình AI và các kĩ thuật </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_1> <label class=md-nav__link for=__nav_2_1_1 id=__nav_2_1_1_label tabindex=0> <span class=md-ellipsis> Lenet-5 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_1> <span class="md-nav__icon md-icon"></span> Lenet-5 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenet5/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenetpart1/ class=md-nav__link> <span class=md-ellipsis> Tutorial part 1 </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenetpart2/ class=md-nav__link> <span class=md-ellipsis> Tutorial part 2 </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/Lenet5_MNIST/ class=md-nav__link> <span class=md-ellipsis> Lenet-5 (Final Project 1) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_2> <label class=md-nav__link for=__nav_2_1_2 id=__nav_2_1_2_label tabindex=0> <span class=md-ellipsis> UNet </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_2> <span class="md-nav__icon md-icon"></span> UNet </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../AI_Model/unet/unet/ class=md-nav__link> <span class=md-ellipsis> Unet tutorial </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/unet/unet_model/ class=md-nav__link> <span class=md-ellipsis> UNet (Final Project 2) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../AI_Model/essential.md class=md-nav__link> <span class=md-ellipsis> Các kiến thức cần thiết </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> Python </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Python/Numpy_1/ class=md-nav__link> <span class=md-ellipsis> Numpy 1 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Numpy_2/ class=md-nav__link> <span class=md-ellipsis> Numpy 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Python_Pandas_exercise1/ class=md-nav__link> <span class=md-ellipsis> Pandas exercise 1 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas2/ class=md-nav__link> <span class=md-ellipsis> Pandas 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Python_Pandas_exercise2/ class=md-nav__link> <span class=md-ellipsis> Pandas exercise 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas_Join_Combine/ class=md-nav__link> <span class=md-ellipsis> Pandas Join Combine </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> PyTorch </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> PyTorch </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Pytorch/00_pytorch_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 0 Pytorch Fundamentals (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/00_pytorch_and_deep_learning_fundamentals.pdf class=md-nav__link> <span class=md-ellipsis> 0 Deep learning fundamentals (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/01_pytorch_workflow/ class=md-nav__link> <span class=md-ellipsis> 1 Pytorch workflow (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/01_pytorch_workflow.pdf class=md-nav__link> <span class=md-ellipsis> 1 Pytorch workflow (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/02_pytorch_classification/ class=md-nav__link> <span class=md-ellipsis> 2 classification (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/03_pytorch_computer_vision/ class=md-nav__link> <span class=md-ellipsis> 3 Computer vision (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/03_pytorch_computer_vision.pdf class=md-nav__link> <span class=md-ellipsis> 3 Compupter vision (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/04_pytorch_custom_datasets/ class=md-nav__link> <span class=md-ellipsis> 4 Custom datasets (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/05_pytorch_going_modular.pdf class=md-nav__link> <span class=md-ellipsis> 5 Going moduler (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/06_pytorch_transfer_learning/ class=md-nav__link> <span class=md-ellipsis> 6 Transfer learning (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/06_pytorch_transfer_learning.pdf class=md-nav__link> <span class=md-ellipsis> 6 Transfer learning (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/07_pytorch_experiment_tracking/ class=md-nav__link> <span class=md-ellipsis> 7 Experiment tracking (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/07_pytorch_experiment_tracking.pdf class=md-nav__link> <span class=md-ellipsis> 7 Experiment tracking (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/08_pytorch_paper_replicating/ class=md-nav__link> <span class=md-ellipsis> 8 Paper replicating (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/08_pytorch_paper_replicating.pdf class=md-nav__link> <span class=md-ellipsis> 8 Paper replicating (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/09_pytorch_model_deployment.pdf class=md-nav__link> <span class=md-ellipsis> 9 Model deployment(pdf) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4 checked> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class=md-ellipsis> Tensorflow </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=true> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Tensorflow </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../00_tensorflow_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 0 Tensorflow fundamentals </span> </a> </li> <li class=md-nav__item> <a href=../01_neural_network_regression_in_tensorflow/ class=md-nav__link> <span class=md-ellipsis> 1 Neural network regression </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 2 Neural network classification </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 2 Neural network classification </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-were-going-to-cover class=md-nav__link> <span class=md-ellipsis> What we're going to cover </span> </a> </li> <li class=md-nav__item> <a href=#how-you-can-use-this-notebook class=md-nav__link> <span class=md-ellipsis> How you can use this notebook </span> </a> </li> <li class=md-nav__item> <a href=#typical-architecture-of-a-classification-neural-network class=md-nav__link> <span class=md-ellipsis> Typical architecture of a classification neural network </span> </a> </li> <li class=md-nav__item> <a href=#creating-data-to-view-and-fit class=md-nav__link> <span class=md-ellipsis> Creating data to view and fit </span> </a> </li> <li class=md-nav__item> <a href=#input-and-output-shapes class=md-nav__link> <span class=md-ellipsis> Input and output shapes </span> </a> </li> <li class=md-nav__item> <a href=#steps-in-modelling class=md-nav__link> <span class=md-ellipsis> Steps in modelling </span> </a> </li> <li class=md-nav__item> <a href=#improving-a-model class=md-nav__link> <span class=md-ellipsis> Improving a model </span> </a> </li> <li class=md-nav__item> <a href=#the-missing-piece-non-linearity class=md-nav__link> <span class=md-ellipsis> The missing piece: Non-linearity </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-and-improving-our-classification-model class=md-nav__link> <span class=md-ellipsis> Evaluating and improving our classification model </span> </a> <nav class=md-nav aria-label="Evaluating and improving our classification model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#plot-the-loss-curves class=md-nav__link> <span class=md-ellipsis> Plot the loss curves </span> </a> </li> <li class=md-nav__item> <a href=#finding-the-best-learning-rate class=md-nav__link> <span class=md-ellipsis> Finding the best learning rate </span> </a> </li> <li class=md-nav__item> <a href=#more-classification-evaluation-methods class=md-nav__link> <span class=md-ellipsis> More classification evaluation methods </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#working-with-a-larger-example-multiclass-classification class=md-nav__link> <span class=md-ellipsis> Working with a larger example (multiclass classification) </span> </a> <nav class=md-nav aria-label="Working with a larger example (multiclass classification)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-patterns-is-our-model-learning class=md-nav__link> <span class=md-ellipsis> What patterns is our model learning? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#how-a-model-learns-in-brief class=md-nav__link> <span class=md-ellipsis> How a model learns (in brief) </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> Exercises 🛠 </span> </a> </li> <li class=md-nav__item> <a href=#extra-curriculum class=md-nav__link> <span class=md-ellipsis> Extra curriculum 📖 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../04_transfer_learning_in_tensorflow_part_1_feature_extraction/ class=md-nav__link> <span class=md-ellipsis> 4 Transfer learning </span> </a> </li> <li class=md-nav__item> <a href=../10_time_series_forecasting_in_tensorflow/ class=md-nav__link> <span class=md-ellipsis> 10 Time series forecasting </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../OOP_Practice/ class=md-nav__link> <span class=md-ellipsis> OOP Practice </span> </a> </li> <li class=md-nav__item> <a href=../../Visualization/ class=md-nav__link> <span class=md-ellipsis> Visualization </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Môn học </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Môn học </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> Xác suất thống kê </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> Xác suất thống kê </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Subject/XSTK/Exercise2/homework/ class=md-nav__link> <span class=md-ellipsis> Bài tập về nhà buổi 2 </span> </a> </li> <li class=md-nav__item> <a href=../../../Subject/XSTK/Exercise3/homework/ class=md-nav__link> <span class=md-ellipsis> Bài tập về nhà buổi 3 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> CP-Problem </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> CP-Problem </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/general/ class=md-nav__link> <span class=md-ellipsis> General </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> COCI 2006 2007 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> COCI 2006 2007 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_2> <label class=md-nav__link for=__nav_4_2_2 id=__nav_4_2_2_label tabindex=0> <span class=md-ellipsis> Contest 1 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_2> <span class="md-nav__icon md-icon"></span> Contest 1 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P1_MODULO/ class=md-nav__link> <span class=md-ellipsis> MODULO </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P2_HERMAN/ class=md-nav__link> <span class=md-ellipsis> HERMAN </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P3_OKVIRI/ class=md-nav__link> <span class=md-ellipsis> OKVIRI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P4_SLIKAR/ class=md-nav__link> <span class=md-ellipsis> SLIKAR </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P5_BOND/ class=md-nav__link> <span class=md-ellipsis> BOND </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P6_DEBUG/ class=md-nav__link> <span class=md-ellipsis> DEBUG </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_3> <label class=md-nav__link for=__nav_4_2_3 id=__nav_4_2_3_label tabindex=0> <span class=md-ellipsis> Contest 2 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_3> <span class="md-nav__icon md-icon"></span> Contest 2 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P1_R2/ class=md-nav__link> <span class=md-ellipsis> R2 </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P2_ABC/ class=md-nav__link> <span class=md-ellipsis> ABC </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P3_KOLONE/ class=md-nav__link> <span class=md-ellipsis> KOLONE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P4_SJECISTA/ class=md-nav__link> <span class=md-ellipsis> SJECISTA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P5_STOL/ class=md-nav__link> <span class=md-ellipsis> STOL </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P6_STRAZA/ class=md-nav__link> <span class=md-ellipsis> STRAZA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_4> <label class=md-nav__link for=__nav_4_2_4 id=__nav_4_2_4_label tabindex=0> <span class=md-ellipsis> Contest 3 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_4> <span class="md-nav__icon md-icon"></span> Contest 3 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P1_PATULJCI/ class=md-nav__link> <span class=md-ellipsis> PATULJCI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P2_NPUZZLE/ class=md-nav__link> <span class=md-ellipsis> NPUZZLE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P3_TROJKE/ class=md-nav__link> <span class=md-ellipsis> TROJKE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P4_TENKICI/ class=md-nav__link> <span class=md-ellipsis> TENKICI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P5_BICIKLI/ class=md-nav__link> <span class=md-ellipsis> BICIKLI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P6_LISTA/ class=md-nav__link> <span class=md-ellipsis> LISTA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_5> <label class=md-nav__link for=__nav_4_2_5 id=__nav_4_2_5_label tabindex=0> <span class=md-ellipsis> Contest 4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_5> <span class="md-nav__icon md-icon"></span> Contest 4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P1_Sibice/ class=md-nav__link> <span class=md-ellipsis> SIBICE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P2_Skener/ class=md-nav__link> <span class=md-ellipsis> SKENER </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P3_Prsteni/ class=md-nav__link> <span class=md-ellipsis> PRSTENI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P4_Zbrka/ class=md-nav__link> <span class=md-ellipsis> ZBRKA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P5_Jogurt/ class=md-nav__link> <span class=md-ellipsis> JOGURT </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P6_Ispiti/ class=md-nav__link> <span class=md-ellipsis> ISPITI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_6> <label class=md-nav__link for=__nav_4_2_6 id=__nav_4_2_6_label tabindex=0> <span class=md-ellipsis> Contest 5 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_6_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_6> <span class="md-nav__icon md-icon"></span> Contest 5 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P1_Trik/ class=md-nav__link> <span class=md-ellipsis> TRIK </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P2_Natrij/ class=md-nav__link> <span class=md-ellipsis> NATRIJ </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P3_Tenis/ class=md-nav__link> <span class=md-ellipsis> TENIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P4_Liga/ class=md-nav__link> <span class=md-ellipsis> LIGA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P5_Ivana/ class=md-nav__link> <span class=md-ellipsis> IVANA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P6_Dvaput/ class=md-nav__link> <span class=md-ellipsis> DVAPUT </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_7> <label class=md-nav__link for=__nav_4_2_7 id=__nav_4_2_7_label tabindex=0> <span class=md-ellipsis> Contest 6 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_7_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_7> <span class="md-nav__icon md-icon"></span> Contest 6 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P1_PRASE/ class=md-nav__link> <span class=md-ellipsis> PRASE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P2_MAGIJA/ class=md-nav__link> <span class=md-ellipsis> MAGIJA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P3_MARATON/ class=md-nav__link> <span class=md-ellipsis> MARATON </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P4_KAMEN/ class=md-nav__link> <span class=md-ellipsis> KAMEN </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P5_V/ class=md-nav__link> <span class=md-ellipsis> V </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P6_PROSTOR/ class=md-nav__link> <span class=md-ellipsis> PROSTOR </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_8> <label class=md-nav__link for=__nav_4_2_8 id=__nav_4_2_8_label tabindex=0> <span class=md-ellipsis> Regional </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_8_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_8> <span class="md-nav__icon md-icon"></span> Regional </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P1_BARD/ class=md-nav__link> <span class=md-ellipsis> BARD </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P2_TETRIS/ class=md-nav__link> <span class=md-ellipsis> TETRIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P4_CIRCLE/ class=md-nav__link> <span class=md-ellipsis> CIRCLE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_9> <label class=md-nav__link for=__nav_4_2_9 id=__nav_4_2_9_label tabindex=0> <span class=md-ellipsis> Croatian Olympiad </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_9_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_9> <span class="md-nav__icon md-icon"></span> Croatian Olympiad </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P1_PATRIK/ class=md-nav__link> <span class=md-ellipsis> PATRIK </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P2_POLICIJA/ class=md-nav__link> <span class=md-ellipsis> POLICIJA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P3_SABOR/ class=md-nav__link> <span class=md-ellipsis> SABOR </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class=md-ellipsis> COCI 2007 2008 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> COCI 2007 2008 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_2> <label class=md-nav__link for=__nav_4_3_2 id=__nav_4_3_2_label tabindex=0> <span class=md-ellipsis> Contest 1 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_2> <span class="md-nav__icon md-icon"></span> Contest 1 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P1_CETVRTA/ class=md-nav__link> <span class=md-ellipsis> CETVRTA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P2_PEG/ class=md-nav__link> <span class=md-ellipsis> PEG </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P3_PRINOVA/ class=md-nav__link> <span class=md-ellipsis> PRINOVA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P4_ZAPIS/ class=md-nav__link> <span class=md-ellipsis> ZAPIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P5_SREDNJI/ class=md-nav__link> <span class=md-ellipsis> SREDNJI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P6_STAZA/ class=md-nav__link> <span class=md-ellipsis> STAZA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_3> <label class=md-nav__link for=__nav_4_3_3 id=__nav_4_3_3_label tabindex=0> <span class=md-ellipsis> Contest 4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_3> <span class="md-nav__icon md-icon"></span> Contest 4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P1_CIRCLE/ class=md-nav__link> <span class=md-ellipsis> CIRCLE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P2_VECI/ class=md-nav__link> <span class=md-ellipsis> VECI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P3_LEKTIRA/ class=md-nav__link> <span class=md-ellipsis> LEKTIRA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P4_MUZICARI/ class=md-nav__link> <span class=md-ellipsis> MUZICARI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P5_POKLON/ class=md-nav__link> <span class=md-ellipsis> POKLON </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P6_KOCKE/ class=md-nav__link> <span class=md-ellipsis> KOCKE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_4> <label class=md-nav__link for=__nav_4_3_4 id=__nav_4_3_4_label tabindex=0> <span class=md-ellipsis> Contest 6 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_4> <span class="md-nav__icon md-icon"></span> Contest 6 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P1_PARKING/ class=md-nav__link> <span class=md-ellipsis> PARKING </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P2_SEMAFORI/ class=md-nav__link> <span class=md-ellipsis> SEMAFORI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P3_GRANICA/ class=md-nav__link> <span class=md-ellipsis> GRANICA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P4_GEORGE/ class=md-nav__link> <span class=md-ellipsis> GEORGE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P5_PRINCEZA/ class=md-nav__link> <span class=md-ellipsis> PRINCEZA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P6_CESTARINE/ class=md-nav__link> <span class=md-ellipsis> CESTARINE </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../other/other/ class=md-nav__link> <span class=md-ellipsis> Other </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-were-going-to-cover class=md-nav__link> <span class=md-ellipsis> What we're going to cover </span> </a> </li> <li class=md-nav__item> <a href=#how-you-can-use-this-notebook class=md-nav__link> <span class=md-ellipsis> How you can use this notebook </span> </a> </li> <li class=md-nav__item> <a href=#typical-architecture-of-a-classification-neural-network class=md-nav__link> <span class=md-ellipsis> Typical architecture of a classification neural network </span> </a> </li> <li class=md-nav__item> <a href=#creating-data-to-view-and-fit class=md-nav__link> <span class=md-ellipsis> Creating data to view and fit </span> </a> </li> <li class=md-nav__item> <a href=#input-and-output-shapes class=md-nav__link> <span class=md-ellipsis> Input and output shapes </span> </a> </li> <li class=md-nav__item> <a href=#steps-in-modelling class=md-nav__link> <span class=md-ellipsis> Steps in modelling </span> </a> </li> <li class=md-nav__item> <a href=#improving-a-model class=md-nav__link> <span class=md-ellipsis> Improving a model </span> </a> </li> <li class=md-nav__item> <a href=#the-missing-piece-non-linearity class=md-nav__link> <span class=md-ellipsis> The missing piece: Non-linearity </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-and-improving-our-classification-model class=md-nav__link> <span class=md-ellipsis> Evaluating and improving our classification model </span> </a> <nav class=md-nav aria-label="Evaluating and improving our classification model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#plot-the-loss-curves class=md-nav__link> <span class=md-ellipsis> Plot the loss curves </span> </a> </li> <li class=md-nav__item> <a href=#finding-the-best-learning-rate class=md-nav__link> <span class=md-ellipsis> Finding the best learning rate </span> </a> </li> <li class=md-nav__item> <a href=#more-classification-evaluation-methods class=md-nav__link> <span class=md-ellipsis> More classification evaluation methods </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#working-with-a-larger-example-multiclass-classification class=md-nav__link> <span class=md-ellipsis> Working with a larger example (multiclass classification) </span> </a> <nav class=md-nav aria-label="Working with a larger example (multiclass classification)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-patterns-is-our-model-learning class=md-nav__link> <span class=md-ellipsis> What patterns is our model learning? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#how-a-model-learns-in-brief class=md-nav__link> <span class=md-ellipsis> How a model learns (in brief) </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> Exercises 🛠 </span> </a> </li> <li class=md-nav__item> <a href=#extra-curriculum class=md-nav__link> <span class=md-ellipsis> Extra curriculum 📖 </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <p><a href=https://colab.research.google.com/github/mrdbourke/tensorflow-deep-learning/blob/main/02_neural_network_classification_in_tensorflow.ipynb target=_parent><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p> <h1 id=02-neural-network-classification-with-tensorflow>02. Neural Network Classification with TensorFlow</h1> <p>Okay, we've seen how to deal with a regression problem in TensorFlow, let's look at how we can approach a classification problem.</p> <p>A <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification problem</a> involves predicting whether something is one thing or another.</p> <p>For example, you might want to: * Predict whether or not someone has heart disease based on their health parameters. This is called <strong>binary classification</strong> since there are only two options. * Decide whether a photo of is of food, a person or a dog. This is called <strong>multi-class classification</strong> since there are more than two options. * Predict what categories should be assigned to a Wikipedia article. This is called <strong>multi-label classification</strong> since a single article could have more than one category assigned.</p> <p>In this notebook, we're going to work through a number of different classification problems with TensorFlow. In other words, taking a set of inputs and predicting what class those set of inputs belong to.</p> <h2 id=what-were-going-to-cover>What we're going to cover</h2> <p>Specifically, we're going to go through doing the following with TensorFlow: - Architecture of a classification model - Input shapes and output shapes - <code>X</code>: features/data (inputs) - <code>y</code>: labels (outputs) - "What class do the inputs belong to?" - Creating custom data to view and fit - Steps in modelling for binary and mutliclass classification - Creating a model - Compiling a model - Defining a loss function - Setting up an optimizer - Finding the best learning rate - Creating evaluation metrics - Fitting a model (getting it to find patterns in our data) - Improving a model - The power of non-linearity - Evaluating classification models - Visualizng the model ("visualize, visualize, visualize") - Looking at training curves - Compare predictions to ground truth (using our evaluation metrics)</p> <h2 id=how-you-can-use-this-notebook>How you can use this notebook</h2> <p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to <strong>write more code</strong>.</p> <h2 id=typical-architecture-of-a-classification-neural-network>Typical architecture of a classification neural network</h2> <p>The word <em>typical</em> is on purpose.</p> <p>Because the architecture of a classification neural network can widely vary depending on the problem you're working on.</p> <p>However, there are some fundamentals all deep neural networks contain: * An input layer. * Some hidden layers. * An output layer.</p> <p>Much of the rest is up to the data analyst creating the model.</p> <p>The following are some standard values you'll often use in your classification neural networks.</p> <table> <thead> <tr> <th><strong>Hyperparameter</strong></th> <th><strong>Binary Classification</strong></th> <th><strong>Multiclass classification</strong></th> </tr> </thead> <tbody> <tr> <td>Input layer shape</td> <td>Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction)</td> <td>Same as binary classification</td> </tr> <tr> <td>Hidden layer(s)</td> <td>Problem specific, minimum = 1, maximum = unlimited</td> <td>Same as binary classification</td> </tr> <tr> <td>Neurons per hidden layer</td> <td>Problem specific, generally 10 to 100</td> <td>Same as binary classification</td> </tr> <tr> <td>Output layer shape</td> <td>1 (one class or the other)</td> <td>1 per class (e.g. 3 for food, person or dog photo)</td> </tr> <tr> <td>Hidden activation</td> <td>Usually <a href=https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning>ReLU</a> (rectified linear unit)</td> <td>Same as binary classification</td> </tr> <tr> <td>Output activation</td> <td><a href=https://en.wikipedia.org/wiki/Sigmoid_function>Sigmoid</a></td> <td><a href=https://en.wikipedia.org/wiki/Softmax_function>Softmax</a></td> </tr> <tr> <td>Loss function</td> <td><a href=https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression>Cross entropy</a> (<a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy><code>tf.keras.losses.BinaryCrossentropy</code></a> in TensorFlow)</td> <td>Cross entropy (<a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy><code>tf.keras.losses.CategoricalCrossentropy</code></a> in TensorFlow)</td> </tr> <tr> <td>Optimizer</td> <td><a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD>SGD</a> (stochastic gradient descent), <a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam>Adam</a></td> <td>Same as binary classification</td> </tr> </tbody> </table> <p>Table 1: Typical architecture of a classification network. Source: Adapted from page 295 of <a href=https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ >Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow Book by Aurélien Géron</a></p> <p>Don't worry if not much of the above makes sense right now, we'll get plenty of experience as we go through this notebook.</p> <p>Let's start by importing TensorFlow as the common alias <code>tf</code>. For this notebook, make sure you're using version 2.x+.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>
<span class=nb>print</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>

<span class=kn>import</span><span class=w> </span><span class=nn>datetime</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Notebook last run (end-to-end): </span><span class=si>{</span><span class=n>datetime</span><span class=o>.</span><span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>2.17.0-dev20240226
Notebook last run (end-to-end): 2024-04-19 14:13:33.923266
</code></pre></div> <h2 id=creating-data-to-view-and-fit>Creating data to view and fit</h2> <p>We could start by importing a classification dataset but let's practice making some of our own classification data.</p> <blockquote> <p>🔑 <strong>Note:</strong> It's a common practice to get you and model you build working on a toy (or simple) dataset before moving to your actual problem. Treat it as a rehersal experiment before the actual experiment(s).</p> </blockquote> <p>Since classification is predicting whether something is one thing or another, let's make some data to reflect that.</p> <p>To do so, we'll use Scikit-Learn's <a href=https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#sklearn.datasets.make_circles><code>make_circles()</code></a> function.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>make_circles</span>

<span class=c1># Make 1000 examples</span>
<span class=n>n_samples</span> <span class=o>=</span> <span class=mi>1000</span>

<span class=c1># Create circles</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>make_circles</span><span class=p>(</span><span class=n>n_samples</span><span class=p>,</span>
                    <span class=n>noise</span><span class=o>=</span><span class=mf>0.03</span><span class=p>,</span>
                    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</code></pre></div> <p>Wonderful, now we've created some data, let's look at the features (<code>X</code>) and labels (<code>y</code>).</p> <div class=highlight><pre><span></span><code><span class=c1># Check out the features</span>
<span class=n>X</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([[ 0.75424625,  0.23148074],
       [-0.75615888,  0.15325888],
       [-0.81539193,  0.17328203],
       ...,
       [-0.13690036, -0.81001183],
       [ 0.67036156, -0.76750154],
       [ 0.28105665,  0.96382443]])
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># See the first 10 labels</span>
<span class=n>y</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0], dtype=int64)
</code></pre></div> <p>Okay, we've seen some of our data and labels, how about we move towards visualizing?</p> <blockquote> <p>🔑 <strong>Note:</strong> One important step of starting any kind of machine learning project is to <a href=https://karpathy.github.io/2019/04/25/recipe/ >become one with the data</a>. And one of the best ways to do this is to visualize the data you're working with as much as possible. The data explorer's motto is "visualize, visualize, visualize".</p> </blockquote> <p>We'll start with a DataFrame.</p> <div class=highlight><pre><span></span><code><span class=c1># Make dataframe of features and labels</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=n>circles</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span><span class=s2>&quot;X0&quot;</span><span class=p>:</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=s2>&quot;X1&quot;</span><span class=p>:</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=s2>&quot;label&quot;</span><span class=p>:</span><span class=n>y</span><span class=p>})</span>
<span class=n>circles</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>X0</th> <th>X1</th> <th>label</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>0.754246</td> <td>0.231481</td> <td>1</td> </tr> <tr> <th>1</th> <td>-0.756159</td> <td>0.153259</td> <td>1</td> </tr> <tr> <th>2</th> <td>-0.815392</td> <td>0.173282</td> <td>1</td> </tr> <tr> <th>3</th> <td>-0.393731</td> <td>0.692883</td> <td>1</td> </tr> <tr> <th>4</th> <td>0.442208</td> <td>-0.896723</td> <td>0</td> </tr> </tbody> </table> </div> <p>What kind of labels are we dealing with?</p> <div class=highlight><pre><span></span><code><span class=c1># Check out the different labels</span>
<span class=n>circles</span><span class=o>.</span><span class=n>label</span><span class=o>.</span><span class=n>value_counts</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>label
1    500
0    500
Name: count, dtype: int64
</code></pre></div> <p>Alright, looks like we're dealing with a <strong>binary classification</strong> problem. It's binary because there are only two labels (0 or 1).</p> <p>If there were more label options (e.g. 0, 1, 2, 3 or 4), it would be called <strong>multiclass classification</strong>.</p> <p>Let's take our visualization a step further and plot our data.</p> <div class=highlight><pre><span></span><code><span class=c1># Visualize with a plot</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>RdYlBu</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_14_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_14_0.png></a></p> <p>Nice! From the plot, can you guess what kind of model we might want to build?</p> <p>How about we try and build one to classify blue or red dots? As in, a model which is able to distinguish blue from red dots.</p> <blockquote> <p>🛠 <strong>Practice:</strong> Before pushing forward, you might want to spend 10 minutes playing around with the <a href="https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2,2&seed=0.93799&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&regularizationRate_hide=true&batchSize_hide=true">TensorFlow Playground</a>. Try adjusting the different hyperparameters you see and click play to see a neural network train. I think you'll find the data very similar to what we've just created.</p> </blockquote> <h2 id=input-and-output-shapes>Input and output shapes</h2> <p>One of the most common issues you'll run into when building neural networks is shape mismatches.</p> <p>More specifically, the shape of the input data and the shape of the output data.</p> <p>In our case, we want to input <code>X</code> and get our model to predict <code>y</code>.</p> <p>So let's check out the shapes of <code>X</code> and <code>y</code>.</p> <div class=highlight><pre><span></span><code><span class=c1># Check the shapes of our features and labels</span>
<span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>((1000, 2), (1000,))
</code></pre></div> <p>Hmm, where do these numbers come from?</p> <div class=highlight><pre><span></span><code><span class=c1># Check how many samples we have</span>
<span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(1000, 1000)
</code></pre></div> <p>So we've got as many <code>X</code> values as we do <code>y</code> values, that makes sense.</p> <p>Let's check out one example of each.</p> <div class=highlight><pre><span></span><code><span class=c1># View the first example of features and labels</span>
<span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([0.75424625, 0.23148074]), 1)
</code></pre></div> <p>Alright, so we've got two <code>X</code> features which lead to one <code>y</code> value.</p> <p>This means our neural network input shape will has to accept a tensor with at least one dimension being two and output a tensor with at least one value.</p> <blockquote> <p>🤔 <strong>Note:</strong> <code>y</code> having a shape of (1000,) can seem confusing. However, this is because all <code>y</code> values are actually scalars (single values) and therefore don't have a dimension. For now, think of your output shape as being at least the same value as one example of <code>y</code> (in our case, the output from our neural network has to be at least one value).</p> </blockquote> <h2 id=steps-in-modelling>Steps in modelling</h2> <p>Now we know what data we have as well as the input and output shapes, let's see how we'd build a neural network to model it.</p> <p>In TensorFlow, there are typically 3 fundamental steps to creating and training a model.</p> <ol> <li><strong>Creating a model</strong> - piece together the layers of a neural network yourself (using the <a href=https://www.tensorflow.org/guide/keras/functional>functional</a> or <a href=https://www.tensorflow.org/api_docs/python/tf/keras/Sequential>sequential API</a>) or import a previously built model (known as transfer learning).</li> <li><strong>Compiling a model</strong> - defining how a model's performance should be measured (loss/metrics) as well as defining how it should improve (optimizer).</li> <li><strong>Fitting a model</strong> - letting the model try to find patterns in the data (how does <code>X</code> get to <code>y</code>).</li> </ol> <p>Let's see these in action using the Sequential API to build a model for our regression data. And then we'll step through each.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># 1. Create the model using the Sequential API</span>
<span class=n>model_1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># 2. Compile the model</span>
<span class=n>model_1</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>BinaryCrossentropy</span><span class=p>(),</span> <span class=c1># binary since we are working with 2 clases (0 &amp; 1)</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(),</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>

<span class=c1># 3. Fit the model</span>
<span class=n>model_1</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/5
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 2ms/step - accuracy: 0.4693 - loss: 4.0181 
Epoch 2/5
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5154 - loss: 0.7888
Epoch 3/5
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5144 - loss: 0.7107
Epoch 4/5
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5164 - loss: 0.6968
Epoch 5/5
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5137 - loss: 0.6938





&lt;keras.src.callbacks.history.History at 0x2017298b5c0&gt;
</code></pre></div> <p>Looking at the accuracy metric, our model performs poorly (50% accuracy on a binary classification problem is the equivalent of guessing), but what if we trained it for longer?</p> <div class=highlight><pre><span></span><code><span class=c1># Train our model for longer (more chances to look at the data)</span>
<span class=n>model_1</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=c1># set verbose=0 to remove training updates</span>
<span class=n>model_1</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4852 - loss: 0.6927





[0.6933518648147583, 0.4729999899864197]
</code></pre></div> <p>Even after 200 passes of the data, it's still performing as if it's guessing.</p> <p>What if we added an extra layer and trained for a little longer?</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># 1. Create the model (same as model_1 but with an extra layer)</span>
<span class=n>model_2</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span> <span class=c1># add an extra layer</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># 2. Compile the model</span>
<span class=n>model_2</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>BinaryCrossentropy</span><span class=p>(),</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(),</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>

<span class=c1># 3. Fit the model</span>
<span class=n>model_2</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=c1># set verbose=0 to make the output print less</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;keras.src.callbacks.history.History at 0x201743d84a0&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate the model</span>
<span class=n>model_2</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4955 - loss: 8.1264





[7.96460485458374, 0.5]
</code></pre></div> <p>Still not even as good as guessing (~50% accuracy)... hmm...?</p> <p>Let's remind ourselves of a couple more ways we can use to improve our models.</p> <h2 id=improving-a-model>Improving a model</h2> <p>To improve our model, we can alter almost every part of the 3 steps we went through before.</p> <ol> <li><strong>Creating a model</strong> - here you might want to add more layers, increase the number of hidden units (also called neurons) within each layer, change the activation functions of each layer.</li> <li><strong>Compiling a model</strong> - you might want to choose a different optimization function (such as the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam>Adam</a> optimizer, which is usually pretty good for many problems) or perhaps change the learning rate of the optimization function.</li> <li><strong>Fitting a model</strong> - perhaps you could fit a model for more epochs (leave it training for longer).</li> </ol> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-improving-a-model-from-model-perspective.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="various options you can use to improve a neural network model" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-improving-a-model-from-model-perspective.png></a> <em>There are many different ways to potentially improve a neural network. Some of the most common include: increasing the number of layers (making the network deeper), increasing the number of hidden units (making the network wider) and changing the learning rate. Because these values are all human-changeable, they're referred to as <a href=https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)>hyperparameters</a> and the practice of trying to find the best hyperparameters is referred to as <a href=https://en.wikipedia.org/wiki/Hyperparameter_optimization>hyperparameter tuning</a>.</em></p> <p>How about we try adding more neurons, an extra layer and our friend the Adam optimizer?</p> <p>Surely doing this will result in predictions better than guessing...</p> <blockquote> <p><strong>Note:</strong> The following message (below this one) can be ignored if you're running TensorFlow 2.8.0+, the error seems to have been fixed.</p> <p><strong>Note:</strong> If you're using TensorFlow 2.7.0+ (but not 2.8.0+) the original code from the following cells may have caused some errors. They've since been updated to fix those errors. You can see explanations on what happened at the following resources: * <a href="https://colab.research.google.com/drive/1_dlrB_DJOBS9c9foYJs49I0YwN7LTakl?usp=sharing">Example Colab Notebook</a> * <a href=https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278>TensorFlow for Deep Learning GitHub Discussion on TensorFlow 2.7.0 breaking changes</a></p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># 1. Create the model (this time 3 layers)</span>
<span class=n>model_3</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=c1># Before TensorFlow 2.7.0</span>
  <span class=c1># tf.keras.layers.Dense(100), # add 100 dense neurons</span>

  <span class=c1># With TensorFlow 2.7.0</span>
  <span class=c1># tf.keras.layers.Dense(100, input_shape=(None, 1)), # add 100 dense neurons</span>

  <span class=c1>## After TensorFlow 2.8.0 ##</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>100</span><span class=p>),</span> <span class=c1># add 100 dense neurons</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>),</span> <span class=c1># add another layer with 10 neurons</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># 2. Compile the model</span>
<span class=n>model_3</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>BinaryCrossentropy</span><span class=p>(),</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span> <span class=c1># use Adam instead of SGD</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>

<span class=c1># 3. Fit the model</span>
<span class=n>model_3</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=c1># fit for 100 passes of the data</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.4713 - loss: 3.7724
Epoch 2/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4204 - loss: 1.5564
Epoch 3/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4075 - loss: 0.8399
Epoch 4/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4339 - loss: 0.8157
Epoch 5/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4728 - loss: 0.7995
Epoch 6/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7855
Epoch 7/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7733
Epoch 8/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7627
Epoch 9/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7534
Epoch 10/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.7453
Epoch 11/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7382
Epoch 12/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7320
Epoch 13/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7265
Epoch 14/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7217
Epoch 15/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.7175
Epoch 16/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.7138
Epoch 17/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7106
Epoch 18/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7078
Epoch 19/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7054
Epoch 20/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7033
Epoch 21/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.7015
Epoch 22/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7001
Epoch 23/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.6988
Epoch 24/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.6978
Epoch 25/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.6969
Epoch 26/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.6962
Epoch 27/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.6957
Epoch 28/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 7ms/step - accuracy: 0.4830 - loss: 0.6952
Epoch 29/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.6949
Epoch 30/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.6946
Epoch 31/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.6943
Epoch 32/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.6942
Epoch 33/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.6940
Epoch 34/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.6939
Epoch 35/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.6939
Epoch 36/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.6938
Epoch 37/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4617 - loss: 0.6937
Epoch 38/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4357 - loss: 0.6937
Epoch 39/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4335 - loss: 0.6937
Epoch 40/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4299 - loss: 0.6937
Epoch 41/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4422 - loss: 0.6937
Epoch 42/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4504 - loss: 0.6936
Epoch 43/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4548 - loss: 0.6936
Epoch 44/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4539 - loss: 0.6936
Epoch 45/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4492 - loss: 0.6936
Epoch 46/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4564 - loss: 0.6936
Epoch 47/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4601 - loss: 0.6936
Epoch 48/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4633 - loss: 0.6936
Epoch 49/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4702 - loss: 0.6937
Epoch 50/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4690 - loss: 0.6937
Epoch 51/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4699 - loss: 0.6937
Epoch 52/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4697 - loss: 0.6937
Epoch 53/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4694 - loss: 0.6937
Epoch 54/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4691 - loss: 0.6937
Epoch 55/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4664 - loss: 0.6937
Epoch 56/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4653 - loss: 0.6937
Epoch 57/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4647 - loss: 0.6937
Epoch 58/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4655 - loss: 0.6937
Epoch 59/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4685 - loss: 0.6937
Epoch 60/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4677 - loss: 0.6937
Epoch 61/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4680 - loss: 0.6937
Epoch 62/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4674 - loss: 0.6937
Epoch 63/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4661 - loss: 0.6938
Epoch 64/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4659 - loss: 0.6938
Epoch 65/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4683 - loss: 0.6938
Epoch 66/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4655 - loss: 0.6938
Epoch 67/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4675 - loss: 0.6938
Epoch 68/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4689 - loss: 0.6938
Epoch 69/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4667 - loss: 0.6938
Epoch 70/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4656 - loss: 0.6938
Epoch 71/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4632 - loss: 0.6938
Epoch 72/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4622 - loss: 0.6938
Epoch 73/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4636 - loss: 0.6938
Epoch 74/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4655 - loss: 0.6939
Epoch 75/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4660 - loss: 0.6939
Epoch 76/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4644 - loss: 0.6939
Epoch 77/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4672 - loss: 0.6939
Epoch 78/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4649 - loss: 0.6939
Epoch 79/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4666 - loss: 0.6939
Epoch 80/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4647 - loss: 0.6939
Epoch 81/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4609 - loss: 0.6939
Epoch 82/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4605 - loss: 0.6939
Epoch 83/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4616 - loss: 0.6940
Epoch 84/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4608 - loss: 0.6940
Epoch 85/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4611 - loss: 0.6940
Epoch 86/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4622 - loss: 0.6940
Epoch 87/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4611 - loss: 0.6940
Epoch 88/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4590 - loss: 0.6940
Epoch 89/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4587 - loss: 0.6940
Epoch 90/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4579 - loss: 0.6940
Epoch 91/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4581 - loss: 0.6940
Epoch 92/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4595 - loss: 0.6941
Epoch 93/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4552 - loss: 0.6941
Epoch 94/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4562 - loss: 0.6941
Epoch 95/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4556 - loss: 0.6941
Epoch 96/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4543 - loss: 0.6941
Epoch 97/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4545 - loss: 0.6941
Epoch 98/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4546 - loss: 0.6941
Epoch 99/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4552 - loss: 0.6942
Epoch 100/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4535 - loss: 0.6942





&lt;keras.src.callbacks.history.History at 0x20172dce720&gt;
</code></pre></div> <p>Still!</p> <p>We've pulled out a few tricks but our model isn't even doing better than guessing.</p> <p>Let's make some visualizations to see what's happening.</p> <blockquote> <p>🔑 <strong>Note:</strong> Whenever your model is performing strangely or there's something going on with your data you're not quite sure of, remember these three words: <strong>visualize, visualize, visualize</strong>. Inspect your data, inspect your model, inpsect your model's predictions.</p> </blockquote> <p>To visualize our model's predictions we're going to create a function <code>plot_decision_boundary()</code> which: * Takes in a trained model, features (<code>X</code>) and labels (<code>y</code>). * Creates a <a href=https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html>meshgrid</a> of the different <code>X</code> values. * Makes predictions across the meshgrid. * Plots the predictions as well as a line between the different zones (where each unique class falls).</p> <p>If this sounds confusing, let's see it in code and then see the output.</p> <blockquote> <p>🔑 <strong>Note:</strong> If you're ever unsure of what a function does, try unraveling it and writing it line by line for yourself to see what it does. Break it into small parts and see what each part outputs.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=k>def</span><span class=w> </span><span class=nf>plot_decision_boundary</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Plots the decision boundary created by a model predicting on X.</span>
<span class=sd>  This function has been adapted from two phenomenal resources:</span>
<span class=sd>   1. CS231n - https://cs231n.github.io/neural-networks-case-study/</span>
<span class=sd>   2. Made with ML basics - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=c1># Define the axis boundaries of the plot and create a meshgrid</span>
  <span class=n>x_min</span><span class=p>,</span> <span class=n>x_max</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span> <span class=o>-</span> <span class=mf>0.1</span><span class=p>,</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=mf>0.1</span>
  <span class=n>y_min</span><span class=p>,</span> <span class=n>y_max</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span> <span class=o>-</span> <span class=mf>0.1</span><span class=p>,</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=mf>0.1</span>
  <span class=n>xx</span><span class=p>,</span> <span class=n>yy</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>x_min</span><span class=p>,</span> <span class=n>x_max</span><span class=p>,</span> <span class=mi>100</span><span class=p>),</span>
                       <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>y_min</span><span class=p>,</span> <span class=n>y_max</span><span class=p>,</span> <span class=mi>100</span><span class=p>))</span>

  <span class=c1># Create X values (we&#39;re going to predict on all of these)</span>
  <span class=n>x_in</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>c_</span><span class=p>[</span><span class=n>xx</span><span class=o>.</span><span class=n>ravel</span><span class=p>(),</span> <span class=n>yy</span><span class=o>.</span><span class=n>ravel</span><span class=p>()]</span> <span class=c1># stack 2D arrays together: https://numpy.org/devdocs/reference/generated/numpy.c_.html</span>

  <span class=c1># Make predictions using the trained model</span>
  <span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_in</span><span class=p>)</span>

  <span class=c1># Check for multi-class</span>
  <span class=k>if</span> <span class=n>model</span><span class=o>.</span><span class=n>output_shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span> <span class=c1># checks the final dimension of the model&#39;s output shape, if this is &gt; (greater than) 1, it&#39;s multi-class</span>
    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;doing multiclass classification...&quot;</span><span class=p>)</span>
    <span class=c1># We have to reshape our predictions to get them ready for plotting</span>
    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>xx</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
  <span class=k>else</span><span class=p>:</span>
    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;doing binary classifcation...&quot;</span><span class=p>)</span>
    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>xx</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>

  <span class=c1># Plot decision boundary</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>contourf</span><span class=p>(</span><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>RdYlBu</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>40</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>RdYlBu</span><span class=p>)</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>xlim</span><span class=p>(</span><span class=n>xx</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>xx</span><span class=o>.</span><span class=n>max</span><span class=p>())</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>ylim</span><span class=p>(</span><span class=n>yy</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>yy</span><span class=o>.</span><span class=n>max</span><span class=p>())</span>
</code></pre></div> <p>Now we've got a function to plot our model's decision boundary (the cut off point its making between red and blue dots), let's try it out.</p> <div class=highlight><pre><span></span><code><span class=c1># Check out the predictions our model is making</span>
<span class=n>plot_decision_boundary</span><span class=p>(</span><span class=n>model_3</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m313/313[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 2ms/step
doing binary classifcation...
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_36_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_36_1.png></a></p> <p>Looks like our model is trying to draw a straight line through the data.</p> <p>What's wrong with doing this?</p> <p>The main issue is our data isn't separable by a straight line.</p> <p>In a regression problem, our model might work. In fact, let's try it.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create some regression data</span>
<span class=n>X_regression</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1000</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>
<span class=n>y_regression</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>1100</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>

<span class=c1># Split it into training and test sets</span>
<span class=n>X_reg_train</span> <span class=o>=</span> <span class=n>X_regression</span><span class=p>[:</span><span class=mi>150</span><span class=p>]</span>
<span class=n>X_reg_test</span> <span class=o>=</span> <span class=n>X_regression</span><span class=p>[</span><span class=mi>150</span><span class=p>:]</span>
<span class=n>y_reg_train</span> <span class=o>=</span> <span class=n>y_regression</span><span class=p>[:</span><span class=mi>150</span><span class=p>]</span>
<span class=n>y_reg_test</span> <span class=o>=</span> <span class=n>y_regression</span><span class=p>[</span><span class=mi>150</span><span class=p>:]</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>model_test</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=c1># Before TensorFlow 2.7.0</span>
  <span class=c1># tf.keras.layers.Dense(100), # add 100 dense neurons</span>

  <span class=c1># With TensorFlow 2.7.0</span>
  <span class=c1># tf.keras.layers.Dense(100, input_shape=(None, 1)), # add 100 dense neurons</span>

  <span class=c1>## After TensorFlow 2.8.0 ##</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>100</span><span class=p>),</span> <span class=c1># add 100 dense neurons</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>),</span> <span class=c1># add another layer with 10 neurons</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># 2. Compile the model</span>
<span class=n>model_test</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span> <span class=c1># use Adam instead of SGD</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;mae&#39;</span><span class=p>])</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Fit our model to the data</span>
<span class=c1># Note: Before TensorFlow 2.7.0, this line would work</span>
<span class=c1># model_3.fit(X_reg_train, y_reg_train, epochs=100)</span>

<span class=c1># After TensorFlow 2.7.0, see here for more: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278</span>
<span class=n>model_test</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>X_reg_train</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>),</span>
            <span class=n>y_reg_train</span><span class=p>,</span>
            <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 6ms/step - loss: 487.9379 - mae: 488.9716
Epoch 2/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 346.9504 - mae: 348.0899 
Epoch 3/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 204.6113 - mae: 205.8709 
Epoch 4/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 70.2593 - mae: 70.2393 
Epoch 5/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 80.1273 - mae: 79.7820 
Epoch 6/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 88.2640 - mae: 88.4543 
Epoch 7/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 53.6145 - mae: 53.7947 
Epoch 8/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 49.0636 - mae: 49.0717 
Epoch 9/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 43.2507 - mae: 43.2126 
Epoch 10/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 46.1210 - mae: 46.0782 
Epoch 11/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 43.7796 - mae: 43.8188 
Epoch 12/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 41.5159 - mae: 41.5407 
Epoch 13/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 40.4414 - mae: 40.4140 
Epoch 14/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 41.3143 - mae: 41.2920 
Epoch 15/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 40.2071 - mae: 40.2172 
Epoch 16/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 40.7613 - mae: 40.7893 
Epoch 17/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.8387 - mae: 39.8053 
Epoch 18/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 40.5822 - mae: 40.5698 
Epoch 19/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.8768 - mae: 39.8807 
Epoch 20/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.9630 - mae: 39.9600 
Epoch 21/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.6956 - mae: 39.6766 
Epoch 22/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.7170 - mae: 39.7061 
Epoch 23/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.7147 - mae: 39.7110 
Epoch 24/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.6130 - mae: 39.6021 
Epoch 25/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.5977 - mae: 39.5890 
Epoch 26/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.5747 - mae: 39.5650 
Epoch 27/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.5323 - mae: 39.5229 
Epoch 28/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.5035 - mae: 39.4919 
Epoch 29/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.4764 - mae: 39.4646 
Epoch 30/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.4674 - mae: 39.4604 
Epoch 31/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.4113 - mae: 39.3988 
Epoch 32/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.3846 - mae: 39.3699 
Epoch 33/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.3622 - mae: 39.3551 
Epoch 34/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.3695 - mae: 39.3610 
Epoch 35/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.2743 - mae: 39.2587 
Epoch 36/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.2665 - mae: 39.2565 
Epoch 37/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.2265 - mae: 39.2171 
Epoch 38/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.1773 - mae: 39.1691 
Epoch 39/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.1630 - mae: 39.1532 
Epoch 40/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.1117 - mae: 39.1014 
Epoch 41/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.1120 - mae: 39.1077 
Epoch 42/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.0677 - mae: 39.0572 
Epoch 43/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.0115 - mae: 38.9980 
Epoch 44/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.0076 - mae: 38.9984 
Epoch 45/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.9650 - mae: 38.9573 
Epoch 46/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.9257 - mae: 38.9179 
Epoch 47/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.8884 - mae: 38.8794 
Epoch 48/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.8522 - mae: 38.8414 
Epoch 49/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.8019 - mae: 38.7935 
Epoch 50/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.7851 - mae: 38.7754 
Epoch 51/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.7265 - mae: 38.7165 
Epoch 52/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.7523 - mae: 38.7445 
Epoch 53/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.6480 - mae: 38.6364 
Epoch 54/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.6548 - mae: 38.6482 
Epoch 55/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.5819 - mae: 38.5667 
Epoch 56/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.5475 - mae: 38.5359 
Epoch 57/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.5345 - mae: 38.5270 
Epoch 58/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.4910 - mae: 38.4816 
Epoch 59/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.4528 - mae: 38.4410 
Epoch 60/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.4010 - mae: 38.3917 
Epoch 61/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.3729 - mae: 38.3618 
Epoch 62/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.3225 - mae: 38.3110 
Epoch 63/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.3463 - mae: 38.3368 
Epoch 64/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.2403 - mae: 38.2266 
Epoch 65/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.2440 - mae: 38.2354 
Epoch 66/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.1954 - mae: 38.1883 
Epoch 67/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.0944 - mae: 38.0809 
Epoch 68/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.1159 - mae: 38.1033 
Epoch 69/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.0538 - mae: 38.0456 
Epoch 70/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.0569 - mae: 38.0423 
Epoch 71/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.9683 - mae: 37.9557 
Epoch 72/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.0189 - mae: 38.0129 
Epoch 73/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.8708 - mae: 37.8589 
Epoch 74/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.8845 - mae: 37.8777 
Epoch 75/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.7954 - mae: 37.7780 
Epoch 76/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.7736 - mae: 37.7602 
Epoch 77/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.8286 - mae: 37.8261 
Epoch 78/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.6436 - mae: 37.6286 
Epoch 79/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 37.6467 - mae: 37.6358 
Epoch 80/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.6812 - mae: 37.6750 
Epoch 81/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.5529 - mae: 37.5367 
Epoch 82/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.5196 - mae: 37.5028 
Epoch 83/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.5093 - mae: 37.4998 
Epoch 84/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.4750 - mae: 37.4621 
Epoch 85/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.3526 - mae: 37.3406  
Epoch 86/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 37.3761 - mae: 37.3615 
Epoch 87/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 37.2650 - mae: 37.2547 
Epoch 88/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.3105 - mae: 37.2961 
Epoch 89/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.1641 - mae: 37.1582 
Epoch 90/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.3469 - mae: 37.3350 
Epoch 91/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.0692 - mae: 37.0674 
Epoch 92/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.4917 - mae: 37.4849 
Epoch 93/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.0107 - mae: 37.0053 
Epoch 94/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.1842 - mae: 37.1742 
Epoch 95/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 36.8774 - mae: 36.8692 
Epoch 96/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.0420 - mae: 37.0377 
Epoch 97/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 36.7804 - mae: 36.7607 
Epoch 98/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 36.7776 - mae: 36.7659 
Epoch 99/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 36.8120 - mae: 36.8078 
Epoch 100/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 36.6546 - mae: 36.6399





&lt;keras.src.callbacks.history.History at 0x201770bb380&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>model_test</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</code></pre></div> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_3"</span>
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                         </span>┃<span style="font-weight: bold"> Output Shape                </span>┃<span style="font-weight: bold">         Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ dense_6 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">100</span>)                 │             <span style="color: #00af00; text-decoration-color: #00af00">200</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_7 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>)                  │           <span style="color: #00af00; text-decoration-color: #00af00">1,010</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_8 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   │              <span style="color: #00af00; text-decoration-color: #00af00">11</span> │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">3,665</span> (14.32 KB)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,221</span> (4.77 KB)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,444</span> (9.55 KB)
</pre> <p>Oh wait... we compiled our model for a binary classification problem.</p> <p>No trouble, we can recreate it for a regression problem.</p> <div class=highlight><pre><span></span><code><span class=c1># Setup random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Recreate the model</span>
<span class=n>model_3</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>100</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Change the loss and metrics of our compiled model</span>
<span class=n>model_3</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span> <span class=c1># change the loss function to be regression-specific</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;mae&#39;</span><span class=p>])</span> <span class=c1># change the metric to be regression-specific</span>

<span class=c1># Fit the recompiled model</span>
<span class=n>model_3</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>X_reg_train</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>),</span>
            <span class=n>y_reg_train</span><span class=p>,</span>
            <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 6ms/step - loss: 488.5253 - mae: 489.1115
Epoch 2/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 396.2791 - mae: 396.9307 
Epoch 3/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 303.3047 - mae: 304.0399 
Epoch 4/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 207.6707 - mae: 208.5094 
Epoch 5/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 107.3864 - mae: 108.3462 
Epoch 6/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 46.9067 - mae: 46.4586 
Epoch 7/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 76.5540 - mae: 76.4521 
Epoch 8/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 67.3932 - mae: 67.5772 
Epoch 9/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 44.7325 - mae: 44.7694 
Epoch 10/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 46.6696 - mae: 46.7539 
Epoch 11/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 40.8703 - mae: 40.8414 
Epoch 12/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 42.4601 - mae: 42.4116 
Epoch 13/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 42.2812 - mae: 42.2912 
Epoch 14/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 40.2232 - mae: 40.2372 
Epoch 15/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 40.4902 - mae: 40.4988 
Epoch 16/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.8062 - mae: 39.7832 
Epoch 17/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.9811 - mae: 39.9660 
Epoch 18/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.8529 - mae: 39.8500 
Epoch 19/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.8665 - mae: 39.8625 
Epoch 20/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.7435 - mae: 39.7289 
Epoch 21/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.7560 - mae: 39.7432 
Epoch 22/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.7272 - mae: 39.7225 
Epoch 23/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.7281 - mae: 39.7228 
Epoch 24/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.6495 - mae: 39.6376 
Epoch 25/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.6456 - mae: 39.6344 
Epoch 26/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.6273 - mae: 39.6196 
Epoch 27/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.5767 - mae: 39.5679 
Epoch 28/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.5691 - mae: 39.5619 
Epoch 29/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.5216 - mae: 39.5119 
Epoch 30/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.5056 - mae: 39.4992 
Epoch 31/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.5064 - mae: 39.5005 
Epoch 32/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.4377 - mae: 39.4266 
Epoch 33/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.4279 - mae: 39.4190 
Epoch 34/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.3995 - mae: 39.3897 
Epoch 35/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.3672 - mae: 39.3576 
Epoch 36/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.3580 - mae: 39.3523 
Epoch 37/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.3224 - mae: 39.3124 
Epoch 38/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.2826 - mae: 39.2706 
Epoch 39/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.2623 - mae: 39.2558 
Epoch 40/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.2603 - mae: 39.2556  
Epoch 41/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.1853 - mae: 39.1753 
Epoch 42/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.1664 - mae: 39.1589 
Epoch 43/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.1551 - mae: 39.1472 
Epoch 44/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.1001 - mae: 39.0920 
Epoch 45/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 39.0835 - mae: 39.0733 
Epoch 46/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.0469 - mae: 39.0359 
Epoch 47/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 39.0374 - mae: 39.0302 
Epoch 48/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.9723 - mae: 38.9629 
Epoch 49/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.9698 - mae: 38.9617 
Epoch 50/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.9351 - mae: 38.9267 
Epoch 51/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.9020 - mae: 38.8923 
Epoch 52/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.8603 - mae: 38.8523 
Epoch 53/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.8433 - mae: 38.8340 
Epoch 54/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.7959 - mae: 38.7860 
Epoch 55/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.7907 - mae: 38.7852 
Epoch 56/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.7447 - mae: 38.7335 
Epoch 57/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.7056 - mae: 38.6914 
Epoch 58/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.6853 - mae: 38.6775 
Epoch 59/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.6660 - mae: 38.6596 
Epoch 60/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.5803 - mae: 38.5672 
Epoch 61/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.5812 - mae: 38.5709 
Epoch 62/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.5689 - mae: 38.5650 
Epoch 63/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.5308 - mae: 38.5219 
Epoch 64/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.4670 - mae: 38.4551 
Epoch 65/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.4609 - mae: 38.4523 
Epoch 66/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.4198 - mae: 38.4117 
Epoch 67/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.3815 - mae: 38.3724 
Epoch 68/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.3447 - mae: 38.3336 
Epoch 69/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 38.3018 - mae: 38.2922 
Epoch 70/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.2952 - mae: 38.2915 
Epoch 71/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.2544 - mae: 38.2448 
Epoch 72/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.1906 - mae: 38.1775 
Epoch 73/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.1825 - mae: 38.1725 
Epoch 74/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.1392 - mae: 38.1295 
Epoch 75/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.0987 - mae: 38.0877 
Epoch 76/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.0507 - mae: 38.0416 
Epoch 77/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 38.0162 - mae: 38.0049 
Epoch 78/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 57ms/step - loss: 37.9768 - mae: 37.9645 
Epoch 79/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 37.9667 - mae: 37.9590 
Epoch 80/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.8756 - mae: 37.8644 
Epoch 81/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.8815 - mae: 37.8714 
Epoch 82/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 11ms/step - loss: 37.8380 - mae: 37.8267 
Epoch 83/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 37.7869 - mae: 37.7776 
Epoch 84/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 37.7489 - mae: 37.7373 
Epoch 85/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.7085 - mae: 37.6958 
Epoch 86/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.6971 - mae: 37.6891 
Epoch 87/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.6475 - mae: 37.6403 
Epoch 88/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.6015 - mae: 37.5931 
Epoch 89/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.5575 - mae: 37.5465 
Epoch 90/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.5048 - mae: 37.4951 
Epoch 91/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 37.5167 - mae: 37.5089 
Epoch 92/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 37.4110 - mae: 37.3989 
Epoch 93/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.4065 - mae: 37.3977 
Epoch 94/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.3554 - mae: 37.3465 
Epoch 95/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.3073 - mae: 37.2964 
Epoch 96/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 37.2494 - mae: 37.2400 
Epoch 97/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.2062 - mae: 37.1938 
Epoch 98/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.1643 - mae: 37.1504 
Epoch 99/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.1485 - mae: 37.1391 
Epoch 100/100
[1m5/5[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - loss: 37.0928 - mae: 37.0840





&lt;keras.src.callbacks.history.History at 0x20177087cb0&gt;
</code></pre></div> <p>Okay, it seems like our model is learning something (the <code>mae</code> value trends down with each epoch), let's plot its predictions.</p> <div class=highlight><pre><span></span><code><span class=c1># Make predictions with our trained model</span>
<span class=n>y_reg_preds</span> <span class=o>=</span> <span class=n>model_3</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_reg_test</span><span class=p>)</span>

<span class=c1># Plot the model&#39;s predictions against our regression data</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_reg_train</span><span class=p>,</span> <span class=n>y_reg_train</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s1>&#39;b&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training data&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_reg_test</span><span class=p>,</span> <span class=n>y_reg_test</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s1>&#39;g&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Testing data&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_reg_test</span><span class=p>,</span> <span class=n>y_reg_preds</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(),</span> <span class=n>c</span><span class=o>=</span><span class=s1>&#39;r&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Predictions&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>();</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m2/2[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 96ms/step
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_45_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_45_1.png></a></p> <p>Okay, the predictions aren't perfect (if the predictions were perfect, the red would line up with the green), but they look better than complete guessing.</p> <p>So this means our model must be learning something...</p> <p>There must be something we're missing out on for our classification problem.</p> <h2 id=the-missing-piece-non-linearity>The missing piece: Non-linearity</h2> <p>Okay, so we saw our neural network can model straight lines (with ability a little bit better than guessing).</p> <p>What about non-straight (non-linear) lines?</p> <p>If we're going to model our classification data (the red and blue circles), we're going to need some non-linear lines.</p> <blockquote> <p>🔨 <strong>Practice:</strong> Before we get to the next steps, I'd encourage you to play around with the <a href="https://playground.tensorflow.org/#activation=linear&batchSize=1&dataset=circle&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=0&networkShape=1&seed=0.09561&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularizationRate_hide=true&discretize_hide=true&regularization_hide=true&dataset_hide=true&noise_hide=true&batchSize_hide=true">TensorFlow Playground</a> (check out what the data has in common with our own classification data) for 10-minutes. In particular the tab which says "activation". Once you're done, come back.</p> </blockquote> <p>Did you try out the activation options? If so, what did you find?</p> <p>If you didn't, don't worry, let's see it in code.</p> <p>We're going to replicate the neural network you can see at this link: <a href="https://playground.tensorflow.org/#activation=linear&batchSize=1&dataset=circle&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=0&networkShape=1&seed=0.09561&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularizationRate_hide=true&discretize_hide=true&regularization_hide=true&dataset_hide=true&noise_hide=true&batchSize_hide=true">TensorFlow Playground</a>.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-simple-net-linear-activation.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="simple neural net created with TensorFlow playground" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-simple-net-linear-activation.png></a> <em>The neural network we're going to recreate with TensorFlow code. See it live at <a href="https://playground.tensorflow.org/#activation=linear&batchSize=1&dataset=circle&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=0&networkShape=1&seed=0.09561&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularizationRate_hide=true&discretize_hide=true&regularization_hide=true&dataset_hide=true&noise_hide=true&batchSize_hide=true">TensorFlow Playground</a>.</em></p> <p>The main change we'll add to models we've built before is the use of the <code>activation</code> keyword.</p> <div class=highlight><pre><span></span><code><span class=c1># Set the random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create the model</span>
<span class=n>model_4</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>activations</span><span class=o>.</span><span class=n>linear</span><span class=p>),</span> <span class=c1># 1 hidden layer with linear activation</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=c1># output layer</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_4</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>binary_crossentropy</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.001</span><span class=p>),</span> <span class=c1># note: &quot;lr&quot; used to be what was used, now &quot;learning_rate&quot; is favoured</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;accuracy&quot;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>model_4</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.4830 - loss: 4.6718
Epoch 2/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 4.3606
Epoch 3/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 4.1655
Epoch 4/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 4.0379
Epoch 5/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 3.8148
Epoch 6/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 3.4429
Epoch 7/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 3.0337
Epoch 8/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 2.3672
Epoch 9/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 1.2097
Epoch 10/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 1.1518
Epoch 11/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 1.1209
Epoch 12/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 1.0960
Epoch 13/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 1.0745
Epoch 14/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 1.0555
Epoch 15/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 1.0383
Epoch 16/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 1.0226
Epoch 17/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 1.0081
Epoch 18/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.9946
Epoch 19/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.9820
Epoch 20/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.9701
Epoch 21/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.9589
Epoch 22/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 0.9482
Epoch 23/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.9382
Epoch 24/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.9286
Epoch 25/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.9194
Epoch 26/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.9107
Epoch 27/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.9023
Epoch 28/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8943
Epoch 29/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8866
Epoch 30/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8792
Epoch 31/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8721
Epoch 32/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8652
Epoch 33/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8587
Epoch 34/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8523
Epoch 35/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8462
Epoch 36/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8403
Epoch 37/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8347
Epoch 38/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8292
Epoch 39/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8239
Epoch 40/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8188
Epoch 41/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8139
Epoch 42/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8092
Epoch 43/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8046
Epoch 44/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.8002
Epoch 45/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7959
Epoch 46/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7918
Epoch 47/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7878
Epoch 48/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7840
Epoch 49/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7803
Epoch 50/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7767
Epoch 51/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7733
Epoch 52/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7699
Epoch 53/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7667
Epoch 54/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7636
Epoch 55/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7607
Epoch 56/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 0.7578
Epoch 57/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4827 - loss: 0.7551
Epoch 58/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4807 - loss: 0.7524
Epoch 59/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4788 - loss: 0.7498
Epoch 60/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4690 - loss: 0.7474
Epoch 61/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4534 - loss: 0.7450
Epoch 62/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4450 - loss: 0.7428
Epoch 63/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4287 - loss: 0.7406
Epoch 64/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4251 - loss: 0.7385
Epoch 65/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4176 - loss: 0.7365
Epoch 66/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4150 - loss: 0.7345
Epoch 67/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4112 - loss: 0.7327
Epoch 68/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4208 - loss: 0.7309
Epoch 69/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4289 - loss: 0.7292
Epoch 70/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4405 - loss: 0.7276
Epoch 71/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4426 - loss: 0.7260
Epoch 72/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4578 - loss: 0.7245
Epoch 73/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4595 - loss: 0.7231
Epoch 74/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4675 - loss: 0.7217
Epoch 75/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4707 - loss: 0.7204
Epoch 76/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4737 - loss: 0.7192
Epoch 77/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4724 - loss: 0.7180
Epoch 78/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4766 - loss: 0.7168
Epoch 79/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4767 - loss: 0.7157
Epoch 80/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4750 - loss: 0.7147
Epoch 81/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4776 - loss: 0.7137
Epoch 82/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4811 - loss: 0.7128
Epoch 83/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4844 - loss: 0.7119
Epoch 84/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4838 - loss: 0.7110
Epoch 85/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4834 - loss: 0.7102
Epoch 86/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4883 - loss: 0.7094
Epoch 87/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4925 - loss: 0.7086
Epoch 88/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4904 - loss: 0.7079
Epoch 89/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4948 - loss: 0.7072
Epoch 90/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - accuracy: 0.4951 - loss: 0.7066
Epoch 91/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4949 - loss: 0.7060
Epoch 92/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4959 - loss: 0.7054
Epoch 93/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4965 - loss: 0.7048
Epoch 94/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4983 - loss: 0.7043
Epoch 95/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5009 - loss: 0.7037
Epoch 96/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5023 - loss: 0.7033
Epoch 97/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5017 - loss: 0.7028
Epoch 98/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5064 - loss: 0.7023
Epoch 99/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5079 - loss: 0.7019
Epoch 100/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5087 - loss: 0.7015
</code></pre></div> <p>Okay, our model performs a little worse than guessing.</p> <p>Let's remind ourselves what our data looks like.</p> <div class=highlight><pre><span></span><code><span class=c1># Check out our data</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>RdYlBu</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_50_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_50_0.png></a></p> <p>And let's see how our model is making predictions on it.</p> <div class=highlight><pre><span></span><code><span class=c1># Check the deicison boundary (blue is blue class, yellow is the crossover, red is red class)</span>
<span class=n>plot_decision_boundary</span><span class=p>(</span><span class=n>model_4</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>WARNING:tensorflow:5 out of the last 316 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x000002017A9149A0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[1m313/313[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 2ms/step
doing binary classifcation...
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_52_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_52_1.png></a></p> <p>Well, it looks like we're getting a straight (linear) line prediction again.</p> <p>But our data is non-linear (not a straight line)...</p> <p>What we're going to have to do is add some non-linearity to our model.</p> <p>To do so, we'll use the <code>activation</code> parameter in on of our layers.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create a model with a non-linear activation</span>
<span class=n>model_5</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>activations</span><span class=o>.</span><span class=n>relu</span><span class=p>),</span> <span class=c1># can also do activation=&#39;relu&#39;</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=c1># output layer</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_5</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>binary_crossentropy</span><span class=p>,</span>
              <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span>
              <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;accuracy&quot;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>model_5</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 2/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 3/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 4/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 5/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 6/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 7/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 8/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 9/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 10/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 11/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 12/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 13/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 14/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 15/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 16/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 17/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 18/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 19/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 20/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 21/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 22/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 23/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 24/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 25/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 26/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 27/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 28/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 29/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 30/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 31/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 32/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 33/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 34/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 35/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 36/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 37/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 38/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 39/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 40/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 41/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 42/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 43/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 44/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 45/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 46/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 47/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 48/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 49/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 50/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 51/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 52/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 53/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 54/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 55/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 56/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 57/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 58/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 59/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 60/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 61/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 62/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 63/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 64/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 65/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 66/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 67/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 68/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 69/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 70/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 71/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 6ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 72/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 73/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 74/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 75/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 76/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 77/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 78/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 79/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 80/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 81/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 82/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 83/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 84/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 85/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 86/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 87/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 88/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 89/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 90/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 91/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 92/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 93/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 94/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 95/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 96/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 97/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 98/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 99/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
Epoch 100/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 8.3336
</code></pre></div> <p>Hmm... still not learning...</p> <p>What we if increased the number of neurons and layers?</p> <p>Say, 2 hidden layers, with <a href=https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu>ReLU</a>, pronounced "rel-u", (short for <a href=https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/ >rectified linear unit</a>), activation on the first one, and 4 neurons each?</p> <p>To see this network in action, check out the <a href="https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=4,4&seed=0.93799&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&regularizationRate_hide=true&batchSize_hide=true&dataset_hide=true">TensorFlow Playground demo</a>.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-two-layer-net-relu-activation.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="multi-layer neural net created with TensorFlow playground" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-two-layer-net-relu-activation.png></a> <em>The neural network we're going to recreate with TensorFlow code. See it live at <a href="https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=4,4&seed=0.93799&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&regularizationRate_hide=true&batchSize_hide=true&dataset_hide=true">TensorFlow Playground</a>.</em></p> <p>Let's try.</p> <p><strong>Note:</strong> in the course, Daniel used <code>lr</code> instead of <code>learning_rate</code>. But for the update, we had changed to <code>learning_rate</code> instead of <code>lr</code>.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create a model</span>
<span class=n>model_6</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>activations</span><span class=o>.</span><span class=n>relu</span><span class=p>),</span> <span class=c1># hidden layer 1, 4 neurons, ReLU activation</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>activations</span><span class=o>.</span><span class=n>relu</span><span class=p>),</span> <span class=c1># hidden layer 2, 4 neurons, ReLU activation</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=c1># ouput layer</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_6</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>binary_crossentropy</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.001</span><span class=p>),</span> <span class=c1># Adam&#39;s default learning rate is 0.001</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>model_6</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.4830 - loss: 6.2658
Epoch 2/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 5.5252
Epoch 3/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 5.2501
Epoch 4/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 4.8496
Epoch 5/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 4.3150
Epoch 6/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4830 - loss: 3.6704
Epoch 7/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 3.3973
Epoch 8/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4830 - loss: 2.9343
Epoch 9/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4718 - loss: 2.0000
Epoch 10/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4598 - loss: 0.9447
Epoch 11/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4639 - loss: 0.8439
Epoch 12/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4701 - loss: 0.8198
Epoch 13/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4667 - loss: 0.8045
Epoch 14/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4479 - loss: 0.7929
Epoch 15/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4448 - loss: 0.7837
Epoch 16/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4414 - loss: 0.7761
Epoch 17/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4468 - loss: 0.7698
Epoch 18/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4510 - loss: 0.7644
Epoch 19/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4549 - loss: 0.7596
Epoch 20/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4599 - loss: 0.7554
Epoch 21/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4607 - loss: 0.7516
Epoch 22/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4612 - loss: 0.7481
Epoch 23/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4584 - loss: 0.7448
Epoch 24/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4633 - loss: 0.7417
Epoch 25/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4619 - loss: 0.7388
Epoch 26/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4645 - loss: 0.7360
Epoch 27/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4676 - loss: 0.7334
Epoch 28/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4676 - loss: 0.7308
Epoch 29/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4690 - loss: 0.7284
Epoch 30/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4673 - loss: 0.7260
Epoch 31/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4697 - loss: 0.7238
Epoch 32/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4712 - loss: 0.7216
Epoch 33/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4714 - loss: 0.7195
Epoch 34/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4714 - loss: 0.7174
Epoch 35/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4735 - loss: 0.7154
Epoch 36/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4735 - loss: 0.7135
Epoch 37/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4735 - loss: 0.7116
Epoch 38/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4754 - loss: 0.7099
Epoch 39/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4767 - loss: 0.7076
Epoch 40/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4923 - loss: 0.7008
Epoch 41/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4872 - loss: 0.6926
Epoch 42/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5324 - loss: 0.6873
Epoch 43/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5235 - loss: 0.6831
Epoch 44/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5190 - loss: 0.6798
Epoch 45/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5297 - loss: 0.6767
Epoch 46/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5052 - loss: 0.6738
Epoch 47/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5081 - loss: 0.6710
Epoch 48/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5163 - loss: 0.6683
Epoch 49/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5159 - loss: 0.6656
Epoch 50/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5228 - loss: 0.6630
Epoch 51/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5381 - loss: 0.6605
Epoch 52/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5544 - loss: 0.6580
Epoch 53/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5516 - loss: 0.6556
Epoch 54/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5663 - loss: 0.6533
Epoch 55/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5756 - loss: 0.6510
Epoch 56/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6219 - loss: 0.6487
Epoch 57/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6336 - loss: 0.6464
Epoch 58/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6372 - loss: 0.6441
Epoch 59/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6598 - loss: 0.6417
Epoch 60/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6663 - loss: 0.6393
Epoch 61/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6657 - loss: 0.6369
Epoch 62/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.6753 - loss: 0.6346
Epoch 63/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.6767 - loss: 0.6322
Epoch 64/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.6832 - loss: 0.6297
Epoch 65/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6833 - loss: 0.6274
Epoch 66/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.6783 - loss: 0.6251
Epoch 67/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.6811 - loss: 0.6226
Epoch 68/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6861 - loss: 0.6202
Epoch 69/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.6926 - loss: 0.6177
Epoch 70/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6965 - loss: 0.6152
Epoch 71/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6944 - loss: 0.6127
Epoch 72/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6972 - loss: 0.6101
Epoch 73/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7003 - loss: 0.6075
Epoch 74/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - accuracy: 0.7015 - loss: 0.6049
Epoch 75/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7054 - loss: 0.6022
Epoch 76/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7125 - loss: 0.5994
Epoch 77/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7230 - loss: 0.5965
Epoch 78/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7276 - loss: 0.5935
Epoch 79/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7334 - loss: 0.5905
Epoch 80/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7345 - loss: 0.5874
Epoch 81/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7369 - loss: 0.5844
Epoch 82/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7392 - loss: 0.5814
Epoch 83/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7444 - loss: 0.5783
Epoch 84/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7495 - loss: 0.5752
Epoch 85/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7571 - loss: 0.5721
Epoch 86/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7626 - loss: 0.5688
Epoch 87/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7681 - loss: 0.5656
Epoch 88/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7670 - loss: 0.5622
Epoch 89/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7736 - loss: 0.5588
Epoch 90/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7793 - loss: 0.5552
Epoch 91/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7798 - loss: 0.5516
Epoch 92/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7833 - loss: 0.5480
Epoch 93/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7841 - loss: 0.5442
Epoch 94/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7903 - loss: 0.5402
Epoch 95/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7913 - loss: 0.5356
Epoch 96/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7928 - loss: 0.5310
Epoch 97/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7949 - loss: 0.5262
Epoch 98/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.8039 - loss: 0.5211
Epoch 99/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.8025 - loss: 0.5157
Epoch 100/100
[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8018 - loss: 0.5104
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate the model</span>
<span class=n>model_6</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7802 - loss: 0.5153





[0.5108292102813721, 0.7940000295639038]
</code></pre></div> <p>We're still hitting 50% accuracy, our model is still practically as good as guessing.</p> <p>How do the predictions look?</p> <div class=highlight><pre><span></span><code><span class=c1># Check out the predictions using 2 hidden layers</span>
<span class=n>plot_decision_boundary</span><span class=p>(</span><span class=n>model_6</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m313/313[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 2ms/step
doing binary classifcation...
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_59_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_59_1.png></a></p> <p>What gives?</p> <p>It seems like our model is the same as the one in the <a href="https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,4&seed=0.93799&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&regularizationRate_hide=true&batchSize_hide=true">TensorFlow Playground</a> but model it's still drawing straight lines...</p> <p>Ideally, the yellow lines go on the inside of the red circle and the blue circle.</p> <p>Okay, okay, let's model this circle once and for all.</p> <p>One more model (I promise... actually, I'm going to have to break that promise... we'll be building plenty more models).</p> <p>This time we'll change the activation function on our output layer too. Remember the architecture of a classification model? For binary classification, the output layer activation is usually the <a href=https://www.tensorflow.org/api_docs/python/tf/math/sigmoid>Sigmoid activation function</a>.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create a model</span>
<span class=n>model_7</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>activations</span><span class=o>.</span><span class=n>relu</span><span class=p>),</span> <span class=c1># hidden layer 1, ReLU activation</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>activations</span><span class=o>.</span><span class=n>relu</span><span class=p>),</span> <span class=c1># hidden layer 2, ReLU activation</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>activations</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>)</span> <span class=c1># ouput layer, sigmoid activation</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_7</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>binary_crossentropy</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>model_7</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate our model</span>
<span class=n>model_7</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m32/32[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.8287 - loss: 0.4659





[0.4603603780269623, 0.8299999833106995]
</code></pre></div> <p>Woah! It looks like our model is getting some incredible results, let's check them out.</p> <div class=highlight><pre><span></span><code><span class=c1># View the predictions of the model with relu and sigmoid activations</span>
<span class=n>plot_decision_boundary</span><span class=p>(</span><span class=n>model_7</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m313/313[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 2ms/step
doing binary classifcation...
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_64_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_64_1.png></a></p> <p>Nice! It looks like our model is almost perfectly (apart from a few examples) separating the two circles.</p> <blockquote> <p>🤔 <strong>Question:</strong> What's wrong with the predictions we've made? Are we really evaluating our model correctly here? Hint: what data did the model learn on and what did we predict on?</p> </blockquote> <p>Before we answer that, it's important to recognize what we've just covered.</p> <blockquote> <p>🔑 <strong>Note:</strong> The combination of <strong>linear (straight lines) and non-linear (non-straight lines) functions</strong> is one of the key fundamentals of neural networks.</p> </blockquote> <p>Think of it like this:</p> <p>If I gave you an unlimited amount of straight lines and non-straight lines, what kind of patterns could you draw?</p> <p>That's essentially what neural networks do to find patterns in data.</p> <p>Now you might be thinking, "but I haven't seen a linear function or a non-linear function before..."</p> <p>Oh but you have.</p> <p>We've been using them the whole time.</p> <p>They're what power the layers in the models we just built.</p> <p>To get some intuition about the activation functions we've just used, let's create them and then try them on some toy data.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a toy tensor (similar to the data we pass into our model)</span>
<span class=n>A</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=o>-</span><span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
<span class=n>A</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,
         1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],
      dtype=float32)&gt;
</code></pre></div> <p>How does this look?</p> <div class=highlight><pre><span></span><code><span class=c1># Visualize our toy tensor</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>A</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_68_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_68_0.png></a></p> <p>A straight (linear) line!</p> <p>Nice, now let's recreate the <a href=https://en.wikipedia.org/wiki/Sigmoid_function>sigmoid function</a> and see what it does to our data. You can also find a pre-built sigmoid function at <a href=https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid><code>tf.keras.activations.sigmoid</code></a>.</p> <div class=highlight><pre><span></span><code><span class=c1># Sigmoid - https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid</span>
<span class=k>def</span><span class=w> </span><span class=nf>sigmoid</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
  <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>tf</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>x</span><span class=p>))</span>

<span class=c1># Use the sigmoid function on our tensor</span>
<span class=n>sigmoid</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=
array([4.5397872e-05, 1.2339458e-04, 3.3535014e-04, 9.1105117e-04,
       2.4726233e-03, 6.6928510e-03, 1.7986210e-02, 4.7425874e-02,
       1.1920292e-01, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,
       8.8079703e-01, 9.5257413e-01, 9.8201376e-01, 9.9330717e-01,
       9.9752742e-01, 9.9908900e-01, 9.9966466e-01, 9.9987662e-01],
      dtype=float32)&gt;
</code></pre></div> <p>And how does it look?</p> <div class=highlight><pre><span></span><code><span class=c1># Plot sigmoid modified tensor</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>A</span><span class=p>));</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_72_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_72_0.png></a></p> <p>A non-straight (non-linear) line!</p> <p>Okay, how about the <a href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/#:~:text=The%20rectified%20linear%20activation%20function,otherwise%2C%20it%20will%20output%20zero.">ReLU function</a> (ReLU turns all negatives to 0 and positive numbers stay the same)?</p> <div class=highlight><pre><span></span><code><span class=c1># ReLU - https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu</span>
<span class=k>def</span><span class=w> </span><span class=nf>relu</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
  <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>

<span class=c1># Pass toy tensor through ReLU function</span>
<span class=n>relu</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6.,
       7., 8., 9.], dtype=float32)&gt;
</code></pre></div> <p>How does the ReLU-modified tensor look?</p> <div class=highlight><pre><span></span><code><span class=c1># Plot ReLU-modified tensor</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>relu</span><span class=p>(</span><span class=n>A</span><span class=p>));</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_76_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_76_0.png></a></p> <p>Another non-straight line!</p> <p>Well, how about TensorFlow's <a href=https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear>linear activation function</a>?</p> <div class=highlight><pre><span></span><code><span class=c1># Linear - https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear (returns input non-modified...)</span>
<span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>activations</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,
         1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],
      dtype=float32)&gt;
</code></pre></div> <p>Hmm, it looks like our inputs are unmodified...</p> <div class=highlight><pre><span></span><code><span class=c1># Does the linear activation change anything?</span>
<span class=n>A</span> <span class=o>==</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>activations</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(20,), dtype=bool, numpy=
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True])&gt;
</code></pre></div> <p>Okay, so it makes sense now the model doesn't really learn anything when using only linear activation functions, because the linear activation function doesn't change our input data in anyway.</p> <p>Where as, with our non-linear functions, our data gets manipulated. A neural network uses these kind of transformations at a large scale to figure draw patterns between its inputs and outputs.</p> <p>Now rather than dive into the guts of neural networks, we're going to keep coding applying what we've learned to different problems but if you want a more in-depth look at what's going on behind the scenes, check out the Extra Curriculum section below.</p> <blockquote> <p>📖 <strong>Resource:</strong> For more on activation functions, check out the <a href=https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#>machine learning cheatsheet page</a> on them.</p> </blockquote> <h2 id=evaluating-and-improving-our-classification-model>Evaluating and improving our classification model</h2> <p>If you answered the question above, you might've picked up what we've been doing wrong.</p> <p>We've been evaluating our model on the same data it was trained on.</p> <p>A better approach would be to split our data into training, validation (optional) and test sets.</p> <p>Once we've done that, we'll train our model on the training set (let it find patterns in the data) and then see how well it learned the patterns by using it to predict values on the test set.</p> <p>Let's do it.</p> <div class=highlight><pre><span></span><code><span class=c1># How many examples are in the whole dataset?</span>
<span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>1000
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Split data into train and test sets</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:</span><span class=mi>800</span><span class=p>],</span> <span class=n>y</span><span class=p>[:</span><span class=mi>800</span><span class=p>]</span> <span class=c1># 80% of the data for the training set</span>
<span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=mi>800</span><span class=p>:],</span> <span class=n>y</span><span class=p>[</span><span class=mi>800</span><span class=p>:]</span> <span class=c1># 20% of the data for the test set</span>

<span class=c1># Check the shapes of the data</span>
<span class=n>X_train</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>X_test</span><span class=o>.</span><span class=n>shape</span> <span class=c1># 800 examples in the training set, 200 examples in the test set</span>
</code></pre></div> <div class=highlight><pre><span></span><code>((800, 2), (200, 2))
</code></pre></div> <p>Great, now we've got training and test sets, let's model the training data and evaluate what our model has learned on the test set.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create the model (same as model_7)</span>
<span class=n>model_8</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span> <span class=c1># hidden layer 1, using &quot;relu&quot; for activation (same as tf.keras.activations.relu)</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;sigmoid&quot;</span><span class=p>)</span> <span class=c1># output layer, using &#39;sigmoid&#39; for the output</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_8</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>binary_crossentropy</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.01</span><span class=p>),</span> <span class=c1># increase learning rate from 0.001 to 0.01 for faster learning</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>model_8</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>25</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.4535 - loss: 0.7185
Epoch 2/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5016 - loss: 0.6954
Epoch 3/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4979 - loss: 0.6930
Epoch 4/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4930 - loss: 0.6922
Epoch 5/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4996 - loss: 0.6921
Epoch 6/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4967 - loss: 0.6917
Epoch 7/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5061 - loss: 0.6906
Epoch 8/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5146 - loss: 0.6878
Epoch 9/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5199 - loss: 0.6785
Epoch 10/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5592 - loss: 0.6611 
Epoch 11/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6009 - loss: 0.6495 
Epoch 12/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6251 - loss: 0.6335 
Epoch 13/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6678 - loss: 0.6180 
Epoch 14/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6856 - loss: 0.6006 
Epoch 15/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6998 - loss: 0.5826 
Epoch 16/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7258 - loss: 0.5563 
Epoch 17/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7777 - loss: 0.5033 
Epoch 18/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8015 - loss: 0.4648 
Epoch 19/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8355 - loss: 0.4368 
Epoch 20/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8548 - loss: 0.4124
Epoch 21/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8757 - loss: 0.3927 
Epoch 22/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8730 - loss: 0.3781 
Epoch 23/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8741 - loss: 0.3656 
Epoch 24/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8624 - loss: 0.3576 
Epoch 25/25
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8707 - loss: 0.3478
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate our model on the test set</span>
<span class=n>loss</span><span class=p>,</span> <span class=n>accuracy</span> <span class=o>=</span> <span class=n>model_8</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Model loss on the test set: </span><span class=si>{</span><span class=n>loss</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Model accuracy on the test set: </span><span class=si>{</span><span class=mi>100</span><span class=o>*</span><span class=n>accuracy</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m7/7[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.8902 - loss: 0.3174  
Model loss on the test set: 0.3311477601528168
Model accuracy on the test set: 88.00%
</code></pre></div> <p>100% accuracy? Nice!</p> <p>Now, when we started to create <code>model_8</code> we said it was going to be the same as <code>model_7</code> but you might've found that to be a little lie.</p> <p>That's because we changed a few things: * <strong>The <code>activation</code> parameter</strong> - We used strings (<code>"relu"</code> &amp; <code>"sigmoid"</code>) instead of using library paths (<code>tf.keras.activations.relu</code>), in TensorFlow, they both offer the same functionality. * <strong>The <code>learning_rate</code> (also <code>lr</code>) parameter</strong> - We increased the <strong>learning rate</strong> parameter in the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam>Adam optimizer</a> to <code>0.01</code> instead of <code>0.001</code> (an increase of 10x). * You can think of the learning rate as how quickly a model learns. The higher the learning rate, the faster the model's capacity to learn, however, there's such a thing as a <em>too high</em> learning rate, where a model tries to learn too fast and doesn't learn anything. We'll see a trick to find the ideal learning rate soon. * <strong>The number of epochs</strong> - We lowered the number of epochs (using the <code>epochs</code> parameter) from 100 to 25 but our model still got an incredible result on both the training and test sets. * One of the reasons our model performed well in even less epochs (remember a single epoch is the model trying to learn patterns in the data by looking at it once, so 25 epochs means the model gets 25 chances) than before is because we increased the learning rate.</p> <p>We know our model is performing well based on the evaluation metrics but let's see how it performs visually.</p> <div class=highlight><pre><span></span><code><span class=c1># Plot the decision boundaries for the training and test sets</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Train&quot;</span><span class=p>)</span>
<span class=n>plot_decision_boundary</span><span class=p>(</span><span class=n>model_8</span><span class=p>,</span> <span class=n>X</span><span class=o>=</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>y_train</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Test&quot;</span><span class=p>)</span>
<span class=n>plot_decision_boundary</span><span class=p>(</span><span class=n>model_8</span><span class=p>,</span> <span class=n>X</span><span class=o>=</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>y_test</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m313/313[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 2ms/step
doing binary classifcation...
[1m313/313[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 2ms/step
doing binary classifcation...
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_89_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_89_1.png></a></p> <p>Check that out! How cool. With a few tweaks, our model is now predicting the blue and red circles almost perfectly.</p> <h3 id=plot-the-loss-curves>Plot the loss curves</h3> <p>Looking at the plots above, we can see the outputs of our model are very good.</p> <p>But how did our model go whilst it was learning?</p> <p>As in, how did the performance change everytime the model had a chance to look at the data (once every epoch)?</p> <p>To figure this out, we can check the <strong>loss curves</strong> (also referred to as the <strong>learning curves</strong>).</p> <p>You might've seen we've been using the variable <code>history</code> when calling the <code>fit()</code> function on a model (<a href=https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit><code>fit()</code> returns a <code>History</code> object</a>).</p> <p>This is where we'll get the information for how our model is performing as it learns.</p> <p>Let's see how we might use it.</p> <div class=highlight><pre><span></span><code><span class=c1># You can access the information in the history variable using the .history attribute</span>
<span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>)</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>accuracy</th> <th>loss</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>0.43375</td> <td>0.707851</td> </tr> <tr> <th>1</th> <td>0.49875</td> <td>0.694228</td> </tr> <tr> <th>2</th> <td>0.48875</td> <td>0.692185</td> </tr> <tr> <th>3</th> <td>0.51625</td> <td>0.690773</td> </tr> <tr> <th>4</th> <td>0.51750</td> <td>0.689282</td> </tr> <tr> <th>5</th> <td>0.53000</td> <td>0.687196</td> </tr> <tr> <th>6</th> <td>0.53625</td> <td>0.684352</td> </tr> <tr> <th>7</th> <td>0.54625</td> <td>0.679538</td> </tr> <tr> <th>8</th> <td>0.56375</td> <td>0.668187</td> </tr> <tr> <th>9</th> <td>0.59875</td> <td>0.652840</td> </tr> <tr> <th>10</th> <td>0.62625</td> <td>0.638076</td> </tr> <tr> <th>11</th> <td>0.64875</td> <td>0.622055</td> </tr> <tr> <th>12</th> <td>0.67750</td> <td>0.605658</td> </tr> <tr> <th>13</th> <td>0.69375</td> <td>0.587679</td> </tr> <tr> <th>14</th> <td>0.71375</td> <td>0.570109</td> </tr> <tr> <th>15</th> <td>0.76500</td> <td>0.531554</td> </tr> <tr> <th>16</th> <td>0.80625</td> <td>0.483048</td> </tr> <tr> <th>17</th> <td>0.82125</td> <td>0.449459</td> </tr> <tr> <th>18</th> <td>0.84875</td> <td>0.419913</td> </tr> <tr> <th>19</th> <td>0.86625</td> <td>0.395091</td> </tr> <tr> <th>20</th> <td>0.87875</td> <td>0.375061</td> </tr> <tr> <th>21</th> <td>0.88250</td> <td>0.360068</td> </tr> <tr> <th>22</th> <td>0.88500</td> <td>0.347148</td> </tr> <tr> <th>23</th> <td>0.88250</td> <td>0.337774</td> </tr> <tr> <th>24</th> <td>0.89000</td> <td>0.328164</td> </tr> </tbody> </table> </div> <p>Inspecting the outputs, we can see the loss values going down and the accuracy going up.</p> <p>How's it look (visualize, visualize, visualize)?</p> <div class=highlight><pre><span></span><code><span class=c1># Plot the loss curves</span>
<span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>)</span><span class=o>.</span><span class=n>plot</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Model_8 training curves&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Text(0.5, 1.0, &#39;Model_8 training curves&#39;)
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_94_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_94_1.png></a></p> <p>Beautiful. This is the ideal plot we'd be looking for when dealing with a classification problem, loss going down, accuracy going up.</p> <blockquote> <p>🔑 <strong>Note:</strong> For many problems, the loss function going down means the model is improving (the predictions it's making are getting closer to the ground truth labels).</p> </blockquote> <h3 id=finding-the-best-learning-rate>Finding the best learning rate</h3> <p>Aside from the architecture itself (the layers, number of neurons, activations, etc), the most important hyperparameter you can tune for your neural network models is the <strong>learning rate</strong>.</p> <p>In <code>model_8</code> you saw we lowered the Adam optimizer's learning rate from the default of <code>0.001</code> (default) to <code>0.01</code>.</p> <p>And you might be wondering why we did this.</p> <p>Put it this way, it was a lucky guess.</p> <p>I just decided to try a lower learning rate and see how the model went.</p> <p>Now you might be thinking, "Seriously? You can do that?"</p> <p>And the answer is yes. You can change any of the hyperparamaters of your neural networks.</p> <p>With practice, you'll start to see what kind of hyperparameters work and what don't.</p> <p>That's an important thing to understand about machine learning and deep learning in general. It's very experimental. You build a model and evaluate it, build a model and evaluate it.</p> <p>That being said, I want to introduce you a trick which will help you find the optimal learning rate (at least to begin training with) for your models going forward.</p> <p>To do so, we're going to use the following: * A <a href=https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler>learning rate <strong>callback</strong></a>. * You can think of a callback as an extra piece of functionality you can add to your model <em>while</em> its training. * Another model (we could use the same ones as above, we we're practicing building models here). * A modified loss curves plot.</p> <p>We'll go through each with code, then explain what's going on.</p> <blockquote> <p>🔑 <strong>Note:</strong> The default hyperparameters of many neural network building blocks in TensorFlow are setup in a way which usually work right out of the box (e.g. the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam>Adam optimizer's</a> default settings can usually get good results on many datasets). So it's a good idea to try the defaults first, then adjust as needed.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create a model (same as model_8)</span>
<span class=n>model_9</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;sigmoid&quot;</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_9</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s2>&quot;binary_crossentropy&quot;</span><span class=p>,</span> <span class=c1># we can use strings here too</span>
              <span class=n>optimizer</span><span class=o>=</span><span class=s2>&quot;Adam&quot;</span><span class=p>,</span> <span class=c1># same as tf.keras.optimizers.Adam() with default settings</span>
              <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;accuracy&quot;</span><span class=p>])</span>

<span class=c1># Create a learning rate scheduler callback</span>
<span class=n>lr_scheduler</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>LearningRateScheduler</span><span class=p>(</span><span class=k>lambda</span> <span class=n>epoch</span><span class=p>:</span> <span class=mf>1e-4</span> <span class=o>*</span> <span class=mi>10</span><span class=o>**</span><span class=p>(</span><span class=n>epoch</span><span class=o>/</span><span class=mi>20</span><span class=p>))</span> <span class=c1># traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch</span>

<span class=c1># Fit the model (passing the lr_scheduler callback)</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>model_9</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span>
                      <span class=n>y_train</span><span class=p>,</span>
                      <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
                      <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>lr_scheduler</span><span class=p>])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.4833 - loss: 0.6941 - learning_rate: 1.0000e-04
Epoch 2/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4852 - loss: 0.6940 - learning_rate: 1.1220e-04
Epoch 3/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4910 - loss: 0.6938 - learning_rate: 1.2589e-04
Epoch 4/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4915 - loss: 0.6936 - learning_rate: 1.4125e-04
Epoch 5/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4861 - loss: 0.6935 - learning_rate: 1.5849e-04
Epoch 6/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4882 - loss: 0.6933 - learning_rate: 1.7783e-04
Epoch 7/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4971 - loss: 0.6931 - learning_rate: 1.9953e-04
Epoch 8/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4999 - loss: 0.6928 - learning_rate: 2.2387e-04
Epoch 9/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5031 - loss: 0.6926 - learning_rate: 2.5119e-04
Epoch 10/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4975 - loss: 0.6923 - learning_rate: 2.8184e-04
Epoch 11/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4963 - loss: 0.6920 - learning_rate: 3.1623e-04
Epoch 12/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4991 - loss: 0.6917 - learning_rate: 3.5481e-04
Epoch 13/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5010 - loss: 0.6913 - learning_rate: 3.9811e-04
Epoch 14/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5177 - loss: 0.6910 - learning_rate: 4.4668e-04
Epoch 15/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5253 - loss: 0.6906 - learning_rate: 5.0119e-04
Epoch 16/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5330 - loss: 0.6901 - learning_rate: 5.6234e-04
Epoch 17/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5289 - loss: 0.6896 - learning_rate: 6.3096e-04
Epoch 18/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5392 - loss: 0.6891 - learning_rate: 7.0795e-04
Epoch 19/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5372 - loss: 0.6886 - learning_rate: 7.9433e-04
Epoch 20/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5464 - loss: 0.6879 - learning_rate: 8.9125e-04
Epoch 21/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5464 - loss: 0.6873 - learning_rate: 0.0010
Epoch 22/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5428 - loss: 0.6865 - learning_rate: 0.0011
Epoch 23/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5475 - loss: 0.6857 - learning_rate: 0.0013
Epoch 24/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5520 - loss: 0.6846 - learning_rate: 0.0014
Epoch 25/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5502 - loss: 0.6836 - learning_rate: 0.0016
Epoch 26/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5615 - loss: 0.6824 - learning_rate: 0.0018
Epoch 27/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5743 - loss: 0.6809 - learning_rate: 0.0020
Epoch 28/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5765 - loss: 0.6795 - learning_rate: 0.0022
Epoch 29/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5904 - loss: 0.6777 - learning_rate: 0.0025
Epoch 30/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5962 - loss: 0.6757 - learning_rate: 0.0028
Epoch 31/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5942 - loss: 0.6733 - learning_rate: 0.0032
Epoch 32/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5907 - loss: 0.6704 - learning_rate: 0.0035
Epoch 33/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5904 - loss: 0.6669 - learning_rate: 0.0040
Epoch 34/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5972 - loss: 0.6629 - learning_rate: 0.0045
Epoch 35/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6077 - loss: 0.6575 - learning_rate: 0.0050
Epoch 36/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6330 - loss: 0.6469 - learning_rate: 0.0056
Epoch 37/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6652 - loss: 0.6245 - learning_rate: 0.0063
Epoch 38/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7217 - loss: 0.5752 - learning_rate: 0.0071
Epoch 39/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7821 - loss: 0.5039 - learning_rate: 0.0079
Epoch 40/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8655 - loss: 0.4148 - learning_rate: 0.0089
Epoch 41/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.8984 - loss: 0.3350 - learning_rate: 0.0100
Epoch 42/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9069 - loss: 0.2828 - learning_rate: 0.0112
Epoch 43/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9246 - loss: 0.2388 - learning_rate: 0.0126
Epoch 44/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9394 - loss: 0.2067 - learning_rate: 0.0141
Epoch 45/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9589 - loss: 0.1811 - learning_rate: 0.0158
Epoch 46/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9509 - loss: 0.1679 - learning_rate: 0.0178
Epoch 47/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9583 - loss: 0.1471 - learning_rate: 0.0200
Epoch 48/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9602 - loss: 0.1298 - learning_rate: 0.0224
Epoch 49/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9649 - loss: 0.1207 - learning_rate: 0.0251
Epoch 50/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9642 - loss: 0.1140 - learning_rate: 0.0282
Epoch 51/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9499 - loss: 0.1238 - learning_rate: 0.0316
Epoch 52/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9611 - loss: 0.1096 - learning_rate: 0.0355
Epoch 53/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9554 - loss: 0.1185 - learning_rate: 0.0398
Epoch 54/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9656 - loss: 0.0894 - learning_rate: 0.0447
Epoch 55/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9559 - loss: 0.1324 - learning_rate: 0.0501
Epoch 56/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9162 - loss: 0.2374 - learning_rate: 0.0562
Epoch 57/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9075 - loss: 0.3001 - learning_rate: 0.0631
Epoch 58/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9578 - loss: 0.1188 - learning_rate: 0.0708
Epoch 59/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9605 - loss: 0.1295 - learning_rate: 0.0794
Epoch 60/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - accuracy: 0.9717 - loss: 0.1014 - learning_rate: 0.0891
Epoch 61/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9619 - loss: 0.1088 - learning_rate: 0.1000
Epoch 62/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9557 - loss: 0.1247 - learning_rate: 0.1122
Epoch 63/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8971 - loss: 0.3277 - learning_rate: 0.1259
Epoch 64/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9014 - loss: 0.3194 - learning_rate: 0.1413
Epoch 65/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9405 - loss: 0.1477 - learning_rate: 0.1585
Epoch 66/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9614 - loss: 0.1023 - learning_rate: 0.1778
Epoch 67/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.7683 - loss: 0.8885 - learning_rate: 0.1995
Epoch 68/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8840 - loss: 0.3150 - learning_rate: 0.2239
Epoch 69/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9152 - loss: 0.2059 - learning_rate: 0.2512
Epoch 70/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8798 - loss: 0.3155 - learning_rate: 0.2818
Epoch 71/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9302 - loss: 0.1901 - learning_rate: 0.3162
Epoch 72/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8791 - loss: 0.3337 - learning_rate: 0.3548
Epoch 73/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7443 - loss: 0.4584 - learning_rate: 0.3981
Epoch 74/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7971 - loss: 0.4335 - learning_rate: 0.4467
Epoch 75/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.8514 - loss: 0.2845 - learning_rate: 0.5012
Epoch 76/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8404 - loss: 0.3574 - learning_rate: 0.5623
Epoch 77/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8778 - loss: 0.3084 - learning_rate: 0.6310
Epoch 78/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8180 - loss: 0.3658 - learning_rate: 0.7079
Epoch 79/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7148 - loss: 0.6216 - learning_rate: 0.7943
Epoch 80/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5139 - loss: 0.7005 - learning_rate: 0.8913
Epoch 81/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5143 - loss: 0.7190 - learning_rate: 1.0000
Epoch 82/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5281 - loss: 0.7205 - learning_rate: 1.1220
Epoch 83/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5281 - loss: 0.7096 - learning_rate: 1.2589
Epoch 84/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5096 - loss: 0.7046 - learning_rate: 1.4125
Epoch 85/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5056 - loss: 0.7081 - learning_rate: 1.5849
Epoch 86/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4885 - loss: 0.7155 - learning_rate: 1.7783
Epoch 87/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5006 - loss: 0.7280 - learning_rate: 1.9953
Epoch 88/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5006 - loss: 0.7439 - learning_rate: 2.2387
Epoch 89/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4899 - loss: 0.7570 - learning_rate: 2.5119
Epoch 90/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5003 - loss: 0.7507 - learning_rate: 2.8184
Epoch 91/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4978 - loss: 0.7172 - learning_rate: 3.1623
Epoch 92/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4781 - loss: 0.7097 - learning_rate: 3.5481
Epoch 93/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4798 - loss: 0.7143 - learning_rate: 3.9811
Epoch 94/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.4732 - loss: 0.7217 - learning_rate: 4.4668
Epoch 95/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4732 - loss: 0.7360 - learning_rate: 5.0119
Epoch 96/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4849 - loss: 0.7729 - learning_rate: 5.6234
Epoch 97/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4844 - loss: 0.8143 - learning_rate: 6.3096
Epoch 98/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4892 - loss: 0.8602 - learning_rate: 7.0795
Epoch 99/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4892 - loss: 0.9049 - learning_rate: 7.9433
Epoch 100/100
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.4892 - loss: 0.9503 - learning_rate: 8.9125
</code></pre></div> <p>Now our model has finished training, let's have a look at the training history.</p> <div class=highlight><pre><span></span><code><span class=c1># Checkout the history</span>
<span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>)</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span><span class=mi>7</span><span class=p>),</span> <span class=n>xlabel</span><span class=o>=</span><span class=s2>&quot;epochs&quot;</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_99_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_99_0.png></a></p> <p>As you you see the learning rate exponentially increases as the number of epochs increases.</p> <p>And you can see the model's accuracy goes up (and loss goes down) at a specific point when the learning rate slowly increases.</p> <p>To figure out where this infliction point is, we can plot the loss versus the log-scale learning rate.</p> <div class=highlight><pre><span></span><code><span class=c1># Plot the learning rate versus the loss</span>
<span class=n>lrs</span> <span class=o>=</span> <span class=mf>1e-4</span> <span class=o>*</span> <span class=p>(</span><span class=mi>10</span> <span class=o>**</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>100</span><span class=p>)</span><span class=o>/</span><span class=mi>20</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>semilogx</span><span class=p>(</span><span class=n>lrs</span><span class=p>,</span> <span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s2>&quot;loss&quot;</span><span class=p>])</span> <span class=c1># we want the x-axis (learning rate) to be log scale</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Learning Rate&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;Loss&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Learning rate vs. loss&quot;</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_101_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_101_0.png></a></p> <p>To figure out the ideal value of the learning rate (at least the ideal value to <em>begin</em> training our model), the rule of thumb is to take the learning rate value where the loss is still decreasing but not quite flattened out (usually about 10x smaller than the bottom of the curve).</p> <p>In this case, our ideal learning rate ends up between <code>0.01</code> (<span class=arithmatex>\(10^{-2}\)</span>) and <code>0.02</code>.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-learning-rate-vs-loss.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="finding the ideal learning rate by plotting learning rate vs. loss" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-learning-rate-vs-loss.png></a></p> <p><em>The ideal learning rate at the start of model training is somewhere just before the loss curve bottoms out (a value where the loss is still decreasing).</em></p> <div class=highlight><pre><span></span><code><span class=c1># Example of other typical learning rate values</span>
<span class=mi>10</span><span class=o>**</span><span class=mi>0</span><span class=p>,</span> <span class=mi>10</span><span class=o>**-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=o>**-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>10</span><span class=o>**-</span><span class=mi>3</span><span class=p>,</span> <span class=mf>1e-4</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(1, 0.1, 0.01, 0.001, 0.0001)
</code></pre></div> <p>Now we've estimated the ideal learning rate (we'll use <code>0.02</code>) for our model, let's refit it.</p> <div class=highlight><pre><span></span><code><span class=c1># Set the random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create the model</span>
<span class=n>model_10</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;sigmoid&quot;</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Compile the model with the ideal learning rate</span>
<span class=n>model_10</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s2>&quot;binary_crossentropy&quot;</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.02</span><span class=p>),</span> <span class=c1># to adjust the learning rate, you need to use tf.keras.optimizers.Adam (not &quot;adam&quot;)</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;accuracy&quot;</span><span class=p>])</span>

<span class=c1># Fit the model for 20 epochs (5 less than before)</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>model_10</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 3ms/step - accuracy: 0.5438 - loss: 0.6876
Epoch 2/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5670 - loss: 0.6777
Epoch 3/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5740 - loss: 0.6697
Epoch 4/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.5743 - loss: 0.6605
Epoch 5/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.5904 - loss: 0.6495
Epoch 6/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.6109 - loss: 0.6349
Epoch 7/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.6123 - loss: 0.6221
Epoch 8/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.6770 - loss: 0.5789
Epoch 9/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7165 - loss: 0.5212 
Epoch 10/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.7473 - loss: 0.4530 
Epoch 11/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8028 - loss: 0.3890
Epoch 12/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.8546 - loss: 0.3222
Epoch 13/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9093 - loss: 0.2580 
Epoch 14/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9508 - loss: 0.2018
Epoch 15/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9624 - loss: 0.1617 
Epoch 16/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9666 - loss: 0.1407 
Epoch 17/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9713 - loss: 0.1266
Epoch 18/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9710 - loss: 0.1212
Epoch 19/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 2ms/step - accuracy: 0.9714 - loss: 0.1165
Epoch 20/20
[1m25/25[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9784 - loss: 0.1051
</code></pre></div> <p>Nice! With a little higher learning rate (<code>0.02</code> instead of <code>0.01</code>) we reach a higher accuracy than <code>model_8</code> in less epochs (<code>20</code> instead of <code>25</code>).</p> <blockquote> <p>🛠 <strong>Practice:</strong> Now you've seen an example of what can happen when you change the learning rate, try changing the learning rate value in the <a href="https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.03154&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&regularizationRate_hide=true&problem_hide=true">TensorFlow Playground</a> and see what happens. What happens if you increase it? What happens if you decrease it?</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Evaluate model on the test dataset</span>
<span class=n>model_10</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m7/7[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9745 - loss: 0.0957





[0.0894651785492897, 0.9750000238418579]
</code></pre></div> <p>Let's see how the predictions look.</p> <div class=highlight><pre><span></span><code><span class=c1># Plot the decision boundaries for the training and test sets</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Train&quot;</span><span class=p>)</span>
<span class=n>plot_decision_boundary</span><span class=p>(</span><span class=n>model_10</span><span class=p>,</span> <span class=n>X</span><span class=o>=</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>y_train</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Test&quot;</span><span class=p>)</span>
<span class=n>plot_decision_boundary</span><span class=p>(</span><span class=n>model_10</span><span class=p>,</span> <span class=n>X</span><span class=o>=</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>y_test</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m313/313[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 2ms/step
doing binary classifcation...
[1m313/313[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 2ms/step
doing binary classifcation...
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_109_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_109_1.png></a></p> <p>And as we can see, almost perfect again.</p> <p>These are the kind of experiments you'll be running often when building your own models.</p> <p>Start with default settings and see how they perform on your data.</p> <p>And if they don't perform as well as you'd like, improve them.</p> <p>Let's look at a few more ways to evaluate our classification models.</p> <h3 id=more-classification-evaluation-methods>More classification evaluation methods</h3> <p>Alongside the visualizations we've been making, there are a number of different evaluation metrics we can use to evaluate our classification models.</p> <table> <thead> <tr> <th><strong>Metric name/Evaluation method</strong></th> <th><strong>Defintion</strong></th> <th><strong>Code</strong></th> </tr> </thead> <tbody> <tr> <td>Accuracy</td> <td>Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct.</td> <td><a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html><code>sklearn.metrics.accuracy_score()</code></a> or <a href=tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy><code>tf.keras.metrics.Accuracy()</code></a></td> </tr> <tr> <td>Precision</td> <td>Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0).</td> <td><a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html><code>sklearn.metrics.precision_score()</code></a> or <a href=tensorflow.org/api_docs/python/tf/keras/metrics/Precision><code>tf.keras.metrics.Precision()</code></a></td> </tr> <tr> <td>Recall</td> <td>Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives.</td> <td><a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html><code>sklearn.metrics.recall_score()</code></a> or <a href=tensorflow.org/api_docs/python/tf/keras/metrics/Recall><code>tf.keras.metrics.Recall()</code></a></td> </tr> <tr> <td>F1-score</td> <td>Combines precision and recall into one metric. 1 is best, 0 is worst.</td> <td><a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html><code>sklearn.metrics.f1_score()</code></a></td> </tr> <tr> <td><a href=https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/ >Confusion matrix</a></td> <td>Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line).</td> <td>Custom function or <a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html><code>sklearn.metrics.plot_confusion_matrix()</code></a></td> </tr> <tr> <td>Classification report</td> <td>Collection of some of the main classification metrics such as precision, recall and f1-score.</td> <td><a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html><code>sklearn.metrics.classification_report()</code></a></td> </tr> </tbody> </table> <blockquote> <p>🔑 <strong>Note:</strong> Every classification problem will require different kinds of evaluation methods. But you should be familiar with at least the ones above.</p> </blockquote> <p>Let's start with accuracy.</p> <p>Because we passed <code>["accuracy"]</code> to the <code>metrics</code> parameter when we compiled our model, calling <code>evaluate()</code> on it will return the loss as well as accuracy.</p> <div class=highlight><pre><span></span><code><span class=c1># Check the accuracy of our model</span>
<span class=n>loss</span><span class=p>,</span> <span class=n>accuracy</span> <span class=o>=</span> <span class=n>model_10</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Model loss on test set: </span><span class=si>{</span><span class=n>loss</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Model accuracy on test set: </span><span class=si>{</span><span class=p>(</span><span class=n>accuracy</span><span class=o>*</span><span class=mi>100</span><span class=p>)</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m7/7[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - accuracy: 0.9745 - loss: 0.0957 
Model loss on test set: 0.0894651785492897
Model accuracy on test set: 97.50%
</code></pre></div> <p>How about a confusion matrix?</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-anatomy-of-a-confusion-matrix.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="anatomy of a confusion matrix" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-anatomy-of-a-confusion-matrix.png></a> <em>Anatomy of a confusion matrix (what we're going to be creating). Correct predictions appear down the diagonal (from top left to bottom right).</em></p> <p>We can make a confusion matrix using <a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html>Scikit-Learn's <code>confusion_matrix</code></a> method.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a confusion matrix</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>confusion_matrix</span>

<span class=c1># Make predictions</span>
<span class=n>y_preds</span> <span class=o>=</span> <span class=n>model_10</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=c1># Create confusion matrix</span>
<span class=n>confusion_matrix</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_preds</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m7/7[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step



---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

Cell In[67], line 8
      5 y_preds = model_10.predict(X_test)
      7 # Create confusion matrix
----&gt; 8 confusion_matrix(y_test, y_preds)


File D:\anaconda\envs\py3-TF2.0\Lib\site-packages\sklearn\utils\_param_validation.py:213, in validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)
    207 try:
    208     with config_context(
    209         skip_parameter_validation=(
    210             prefer_skip_nested_validation or global_skip_validation
    211         )
    212     ):
--&gt; 213         return func(*args, **kwargs)
    214 except InvalidParameterError as e:
    215     # When the function is just a wrapper around an estimator, we allow
    216     # the function to delegate validation to the estimator, but we replace
    217     # the name of the estimator by the name of the function in the error
    218     # message to avoid confusion.
    219     msg = re.sub(
    220         r&quot;parameter of \w+ must be&quot;,
    221         f&quot;parameter of {func.__qualname__} must be&quot;,
    222         str(e),
    223     )


File D:\anaconda\envs\py3-TF2.0\Lib\site-packages\sklearn\metrics\_classification.py:319, in confusion_matrix(y_true, y_pred, labels, sample_weight, normalize)
    224 @validate_params(
    225     {
    226         &quot;y_true&quot;: [&quot;array-like&quot;],
   (...)
    235     y_true, y_pred, *, labels=None, sample_weight=None, normalize=None
    236 ):
    237     &quot;&quot;&quot;Compute confusion matrix to evaluate the accuracy of a classification.
    238 
    239     By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`
   (...)
    317     (0, 2, 1, 1)
    318     &quot;&quot;&quot;
--&gt; 319     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    320     if y_type not in (&quot;binary&quot;, &quot;multiclass&quot;):
    321         raise ValueError(&quot;%s is not supported&quot; % y_type)


File D:\anaconda\envs\py3-TF2.0\Lib\site-packages\sklearn\metrics\_classification.py:94, in _check_targets(y_true, y_pred)
     91     y_type = {&quot;multiclass&quot;}
     93 if len(y_type) &gt; 1:
---&gt; 94     raise ValueError(
     95         &quot;Classification metrics can&#39;t handle a mix of {0} and {1} targets&quot;.format(
     96             type_true, type_pred
     97         )
     98     )
    100 # We can&#39;t have more than one value on y_type =&gt; The set is no more needed
    101 y_type = y_type.pop()


ValueError: Classification metrics can&#39;t handle a mix of binary and continuous targets
</code></pre></div> <p>Ahh, it seems our predictions aren't in the format they need to be.</p> <p>Let's check them out.</p> <div class=highlight><pre><span></span><code><span class=c1># View the first 10 predictions</span>
<span class=n>y_preds</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([[9.5806485e-01],
       [9.9525082e-01],
       [9.9885440e-01],
       [9.7500271e-01],
       [1.0511072e-01],
       [1.7831451e-04],
       [9.9182343e-01],
       [2.9515686e-05],
       [9.6916127e-01],
       [1.7080535e-04]], dtype=float32)
</code></pre></div> <p>What about our test labels?</p> <div class=highlight><pre><span></span><code><span class=c1># View the first 10 test labels</span>
<span class=n>y_test</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([1, 1, 1, 1, 0, 0, 1, 0, 1, 0], dtype=int64)
</code></pre></div> <p>It looks like we need to get our predictions into the binary format (0 or 1).</p> <p>But you might be wondering, what format are they currently in?</p> <p>In their current format (<code>9.8526537e-01</code>), they're in a form called <strong>prediction probabilities</strong>.</p> <p>You'll see this often with the outputs of neural networks. Often they won't be exact values but more a probability of how <em>likely</em> they are to be one value or another.</p> <p>So one of the steps you'll often see after making predicitons with a neural network is converting the prediction probabilities into labels.</p> <p>In our case, since our ground truth labels (<code>y_test</code>) are binary (0 or 1), we can convert the prediction probabilities using to their binary form using <a href=https://www.tensorflow.org/api_docs/python/tf/math/round><code>tf.round()</code></a>.</p> <div class=highlight><pre><span></span><code><span class=c1># Convert prediction probabilities to binary format and view the first 10</span>
<span class=n>tf</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>y_preds</span><span class=p>)[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(10, 1), dtype=float32, numpy=
array([[1.],
       [1.],
       [1.],
       [1.],
       [0.],
       [0.],
       [1.],
       [0.],
       [1.],
       [0.]], dtype=float32)&gt;
</code></pre></div> <p>Wonderful! Now we can use the <code>confusion_matrix</code> function.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a confusion matrix</span>
<span class=n>confusion_matrix</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>y_preds</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([[98,  3],
       [ 2, 97]], dtype=int64)
</code></pre></div> <p>Alright, we can see the highest numbers are down the diagonal (from top left to bottom right) so this a good sign, but the rest of the matrix doesn't really tell us much.</p> <p>How about we make a function to make our confusion matrix a little more visual?</p> <div class=highlight><pre><span></span><code><span class=c1># Note: The following confusion matrix code is a remix of Scikit-Learn&#39;s</span>
<span class=c1># plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html</span>
<span class=c1># and Made with ML&#39;s introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb</span>
<span class=kn>import</span><span class=w> </span><span class=nn>itertools</span>

<span class=n>figsize</span> <span class=o>=</span> <span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>

<span class=c1># Create the confusion matrix</span>
<span class=n>cm</span> <span class=o>=</span> <span class=n>confusion_matrix</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>y_preds</span><span class=p>))</span>
<span class=n>cm_norm</span> <span class=o>=</span> <span class=n>cm</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s2>&quot;float&quot;</span><span class=p>)</span> <span class=o>/</span> <span class=n>cm</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)[:,</span> <span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>]</span> <span class=c1># normalize it</span>
<span class=n>n_classes</span> <span class=o>=</span> <span class=n>cm</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>

<span class=c1># Let&#39;s prettify it</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=n>figsize</span><span class=p>)</span>
<span class=c1># Create a matrix plot</span>
<span class=n>cax</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>matshow</span><span class=p>(</span><span class=n>cm</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>Blues</span><span class=p>)</span> <span class=c1># https://matplotlib.org/3.2.0/api/_as_gen/matplotlib.axes.Axes.matshow.html</span>
<span class=n>fig</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>cax</span><span class=p>)</span>

<span class=c1># Create classes</span>
<span class=n>classes</span> <span class=o>=</span> <span class=kc>False</span>

<span class=k>if</span> <span class=n>classes</span><span class=p>:</span>
  <span class=n>labels</span> <span class=o>=</span> <span class=n>classes</span>
<span class=k>else</span><span class=p>:</span>
  <span class=n>labels</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>cm</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>

<span class=c1># Label the axes</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=n>title</span><span class=o>=</span><span class=s2>&quot;Confusion Matrix&quot;</span><span class=p>,</span>
       <span class=n>xlabel</span><span class=o>=</span><span class=s2>&quot;Predicted label&quot;</span><span class=p>,</span>
       <span class=n>ylabel</span><span class=o>=</span><span class=s2>&quot;True label&quot;</span><span class=p>,</span>
       <span class=n>xticks</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>n_classes</span><span class=p>),</span>
       <span class=n>yticks</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>n_classes</span><span class=p>),</span>
       <span class=n>xticklabels</span><span class=o>=</span><span class=n>labels</span><span class=p>,</span>
       <span class=n>yticklabels</span><span class=o>=</span><span class=n>labels</span><span class=p>)</span>

<span class=c1># Set x-axis labels to bottom</span>
<span class=n>ax</span><span class=o>.</span><span class=n>xaxis</span><span class=o>.</span><span class=n>set_label_position</span><span class=p>(</span><span class=s2>&quot;bottom&quot;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>xaxis</span><span class=o>.</span><span class=n>tick_bottom</span><span class=p>()</span>

<span class=c1># Adjust label size</span>
<span class=n>ax</span><span class=o>.</span><span class=n>xaxis</span><span class=o>.</span><span class=n>label</span><span class=o>.</span><span class=n>set_size</span><span class=p>(</span><span class=mi>20</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>yaxis</span><span class=o>.</span><span class=n>label</span><span class=o>.</span><span class=n>set_size</span><span class=p>(</span><span class=mi>20</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>title</span><span class=o>.</span><span class=n>set_size</span><span class=p>(</span><span class=mi>20</span><span class=p>)</span>

<span class=c1># Set threshold for different colors</span>
<span class=n>threshold</span> <span class=o>=</span> <span class=p>(</span><span class=n>cm</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=n>cm</span><span class=o>.</span><span class=n>min</span><span class=p>())</span> <span class=o>/</span> <span class=mf>2.</span>

<span class=c1># Plot the text on each cell</span>
<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>j</span> <span class=ow>in</span> <span class=n>itertools</span><span class=o>.</span><span class=n>product</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>cm</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=nb>range</span><span class=p>(</span><span class=n>cm</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>])):</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>j</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>cm</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=w> </span><span class=n>j</span><span class=p>]</span><span class=si>}</span><span class=s2> (</span><span class=si>{</span><span class=n>cm_norm</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=w> </span><span class=n>j</span><span class=p>]</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%)&quot;</span><span class=p>,</span>
           <span class=n>horizontalalignment</span><span class=o>=</span><span class=s2>&quot;center&quot;</span><span class=p>,</span>
           <span class=n>color</span><span class=o>=</span><span class=s2>&quot;white&quot;</span> <span class=k>if</span> <span class=n>cm</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>threshold</span> <span class=k>else</span> <span class=s2>&quot;black&quot;</span><span class=p>,</span>
           <span class=n>size</span><span class=o>=</span><span class=mi>15</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_124_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_124_0.png></a></p> <p>That looks much better. It seems our model has made almost perfect predictions on the test set except for two false positives (top right corner).</p> <div class=highlight><pre><span></span><code><span class=c1># What does itertools.product do? Combines two things into each combination</span>
<span class=kn>import</span><span class=w> </span><span class=nn>itertools</span>
<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>j</span> <span class=ow>in</span> <span class=n>itertools</span><span class=o>.</span><span class=n>product</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>cm</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=nb>range</span><span class=p>(</span><span class=n>cm</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>])):</span>
  <span class=nb>print</span><span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>0 0
0 1
1 0
1 1
</code></pre></div> <h2 id=working-with-a-larger-example-multiclass-classification>Working with a larger example (multiclass classification)</h2> <p>We've seen a binary classification example (predicting if a data point is part of a red circle or blue circle) but what if you had multiple different classes of things?</p> <p>For example, say you were a fashion company and you wanted to build a neural network to predict whether a piece of clothing was a shoe, a shirt or a jacket (3 different options).</p> <p>When you have more than two classes as an option, this is known as <strong>multiclass classification</strong>.</p> <p>The good news is, the things we've learned so far (with a few tweaks) can be applied to multiclass classification problems as well.</p> <p>Let's see it in action.</p> <p>To start, we'll need some data. The good thing for us is TensorFlow has a multiclass classication dataset known as <a href=https://github.com/zalandoresearch/fashion-mnist>Fashion MNIST built-in</a>. Meaning we can get started straight away.</p> <p>We can import it using the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/datasets><code>tf.keras.datasets</code></a> module.</p> <blockquote> <p>📖 <strong>Resource:</strong> The following multiclass classification problem has been adapted from the <a href=https://www.tensorflow.org/tutorials/keras/classification>TensorFlow classification guide</a>. A good exercise would be to once you've gone through the following example, replicate the TensorFlow guide.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>
<span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>fashion_mnist</span>

<span class=c1># The data has already been sorted into training and test sets for us</span>
<span class=p>(</span><span class=n>train_data</span><span class=p>,</span> <span class=n>train_labels</span><span class=p>),</span> <span class=p>(</span><span class=n>test_data</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>)</span> <span class=o>=</span> <span class=n>fashion_mnist</span><span class=o>.</span><span class=n>load_data</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz
[1m29515/29515[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz
[1m26421880/26421880[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m40s[0m 2us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz
[1m5148/5148[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 1us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz
[1m4422102/4422102[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m7s[0m 1us/step
</code></pre></div> <p>Now let's check out an example.</p> <div class=highlight><pre><span></span><code><span class=c1># Show the first training example</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Training sample:</span><span class=se>\n</span><span class=si>{</span><span class=n>train_data</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=se>\n</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Training label: </span><span class=si>{</span><span class=n>train_labels</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Training sample:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0
    0   1   4   0   0   0   0   1   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62
   54   0   0   0   1   3   4   0   0   3]
 [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134
  144 123  23   0   0   0   0  12  10   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178
  107 156 161 109  64  23  77 130  72  15]
 [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216
  216 163 127 121 122 146 141  88 172  66]
 [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229
  223 223 215 213 164 127 123 196 229   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228
  235 227 224 222 224 221 223 245 173   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198
  180 212 210 211 213 223 220 243 202   0]
 [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192
  169 227 208 218 224 212 226 197 209  52]
 [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203
  198 221 215 213 222 220 245 119 167  56]
 [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240
  232 213 218 223 234 217 217 209  92   0]
 [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219
  222 221 216 223 229 215 218 255  77   0]
 [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208
  211 218 224 223 219 215 224 244 159   0]
 [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230
  224 234 176 188 250 248 233 238 215   0]
 [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223
  255 255 221 234 221 211 220 232 246   0]
 [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221
  188 154 191 210 204 209 222 228 225   0]
 [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117
  168 219 221 215 217 223 223 224 229  29]
 [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245
  239 223 218 212 209 222 220 221 230  67]
 [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216
  199 206 186 181 177 172 181 205 206 115]
 [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191
  195 191 198 192 176 156 167 177 210  92]
 [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209
  210 210 211 188 188 194 192 216 170   0]
 [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179
  182 182 181 176 166 168  99  58   0   0]
 [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0]]

Training label: 9
</code></pre></div> <p>Woah, we get a large list of numbers, followed (the data) by a single number (the class label).</p> <p>What about the shapes?</p> <div class=highlight><pre><span></span><code><span class=c1># Check the shape of our data</span>
<span class=n>train_data</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>train_labels</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>test_data</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>test_labels</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check shape of a single example</span>
<span class=n>train_data</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>train_labels</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>((28, 28), ())
</code></pre></div> <p>Okay, 60,000 training examples each with shape (28, 28) and a label each as well as 10,000 test examples of shape (28, 28).</p> <p>But these are just numbers, let's visualize.</p> <div class=highlight><pre><span></span><code><span class=c1># Plot a single example</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>train_data</span><span class=p>[</span><span class=mi>7</span><span class=p>]);</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_135_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_135_0.png></a></p> <p>Hmm, but what about its label?</p> <div class=highlight><pre><span></span><code><span class=c1># Check our samples label</span>
<span class=n>train_labels</span><span class=p>[</span><span class=mi>7</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>2
</code></pre></div> <p>It looks like our labels are in numerical form. And while this is fine for a neural network, you might want to have them in human readable form.</p> <p>Let's create a small list of the class names (we can find them on <a href=https://github.com/zalandoresearch/fashion-mnist#labels>the dataset's GitHub page</a>).</p> <blockquote> <p>🔑 <strong>Note:</strong> Whilst this dataset has been prepared for us and ready to go, it's important to remember many datasets won't be ready to go like this one. Often you'll have to do a few preprocessing steps to have it ready to use with a neural network (we'll see more of this when we work with our own data later).</p> </blockquote> <div class=highlight><pre><span></span><code><span class=n>class_names</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;T-shirt/top&#39;</span><span class=p>,</span> <span class=s1>&#39;Trouser&#39;</span><span class=p>,</span> <span class=s1>&#39;Pullover&#39;</span><span class=p>,</span> <span class=s1>&#39;Dress&#39;</span><span class=p>,</span> <span class=s1>&#39;Coat&#39;</span><span class=p>,</span>
               <span class=s1>&#39;Sandal&#39;</span><span class=p>,</span> <span class=s1>&#39;Shirt&#39;</span><span class=p>,</span> <span class=s1>&#39;Sneaker&#39;</span><span class=p>,</span> <span class=s1>&#39;Bag&#39;</span><span class=p>,</span> <span class=s1>&#39;Ankle boot&#39;</span><span class=p>]</span>

<span class=c1># How many classes are there (this&#39;ll be our output shape)?</span>
<span class=nb>len</span><span class=p>(</span><span class=n>class_names</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>10
</code></pre></div> <p>Now we have these, let's plot another example.</p> <blockquote> <p>🤔 <strong>Question:</strong> Pay particular attention to what the data we're working with <em>looks</em> like. Is it only straight lines? Or does it have non-straight lines as well? Do you think if we wanted to find patterns in the photos of clothes (which are actually collections of pixels), will our model need non-linearities (non-straight lines) or not?</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Plot an example image and its label</span>
<span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>train_data</span><span class=p>[</span><span class=mi>17</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>binary</span><span class=p>)</span> <span class=c1># change the colours to black &amp; white</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=n>class_names</span><span class=p>[</span><span class=n>train_labels</span><span class=p>[</span><span class=mi>17</span><span class=p>]]);</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_141_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_141_0.png></a></p> <div class=highlight><pre><span></span><code><span class=c1># Plot multiple random images of fashion MNIST</span>
<span class=kn>import</span><span class=w> </span><span class=nn>random</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>7</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>4</span><span class=p>):</span>
  <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
  <span class=n>rand_index</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train_data</span><span class=p>)))</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>train_data</span><span class=p>[</span><span class=n>rand_index</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>binary</span><span class=p>)</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=n>class_names</span><span class=p>[</span><span class=n>train_labels</span><span class=p>[</span><span class=n>rand_index</span><span class=p>]])</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_142_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_142_0.png></a></p> <p>Alright, let's build a model to figure out the relationship between the pixel values and their labels.</p> <p>Since this is a multiclass classification problem, we'll need to make a few changes to our architecture (inline with Table 1 above):</p> <ul> <li>The <strong>input shape</strong> will have to deal with 28x28 tensors (the height and width of our images).</li> <li>We're actually going to squash the input into a tensor (vector) of shape <code>(784)</code>.</li> <li>The <strong>output shape</strong> will have to be 10 because we need our model to predict for 10 different classes.</li> <li>We'll also change the <code>activation</code> parameter of our output layer to be <a href=https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax><code>"softmax"</code></a> instead of <code>'sigmoid'</code>. As we'll see the <code>"softmax"</code> activation function outputs a series of values between 0 &amp; 1 (the same shape as <strong>output shape</strong>, which together add up to ~1. The index with the highest value is predicted by the model to be the most <em>likely</em> class.</li> <li>We'll need to change our loss function from a binary loss function to a multiclass loss function.</li> <li>More specifically, since our labels are in integer form, we'll use <a href=https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy><code>tf.keras.losses.SparseCategoricalCrossentropy()</code></a>, if our labels were one-hot encoded (e.g. they looked something like <code>[0, 0, 1, 0, 0...]</code>), we'd use <a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy><code>tf.keras.losses.CategoricalCrossentropy()</code></a>.</li> <li>We'll also use the <code>validation_data</code> parameter when calling the <code>fit()</code> function. This will give us an idea of how the model performs on the test set during training.</li> </ul> <p>You ready? Let's go.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create the model</span>
<span class=n>model_11</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Flatten</span><span class=p>(</span><span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)),</span> <span class=c1># input layer (we had to reshape 28x28 to 784, the Flatten layer does this for us)</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;softmax&quot;</span><span class=p>)</span> <span class=c1># output shape is 10, activation is softmax</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_11</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>SparseCategoricalCrossentropy</span><span class=p>(),</span> <span class=c1># different loss function for multiclass classifcation</span>
                 <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span>
                 <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;accuracy&quot;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=n>non_norm_history</span> <span class=o>=</span> <span class=n>model_11</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_data</span><span class=p>,</span>
                                <span class=n>train_labels</span><span class=p>,</span>
                                <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
                                <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>test_data</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>))</span> <span class=c1># see how the model performs on the test set during training</span>
</code></pre></div> <div class=highlight><pre><span></span><code>D:\anaconda\envs\py3-TF2.0\Lib\site-packages\keras\src\layers\reshaping\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)


Epoch 1/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m7s[0m 3ms/step - accuracy: 0.0983 - loss: 2.4172 - val_accuracy: 0.1001 - val_loss: 2.3025
Epoch 2/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.0986 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025
Epoch 3/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m6s[0m 3ms/step - accuracy: 0.0986 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025
Epoch 4/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m6s[0m 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025
Epoch 5/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025
Epoch 6/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025
Epoch 7/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025
Epoch 8/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025
Epoch 9/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m6s[0m 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025
Epoch 10/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.0985 - loss: 2.3028 - val_accuracy: 0.1001 - val_loss: 2.3025
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check the shapes of our model</span>
<span class=c1># Note: the &quot;None&quot; in (None, 784) is for batch_size, we&#39;ll cover this in a later module</span>
<span class=n>model_11</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</code></pre></div> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_12"</span>
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                         </span>┃<span style="font-weight: bold"> Output Shape                </span>┃<span style="font-weight: bold">         Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ flatten (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)                    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">784</span>)                 │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_31 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">4</span>)                   │           <span style="color: #00af00; text-decoration-color: #00af00">3,140</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_32 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">4</span>)                   │              <span style="color: #00af00; text-decoration-color: #00af00">20</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_33 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>)                  │              <span style="color: #00af00; text-decoration-color: #00af00">50</span> │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">9,632</span> (37.63 KB)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">3,210</span> (12.54 KB)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">6,422</span> (25.09 KB)
</pre> <p>Alright, our model gets to about ~35% accuracy after 10 epochs using a similar style model to what we used on our binary classification problem.</p> <p>Which is better than guessing (guessing with 10 classes would result in about 10% accuracy) but we can do better.</p> <p>Do you remember when we talked about neural networks preferring numbers between 0 and 1? (if not, treat this as a reminder)</p> <p>Well, right now, the data we have isn't between 0 and 1, in other words, it's not normalized (hence why we used the <code>non_norm_history</code> variable when calling <code>fit()</code>). It's pixel values are between 0 and 255.</p> <p>Let's see.</p> <div class=highlight><pre><span></span><code><span class=c1># Check the min and max values of the training data</span>
<span class=n>train_data</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>train_data</span><span class=o>.</span><span class=n>max</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(0, 255)
</code></pre></div> <p>We can get these values between 0 and 1 by dividing the entire array by the maximum: <code>255.0</code> (dividing by a float also converts to a float).</p> <p>Doing so will result in all of our data being between 0 and 1 (known as <strong>scaling</strong> or <strong>normalization</strong>).</p> <div class=highlight><pre><span></span><code><span class=c1># Divide train and test images by the maximum value (normalize it)</span>
<span class=n>train_data</span> <span class=o>=</span> <span class=n>train_data</span> <span class=o>/</span> <span class=mf>255.0</span>
<span class=n>test_data</span> <span class=o>=</span> <span class=n>test_data</span> <span class=o>/</span> <span class=mf>255.0</span>

<span class=c1># Check the min and max values of the training data</span>
<span class=n>train_data</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>train_data</span><span class=o>.</span><span class=n>max</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(0.0, 1.0)
</code></pre></div> <p>Beautiful! Now our data is between 0 and 1. Let's see what happens when we model it.</p> <p>We'll use the same model as before (<code>model_11</code>) except this time the data will be normalized.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create the model</span>
<span class=n>model_12</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Flatten</span><span class=p>(</span><span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)),</span> <span class=c1># input layer (we had to reshape 28x28 to 784)</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;softmax&quot;</span><span class=p>)</span> <span class=c1># output shape is 10, activation is softmax</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_12</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>SparseCategoricalCrossentropy</span><span class=p>(),</span>
                 <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span>
                 <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;accuracy&quot;</span><span class=p>])</span>

<span class=c1># Fit the model (to the normalized data)</span>
<span class=n>norm_history</span> <span class=o>=</span> <span class=n>model_12</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_data</span><span class=p>,</span>
                            <span class=n>train_labels</span><span class=p>,</span>
                            <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
                            <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>test_data</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m8s[0m 3ms/step - accuracy: 0.2158 - loss: 1.9761 - val_accuracy: 0.4401 - val_loss: 1.3918
Epoch 2/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.4754 - loss: 1.3161 - val_accuracy: 0.5242 - val_loss: 1.1941
Epoch 3/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.5355 - loss: 1.1648 - val_accuracy: 0.5452 - val_loss: 1.1227
Epoch 4/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.5472 - loss: 1.1011 - val_accuracy: 0.5576 - val_loss: 1.0870
Epoch 5/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.5635 - loss: 1.0678 - val_accuracy: 0.5718 - val_loss: 1.0619
Epoch 6/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 2ms/step - accuracy: 0.5744 - loss: 1.0477 - val_accuracy: 0.5874 - val_loss: 1.0474
Epoch 7/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 2ms/step - accuracy: 0.5877 - loss: 1.0305 - val_accuracy: 0.6094 - val_loss: 1.0314
Epoch 8/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 2ms/step - accuracy: 0.6136 - loss: 1.0109 - val_accuracy: 0.6562 - val_loss: 1.0011
Epoch 9/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 2ms/step - accuracy: 0.6583 - loss: 0.9772 - val_accuracy: 0.6928 - val_loss: 0.9538
Epoch 10/10
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m6s[0m 3ms/step - accuracy: 0.6958 - loss: 0.9224 - val_accuracy: 0.7093 - val_loss: 0.8947
</code></pre></div> <p>Woah, we used the exact same model as before but we with normalized data we're now seeing a much higher accuracy value!</p> <p>Let's plot each model's history (their loss curves).</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=c1># Plot non-normalized data loss curves</span>
<span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>non_norm_history</span><span class=o>.</span><span class=n>history</span><span class=p>)</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>title</span><span class=o>=</span><span class=s2>&quot;Non-normalized Data&quot;</span><span class=p>)</span>
<span class=c1># Plot normalized data loss curves</span>
<span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>norm_history</span><span class=o>.</span><span class=n>history</span><span class=p>)</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>title</span><span class=o>=</span><span class=s2>&quot;Normalized data&quot;</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_153_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_153_0.png></a></p> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_153_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_153_1.png></a></p> <p>Wow. From these two plots, we can see how much quicker our model with the normalized data (<code>model_12</code>) improved than the model with the non-normalized data (<code>model_11</code>).</p> <blockquote> <p>🔑 <strong>Note:</strong> The same model with even <em>slightly</em> different data can produce <em>dramatically</em> different results. So when you're comparing models, it's important to make sure you're comparing them on the same criteria (e.g. same architecture but different data or same data but different architecture).</p> </blockquote> <p>How about we find the ideal learning rate and see what happens?</p> <p>We'll use the same architecture we've been using.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create the model</span>
<span class=n>model_13</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Flatten</span><span class=p>(</span><span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)),</span> <span class=c1># input layer (we had to reshape 28x28 to 784)</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;softmax&quot;</span><span class=p>)</span> <span class=c1># output shape is 10, activation is softmax</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_13</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>SparseCategoricalCrossentropy</span><span class=p>(),</span>
                 <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span>
                 <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;accuracy&quot;</span><span class=p>])</span>

<span class=c1># Create the learning rate callback</span>
<span class=n>lr_scheduler</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>LearningRateScheduler</span><span class=p>(</span><span class=k>lambda</span> <span class=n>epoch</span><span class=p>:</span> <span class=mf>1e-3</span> <span class=o>*</span> <span class=mi>10</span><span class=o>**</span><span class=p>(</span><span class=n>epoch</span><span class=o>/</span><span class=mi>20</span><span class=p>))</span>

<span class=c1># Fit the model</span>
<span class=n>find_lr_history</span> <span class=o>=</span> <span class=n>model_13</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_data</span><span class=p>,</span>
                               <span class=n>train_labels</span><span class=p>,</span>
                               <span class=n>epochs</span><span class=o>=</span><span class=mi>40</span><span class=p>,</span> <span class=c1># model already doing pretty good with current LR, probably don&#39;t need 100 epochs</span>
                               <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>test_data</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>),</span>
                               <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>lr_scheduler</span><span class=p>])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>D:\anaconda\envs\py3-TF2.0\Lib\site-packages\keras\src\layers\reshaping\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)


Epoch 1/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m7s[0m 3ms/step - accuracy: 0.3501 - loss: 1.6252 - val_accuracy: 0.5883 - val_loss: 1.0367 - learning_rate: 0.0010
Epoch 2/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.6031 - loss: 0.9875 - val_accuracy: 0.6861 - val_loss: 0.8441 - learning_rate: 0.0011
Epoch 3/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7117 - loss: 0.7836 - val_accuracy: 0.7452 - val_loss: 0.7132 - learning_rate: 0.0013
Epoch 4/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 2ms/step - accuracy: 0.7515 - loss: 0.6917 - val_accuracy: 0.7586 - val_loss: 0.6820 - learning_rate: 0.0014
Epoch 5/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7632 - loss: 0.6543 - val_accuracy: 0.7611 - val_loss: 0.6566 - learning_rate: 0.0016
Epoch 6/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 2ms/step - accuracy: 0.7689 - loss: 0.6329 - val_accuracy: 0.7647 - val_loss: 0.6471 - learning_rate: 0.0018
Epoch 7/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 2ms/step - accuracy: 0.7719 - loss: 0.6226 - val_accuracy: 0.7652 - val_loss: 0.6440 - learning_rate: 0.0020
Epoch 8/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7735 - loss: 0.6182 - val_accuracy: 0.7667 - val_loss: 0.6424 - learning_rate: 0.0022
Epoch 9/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7736 - loss: 0.6158 - val_accuracy: 0.7641 - val_loss: 0.6441 - learning_rate: 0.0025
Epoch 10/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7735 - loss: 0.6150 - val_accuracy: 0.7620 - val_loss: 0.6458 - learning_rate: 0.0028
Epoch 11/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7742 - loss: 0.6144 - val_accuracy: 0.7584 - val_loss: 0.6536 - learning_rate: 0.0032
Epoch 12/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7731 - loss: 0.6157 - val_accuracy: 0.7560 - val_loss: 0.6611 - learning_rate: 0.0035
Epoch 13/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7739 - loss: 0.6160 - val_accuracy: 0.7545 - val_loss: 0.6638 - learning_rate: 0.0040
Epoch 14/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7731 - loss: 0.6164 - val_accuracy: 0.7525 - val_loss: 0.6669 - learning_rate: 0.0045
Epoch 15/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7725 - loss: 0.6174 - val_accuracy: 0.7508 - val_loss: 0.6720 - learning_rate: 0.0050
Epoch 16/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7721 - loss: 0.6178 - val_accuracy: 0.7505 - val_loss: 0.6710 - learning_rate: 0.0056
Epoch 17/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7742 - loss: 0.6178 - val_accuracy: 0.7558 - val_loss: 0.6642 - learning_rate: 0.0063
Epoch 18/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7756 - loss: 0.6190 - val_accuracy: 0.7582 - val_loss: 0.6623 - learning_rate: 0.0071
Epoch 19/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7759 - loss: 0.6211 - val_accuracy: 0.7580 - val_loss: 0.6633 - learning_rate: 0.0079
Epoch 20/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7744 - loss: 0.6238 - val_accuracy: 0.7560 - val_loss: 0.6689 - learning_rate: 0.0089
Epoch 21/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7726 - loss: 0.6282 - val_accuracy: 0.7554 - val_loss: 0.6709 - learning_rate: 0.0100
Epoch 22/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7708 - loss: 0.6316 - val_accuracy: 0.7494 - val_loss: 0.6829 - learning_rate: 0.0112
Epoch 23/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7701 - loss: 0.6387 - val_accuracy: 0.7481 - val_loss: 0.6884 - learning_rate: 0.0126
Epoch 24/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7688 - loss: 0.6435 - val_accuracy: 0.7453 - val_loss: 0.6955 - learning_rate: 0.0141
Epoch 25/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 2ms/step - accuracy: 0.7645 - loss: 0.6528 - val_accuracy: 0.7411 - val_loss: 0.7083 - learning_rate: 0.0158
Epoch 26/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7639 - loss: 0.6578 - val_accuracy: 0.7323 - val_loss: 0.7232 - learning_rate: 0.0178
Epoch 27/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7601 - loss: 0.6704 - val_accuracy: 0.7353 - val_loss: 0.7220 - learning_rate: 0.0200
Epoch 28/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7560 - loss: 0.6822 - val_accuracy: 0.7359 - val_loss: 0.7109 - learning_rate: 0.0224
Epoch 29/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7494 - loss: 0.6977 - val_accuracy: 0.7361 - val_loss: 0.7232 - learning_rate: 0.0251
Epoch 30/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7448 - loss: 0.7072 - val_accuracy: 0.7295 - val_loss: 0.7468 - learning_rate: 0.0282
Epoch 31/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7308 - loss: 0.7486 - val_accuracy: 0.7294 - val_loss: 0.7358 - learning_rate: 0.0316
Epoch 32/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7338 - loss: 0.7515 - val_accuracy: 0.7248 - val_loss: 0.7697 - learning_rate: 0.0355
Epoch 33/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7263 - loss: 0.7760 - val_accuracy: 0.7163 - val_loss: 0.7975 - learning_rate: 0.0398
Epoch 34/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7080 - loss: 0.8292 - val_accuracy: 0.6639 - val_loss: 0.8910 - learning_rate: 0.0447
Epoch 35/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.6835 - loss: 0.8798 - val_accuracy: 0.6585 - val_loss: 0.9027 - learning_rate: 0.0501
Epoch 36/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.6496 - loss: 0.9609 - val_accuracy: 0.6823 - val_loss: 0.9020 - learning_rate: 0.0562
Epoch 37/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.4564 - loss: 1.3500 - val_accuracy: 0.3779 - val_loss: 1.3239 - learning_rate: 0.0631
Epoch 38/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.3945 - loss: 1.3394 - val_accuracy: 0.4003 - val_loss: 1.4029 - learning_rate: 0.0708
Epoch 39/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.3927 - loss: 1.3779 - val_accuracy: 0.3692 - val_loss: 1.3400 - learning_rate: 0.0794
Epoch 40/40
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.3832 - loss: 1.4105 - val_accuracy: 0.3095 - val_loss: 1.5997 - learning_rate: 0.0891
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Plot the learning rate decay curve</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=n>lrs</span> <span class=o>=</span> <span class=mf>1e-3</span> <span class=o>*</span> <span class=p>(</span><span class=mi>10</span><span class=o>**</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>40</span><span class=p>)</span><span class=o>/</span><span class=mi>20</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>semilogx</span><span class=p>(</span><span class=n>lrs</span><span class=p>,</span> <span class=n>find_lr_history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s2>&quot;loss&quot;</span><span class=p>])</span> <span class=c1># want the x-axis to be log-scale</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Learning rate&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;Loss&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Finding the ideal learning rate&quot;</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_156_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_156_0.png></a></p> <p>In this case, it looks like somewhere close to the default learning rate of the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam>Adam optimizer</a> (<code>0.001</code>) is the ideal learning rate.</p> <p>Let's refit a model using the ideal learning rate.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create the model</span>
<span class=n>model_14</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Flatten</span><span class=p>(</span><span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)),</span> <span class=c1># input layer (we had to reshape 28x28 to 784)</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;relu&quot;</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s2>&quot;softmax&quot;</span><span class=p>)</span> <span class=c1># output shape is 10, activation is softmax</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_14</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>SparseCategoricalCrossentropy</span><span class=p>(),</span>
                 <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.001</span><span class=p>),</span> <span class=c1># ideal learning rate (same as default)</span>
                 <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;accuracy&quot;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>model_14</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_data</span><span class=p>,</span>
                       <span class=n>train_labels</span><span class=p>,</span>
                       <span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
                       <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>test_data</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m7s[0m 3ms/step - accuracy: 0.4631 - loss: 1.4866 - val_accuracy: 0.6686 - val_loss: 0.9178
Epoch 2/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.6946 - loss: 0.8475 - val_accuracy: 0.7302 - val_loss: 0.7608
Epoch 3/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7487 - loss: 0.7210 - val_accuracy: 0.7604 - val_loss: 0.6907
Epoch 4/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7755 - loss: 0.6521 - val_accuracy: 0.7787 - val_loss: 0.6429
Epoch 5/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7917 - loss: 0.6083 - val_accuracy: 0.7880 - val_loss: 0.6178
Epoch 6/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.7995 - loss: 0.5839 - val_accuracy: 0.7958 - val_loss: 0.6021
Epoch 7/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8034 - loss: 0.5680 - val_accuracy: 0.8004 - val_loss: 0.5919
Epoch 8/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8070 - loss: 0.5576 - val_accuracy: 0.8037 - val_loss: 0.5856
Epoch 9/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8102 - loss: 0.5496 - val_accuracy: 0.8069 - val_loss: 0.5805
Epoch 10/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8127 - loss: 0.5430 - val_accuracy: 0.8076 - val_loss: 0.5766
Epoch 11/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8144 - loss: 0.5377 - val_accuracy: 0.8083 - val_loss: 0.5737
Epoch 12/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8154 - loss: 0.5335 - val_accuracy: 0.8091 - val_loss: 0.5708
Epoch 13/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8170 - loss: 0.5298 - val_accuracy: 0.8097 - val_loss: 0.5691
Epoch 14/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8175 - loss: 0.5269 - val_accuracy: 0.8111 - val_loss: 0.5674
Epoch 15/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8188 - loss: 0.5242 - val_accuracy: 0.8110 - val_loss: 0.5662
Epoch 16/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8197 - loss: 0.5219 - val_accuracy: 0.8110 - val_loss: 0.5650
Epoch 17/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8209 - loss: 0.5198 - val_accuracy: 0.8116 - val_loss: 0.5645
Epoch 18/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8222 - loss: 0.5179 - val_accuracy: 0.8114 - val_loss: 0.5640
Epoch 19/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8221 - loss: 0.5163 - val_accuracy: 0.8121 - val_loss: 0.5632
Epoch 20/20
[1m1875/1875[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m5s[0m 3ms/step - accuracy: 0.8225 - loss: 0.5147 - val_accuracy: 0.8127 - val_loss: 0.5627
</code></pre></div> <p>Now we've got a model trained with a close-to-ideal learning rate and performing pretty well, we've got a couple of options.</p> <p>We could: * Evaluate its performance using other classification metrics (such as a <a href=https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py>confusion matrix</a> or <a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html>classification report</a>). * Assess some of its predictions (through visualizations). * Improve its accuracy (by training it for longer or changing the architecture). * Save and export it for use in an application.</p> <p>Let's go through the first two options.</p> <p>First we'll create a classification matrix to visualize its predictions across the different classes.</p> <div class=highlight><pre><span></span><code><span class=c1># Note: The following confusion matrix code is a remix of Scikit-Learn&#39;s</span>
<span class=c1># plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html</span>
<span class=c1># and Made with ML&#39;s introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb</span>
<span class=kn>import</span><span class=w> </span><span class=nn>itertools</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>confusion_matrix</span>

<span class=c1># Our function needs a different name to sklearn&#39;s plot_confusion_matrix</span>
<span class=k>def</span><span class=w> </span><span class=nf>make_confusion_matrix</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=n>classes</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span> <span class=n>text_size</span><span class=o>=</span><span class=mi>15</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;Makes a labelled confusion matrix comparing predictions and ground truth labels.</span>

<span class=sd>  If classes is passed, confusion matrix will be labelled, if not, integer class values</span>
<span class=sd>  will be used.</span>

<span class=sd>  Args:</span>
<span class=sd>    y_true: Array of truth labels (must be same shape as y_pred).</span>
<span class=sd>    y_pred: Array of predicted labels (must be same shape as y_true).</span>
<span class=sd>    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.</span>
<span class=sd>    figsize: Size of output figure (default=(10, 10)).</span>
<span class=sd>    text_size: Size of output figure text (default=15).</span>

<span class=sd>  Returns:</span>
<span class=sd>    A labelled confusion matrix plot comparing y_true and y_pred.</span>

<span class=sd>  Example usage:</span>
<span class=sd>    make_confusion_matrix(y_true=test_labels, # ground truth test labels</span>
<span class=sd>                          y_pred=y_preds, # predicted labels</span>
<span class=sd>                          classes=class_names, # array of class label names</span>
<span class=sd>                          figsize=(15, 15),</span>
<span class=sd>                          text_size=10)</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=c1># Create the confustion matrix</span>
  <span class=n>cm</span> <span class=o>=</span> <span class=n>confusion_matrix</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
  <span class=n>cm_norm</span> <span class=o>=</span> <span class=n>cm</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s2>&quot;float&quot;</span><span class=p>)</span> <span class=o>/</span> <span class=n>cm</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)[:,</span> <span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>]</span> <span class=c1># normalize it</span>
  <span class=n>n_classes</span> <span class=o>=</span> <span class=n>cm</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=c1># find the number of classes we&#39;re dealing with</span>

  <span class=c1># Plot the figure and make it pretty</span>
  <span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=n>figsize</span><span class=p>)</span>
  <span class=n>cax</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>matshow</span><span class=p>(</span><span class=n>cm</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>Blues</span><span class=p>)</span> <span class=c1># colors will represent how &#39;correct&#39; a class is, darker == better</span>
  <span class=n>fig</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>cax</span><span class=p>)</span>

  <span class=c1># Are there a list of classes?</span>
  <span class=k>if</span> <span class=n>classes</span><span class=p>:</span>
    <span class=n>labels</span> <span class=o>=</span> <span class=n>classes</span>
  <span class=k>else</span><span class=p>:</span>
    <span class=n>labels</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>cm</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>

  <span class=c1># Label the axes</span>
  <span class=n>ax</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=n>title</span><span class=o>=</span><span class=s2>&quot;Confusion Matrix&quot;</span><span class=p>,</span>
         <span class=n>xlabel</span><span class=o>=</span><span class=s2>&quot;Predicted label&quot;</span><span class=p>,</span>
         <span class=n>ylabel</span><span class=o>=</span><span class=s2>&quot;True label&quot;</span><span class=p>,</span>
         <span class=n>xticks</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>n_classes</span><span class=p>),</span> <span class=c1># create enough axis slots for each class</span>
         <span class=n>yticks</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>n_classes</span><span class=p>),</span>
         <span class=n>xticklabels</span><span class=o>=</span><span class=n>labels</span><span class=p>,</span> <span class=c1># axes will labeled with class names (if they exist) or ints</span>
         <span class=n>yticklabels</span><span class=o>=</span><span class=n>labels</span><span class=p>)</span>

  <span class=c1># Make x-axis labels appear on bottom</span>
  <span class=n>ax</span><span class=o>.</span><span class=n>xaxis</span><span class=o>.</span><span class=n>set_label_position</span><span class=p>(</span><span class=s2>&quot;bottom&quot;</span><span class=p>)</span>
  <span class=n>ax</span><span class=o>.</span><span class=n>xaxis</span><span class=o>.</span><span class=n>tick_bottom</span><span class=p>()</span>

  <span class=c1># Set the threshold for different colors</span>
  <span class=n>threshold</span> <span class=o>=</span> <span class=p>(</span><span class=n>cm</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=n>cm</span><span class=o>.</span><span class=n>min</span><span class=p>())</span> <span class=o>/</span> <span class=mf>2.</span>

  <span class=c1># Plot the text on each cell</span>
  <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>j</span> <span class=ow>in</span> <span class=n>itertools</span><span class=o>.</span><span class=n>product</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>cm</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=nb>range</span><span class=p>(</span><span class=n>cm</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>])):</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>j</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>cm</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=w> </span><span class=n>j</span><span class=p>]</span><span class=si>}</span><span class=s2> (</span><span class=si>{</span><span class=n>cm_norm</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=w> </span><span class=n>j</span><span class=p>]</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%)&quot;</span><span class=p>,</span>
             <span class=n>horizontalalignment</span><span class=o>=</span><span class=s2>&quot;center&quot;</span><span class=p>,</span>
             <span class=n>color</span><span class=o>=</span><span class=s2>&quot;white&quot;</span> <span class=k>if</span> <span class=n>cm</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>threshold</span> <span class=k>else</span> <span class=s2>&quot;black&quot;</span><span class=p>,</span>
             <span class=n>size</span><span class=o>=</span><span class=n>text_size</span><span class=p>)</span>
</code></pre></div> <p>Since a confusion matrix compares the truth labels (<code>test_labels</code>) to the predicted labels, we have to make some predictions with our model.</p> <div class=highlight><pre><span></span><code><span class=c1># Make predictions with the most recent model</span>
<span class=n>y_probs</span> <span class=o>=</span> <span class=n>model_14</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span> <span class=c1># &quot;probs&quot; is short for probabilities</span>

<span class=c1># View the first 5 predictions</span>
<span class=n>y_probs</span><span class=p>[:</span><span class=mi>5</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m313/313[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 3ms/step





array([[3.32330121e-03, 2.81442535e-05, 1.14267032e-05, 2.78759544e-05,
        1.65281119e-06, 1.43556625e-01, 8.79438361e-04, 1.46590874e-01,
        2.21113864e-04, 7.05359578e-01],
       [1.41393888e-04, 5.80133497e-10, 7.16942549e-01, 3.28307215e-05,
        2.21403509e-01, 0.00000000e+00, 6.14793114e-02, 0.00000000e+00,
        3.05352700e-07, 0.00000000e+00],
       [1.92334678e-03, 9.97694075e-01, 4.38925463e-06, 3.76133510e-04,
        1.18040466e-10, 1.25610046e-29, 1.94088898e-06, 1.92841159e-13,
        4.23684901e-16, 6.21246943e-11],
       [3.18397913e-04, 9.98650610e-01, 2.81396501e-06, 1.02779898e-03,
        1.53851765e-10, 6.76019936e-31, 4.12143322e-07, 1.32739143e-13,
        8.32940635e-17, 1.84258511e-12],
       [1.31434500e-01, 1.67336722e-04, 1.86377615e-01, 1.60328615e-02,
        4.82553691e-02, 6.68696530e-18, 6.15544081e-01, 2.38717523e-20,
        2.18813238e-03, 2.09246382e-15]], dtype=float32)
</code></pre></div> <p>Our model outputs a list of <strong>prediction probabilities</strong>, meaning, it outputs a number for how likely it thinks a particular class is to be the label.</p> <p>The higher the number in the prediction probabilities list, the more likely the model believes that is the right class.</p> <p>To find the highest value we can use the <a href=https://numpy.org/doc/stable/reference/generated/numpy.argmax.html><code>argmax()</code></a> method.</p> <div class=highlight><pre><span></span><code><span class=c1># See the predicted class number and label for the first example</span>
<span class=n>y_probs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>argmax</span><span class=p>(),</span> <span class=n>class_names</span><span class=p>[</span><span class=n>y_probs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>argmax</span><span class=p>()]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(9, &#39;Ankle boot&#39;)
</code></pre></div> <p>Now let's do the same for all of the predictions.</p> <div class=highlight><pre><span></span><code><span class=c1># Convert all of the predictions from probabilities to labels</span>
<span class=n>y_preds</span> <span class=o>=</span> <span class=n>y_probs</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=c1># View the first 10 prediction labels</span>
<span class=n>y_preds</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([9, 2, 1, 1, 6, 1, 6, 6, 5, 7], dtype=int64)
</code></pre></div> <p>Wonderful, now we've got our model's predictions in label form, let's create a confusion matrix to view them against the truth labels.</p> <div class=highlight><pre><span></span><code><span class=c1># Check out the non-prettified confusion matrix</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>confusion_matrix</span>
<span class=n>confusion_matrix</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>test_labels</span><span class=p>,</span>
                 <span class=n>y_pred</span><span class=o>=</span><span class=n>y_preds</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([[756,   2,  32,  59,   5,   2, 129,   3,  12,   0],
       [ 17, 931,   2,  43,   3,   0,   2,   0,   2,   0],
       [ 13,   0, 608,  47, 177,   0, 149,   0,   6,   0],
       [ 17,   8,  19, 852,  50,   1,  39,   3,  11,   0],
       [  2,   0, 102,  50, 764,   0,  70,   0,  12,   0],
       [  2,   0,   0,   0,   0, 899,   0,  61,   6,  32],
       [126,   0, 122,  56, 130,   1, 537,   0,  28,   0],
       [  0,   0,   0,   0,   0,  35,   0, 935,   0,  30],
       [  5,   0,   1,   9,  17,  14,  45,   4, 905,   0],
       [  0,   0,   0,   0,   0,  10,   1,  48,   1, 940]], dtype=int64)
</code></pre></div> <p>That confusion matrix is hard to comprehend, let's make it prettier using the function we created before.</p> <div class=highlight><pre><span></span><code><span class=c1># Make a prettier confusion matrix</span>
<span class=n>make_confusion_matrix</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>test_labels</span><span class=p>,</span>
                      <span class=n>y_pred</span><span class=o>=</span><span class=n>y_preds</span><span class=p>,</span>
                      <span class=n>classes</span><span class=o>=</span><span class=n>class_names</span><span class=p>,</span>
                      <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>15</span><span class=p>),</span>
                      <span class=n>text_size</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_170_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_170_0.png></a></p> <p>That looks much better! (one of my favourites sights in the world is a confusion matrix with dark squares down the diagonal)</p> <p>Except the results aren't as good as they could be...</p> <p>It looks like our model is getting confused between the <code>Shirt</code> and <code>T-shirt/top</code> classes (e.g. predicting <code>Shirt</code> when it's actually a <code>T-shirt/top</code>).</p> <blockquote> <p>🤔 <strong>Question:</strong> Does it make sense that our model is getting confused between the <code>Shirt</code> and <code>T-shirt/top</code> classes? Why do you think this might be? What's one way you could investigate?</p> </blockquote> <p>We've seen how our models predictions line up to the truth labels using a confusion matrix, but how about we visualize some?</p> <p>Let's create a function to plot a random image along with its prediction.</p> <blockquote> <p>🔑 <strong>Note:</strong> Often when working with images and other forms of visual data, it's a good idea to visualize as much as possible to develop a further understanding of the data and the outputs of your model.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>random</span>

<span class=c1># Create a function for plotting a random image along with its prediction</span>
<span class=k>def</span><span class=w> </span><span class=nf>plot_random_image</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>images</span><span class=p>,</span> <span class=n>true_labels</span><span class=p>,</span> <span class=n>classes</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;Picks a random image, plots it and labels it with a predicted and truth label.</span>

<span class=sd>  Args:</span>
<span class=sd>    model: a trained model (trained on data similar to what&#39;s in images).</span>
<span class=sd>    images: a set of random images (in tensor form).</span>
<span class=sd>    true_labels: array of ground truth labels for images.</span>
<span class=sd>    classes: array of class names for images.</span>

<span class=sd>  Returns:</span>
<span class=sd>    A plot of a random image from `images` with a predicted class label from `model`</span>
<span class=sd>    as well as the truth class label from `true_labels`.</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=c1># Setup random integer</span>
  <span class=n>i</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>images</span><span class=p>))</span>

  <span class=c1># Create predictions and targets</span>
  <span class=n>target_image</span> <span class=o>=</span> <span class=n>images</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
  <span class=n>pred_probs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>target_image</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>))</span> <span class=c1># have to reshape to get into right size for model</span>
  <span class=n>pred_label</span> <span class=o>=</span> <span class=n>classes</span><span class=p>[</span><span class=n>pred_probs</span><span class=o>.</span><span class=n>argmax</span><span class=p>()]</span>
  <span class=n>true_label</span> <span class=o>=</span> <span class=n>classes</span><span class=p>[</span><span class=n>true_labels</span><span class=p>[</span><span class=n>i</span><span class=p>]]</span>

  <span class=c1># Plot the target image</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>target_image</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>binary</span><span class=p>)</span>

  <span class=c1># Change the color of the titles depending on if the prediction is right or wrong</span>
  <span class=k>if</span> <span class=n>pred_label</span> <span class=o>==</span> <span class=n>true_label</span><span class=p>:</span>
    <span class=n>color</span> <span class=o>=</span> <span class=s2>&quot;green&quot;</span>
  <span class=k>else</span><span class=p>:</span>
    <span class=n>color</span> <span class=o>=</span> <span class=s2>&quot;red&quot;</span>

  <span class=c1># Add xlabel information (prediction/true label)</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Pred: </span><span class=si>{}</span><span class=s2> </span><span class=si>{:2.0f}</span><span class=s2>% (True: </span><span class=si>{}</span><span class=s2>)&quot;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>pred_label</span><span class=p>,</span>
                                                   <span class=mi>100</span><span class=o>*</span><span class=n>tf</span><span class=o>.</span><span class=n>reduce_max</span><span class=p>(</span><span class=n>pred_probs</span><span class=p>),</span>
                                                   <span class=n>true_label</span><span class=p>),</span>
             <span class=n>color</span><span class=o>=</span><span class=n>color</span><span class=p>)</span> <span class=c1># set the color to green or red</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check out a random image as well as its prediction</span>
<span class=n>plot_random_image</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>model_14</span><span class=p>,</span>
                  <span class=n>images</span><span class=o>=</span><span class=n>test_data</span><span class=p>,</span>
                  <span class=n>true_labels</span><span class=o>=</span><span class=n>test_labels</span><span class=p>,</span>
                  <span class=n>classes</span><span class=o>=</span><span class=n>class_names</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 51ms/step
</code></pre></div> <p><a class=glightbox href=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_173_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../02_neural_network_classification_in_tensorflow_files/02_neural_network_classification_in_tensorflow_173_1.png></a></p> <p>After running the cell above a few times you'll start to get a visual understanding of the relationship between the model's predictions and the true labels.</p> <p>Did you figure out which predictions the model gets confused on?</p> <p>It seems to mix up classes which are similar, for example, <code>Sneaker</code> with <code>Ankle boot</code>.</p> <p>Looking at the images, you can see how this might be the case.</p> <p>The overall shape of a <code>Sneaker</code> and an <code>Ankle Boot</code> are similar.</p> <p>The overall shape might be one of the patterns the model has learned and so therefore when two images have a similar shape, their predictions get mixed up.</p> <h3 id=what-patterns-is-our-model-learning>What patterns is our model learning?</h3> <p>We've been talking a lot about how a neural network finds patterns in numbers, but what exactly do these patterns look like?</p> <p>Let's crack open one of our models and find out.</p> <p>First, we'll get a list of layers in our most recent model (<code>model_14</code>) using the <code>layers</code> attribute.</p> <div class=highlight><pre><span></span><code><span class=c1># Find the layers of our most recent model</span>
<span class=n>model_14</span><span class=o>.</span><span class=n>layers</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[&lt;Flatten name=flatten_3, built=True&gt;,
 &lt;Dense name=dense_40, built=True&gt;,
 &lt;Dense name=dense_41, built=True&gt;,
 &lt;Dense name=dense_42, built=True&gt;]
</code></pre></div> <p>We can access a target layer using indexing.</p> <div class=highlight><pre><span></span><code><span class=c1># Extract a particular layer</span>
<span class=n>model_14</span><span class=o>.</span><span class=n>layers</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;Dense name=dense_40, built=True&gt;
</code></pre></div> <p>And we can find the patterns learned by a particular layer using the <code>get_weights()</code> method.</p> <p>The <code>get_weights()</code> method returns the <strong>weights</strong> (also known as a weights matrix) and biases (also known as a bias vector) of a particular layer.</p> <div class=highlight><pre><span></span><code><span class=c1># Get the patterns of a layer in our network</span>
<span class=n>weights</span><span class=p>,</span> <span class=n>biases</span> <span class=o>=</span> <span class=n>model_14</span><span class=o>.</span><span class=n>layers</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>get_weights</span><span class=p>()</span>

<span class=c1># Shape = 1 weight matrix the size of our input data (28x28) per neuron (4)</span>
<span class=n>weights</span><span class=p>,</span> <span class=n>weights</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([[ 0.45907176, -0.46185845,  0.569868  ,  0.39837158],
        [ 0.34134197, -1.0099722 ,  0.4683142 ,  0.6066443 ],
        [ 0.7053194 , -0.930662  ,  0.39949802,  0.49607873],
        ...,
        [ 0.28680536,  0.03652136,  0.0915413 , -0.4339213 ],
        [-0.49932057,  0.13051574, -0.31600925,  0.16438061],
        [ 0.48601484, -0.43576252,  0.22481441,  0.23154569]],
       dtype=float32),
 (784, 4))
</code></pre></div> <p>The weights matrix is the same shape as the input data, which in our case is 784 (28x28 pixels). And there's a copy of the weights matrix for each neuron the in the selected layer (our selected layer has 4 neurons).</p> <p>Each value in the weights matrix corresponds to how a particular value in the input data influences the network's decisions.</p> <p>These values start out as random numbers (they're set by the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense><code>kernel_initializer</code> parameter</a> when creating a layer, the default is <a href=https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform><code>"glorot_uniform"</code></a>) and are then updated to better representative values of the data (non-random) by the neural network during training.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-fashion-mnist-learning.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="neural network supervised learning weight updates" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-fashion-mnist-learning.png></a> <em>Example workflow of how a supervised neural network starts with random weights and updates them to better represent the data by looking at examples of ideal outputs.</em></p> <p>Now let's check out the bias vector.</p> <div class=highlight><pre><span></span><code><span class=c1># Shape = 1 bias per neuron (we use 4 neurons in the first layer)</span>
<span class=n>biases</span><span class=p>,</span> <span class=n>biases</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([ 1.0610301 ,  1.841785  ,  0.5734505 , -0.34848416], dtype=float32),
 (4,))
</code></pre></div> <p>Every neuron has a bias vector. Each of these is paired with a weight matrix.</p> <p>The bias values get initialized as zeroes by default (using the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense><code>bias_initializer</code> parameter</a>).</p> <p>The bias vector dictates how much the patterns within the corresponding weights matrix should influence the next layer.</p> <div class=highlight><pre><span></span><code><span class=c1># Can now calculate the number of paramters in our model</span>
<span class=n>model_14</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</code></pre></div> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_15"</span>
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                         </span>┃<span style="font-weight: bold"> Output Shape                </span>┃<span style="font-weight: bold">         Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ flatten_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)                  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">784</span>)                 │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_40 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">4</span>)                   │           <span style="color: #00af00; text-decoration-color: #00af00">3,140</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_41 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">4</span>)                   │              <span style="color: #00af00; text-decoration-color: #00af00">20</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_42 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>)                  │              <span style="color: #00af00; text-decoration-color: #00af00">50</span> │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">9,632</span> (37.63 KB)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">3,210</span> (12.54 KB)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">6,422</span> (25.09 KB)
</pre> <p>Now we've built a few deep learning models, it's a good time to point out the whole concept of inputs and outputs not only relates to a model as a whole but to <em>every</em> layer within a model.</p> <p>You might've already guessed this, but starting from the input layer, each subsequent layer's input is the output of the previous layer.</p> <p>We can see this clearly using the utility <a href=https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model><code>plot_model()</code></a>.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras.utils</span><span class=w> </span><span class=kn>import</span> <span class=n>plot_model</span>

<span class=c1># See the inputs and outputs of each layer</span>
<span class=n>plot_model</span><span class=p>(</span><span class=n>model_14</span><span class=p>,</span> <span class=n>show_shapes</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.
</code></pre></div> <h2 id=how-a-model-learns-in-brief>How a model learns (in brief)</h2> <p>Alright, we've trained a bunch of models, but we've never really discussed what's going on under the hood. So how exactly does a model learn?</p> <p>A model learns by updating and improving its weight matrices and biases values every epoch (in our case, when we call the <code>fit()</code> fucntion).</p> <p>It does so by comparing the patterns its learned between the data and labels to the actual labels.</p> <p>If the current patterns (weight matrices and bias values) don't result in a desirable decrease in the loss function (higher loss means worse predictions), the optimizer tries to steer the model to update its patterns in the right way (using the real labels as a reference).</p> <p>This process of using the real labels as a reference to improve the model's predictions is called <a href=https://en.wikipedia.org/wiki/Backpropagation><strong>backpropagation</strong></a>.</p> <p>In other words, data and labels pass through a model (<strong>forward pass</strong>) and it attempts to learn the relationship between the data and labels.</p> <p>And if this learned relationship isn't close to the actual relationship or it could be improved, the model does so by going back through itself (<strong>backward pass</strong>) and tweaking its weights matrices and bias values to better represent the data.</p> <p>If all of this sounds confusing (and it's fine if it does, the above is a very succinct description), check out the resources in the extra-curriculum section for more.</p> <h2 id=exercises>Exercises 🛠</h2> <ol> <li>Play with neural networks in the <a href=https://playground.tensorflow.org/ >TensorFlow Playground</a> for 10-minutes. Especially try different values of the learning, what happens when you decrease it? What happens when you increase it?</li> <li>Replicate the model pictured in the <a href="https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6,6,6,6,6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true">TensorFlow Playground diagram</a> below using TensorFlow code. Compile it using the Adam optimizer, binary crossentropy loss and accuracy metric. Once it's compiled check a summary of the model. <a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-replication-exercise.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="tensorflow playground example neural network" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-replication-exercise.png></a> <em>Try this network out for yourself on the <a href="https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6,6,6,6,6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true">TensorFlow Playground website</a>. Hint: there are 5 hidden layers but the output layer isn't pictured, you'll have to decide what the output layer should be based on the input data.</em></li> <li>Create a classification dataset using Scikit-Learn's <a href=https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html><code>make_moons()</code></a> function, visualize it and then build a model to fit it at over 85% accuracy.</li> <li>Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediciton labels at the same time. Hint: see the <a href=https://www.tensorflow.org/tutorials/keras/classification>classifcation tutorial in the TensorFlow documentation</a> for ideas.</li> <li>Recreate <a href=https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax>TensorFlow's</a> <a href=https://en.wikipedia.org/wiki/Softmax_function>softmax activation function</a> in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it.</li> <li>Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after.</li> <li>Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example, plot 3 images of the <code>T-shirt</code> class with their predictions.</li> </ol> <h2 id=extra-curriculum>Extra curriculum 📖</h2> <ul> <li>Watch 3Blue1Brown's neural networks video 2: <a href="https://www.youtube.com/watch?v=IHZwWFHWa-w"><em>Gradient descent, how neural networks learn</em></a>. After you're done, write 100 words about what you've learned.</li> <li>If you haven't already, watch video 1: <a href=https://youtu.be/aircAruvnKk><em>But what is a Neural Network?</em></a>. Note the activation function they talk about at the end.</li> <li>Watch <a href=https://youtu.be/njKP3FqW3Sk>MIT's introduction to deep learning lecture 1</a> (if you haven't already) to get an idea of the concepts behind using linear and non-linear functions.</li> <li>Spend 1-hour reading <a href=http://neuralnetworksanddeeplearning.com/index.html>Michael Nielsen's Neural Networks and Deep Learning book</a>.</li> <li>Read the <a href=https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html>ML-Glossary documentation on activation functions</a>. Which one is your favourite?</li> <li>After you've read the ML-Glossary, see which activation functions are available in TensorFlow by searching "tensorflow activation functions".</li> </ul> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.instant", "navigation.path", "navigation.indexes", "navigation.top", "navigation.tracking"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>