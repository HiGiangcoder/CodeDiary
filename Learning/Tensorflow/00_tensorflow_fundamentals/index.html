<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=prev href=../../Pytorch/09_pytorch_model_deployment.pdf><link href=../01_neural_network_regression_in_tensorflow/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.7"><title>0 Tensorflow fundamentals - My Documentation</title><link rel=stylesheet href=../../../assets/stylesheets/main.8608ea7d.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Merriweather";--md-code-font:"JetBrains Mono"}</style><link rel=stylesheet href=../../../assets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script> <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#00-getting-started-with-tensorflow-a-guide-to-the-fundamentals class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="My Documentation" class="md-header__button md-logo" aria-label="My Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> My Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 0 Tensorflow fundamentals </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../home/ class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../AI_Model/lenet/lenet5/ class=md-tabs__link> Ki·∫øn th·ª©c l·∫≠p tr√¨nh </a> </li> <li class=md-tabs__item> <a href=../../../Subject/XSTK/Exercise2/homework/ class=md-tabs__link> M√¥n h·ªçc </a> </li> <li class=md-tabs__item> <a href=../../../Problem/general/ class=md-tabs__link> CP-Problem </a> </li> <li class=md-tabs__item> <a href=../../../other/other/ class=md-tabs__link> Other </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="My Documentation" class="md-nav__button md-logo" aria-label="My Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> My Documentation </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../../../home/ class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../home/about/ class=md-nav__link> <span class=md-ellipsis> About </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Ki·∫øn th·ª©c l·∫≠p tr√¨nh </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Ki·∫øn th·ª©c l·∫≠p tr√¨nh </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1> <label class=md-nav__link for=__nav_2_1 id=__nav_2_1_label tabindex=0> <span class=md-ellipsis> C√°c ki·∫øn tr√∫c, m√¥ h√¨nh AI v√† c√°c kƒ© thu·∫≠t </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> C√°c ki·∫øn tr√∫c, m√¥ h√¨nh AI v√† c√°c kƒ© thu·∫≠t </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_1> <label class=md-nav__link for=__nav_2_1_1 id=__nav_2_1_1_label tabindex=0> <span class=md-ellipsis> Lenet-5 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_1> <span class="md-nav__icon md-icon"></span> Lenet-5 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenet5/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenetpart1/ class=md-nav__link> <span class=md-ellipsis> Tutorial part 1 </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenetpart2/ class=md-nav__link> <span class=md-ellipsis> Tutorial part 2 </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/Lenet5_MNIST/ class=md-nav__link> <span class=md-ellipsis> Lenet-5 (Final Project 1) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_2> <label class=md-nav__link for=__nav_2_1_2 id=__nav_2_1_2_label tabindex=0> <span class=md-ellipsis> UNet </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_2> <span class="md-nav__icon md-icon"></span> UNet </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../AI_Model/unet/unet/ class=md-nav__link> <span class=md-ellipsis> Unet tutorial </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/unet/unet_model/ class=md-nav__link> <span class=md-ellipsis> UNet (Final Project 2) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../AI_Model/essential/essential/ class=md-nav__link> <span class=md-ellipsis> C√°c ki·∫øn th·ª©c c·∫ßn thi·∫øt </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> Python </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Python/Numpy_1/ class=md-nav__link> <span class=md-ellipsis> Numpy 1 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Numpy_2/ class=md-nav__link> <span class=md-ellipsis> Numpy 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Python_Pandas_exercise1/ class=md-nav__link> <span class=md-ellipsis> Pandas exercise 1 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas2/ class=md-nav__link> <span class=md-ellipsis> Pandas 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Python_Pandas_exercise2/ class=md-nav__link> <span class=md-ellipsis> Pandas exercise 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas_Join_Combine/ class=md-nav__link> <span class=md-ellipsis> Pandas Join Combine </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> PyTorch </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> PyTorch </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Pytorch/00_pytorch_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 0 Pytorch Fundamentals (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/00_pytorch_and_deep_learning_fundamentals.pdf class=md-nav__link> <span class=md-ellipsis> 0 Deep learning fundamentals (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/01_pytorch_workflow/ class=md-nav__link> <span class=md-ellipsis> 1 Pytorch workflow (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/01_pytorch_workflow.pdf class=md-nav__link> <span class=md-ellipsis> 1 Pytorch workflow (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/02_pytorch_classification/ class=md-nav__link> <span class=md-ellipsis> 2 classification (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/03_pytorch_computer_vision/ class=md-nav__link> <span class=md-ellipsis> 3 Computer vision (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/03_pytorch_computer_vision.pdf class=md-nav__link> <span class=md-ellipsis> 3 Compupter vision (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/04_pytorch_custom_datasets/ class=md-nav__link> <span class=md-ellipsis> 4 Custom datasets (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/05_pytorch_going_modular.pdf class=md-nav__link> <span class=md-ellipsis> 5 Going moduler (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/06_pytorch_transfer_learning/ class=md-nav__link> <span class=md-ellipsis> 6 Transfer learning (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/06_pytorch_transfer_learning.pdf class=md-nav__link> <span class=md-ellipsis> 6 Transfer learning (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/07_pytorch_experiment_tracking/ class=md-nav__link> <span class=md-ellipsis> 7 Experiment tracking (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/07_pytorch_experiment_tracking.pdf class=md-nav__link> <span class=md-ellipsis> 7 Experiment tracking (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/08_pytorch_paper_replicating/ class=md-nav__link> <span class=md-ellipsis> 8 Paper replicating (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/08_pytorch_paper_replicating.pdf class=md-nav__link> <span class=md-ellipsis> 8 Paper replicating (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/09_pytorch_model_deployment.pdf class=md-nav__link> <span class=md-ellipsis> 9 Model deployment(pdf) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4 checked> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class=md-ellipsis> Tensorflow </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=true> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Tensorflow </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 0 Tensorflow fundamentals </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 0 Tensorflow fundamentals </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-tensorflow class=md-nav__link> <span class=md-ellipsis> What is TensorFlow? </span> </a> </li> <li class=md-nav__item> <a href=#why-use-tensorflow class=md-nav__link> <span class=md-ellipsis> Why use TensorFlow? </span> </a> </li> <li class=md-nav__item> <a href=#what-were-going-to-cover class=md-nav__link> <span class=md-ellipsis> What we're going to cover </span> </a> </li> <li class=md-nav__item> <a href=#introduction-to-tensors class=md-nav__link> <span class=md-ellipsis> Introduction to Tensors </span> </a> <nav class=md-nav aria-label="Introduction to Tensors"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#creating-tensors-with-tfconstant class=md-nav__link> <span class=md-ellipsis> Creating Tensors with tf.constant() </span> </a> </li> <li class=md-nav__item> <a href=#creating-tensors-with-tfvariable class=md-nav__link> <span class=md-ellipsis> Creating Tensors with tf.Variable() </span> </a> </li> <li class=md-nav__item> <a href=#creating-random-tensors class=md-nav__link> <span class=md-ellipsis> Creating random tensors </span> </a> </li> <li class=md-nav__item> <a href=#other-ways-to-make-tensors class=md-nav__link> <span class=md-ellipsis> Other ways to make tensors </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#getting-information-from-tensors-shape-rank-size class=md-nav__link> <span class=md-ellipsis> Getting information from tensors (shape, rank, size) </span> </a> </li> <li class=md-nav__item> <a href=#manipulating-tensors-tensor-operations class=md-nav__link> <span class=md-ellipsis> Manipulating tensors (tensor operations) </span> </a> <nav class=md-nav aria-label="Manipulating tensors (tensor operations)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#basic-operations class=md-nav__link> <span class=md-ellipsis> Basic operations </span> </a> </li> <li class=md-nav__item> <a href=#matrix-mutliplication class=md-nav__link> <span class=md-ellipsis> Matrix mutliplication </span> </a> </li> <li class=md-nav__item> <a href=#the-dot-product class=md-nav__link> <span class=md-ellipsis> The dot product </span> </a> </li> <li class=md-nav__item> <a href=#matrix-multiplication-tidbits class=md-nav__link> <span class=md-ellipsis> Matrix multiplication tidbits </span> </a> </li> <li class=md-nav__item> <a href=#changing-the-datatype-of-a-tensor class=md-nav__link> <span class=md-ellipsis> Changing the datatype of a tensor </span> </a> </li> <li class=md-nav__item> <a href=#getting-the-absolute-value class=md-nav__link> <span class=md-ellipsis> Getting the absolute value </span> </a> </li> <li class=md-nav__item> <a href=#finding-the-min-max-mean-sum-aggregation class=md-nav__link> <span class=md-ellipsis> Finding the min, max, mean, sum (aggregation) </span> </a> </li> <li class=md-nav__item> <a href=#finding-the-positional-maximum-and-minimum class=md-nav__link> <span class=md-ellipsis> Finding the positional maximum and minimum </span> </a> </li> <li class=md-nav__item> <a href=#squeezing-a-tensor-removing-all-single-dimensions class=md-nav__link> <span class=md-ellipsis> Squeezing a tensor (removing all single dimensions) </span> </a> </li> <li class=md-nav__item> <a href=#one-hot-encoding class=md-nav__link> <span class=md-ellipsis> One-hot encoding </span> </a> </li> <li class=md-nav__item> <a href=#squaring-log-square-root class=md-nav__link> <span class=md-ellipsis> Squaring, log, square root </span> </a> </li> <li class=md-nav__item> <a href=#manipulating-tfvariable-tensors class=md-nav__link> <span class=md-ellipsis> Manipulating tf.Variable tensors </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#tensors-and-numpy class=md-nav__link> <span class=md-ellipsis> Tensors and NumPy </span> </a> </li> <li class=md-nav__item> <a href=#using-tffunction class=md-nav__link> <span class=md-ellipsis> Using @tf.function </span> </a> </li> <li class=md-nav__item> <a href=#finding-access-to-gpus class=md-nav__link> <span class=md-ellipsis> Finding access to GPUs </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> üõ† Exercises </span> </a> </li> <li class=md-nav__item> <a href=#extra-curriculum class=md-nav__link> <span class=md-ellipsis> üìñ Extra-curriculum </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../01_neural_network_regression_in_tensorflow/ class=md-nav__link> <span class=md-ellipsis> 1 Neural network regression </span> </a> </li> <li class=md-nav__item> <a href=../02_neural_network_classification_in_tensorflow/ class=md-nav__link> <span class=md-ellipsis> 2 Neural network classification </span> </a> </li> <li class=md-nav__item> <a href=../04_transfer_learning_in_tensorflow_part_1_feature_extraction/ class=md-nav__link> <span class=md-ellipsis> 4 Transfer learning </span> </a> </li> <li class=md-nav__item> <a href=../10_time_series_forecasting_in_tensorflow/ class=md-nav__link> <span class=md-ellipsis> 10 Time series forecasting </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../OOP_Practice/ class=md-nav__link> <span class=md-ellipsis> OOP Practice </span> </a> </li> <li class=md-nav__item> <a href=../../Visualization/ class=md-nav__link> <span class=md-ellipsis> Visualization </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> M√¥n h·ªçc </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> M√¥n h·ªçc </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> X√°c su·∫•t th·ªëng k√™ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> X√°c su·∫•t th·ªëng k√™ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Subject/XSTK/Exercise2/homework/ class=md-nav__link> <span class=md-ellipsis> B√†i t·∫≠p v·ªÅ nh√† bu·ªïi 2 </span> </a> </li> <li class=md-nav__item> <a href=../../../Subject/XSTK/Exercise3/homework/ class=md-nav__link> <span class=md-ellipsis> B√†i t·∫≠p v·ªÅ nh√† bu·ªïi 3 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> CP-Problem </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> CP-Problem </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/general/ class=md-nav__link> <span class=md-ellipsis> General </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> COCI 2006 2007 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> COCI 2006 2007 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_2> <label class=md-nav__link for=__nav_4_2_2 id=__nav_4_2_2_label tabindex=0> <span class=md-ellipsis> Contest 1 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_2> <span class="md-nav__icon md-icon"></span> Contest 1 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P1_MODULO/ class=md-nav__link> <span class=md-ellipsis> MODULO </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P2_HERMAN/ class=md-nav__link> <span class=md-ellipsis> HERMAN </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P3_OKVIRI/ class=md-nav__link> <span class=md-ellipsis> OKVIRI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P4_SLIKAR/ class=md-nav__link> <span class=md-ellipsis> SLIKAR </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P5_BOND/ class=md-nav__link> <span class=md-ellipsis> BOND </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P6_DEBUG/ class=md-nav__link> <span class=md-ellipsis> DEBUG </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_3> <label class=md-nav__link for=__nav_4_2_3 id=__nav_4_2_3_label tabindex=0> <span class=md-ellipsis> Contest 2 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_3> <span class="md-nav__icon md-icon"></span> Contest 2 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P1_R2/ class=md-nav__link> <span class=md-ellipsis> R2 </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P2_ABC/ class=md-nav__link> <span class=md-ellipsis> ABC </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P3_KOLONE/ class=md-nav__link> <span class=md-ellipsis> KOLONE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P4_SJECISTA/ class=md-nav__link> <span class=md-ellipsis> SJECISTA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P5_STOL/ class=md-nav__link> <span class=md-ellipsis> STOL </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P6_STRAZA/ class=md-nav__link> <span class=md-ellipsis> STRAZA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_4> <label class=md-nav__link for=__nav_4_2_4 id=__nav_4_2_4_label tabindex=0> <span class=md-ellipsis> Contest 3 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_4> <span class="md-nav__icon md-icon"></span> Contest 3 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P1_PATULJCI/ class=md-nav__link> <span class=md-ellipsis> PATULJCI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P2_NPUZZLE/ class=md-nav__link> <span class=md-ellipsis> NPUZZLE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P3_TROJKE/ class=md-nav__link> <span class=md-ellipsis> TROJKE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P4_TENKICI/ class=md-nav__link> <span class=md-ellipsis> TENKICI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P5_BICIKLI/ class=md-nav__link> <span class=md-ellipsis> BICIKLI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P6_LISTA/ class=md-nav__link> <span class=md-ellipsis> LISTA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_5> <label class=md-nav__link for=__nav_4_2_5 id=__nav_4_2_5_label tabindex=0> <span class=md-ellipsis> Contest 4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_5> <span class="md-nav__icon md-icon"></span> Contest 4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P1_Sibice/ class=md-nav__link> <span class=md-ellipsis> SIBICE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P2_Skener/ class=md-nav__link> <span class=md-ellipsis> SKENER </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P3_Prsteni/ class=md-nav__link> <span class=md-ellipsis> PRSTENI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P4_Zbrka/ class=md-nav__link> <span class=md-ellipsis> ZBRKA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P5_Jogurt/ class=md-nav__link> <span class=md-ellipsis> JOGURT </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P6_Ispiti/ class=md-nav__link> <span class=md-ellipsis> ISPITI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_6> <label class=md-nav__link for=__nav_4_2_6 id=__nav_4_2_6_label tabindex=0> <span class=md-ellipsis> Contest 5 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_6_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_6> <span class="md-nav__icon md-icon"></span> Contest 5 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P1_Trik/ class=md-nav__link> <span class=md-ellipsis> TRIK </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P2_Natrij/ class=md-nav__link> <span class=md-ellipsis> NATRIJ </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P3_Tenis/ class=md-nav__link> <span class=md-ellipsis> TENIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P4_Liga/ class=md-nav__link> <span class=md-ellipsis> LIGA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P5_Ivana/ class=md-nav__link> <span class=md-ellipsis> IVANA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P6_Dvaput/ class=md-nav__link> <span class=md-ellipsis> DVAPUT </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_7> <label class=md-nav__link for=__nav_4_2_7 id=__nav_4_2_7_label tabindex=0> <span class=md-ellipsis> Contest 6 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_7_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_7> <span class="md-nav__icon md-icon"></span> Contest 6 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P1_PRASE/ class=md-nav__link> <span class=md-ellipsis> PRASE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P2_MAGIJA/ class=md-nav__link> <span class=md-ellipsis> MAGIJA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P3_MARATON/ class=md-nav__link> <span class=md-ellipsis> MARATON </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P4_KAMEN/ class=md-nav__link> <span class=md-ellipsis> KAMEN </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P5_V/ class=md-nav__link> <span class=md-ellipsis> V </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P6_PROSTOR/ class=md-nav__link> <span class=md-ellipsis> PROSTOR </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_8> <label class=md-nav__link for=__nav_4_2_8 id=__nav_4_2_8_label tabindex=0> <span class=md-ellipsis> Regional </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_8_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_8> <span class="md-nav__icon md-icon"></span> Regional </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P1_BARD/ class=md-nav__link> <span class=md-ellipsis> BARD </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P2_TETRIS/ class=md-nav__link> <span class=md-ellipsis> TETRIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P4_CIRCLE/ class=md-nav__link> <span class=md-ellipsis> CIRCLE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_9> <label class=md-nav__link for=__nav_4_2_9 id=__nav_4_2_9_label tabindex=0> <span class=md-ellipsis> Croatian Olympiad </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_9_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_9> <span class="md-nav__icon md-icon"></span> Croatian Olympiad </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P1_PATRIK/ class=md-nav__link> <span class=md-ellipsis> PATRIK </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P2_POLICIJA/ class=md-nav__link> <span class=md-ellipsis> POLICIJA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P3_SABOR/ class=md-nav__link> <span class=md-ellipsis> SABOR </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class=md-ellipsis> COCI 2007 2008 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> COCI 2007 2008 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_2> <label class=md-nav__link for=__nav_4_3_2 id=__nav_4_3_2_label tabindex=0> <span class=md-ellipsis> Contest 1 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_2> <span class="md-nav__icon md-icon"></span> Contest 1 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P1_CETVRTA/ class=md-nav__link> <span class=md-ellipsis> CETVRTA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P2_PEG/ class=md-nav__link> <span class=md-ellipsis> PEG </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P3_PRINOVA/ class=md-nav__link> <span class=md-ellipsis> PRINOVA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P4_ZAPIS/ class=md-nav__link> <span class=md-ellipsis> ZAPIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P5_SREDNJI/ class=md-nav__link> <span class=md-ellipsis> SREDNJI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P6_STAZA/ class=md-nav__link> <span class=md-ellipsis> STAZA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_3> <label class=md-nav__link for=__nav_4_3_3 id=__nav_4_3_3_label tabindex=0> <span class=md-ellipsis> Contest 4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_3> <span class="md-nav__icon md-icon"></span> Contest 4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P1_CIRCLE/ class=md-nav__link> <span class=md-ellipsis> CIRCLE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P2_VECI/ class=md-nav__link> <span class=md-ellipsis> VECI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P3_LEKTIRA/ class=md-nav__link> <span class=md-ellipsis> LEKTIRA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P4_MUZICARI/ class=md-nav__link> <span class=md-ellipsis> MUZICARI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P5_POKLON/ class=md-nav__link> <span class=md-ellipsis> POKLON </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P6_KOCKE/ class=md-nav__link> <span class=md-ellipsis> KOCKE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_4> <label class=md-nav__link for=__nav_4_3_4 id=__nav_4_3_4_label tabindex=0> <span class=md-ellipsis> Contest 6 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_4> <span class="md-nav__icon md-icon"></span> Contest 6 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P1_PARKING/ class=md-nav__link> <span class=md-ellipsis> PARKING </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P2_SEMAFORI/ class=md-nav__link> <span class=md-ellipsis> SEMAFORI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P3_GRANICA/ class=md-nav__link> <span class=md-ellipsis> GRANICA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P4_GEORGE/ class=md-nav__link> <span class=md-ellipsis> GEORGE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P5_PRINCEZA/ class=md-nav__link> <span class=md-ellipsis> PRINCEZA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P6_CESTARINE/ class=md-nav__link> <span class=md-ellipsis> CESTARINE </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../other/other/ class=md-nav__link> <span class=md-ellipsis> Other </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-tensorflow class=md-nav__link> <span class=md-ellipsis> What is TensorFlow? </span> </a> </li> <li class=md-nav__item> <a href=#why-use-tensorflow class=md-nav__link> <span class=md-ellipsis> Why use TensorFlow? </span> </a> </li> <li class=md-nav__item> <a href=#what-were-going-to-cover class=md-nav__link> <span class=md-ellipsis> What we're going to cover </span> </a> </li> <li class=md-nav__item> <a href=#introduction-to-tensors class=md-nav__link> <span class=md-ellipsis> Introduction to Tensors </span> </a> <nav class=md-nav aria-label="Introduction to Tensors"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#creating-tensors-with-tfconstant class=md-nav__link> <span class=md-ellipsis> Creating Tensors with tf.constant() </span> </a> </li> <li class=md-nav__item> <a href=#creating-tensors-with-tfvariable class=md-nav__link> <span class=md-ellipsis> Creating Tensors with tf.Variable() </span> </a> </li> <li class=md-nav__item> <a href=#creating-random-tensors class=md-nav__link> <span class=md-ellipsis> Creating random tensors </span> </a> </li> <li class=md-nav__item> <a href=#other-ways-to-make-tensors class=md-nav__link> <span class=md-ellipsis> Other ways to make tensors </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#getting-information-from-tensors-shape-rank-size class=md-nav__link> <span class=md-ellipsis> Getting information from tensors (shape, rank, size) </span> </a> </li> <li class=md-nav__item> <a href=#manipulating-tensors-tensor-operations class=md-nav__link> <span class=md-ellipsis> Manipulating tensors (tensor operations) </span> </a> <nav class=md-nav aria-label="Manipulating tensors (tensor operations)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#basic-operations class=md-nav__link> <span class=md-ellipsis> Basic operations </span> </a> </li> <li class=md-nav__item> <a href=#matrix-mutliplication class=md-nav__link> <span class=md-ellipsis> Matrix mutliplication </span> </a> </li> <li class=md-nav__item> <a href=#the-dot-product class=md-nav__link> <span class=md-ellipsis> The dot product </span> </a> </li> <li class=md-nav__item> <a href=#matrix-multiplication-tidbits class=md-nav__link> <span class=md-ellipsis> Matrix multiplication tidbits </span> </a> </li> <li class=md-nav__item> <a href=#changing-the-datatype-of-a-tensor class=md-nav__link> <span class=md-ellipsis> Changing the datatype of a tensor </span> </a> </li> <li class=md-nav__item> <a href=#getting-the-absolute-value class=md-nav__link> <span class=md-ellipsis> Getting the absolute value </span> </a> </li> <li class=md-nav__item> <a href=#finding-the-min-max-mean-sum-aggregation class=md-nav__link> <span class=md-ellipsis> Finding the min, max, mean, sum (aggregation) </span> </a> </li> <li class=md-nav__item> <a href=#finding-the-positional-maximum-and-minimum class=md-nav__link> <span class=md-ellipsis> Finding the positional maximum and minimum </span> </a> </li> <li class=md-nav__item> <a href=#squeezing-a-tensor-removing-all-single-dimensions class=md-nav__link> <span class=md-ellipsis> Squeezing a tensor (removing all single dimensions) </span> </a> </li> <li class=md-nav__item> <a href=#one-hot-encoding class=md-nav__link> <span class=md-ellipsis> One-hot encoding </span> </a> </li> <li class=md-nav__item> <a href=#squaring-log-square-root class=md-nav__link> <span class=md-ellipsis> Squaring, log, square root </span> </a> </li> <li class=md-nav__item> <a href=#manipulating-tfvariable-tensors class=md-nav__link> <span class=md-ellipsis> Manipulating tf.Variable tensors </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#tensors-and-numpy class=md-nav__link> <span class=md-ellipsis> Tensors and NumPy </span> </a> </li> <li class=md-nav__item> <a href=#using-tffunction class=md-nav__link> <span class=md-ellipsis> Using @tf.function </span> </a> </li> <li class=md-nav__item> <a href=#finding-access-to-gpus class=md-nav__link> <span class=md-ellipsis> Finding access to GPUs </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> üõ† Exercises </span> </a> </li> <li class=md-nav__item> <a href=#extra-curriculum class=md-nav__link> <span class=md-ellipsis> üìñ Extra-curriculum </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <p><a href=https://colab.research.google.com/github/mrdbourke/tensorflow-deep-learning/blob/main/00_tensorflow_fundamentals.ipynb target=_parent><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p> <h1 id=00-getting-started-with-tensorflow-a-guide-to-the-fundamentals>00. Getting started with TensorFlow: A guide to the fundamentals</h1> <h2 id=what-is-tensorflow>What is TensorFlow?</h2> <p><a href=https://www.tensorflow.org/ >TensorFlow</a> is an open-source end-to-end machine learning library for preprocessing data, modelling data and serving models (getting them into the hands of others).</p> <h2 id=why-use-tensorflow>Why use TensorFlow?</h2> <p>Rather than building machine learning and deep learning models from scratch, it's more likely you'll use a library such as TensorFlow. This is because it contains many of the most common machine learning functions you'll want to use.</p> <h2 id=what-were-going-to-cover>What we're going to cover</h2> <p>TensorFlow is vast. But the main premise is simple: turn data into numbers (tensors) and build machine learning algorithms to find patterns in them.</p> <p>In this notebook we cover some of the most fundamental TensorFlow operations, more specificially: * Introduction to tensors (creating tensors) * Getting information from tensors (tensor attributes) * Manipulating tensors (tensor operations) * Tensors and NumPy * Using @tf.function (a way to speed up your regular Python functions) * Using GPUs with TensorFlow * Exercises to try</p> <p>Things to note: * Many of the conventions here will happen automatically behind the scenes (when you build a model) but it's worth knowing so if you see any of these things, you know what's happening. * For any TensorFlow function you see, it's important to be able to check it out in the documentation, for example, going to the Python API docs for all functions and searching for what you need: https://www.tensorflow.org/api_docs/python/ (don't worry if this seems overwhelming at first, with enough practice, you'll get used to navigating the documentaiton).</p> <div class=highlight><pre><span></span><code><span class=c1># Create timestamp</span>
<span class=kn>import</span><span class=w> </span><span class=nn>datetime</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Notebook last run (end-to-end): </span><span class=si>{</span><span class=n>datetime</span><span class=o>.</span><span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Notebook last run (end-to-end): 2024-04-18 13:54:27.276504
</code></pre></div> <h2 id=introduction-to-tensors>Introduction to Tensors</h2> <p>If you've ever used NumPy, <a href=https://www.tensorflow.org/guide/tensor>tensors</a> are kind of like NumPy arrays (we'll see more on this later).</p> <p>For the sake of this notebook and going forward, you can think of a tensor as a multi-dimensional numerical representation (also referred to as n-dimensional, where n can be any number) of something. Where something can be almost anything you can imagine: * It could be numbers themselves (using tensors to represent the price of houses). * It could be an image (using tensors to represent the pixels of an image). * It could be text (using tensors to represent words). * Or it could be some other form of information (or data) you want to represent with numbers.</p> <p>The main difference between tensors and NumPy arrays (also an n-dimensional array of numbers) is that tensors can be used on <a href=https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/ >GPUs (graphical processing units)</a> and <a href=https://en.wikipedia.org/wiki/Tensor_processing_unit>TPUs (tensor processing units)</a>. </p> <p>The benefit of being able to run on GPUs and TPUs is faster computation, this means, if we wanted to find patterns in the numerical representations of our data, we can generally find them faster using GPUs and TPUs.</p> <p>Okay, we've been talking enough about tensors, let's see them.</p> <p>The first thing we'll do is import TensorFlow under the common alias <code>tf</code>.</p> <div class=highlight><pre><span></span><code><span class=c1># Import TensorFlow</span>
<span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>
<span class=nb>print</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span> <span class=c1># find the version number (should be 2.x+)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>2.17.0-dev20240226
</code></pre></div> <h3 id=creating-tensors-with-tfconstant>Creating Tensors with <code>tf.constant()</code></h3> <p>As mentioned before, in general, you usually won't create tensors yourself. This is because TensorFlow has modules built-in (such as <a href=https://www.tensorflow.org/api_docs/python/tf/io><code>tf.io</code></a> and <a href=https://www.tensorflow.org/guide/data><code>tf.data</code></a>) which are able to read your data sources and automatically convert them to tensors and then later on, neural network models will process these for us.</p> <p>But for now, because we're getting familar with tensors themselves and how to manipulate them, we'll see how we can create them ourselves.</p> <p>We'll begin by using <a href=https://www.tensorflow.org/api_docs/python/tf/constant><code>tf.constant()</code></a>.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a scalar (rank 0 tensor)</span>
<span class=n>scalar</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=mi>7</span><span class=p>)</span>
<span class=n>scalar</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(), dtype=int32, numpy=7&gt;
</code></pre></div> <p>A scalar is known as a rank 0 tensor. Because it has no dimensions (it's just a number).</p> <blockquote> <p>üîë <strong>Note:</strong> For now, you don't need to know too much about the different ranks of tensors (but we will see more on this later). The important point is knowing tensors can have an unlimited range of dimensions (the exact amount will depend on what data you're representing).</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Check the number of dimensions of a tensor (ndim stands for number of dimensions)</span>
<span class=n>scalar</span><span class=o>.</span><span class=n>ndim</span>
</code></pre></div> <div class=highlight><pre><span></span><code>0
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Create a vector (more than 0 dimensions)</span>
<span class=n>vector</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>])</span>
<span class=n>vector</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([10, 10])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check the number of dimensions of our vector tensor</span>
<span class=n>vector</span><span class=o>.</span><span class=n>ndim</span>
</code></pre></div> <div class=highlight><pre><span></span><code>1
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Create a matrix (more than 1 dimension)</span>
<span class=n>matrix</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span>
                      <span class=p>[</span><span class=mi>7</span><span class=p>,</span> <span class=mi>10</span><span class=p>]])</span>
<span class=n>matrix</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[10,  7],
       [ 7, 10]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>matrix</span><span class=o>.</span><span class=n>ndim</span>
</code></pre></div> <div class=highlight><pre><span></span><code>2
</code></pre></div> <p>By default, TensorFlow creates tensors with either an <code>int32</code> or <code>float32</code> datatype.</p> <p>This is known as [32-bit precision](https://en.wikipedia.org/wiki/Precision_(computer_science) (the higher the number, the more precise the number, the more space it takes up on your computer).</p> <div class=highlight><pre><span></span><code><span class=c1># Create another matrix and define the datatype</span>
<span class=n>another_matrix</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mf>10.</span><span class=p>,</span> <span class=mf>7.</span><span class=p>],</span>
                              <span class=p>[</span><span class=mf>3.</span><span class=p>,</span> <span class=mf>2.</span><span class=p>],</span>
                              <span class=p>[</span><span class=mf>8.</span><span class=p>,</span> <span class=mf>9.</span><span class=p>]],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span> <span class=c1># specify the datatype with &#39;dtype&#39;</span>
<span class=n>another_matrix</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 2), dtype=float16, numpy=
array([[10.,  7.],
       [ 3.,  2.],
       [ 8.,  9.]], dtype=float16)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Even though another_matrix contains more numbers, its dimensions stay the same</span>
<span class=n>another_matrix</span><span class=o>.</span><span class=n>ndim</span>
</code></pre></div> <div class=highlight><pre><span></span><code>2
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># How about a tensor? (more than 2 dimensions, although, all of the above items are also technically tensors)</span>
<span class=n>tensor</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span>
                       <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>]],</span>
                      <span class=p>[[</span><span class=mi>7</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>9</span><span class=p>],</span>
                       <span class=p>[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>11</span><span class=p>,</span> <span class=mi>12</span><span class=p>]],</span>
                      <span class=p>[[</span><span class=mi>13</span><span class=p>,</span> <span class=mi>14</span><span class=p>,</span> <span class=mi>15</span><span class=p>],</span>
                       <span class=p>[</span><span class=mi>16</span><span class=p>,</span> <span class=mi>17</span><span class=p>,</span> <span class=mi>18</span><span class=p>]]])</span>
<span class=n>tensor</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy=
array([[[ 1,  2,  3],
        [ 4,  5,  6]],

       [[ 7,  8,  9],
        [10, 11, 12]],

       [[13, 14, 15],
        [16, 17, 18]]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>tensor</span><span class=o>.</span><span class=n>ndim</span>
</code></pre></div> <div class=highlight><pre><span></span><code>3
</code></pre></div> <p>This is known as a rank 3 tensor (3-dimensions), however a tensor can have an arbitrary (unlimited) amount of dimensions.</p> <p>For example, you might turn a series of images into tensors with shape (224, 224, 3, 32), where: * 224, 224 (the first 2 dimensions) are the height and width of the images in pixels. * 3 is the number of colour channels of the image (red, green blue). * 32 is the batch size (the number of images a neural network sees at any one time).</p> <p>All of the above variables we've created are actually tensors. But you may also hear them referred to as their different names (the ones we gave them): * <strong>scalar</strong>: a single number. * <strong>vector</strong>: a number with direction (e.g. wind speed with direction). * <strong>matrix</strong>: a 2-dimensional array of numbers. * <strong>tensor</strong>: an n-dimensional arrary of numbers (where n can be any number, a 0-dimension tensor is a scalar, a 1-dimension tensor is a vector). </p> <p>To add to the confusion, the terms matrix and tensor are often used interchangably.</p> <p>Going forward since we're using TensorFlow, everything we refer to and use will be tensors.</p> <p>For more on the mathematical difference between scalars, vectors and matrices see the <a href=https://www.mathsisfun.com/algebra/scalar-vector-matrix.html>visual algebra post by Math is Fun</a>.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/00-scalar-vector-matrix-tensor.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="difference between scalar, vector, matrix, tensor" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/00-scalar-vector-matrix-tensor.png></a></p> <h3 id=creating-tensors-with-tfvariable>Creating Tensors with <code>tf.Variable()</code></h3> <p>You can also (although you likely rarely will, because often, when working with data, tensors are created for you automatically) create tensors using <a href=https://www.tensorflow.org/api_docs/python/tf/Variable><code>tf.Variable()</code></a>.</p> <p>The difference between <code>tf.Variable()</code> and <code>tf.constant()</code> is tensors created with <code>tf.constant()</code> are immutable (can't be changed, can only be used to create a new tensor), where as, tensors created with <code>tf.Variable()</code> are mutable (can be changed).</p> <div class=highlight><pre><span></span><code><span class=c1># Create the same tensor with tf.Variable() and tf.constant()</span>
<span class=n>changeable_tensor</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>([</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>])</span>
<span class=n>unchangeable_tensor</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>])</span>
<span class=n>changeable_tensor</span><span class=p>,</span> <span class=n>unchangeable_tensor</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([10,  7])&gt;,
 &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([10,  7])&gt;)
</code></pre></div> <p>Now let's try to change one of the elements of the changable tensor.</p> <div class=highlight><pre><span></span><code><span class=c1># Will error (requires the .assign() method)</span>
<span class=n>changeable_tensor</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mi>7</span>
<span class=n>changeable_tensor</span>
</code></pre></div> <div class=highlight><pre><span></span><code>---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

Cell In[15], line 2
      1 # Will error (requires the .assign() method)
----&gt; 2 changeable_tensor[0] = 7
      3 changeable_tensor


TypeError: &#39;ResourceVariable&#39; object does not support item assignment
</code></pre></div> <p>To change an element of a <code>tf.Variable()</code> tensor requires the <code>assign()</code> method.</p> <div class=highlight><pre><span></span><code><span class=c1># Won&#39;t error</span>
<span class=n>changeable_tensor</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=mi>7</span><span class=p>)</span>
<span class=n>changeable_tensor</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([7, 7])&gt;
</code></pre></div> <p>Now let's try to change a value in a <code>tf.constant()</code> tensor.</p> <div class=highlight><pre><span></span><code><span class=c1># Will error (can&#39;t change tf.constant())</span>
<span class=n>unchangeable_tensor</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=mi>7</span><span class=p>)</span>
<span class=n>unchangleable_tensor</span>
</code></pre></div> <div class=highlight><pre><span></span><code>---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

Cell In[17], line 2
      1 # Will error (can&#39;t change tf.constant())
----&gt; 2 unchangeable_tensor[0].assign(7)
      3 unchangleable_tensor


File D:\anaconda\envs\py3-TF2.0\Lib\site-packages\tensorflow\python\framework\tensor.py:260, in Tensor.__getattr__(self, name)
    252 if name in {&quot;T&quot;, &quot;astype&quot;, &quot;ravel&quot;, &quot;transpose&quot;, &quot;reshape&quot;, &quot;clip&quot;, &quot;size&quot;,
    253             &quot;tolist&quot;, &quot;data&quot;}:
    254   # TODO(wangpeng): Export the enable_numpy_behavior knob
    255   raise AttributeError(
    256       f&quot;{type(self).__name__} object has no attribute &#39;{name}&#39;. &quot; + &quot;&quot;&quot;
    257     If you are looking for numpy-related methods, please run the following:
    258     tf.experimental.numpy.experimental_enable_numpy_behavior()
    259   &quot;&quot;&quot;)
--&gt; 260 self.__getattribute__(name)


AttributeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object has no attribute &#39;assign&#39;
</code></pre></div> <p>Which one should you use? <code>tf.constant()</code> or <code>tf.Variable()</code>?</p> <p>It will depend on what your problem requires. However, most of the time, TensorFlow will automatically choose for you (when loading data or modelling data).</p> <h3 id=creating-random-tensors>Creating random tensors</h3> <p>Random tensors are tensors of some abitrary size which contain random numbers.</p> <p>Why would you want to create random tensors? </p> <p>This is what neural networks use to intialize their weights (patterns) that they're trying to learn in the data.</p> <p>For example, the process of a neural network learning often involves taking a random n-dimensional array of numbers and refining them until they represent some kind of pattern (a compressed way to represent the original data).</p> <p><strong>How a network learns</strong> <a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/00-how-a-network-learns.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="how a network learns" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/00-how-a-network-learns.png></a> <em>A network learns by starting with random patterns (1) then going through demonstrative examples of data (2) whilst trying to update its random patterns to represent the examples (3).</em></p> <p>We can create random tensors by using the <a href=https://www.tensorflow.org/guide/random_numbers#the_tfrandomgenerator_class><code>tf.random.Generator</code></a> class.</p> <div class=highlight><pre><span></span><code><span class=c1># Create two random (but the same) tensors</span>
<span class=n>random_1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>Generator</span><span class=o>.</span><span class=n>from_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span> <span class=c1># set the seed for reproducibility</span>
<span class=n>random_1</span> <span class=o>=</span> <span class=n>random_1</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span> <span class=c1># create tensor from a normal distribution </span>
<span class=n>random_2</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>Generator</span><span class=o>.</span><span class=n>from_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
<span class=n>random_2</span> <span class=o>=</span> <span class=n>random_2</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>

<span class=c1># Are they equal?</span>
<span class=n>random_1</span><span class=p>,</span> <span class=n>random_2</span><span class=p>,</span> <span class=n>random_1</span> <span class=o>==</span> <span class=n>random_2</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
 array([[-0.7565803 , -0.06854702],
        [ 0.07595026, -1.2573844 ],
        [-0.23193763, -1.8107855 ]], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
 array([[-0.7565803 , -0.06854702],
        [ 0.07595026, -1.2573844 ],
        [-0.23193763, -1.8107855 ]], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy=
 array([[ True,  True],
        [ True,  True],
        [ True,  True]])&gt;)
</code></pre></div> <p>The random tensors we've made are actually <a href=https://www.computerhope.com/jargon/p/pseudo-random.htm>pseudorandom numbers</a> (they appear as random, but really aren't).</p> <p>If we set a seed we'll get the same random numbers (if you've ever used NumPy, this is similar to <code>np.random.seed(42)</code>). </p> <p>Setting the seed says, "hey, create some random numbers, but flavour them with X" (X is the seed).</p> <p>What do you think will happen when we change the seed?</p> <div class=highlight><pre><span></span><code><span class=c1># Create two random (and different) tensors</span>
<span class=n>random_3</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>Generator</span><span class=o>.</span><span class=n>from_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
<span class=n>random_3</span> <span class=o>=</span> <span class=n>random_3</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
<span class=n>random_4</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>Generator</span><span class=o>.</span><span class=n>from_seed</span><span class=p>(</span><span class=mi>11</span><span class=p>)</span>
<span class=n>random_4</span> <span class=o>=</span> <span class=n>random_4</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>

<span class=c1># Check the tensors and see if they are equal</span>
<span class=n>random_3</span><span class=p>,</span> <span class=n>random_4</span><span class=p>,</span> <span class=n>random_1</span> <span class=o>==</span> <span class=n>random_3</span><span class=p>,</span> <span class=n>random_3</span> <span class=o>==</span> <span class=n>random_4</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
 array([[-0.7565803 , -0.06854702],
        [ 0.07595026, -1.2573844 ],
        [-0.23193763, -1.8107855 ]], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
 array([[ 0.27305737, -0.29925638],
        [-0.3652325 ,  0.61883307],
        [-1.0130816 ,  0.28291714]], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy=
 array([[ True,  True],
        [ True,  True],
        [ True,  True]])&gt;,
 &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy=
 array([[False, False],
        [False, False],
        [False, False]])&gt;)
</code></pre></div> <p>What if you wanted to shuffle the order of a tensor?</p> <p>Wait, why would you want to do that?</p> <p>Let's say you working with 15,000 images of cats and dogs and the first 10,000 images of were of cats and the next 5,000 were of dogs. This order could effect how a neural network learns (it may overfit by learning the order of the data), instead, it might be a good idea to move your data around.</p> <div class=highlight><pre><span></span><code><span class=c1># Shuffle a tensor (valuable for when you want to shuffle your data)</span>
<span class=n>not_shuffled</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span>
                            <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
                            <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>5</span><span class=p>]])</span>
<span class=c1># Gets different results each time</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>not_shuffled</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 3,  4],
       [ 2,  5],
       [10,  7]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Shuffle in the same order every time using the seed parameter (won&#39;t acutally be the same)</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>not_shuffled</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 2,  5],
       [ 3,  4],
       [10,  7]])&gt;
</code></pre></div> <p>Wait... why didn't the numbers come out the same?</p> <p>It's due to rule #4 of the <a href=https://www.tensorflow.org/api_docs/python/tf/random/set_seed><code>tf.random.set_seed()</code></a> documentation.</p> <blockquote> <p>"4. If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence."</p> </blockquote> <p><code>tf.random.set_seed(42)</code> sets the global seed, and the <code>seed</code> parameter in <code>tf.random.shuffle(seed=42)</code> sets the operation seed.</p> <p>Because, "Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed."</p> <div class=highlight><pre><span></span><code><span class=c1># Shuffle in the same order every time</span>

<span class=c1># Set the global random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Set the operation random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>not_shuffled</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[10,  7],
       [ 3,  4],
       [ 2,  5]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Set the global random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span> <span class=c1># if you comment this out you&#39;ll get different results</span>

<span class=c1># Set the operation random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>not_shuffled</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 3,  4],
       [ 2,  5],
       [10,  7]])&gt;
</code></pre></div> <h3 id=other-ways-to-make-tensors>Other ways to make tensors</h3> <p>Though you might rarely use these (remember, many tensor operations are done behind the scenes for you), you can use <a href=https://www.tensorflow.org/api_docs/python/tf/ones><code>tf.ones()</code></a> to create a tensor of all ones and <a href=https://www.tensorflow.org/api_docs/python/tf/zeros><code>tf.zeros()</code></a> to create a tensor of all zeros.</p> <div class=highlight><pre><span></span><code><span class=c1># Make a tensor of all ones</span>
<span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[1., 1.],
       [1., 1.],
       [1., 1.]], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make a tensor of all zeros</span>
<span class=n>tf</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=
array([[0., 0.],
       [0., 0.],
       [0., 0.]], dtype=float32)&gt;
</code></pre></div> <p>You can also turn NumPy arrays in into tensors.</p> <p>Remember, the main difference between tensors and NumPy arrays is that tensors can be run on GPUs.</p> <blockquote> <p>üîë <strong>Note:</strong> A matrix or tensor is typically represented by a capital letter (e.g. <code>X</code> or <code>A</code>) where as a vector is typically represented by a lowercase letter (e.g. <code>y</code> or <code>b</code>).</p> </blockquote> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=n>numpy_A</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span> <span class=c1># create a NumPy array between 1 and 25</span>
<span class=n>A</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>numpy_A</span><span class=p>,</span>  
                <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span> <span class=c1># note: the shape total (2*4*3) has to match the number of elements in the array</span>
<span class=n>numpy_A</span><span class=p>,</span> <span class=n>A</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24]),
 &lt;tf.Tensor: shape=(2, 4, 3), dtype=int32, numpy=
 array([[[ 1,  2,  3],
         [ 4,  5,  6],
         [ 7,  8,  9],
         [10, 11, 12]],

        [[13, 14, 15],
         [16, 17, 18],
         [19, 20, 21],
         [22, 23, 24]]])&gt;)
</code></pre></div> <h2 id=getting-information-from-tensors-shape-rank-size>Getting information from tensors (shape, rank, size)</h2> <p>There will be times when you'll want to get different pieces of information from your tensors, in particuluar, you should know the following tensor vocabulary: * <strong>Shape:</strong> The length (number of elements) of each of the dimensions of a tensor. * <strong>Rank:</strong> The number of tensor dimensions. A scalar has rank 0, a vector has rank 1, a matrix is rank 2, a tensor has rank n. * <strong>Axis</strong> or <strong>Dimension:</strong> A particular dimension of a tensor. * <strong>Size:</strong> The total number of items in the tensor.</p> <p>You'll use these especially when you're trying to line up the shapes of your data to the shapes of your model. For example, making sure the shape of your image tensors are the same shape as your models input layer.</p> <p>We've already seen one of these before using the <code>ndim</code> attribute. Let's see the rest.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a rank 4 tensor (4 dimensions)</span>
<span class=n>rank_4_tensor</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>])</span>
<span class=n>rank_4_tensor</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]],


       [[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]]], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>rank_4_tensor</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>rank_4_tensor</span><span class=o>.</span><span class=n>ndim</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=n>rank_4_tensor</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(TensorShape([2, 3, 4, 5]), 4, &lt;tf.Tensor: shape=(), dtype=int32, numpy=120&gt;)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Get various attributes of tensor</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Datatype of every element:&quot;</span><span class=p>,</span> <span class=n>rank_4_tensor</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Number of dimensions (rank):&quot;</span><span class=p>,</span> <span class=n>rank_4_tensor</span><span class=o>.</span><span class=n>ndim</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Shape of tensor:&quot;</span><span class=p>,</span> <span class=n>rank_4_tensor</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Elements along axis 0 of tensor:&quot;</span><span class=p>,</span> <span class=n>rank_4_tensor</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Elements along last axis of tensor:&quot;</span><span class=p>,</span> <span class=n>rank_4_tensor</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Total number of elements (2*3*4*5):&quot;</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=n>rank_4_tensor</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>())</span> <span class=c1># .numpy() converts to NumPy array</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Datatype of every element: &lt;dtype: &#39;float32&#39;&gt;
Number of dimensions (rank): 4
Shape of tensor: (2, 3, 4, 5)
Elements along axis 0 of tensor: 2
Elements along last axis of tensor: 5
Total number of elements (2*3*4*5): 120
</code></pre></div> <p>You can also index tensors just like Python lists.</p> <div class=highlight><pre><span></span><code><span class=c1># Get the first 2 items of each dimension</span>
<span class=n>rank_4_tensor</span><span class=p>[:</span><span class=mi>2</span><span class=p>,</span> <span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=p>:</span><span class=mi>2</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2, 2, 2), dtype=float32, numpy=
array([[[[0., 0.],
         [0., 0.]],

        [[0., 0.],
         [0., 0.]]],


       [[[0., 0.],
         [0., 0.]],

        [[0., 0.],
         [0., 0.]]]], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Get the dimension from each index except for the final one</span>
<span class=n>rank_4_tensor</span><span class=p>[:</span><span class=mi>1</span><span class=p>,</span> <span class=p>:</span><span class=mi>1</span><span class=p>,</span> <span class=p>:</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(1, 1, 1, 5), dtype=float32, numpy=array([[[[0., 0., 0., 0., 0.]]]], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Create a rank 2 tensor (2 dimensions)</span>
<span class=n>rank_2_tensor</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span>
                             <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>]])</span>

<span class=c1># Get the last item of each row</span>
<span class=n>rank_2_tensor</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([7, 4])&gt;
</code></pre></div> <p>You can also add dimensions to your tensor whilst keeping the same information present using <code>tf.newaxis</code>. </p> <div class=highlight><pre><span></span><code><span class=c1># Add an extra dimension (to the end)</span>
<span class=n>rank_3_tensor</span> <span class=o>=</span> <span class=n>rank_2_tensor</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>newaxis</span><span class=p>]</span> <span class=c1># in Python &quot;...&quot; means &quot;all dimensions prior to&quot;</span>
<span class=n>rank_2_tensor</span><span class=p>,</span> <span class=n>rank_3_tensor</span> <span class=c1># shape (2, 2), shape (2, 2, 1)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
 array([[10,  7],
        [ 3,  4]])&gt;,
 &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=
 array([[[10],
         [ 7]],

        [[ 3],
         [ 4]]])&gt;)
</code></pre></div> <p>You can achieve the same using <a href=https://www.tensorflow.org/api_docs/python/tf/expand_dims><code>tf.expand_dims()</code></a>.</p> <div class=highlight><pre><span></span><code><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>rank_2_tensor</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span> <span class=c1># &quot;-1&quot; means last axis</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=
array([[[10],
        [ 7]],

       [[ 3],
        [ 4]]])&gt;
</code></pre></div> <h2 id=manipulating-tensors-tensor-operations>Manipulating tensors (tensor operations)</h2> <p>Finding patterns in tensors (numberical representation of data) requires manipulating them.</p> <p>Again, when building models in TensorFlow, much of this pattern discovery is done for you.</p> <h3 id=basic-operations>Basic operations</h3> <p>You can perform many of the basic mathematical operations directly on tensors using Pyhton operators such as, <code>+</code>, <code>-</code>, <code>*</code>.</p> <div class=highlight><pre><span></span><code><span class=c1># You can add values to a tensor using the addition operator</span>
<span class=n>tensor</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>]])</span>
<span class=n>tensor</span> <span class=o>+</span> <span class=mi>10</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[20, 17],
       [13, 14]])&gt;
</code></pre></div> <p>Since we used <code>tf.constant()</code>, the original tensor is unchanged (the addition gets done on a copy).</p> <div class=highlight><pre><span></span><code><span class=c1># Original tensor unchanged</span>
<span class=n>tensor</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[10,  7],
       [ 3,  4]])&gt;
</code></pre></div> <p>Other operators also work.</p> <div class=highlight><pre><span></span><code><span class=c1># Multiplication (known as element-wise multiplication)</span>
<span class=n>tensor</span> <span class=o>*</span> <span class=mi>10</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[100,  70],
       [ 30,  40]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Subtraction</span>
<span class=n>tensor</span> <span class=o>-</span> <span class=mi>10</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 0, -3],
       [-7, -6]])&gt;
</code></pre></div> <p>You can also use the equivalent TensorFlow function. Using the TensorFlow function (where possible) has the advantage of being sped up later down the line when running as part of a <a href=https://www.tensorflow.org/tensorboard/graphs>TensorFlow graph</a>.</p> <div class=highlight><pre><span></span><code><span class=c1># Use the tensorflow function equivalent of the &#39;*&#39; (multiply) operator</span>
<span class=n>tf</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[100,  70],
       [ 30,  40]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># The original tensor is still unchanged</span>
<span class=n>tensor</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[10,  7],
       [ 3,  4]])&gt;
</code></pre></div> <h3 id=matrix-mutliplication>Matrix mutliplication</h3> <p>One of the most common operations in machine learning algorithms is <a href=https://www.mathsisfun.com/algebra/matrix-multiplying.html>matrix multiplication</a>.</p> <p>TensorFlow implements this matrix multiplication functionality in the <a href=https://www.tensorflow.org/api_docs/python/tf/linalg/matmul><code>tf.matmul()</code></a> method.</p> <p>The main two rules for matrix multiplication to remember are: 1. The inner dimensions must match: * <code>(3, 5) @ (3, 5)</code> won't work * <code>(5, 3) @ (3, 5)</code> will work * <code>(3, 5) @ (5, 3)</code> will work 2. The resulting matrix has the shape of the outer dimensions: * <code>(5, 3) @ (3, 5)</code> -&gt; <code>(5, 5)</code> * <code>(3, 5) @ (5, 3)</code> -&gt; <code>(3, 3)</code></p> <blockquote> <p>üîë <strong>Note:</strong> '<code>@</code>' in Python is the symbol for matrix multiplication.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Matrix multiplication in TensorFlow</span>
<span class=nb>print</span><span class=p>(</span><span class=n>tensor</span><span class=p>)</span>
<span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>tensor</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>tf.Tensor(
[[10  7]
 [ 3  4]], shape=(2, 2), dtype=int32)





&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[121,  98],
       [ 42,  37]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Matrix multiplication with Python operator &#39;@&#39;</span>
<span class=n>tensor</span> <span class=o>@</span> <span class=n>tensor</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[121,  98],
       [ 42,  37]])&gt;
</code></pre></div> <p>Both of these examples work because our <code>tensor</code> variable is of shape (2, 2).</p> <p>What if we created some tensors which had mismatched shapes?</p> <div class=highlight><pre><span></span><code><span class=c1># Create (3, 2) tensor</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span>
                 <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
                 <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>]])</span>

<span class=c1># Create another (3, 2) tensor</span>
<span class=n>Y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([[</span><span class=mi>7</span><span class=p>,</span> <span class=mi>8</span><span class=p>],</span>
                 <span class=p>[</span><span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>],</span>
                 <span class=p>[</span><span class=mi>11</span><span class=p>,</span> <span class=mi>12</span><span class=p>]])</span>
<span class=n>X</span><span class=p>,</span> <span class=n>Y</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
 array([[1, 2],
        [3, 4],
        [5, 6]])&gt;,
 &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
 array([[ 7,  8],
        [ 9, 10],
        [11, 12]])&gt;)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Try to matrix multiply them (will error)</span>
<span class=n>X</span> <span class=o>@</span> <span class=n>Y</span>
</code></pre></div> <div class=highlight><pre><span></span><code>---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

Cell In[47], line 2
      1 # Try to matrix multiply them (will error)
----&gt; 2 X @ Y


File D:\anaconda\envs\py3-TF2.0\Lib\site-packages\tensorflow\python\util\traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--&gt; 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb


File D:\anaconda\envs\py3-TF2.0\Lib\site-packages\tensorflow\python\framework\ops.py:5983, in raise_from_not_ok_status(e, name)
   5981 def raise_from_not_ok_status(e, name) -&gt; NoReturn:
   5982   e.message += (&quot; name: &quot; + str(name if name is not None else &quot;&quot;))
-&gt; 5983   raise core._status_to_exception(e) from None


InvalidArgumentError: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [3,2], In[1]: [3,2] [Op:MatMul] name:
</code></pre></div> <p>Trying to matrix multiply two tensors with the shape <code>(3, 2)</code> errors because the inner dimensions don't match.</p> <p>We need to either: * Reshape X to <code>(2, 3)</code> so it's <code>(2, 3) @ (3, 2)</code>. * Reshape Y to <code>(3, 2)</code> so it's <code>(3, 2) @ (2, 3)</code>.</p> <p>We can do this with either: * <a href=https://www.tensorflow.org/api_docs/python/tf/reshape><code>tf.reshape()</code></a> - allows us to reshape a tensor into a defined shape. * <a href=https://www.tensorflow.org/api_docs/python/tf/transpose><code>tf.transpose()</code></a> - switches the dimensions of a given tensor.</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/00-lining-up-dot-products.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="lining up dimensions for dot products" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/00-lining-up-dot-products.png></a></p> <p>Let's try <code>tf.reshape()</code> first.</p> <div class=highlight><pre><span></span><code><span class=c1># Example of reshape (3, 2) -&gt; (2, 3)</span>
<span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>Y</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[ 7,  8,  9],
       [10, 11, 12]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Try matrix multiplication with reshaped Y</span>
<span class=n>X</span> <span class=o>@</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>Y</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=
array([[ 27,  30,  33],
       [ 61,  68,  75],
       [ 95, 106, 117]])&gt;
</code></pre></div> <p>It worked, let's try the same with a reshaped <code>X</code>, except this time we'll use <a href=https://www.tensorflow.org/api_docs/python/tf/transpose><code>tf.transpose()</code></a> and <code>tf.matmul()</code>.</p> <div class=highlight><pre><span></span><code><span class=c1># Example of transpose (3, 2) -&gt; (2, 3)</span>
<span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 3, 5],
       [2, 4, 6]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Try matrix multiplication </span>
<span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>X</span><span class=p>),</span> <span class=n>Y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 89,  98],
       [116, 128]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># You can achieve the same result with parameters</span>
<span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>a</span><span class=o>=</span><span class=n>X</span><span class=p>,</span> <span class=n>b</span><span class=o>=</span><span class=n>Y</span><span class=p>,</span> <span class=n>transpose_a</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>transpose_b</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 89,  98],
       [116, 128]])&gt;
</code></pre></div> <p>Notice the difference in the resulting shapes when tranposing <code>X</code> or reshaping <code>Y</code>.</p> <p>This is because of the 2nd rule mentioned above: * <code>(3, 2) @ (2, 3)</code> -&gt; <code>(3, 3)</code> done with <code>X @ tf.reshape(Y, shape=(2, 3))</code> * <code>(2, 3) @ (3, 2)</code> -&gt; <code>(2, 2)</code> done with <code>tf.matmul(tf.transpose(X), Y)</code></p> <p>This kind of data manipulation is a reminder: you'll spend a lot of your time in machine learning and working with neural networks reshaping data (in the form of tensors) to prepare it to be used with various operations (such as feeding it to a model).</p> <h3 id=the-dot-product>The dot product</h3> <p>Multiplying matrices by eachother is also referred to as the dot product.</p> <p>You can perform the <code>tf.matmul()</code> operation using <a href=https://www.tensorflow.org/api_docs/python/tf/tensordot><code>tf.tensordot()</code></a>. </p> <div class=highlight><pre><span></span><code><span class=c1># Perform the dot product on X and Y (requires X to be transposed)</span>
<span class=n>tf</span><span class=o>.</span><span class=n>tensordot</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>X</span><span class=p>),</span> <span class=n>Y</span><span class=p>,</span> <span class=n>axes</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 89,  98],
       [116, 128]])&gt;
</code></pre></div> <p>You might notice that although using both <code>reshape</code> and <code>tranpose</code> work, you get different results when using each.</p> <p>Let's see an example, first with <code>tf.transpose()</code> then with <code>tf.reshape()</code>.</p> <div class=highlight><pre><span></span><code><span class=c1># Perform matrix multiplication between X and Y (transposed)</span>
<span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>Y</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=
array([[ 23,  29,  35],
       [ 53,  67,  81],
       [ 83, 105, 127]])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Perform matrix multiplication between X and Y (reshaped)</span>
<span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>Y</span><span class=p>,</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=
array([[ 27,  30,  33],
       [ 61,  68,  75],
       [ 95, 106, 117]])&gt;
</code></pre></div> <p>Hmm... they result in different values.</p> <p>Which is strange because when dealing with <code>Y</code> (a <code>(3x2)</code> matrix), reshaping to <code>(2, 3)</code> and tranposing it result in the same shape.</p> <div class=highlight><pre><span></span><code><span class=c1># Check shapes of Y, reshaped Y and tranposed Y</span>
<span class=n>Y</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>Y</span><span class=p>,</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(TensorShape([3, 2]), TensorShape([2, 3]), TensorShape([2, 3]))
</code></pre></div> <p>But calling <code>tf.reshape()</code> and <code>tf.transpose()</code> on <code>Y</code> don't necessarily result in the same values.</p> <div class=highlight><pre><span></span><code><span class=c1># Check values of Y, reshape Y and tranposed Y</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Normal Y:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>Y</span><span class=p>,</span> <span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span><span class=p>)</span> <span class=c1># &quot;\n&quot; for newline</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Y reshaped to (2, 3):&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>Y</span><span class=p>,</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)),</span> <span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Y transposed:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>Y</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Normal Y:
tf.Tensor(
[[ 7  8]
 [ 9 10]
 [11 12]], shape=(3, 2), dtype=int32)

Y reshaped to (2, 3):
tf.Tensor(
[[ 7  8  9]
 [10 11 12]], shape=(2, 3), dtype=int32)

Y transposed:
tf.Tensor(
[[ 7  9 11]
 [ 8 10 12]], shape=(2, 3), dtype=int32)
</code></pre></div> <p>As you can see, the outputs of <code>tf.reshape()</code> and <code>tf.transpose()</code> when called on <code>Y</code>, even though they have the same shape, are different.</p> <p>This can be explained by the default behaviour of each method: * <a href=https://www.tensorflow.org/api_docs/python/tf/reshape><code>tf.reshape()</code></a> - change the shape of the given tensor (first) and then insert values in order they appear (in our case, 7, 8, 9, 10, 11, 12). * <a href=https://www.tensorflow.org/api_docs/python/tf/transpose><code>tf.transpose()</code></a> - swap the order of the axes, by default the last axis becomes the first, however the order can be changed using the <a href=https://www.tensorflow.org/api_docs/python/tf/transpose><code>perm</code> parameter</a>.</p> <p>So which should you use?</p> <p>Again, most of the time these operations (when they need to be run, such as during the training a neural network, will be implemented for you).</p> <p>But generally, whenever performing a matrix multiplication and the shapes of two matrices don't line up, you will transpose (not reshape) one of them in order to line them up.</p> <h3 id=matrix-multiplication-tidbits>Matrix multiplication tidbits</h3> <ul> <li>If we transposed <code>Y</code>, it would be represented as <span class=arithmatex>\(\mathbf{Y}^\mathsf{T}\)</span> (note the capital T for tranpose).</li> <li>Get an illustrative view of matrix multiplication <a href=https://www.mathsisfun.com/algebra/matrix-multiplying.html>by Math is Fun</a>.</li> <li>Try a hands-on demo of matrix multiplcation: http://matrixmultiplication.xyz/ (shown below).</li> </ul> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/00-matrix-multiply-crop.gif data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="visual demo of matrix multiplication" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/00-matrix-multiply-crop.gif></a></p> <h3 id=changing-the-datatype-of-a-tensor>Changing the datatype of a tensor</h3> <p>Sometimes you'll want to alter the default datatype of your tensor. </p> <p>This is common when you want to compute using less precision (e.g. 16-bit floating point numbers vs. 32-bit floating point numbers). </p> <p>Computing with less precision is useful on devices with less computing capacity such as mobile devices (because the less bits, the less space the computations require).</p> <p>You can change the datatype of a tensor using <a href=https://www.tensorflow.org/api_docs/python/tf/cast><code>tf.cast()</code></a>.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a new tensor with default datatype (float32)</span>
<span class=n>B</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>1.7</span><span class=p>,</span> <span class=mf>7.4</span><span class=p>])</span>

<span class=c1># Create a new tensor with default datatype (int32)</span>
<span class=n>C</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>7</span><span class=p>])</span>
<span class=n>B</span><span class=p>,</span> <span class=n>C</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.7, 7.4], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 7])&gt;)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Change from float32 to float16 (reduced precision)</span>
<span class=n>B</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
<span class=n>B</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2,), dtype=float16, numpy=array([1.7, 7.4], dtype=float16)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Change from int32 to float32</span>
<span class=n>C</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
<span class=n>C</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 7.], dtype=float32)&gt;
</code></pre></div> <h3 id=getting-the-absolute-value>Getting the absolute value</h3> <p>Sometimes you'll want the absolute values (all values are positive) of elements in your tensors.</p> <p>To do so, you can use <a href=https://www.tensorflow.org/api_docs/python/tf/math/abs><code>tf.abs()</code></a>.</p> <div class=highlight><pre><span></span><code><span class=c1># Create tensor with negative values</span>
<span class=n>D</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=o>-</span><span class=mi>7</span><span class=p>,</span> <span class=o>-</span><span class=mi>10</span><span class=p>])</span>
<span class=n>D</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ -7, -10])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Get the absolute values</span>
<span class=n>tf</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>D</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 7, 10])&gt;
</code></pre></div> <h3 id=finding-the-min-max-mean-sum-aggregation>Finding the min, max, mean, sum (aggregation)</h3> <p>You can quickly aggregate (perform a calculation on a whole tensor) tensors to find things like the minimum value, maximum value, mean and sum of all the elements.</p> <p>To do so, aggregation methods typically have the syntax <code>reduce()_[action]</code>, such as: * <a href=https://www.tensorflow.org/api_docs/python/tf/math/reduce_min><code>tf.reduce_min()</code></a> - find the minimum value in a tensor. * <a href=https://www.tensorflow.org/api_docs/python/tf/math/reduce_max><code>tf.reduce_max()</code></a> - find the maximum value in a tensor (helpful for when you want to find the highest prediction probability). * <a href=https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean><code>tf.reduce_mean()</code></a> - find the mean of all elements in a tensor. * <a href=https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum><code>tf.reduce_sum()</code></a> - find the sum of all elements in a tensor. * <strong>Note:</strong> typically, each of these is under the <code>math</code> module, e.g. <code>tf.math.reduce_min()</code> but you can use the alias <code>tf.reduce_min()</code>.</p> <p>Let's see them in action.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a tensor with 50 random values between 0 and 100</span>
<span class=n>E</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=n>low</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>high</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>50</span><span class=p>))</span>
<span class=n>E</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(50,), dtype=int32, numpy=
array([ 3, 98, 57, 41, 33, 29, 95, 93, 61, 71, 64,  2,  3, 60, 93, 22, 50,
       47, 35, 60, 26, 52, 38, 18, 64, 89, 98, 38, 78, 51,  4, 92, 45, 39,
       77, 43, 92, 49, 26, 84,  3, 33, 70,  2, 43,  4, 87, 66, 21, 92])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Find the minimum</span>
<span class=n>tf</span><span class=o>.</span><span class=n>reduce_min</span><span class=p>(</span><span class=n>E</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Find the maximum</span>
<span class=n>tf</span><span class=o>.</span><span class=n>reduce_max</span><span class=p>(</span><span class=n>E</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(), dtype=int32, numpy=98&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Find the mean</span>
<span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>E</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(), dtype=int32, numpy=50&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Find the sum</span>
<span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>E</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(), dtype=int32, numpy=2541&gt;
</code></pre></div> <p>You can also find the standard deviation (<a href=https://www.tensorflow.org/api_docs/python/tf/math/reduce_std><code>tf.reduce_std()</code></a>) and variance (<a href=https://www.tensorflow.org/api_docs/python/tf/math/reduce_variance><code>tf.reduce_variance()</code></a>) of elements in a tensor using similar methods.</p> <h3 id=finding-the-positional-maximum-and-minimum>Finding the positional maximum and minimum</h3> <p>How about finding the position a tensor where the maximum value occurs?</p> <p>This is helpful when you want to line up your labels (say <code>['Green', 'Blue', 'Red']</code>) with your prediction probabilities tensor (e.g. <code>[0.98, 0.01, 0.01]</code>).</p> <p>In this case, the predicted label (the one with the highest prediction probability) would be <code>'Green'</code>.</p> <p>You can do the same for the minimum (if required) with the following: * <a href=https://www.tensorflow.org/api_docs/python/tf/math/argmax><code>tf.argmax()</code></a> - find the position of the maximum element in a given tensor. * <a href=https://www.tensorflow.org/api_docs/python/tf/math/argmin><code>tf.argmin()</code></a> - find the position of the minimum element in a given tensor.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a tensor with 50 values between 0 and 1</span>
<span class=n>F</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>(</span><span class=mi>50</span><span class=p>))</span>
<span class=n>F</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(50,), dtype=float64, numpy=
array([0.33963509, 0.9672647 , 0.43272587, 0.27532735, 0.00204467,
       0.25983684, 0.25954456, 0.1435441 , 0.97031337, 0.26929533,
       0.14032421, 0.9082024 , 0.47425121, 0.77826693, 0.81446725,
       0.67483163, 0.79334419, 0.87420022, 0.18274336, 0.31707395,
       0.24255584, 0.31444067, 0.05813784, 0.23171223, 0.65060714,
       0.80601657, 0.27223314, 0.00410714, 0.92168893, 0.35474602,
       0.69438562, 0.85685611, 0.35194301, 0.00165329, 0.93576894,
       0.08478894, 0.30160467, 0.88888045, 0.38154264, 0.62620644,
       0.28666214, 0.00113414, 0.8597382 , 0.73687862, 0.26745934,
       0.43648347, 0.28677194, 0.43218163, 0.05640713, 0.68271093])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Find the maximum element position of F</span>
<span class=n>tf</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>F</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(), dtype=int64, numpy=8&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Find the minimum element position of F</span>
<span class=n>tf</span><span class=o>.</span><span class=n>argmin</span><span class=p>(</span><span class=n>F</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(), dtype=int64, numpy=41&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Find the maximum element position of F</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;The maximum value of F is at position: </span><span class=si>{</span><span class=n>tf</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>F</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span> 
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;The maximum value of F is: </span><span class=si>{</span><span class=n>tf</span><span class=o>.</span><span class=n>reduce_max</span><span class=p>(</span><span class=n>F</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span> 
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Using tf.argmax() to index F, the maximum value of F is: </span><span class=si>{</span><span class=n>F</span><span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>F</span><span class=p>)]</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Are the two max values the same (they should be)? </span><span class=si>{</span><span class=n>F</span><span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>F</span><span class=p>)]</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=w> </span><span class=o>==</span><span class=w> </span><span class=n>tf</span><span class=o>.</span><span class=n>reduce_max</span><span class=p>(</span><span class=n>F</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>The maximum value of F is at position: 8
The maximum value of F is: 0.9703133683461462
Using tf.argmax() to index F, the maximum value of F is: 0.9703133683461462
Are the two max values the same (they should be)? True
</code></pre></div> <h3 id=squeezing-a-tensor-removing-all-single-dimensions>Squeezing a tensor (removing all single dimensions)</h3> <p>If you need to remove single-dimensions from a tensor (dimensions with size 1), you can use <code>tf.squeeze()</code>.</p> <ul> <li><a href=https://www.tensorflow.org/api_docs/python/tf/squeeze><code>tf.squeeze()</code></a> - remove all dimensions of 1 from a tensor.</li> </ul> <div class=highlight><pre><span></span><code><span class=c1># Create a rank 5 (5 dimensions) tensor of 50 numbers between 0 and 100</span>
<span class=n>G</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>50</span><span class=p>),</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>50</span><span class=p>))</span>
<span class=n>G</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>G</span><span class=o>.</span><span class=n>ndim</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(TensorShape([1, 1, 1, 1, 50]), 5)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Squeeze tensor G (remove all 1 dimensions)</span>
<span class=n>G_squeezed</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>G</span><span class=p>)</span>
<span class=n>G_squeezed</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>G_squeezed</span><span class=o>.</span><span class=n>ndim</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(TensorShape([50]), 1)
</code></pre></div> <h3 id=one-hot-encoding>One-hot encoding</h3> <p>If you have a tensor of indicies and would like to one-hot encode it, you can use <a href=https://www.tensorflow.org/api_docs/python/tf/one_hot><code>tf.one_hot()</code></a>.</p> <p>You should also specify the <code>depth</code> parameter (the level which you want to one-hot encode to).</p> <div class=highlight><pre><span></span><code><span class=c1># Create a list of indices</span>
<span class=n>some_list</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]</span>

<span class=c1># One hot encode them</span>
<span class=n>tf</span><span class=o>.</span><span class=n>one_hot</span><span class=p>(</span><span class=n>some_list</span><span class=p>,</span> <span class=n>depth</span><span class=o>=</span><span class=mi>4</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 0.],
       [0., 1., 0., 0.],
       [0., 0., 1., 0.],
       [0., 0., 0., 1.]], dtype=float32)&gt;
</code></pre></div> <p>You can also specify values for <code>on_value</code> and <code>off_value</code> instead of the default <code>0</code> and <code>1</code>.</p> <div class=highlight><pre><span></span><code><span class=c1># Specify custom values for on and off encoding</span>
<span class=n>tf</span><span class=o>.</span><span class=n>one_hot</span><span class=p>(</span><span class=n>some_list</span><span class=p>,</span> <span class=n>depth</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>on_value</span><span class=o>=</span><span class=s2>&quot;We&#39;re live!&quot;</span><span class=p>,</span> <span class=n>off_value</span><span class=o>=</span><span class=s2>&quot;Offline&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(4, 4), dtype=string, numpy=
array([[b&quot;We&#39;re live!&quot;, b&#39;Offline&#39;, b&#39;Offline&#39;, b&#39;Offline&#39;],
       [b&#39;Offline&#39;, b&quot;We&#39;re live!&quot;, b&#39;Offline&#39;, b&#39;Offline&#39;],
       [b&#39;Offline&#39;, b&#39;Offline&#39;, b&quot;We&#39;re live!&quot;, b&#39;Offline&#39;],
       [b&#39;Offline&#39;, b&#39;Offline&#39;, b&#39;Offline&#39;, b&quot;We&#39;re live!&quot;]], dtype=object)&gt;
</code></pre></div> <h3 id=squaring-log-square-root>Squaring, log, square root</h3> <p>Many other common mathematical operations you'd like to perform at some stage, probably exist.</p> <p>Let's take a look at: * <a href=https://www.tensorflow.org/api_docs/python/tf/math/square><code>tf.square()</code></a> - get the square of every value in a tensor. * <a href=https://www.tensorflow.org/api_docs/python/tf/math/sqrt><code>tf.sqrt()</code></a> - get the squareroot of every value in a tensor (<strong>note:</strong> the elements need to be floats or this will error). * <a href=https://www.tensorflow.org/api_docs/python/tf/math/log><code>tf.math.log()</code></a> - get the natural log of every value in a tensor (elements need to floats).</p> <div class=highlight><pre><span></span><code><span class=c1># Create a new tensor</span>
<span class=n>H</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
<span class=n>H</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(9,), dtype=int32, numpy=array([1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Square it</span>
<span class=n>tf</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>H</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(9,), dtype=int32, numpy=array([ 1,  4,  9, 16, 25, 36, 49, 64, 81])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Find the squareroot (will error), needs to be non-integer</span>
<span class=n>tf</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>H</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

Cell In[88], line 2
      1 # Find the squareroot (will error), needs to be non-integer
----&gt; 2 tf.sqrt(H)


File D:\anaconda\envs\py3-TF2.0\Lib\site-packages\tensorflow\python\ops\weak_tensor_ops.py:88, in weak_tensor_unary_op_wrapper.&lt;locals&gt;.wrapper(*args, **kwargs)
     86 def wrapper(*args, **kwargs):
     87   if not ops.is_auto_dtype_conversion_enabled():
---&gt; 88     return op(*args, **kwargs)
     89   bound_arguments = signature.bind(*args, **kwargs)
     90   bound_arguments.apply_defaults()


File D:\anaconda\envs\py3-TF2.0\Lib\site-packages\tensorflow\python\util\traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--&gt; 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb


File D:\anaconda\envs\py3-TF2.0\Lib\site-packages\tensorflow\python\framework\ops.py:5983, in raise_from_not_ok_status(e, name)
   5981 def raise_from_not_ok_status(e, name) -&gt; NoReturn:
   5982   e.message += (&quot; name: &quot; + str(name if name is not None else &quot;&quot;))
-&gt; 5983   raise core._status_to_exception(e) from None


InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128
    ; NodeDef: {{node Sqrt}}; Op&lt;name=Sqrt; signature=x:T -&gt; y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:Sqrt] name:
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Change H to float32</span>
<span class=n>H</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>H</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
<span class=n>H</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(9,), dtype=float32, numpy=array([1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Find the square root</span>
<span class=n>tf</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>H</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(9,), dtype=float32, numpy=
array([1.       , 1.4142135, 1.7320508, 2.       , 2.236068 , 2.4494898,
       2.6457512, 2.828427 , 3.       ], dtype=float32)&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Find the log (input also needs to be float)</span>
<span class=n>tf</span><span class=o>.</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>H</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(9,), dtype=float32, numpy=
array([0.       , 0.6931472, 1.0986123, 1.3862944, 1.609438 , 1.7917595,
       1.9459102, 2.0794415, 2.1972246], dtype=float32)&gt;
</code></pre></div> <h3 id=manipulating-tfvariable-tensors>Manipulating <code>tf.Variable</code> tensors</h3> <p>Tensors created with <code>tf.Variable()</code> can be changed in place using methods such as:</p> <ul> <li><a href=https://www.tensorflow.org/api_docs/python/tf/Variable#assign><code>.assign()</code></a> - assign a different value to a particular index of a variable tensor.</li> <li><a href=https://www.tensorflow.org/api_docs/python/tf/Variable#assign_add><code>.add_assign()</code></a> - add to an existing value and reassign it at a particular index of a variable tensor.</li> </ul> <div class=highlight><pre><span></span><code><span class=c1># Create a variable tensor</span>
<span class=n>I</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
<span class=n>I</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Variable &#39;Variable:0&#39; shape=(5,) dtype=int32, numpy=array([0, 1, 2, 3, 4])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Assign the final value a new value of 50</span>
<span class=n>I</span><span class=o>.</span><span class=n>assign</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>50</span><span class=p>])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Variable &#39;UnreadVariable&#39; shape=(5,) dtype=int32, numpy=array([ 0,  1,  2,  3, 50])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># The change happens in place (the last value is now 50, not 4)</span>
<span class=n>I</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Variable &#39;Variable:0&#39; shape=(5,) dtype=int32, numpy=array([ 0,  1,  2,  3, 50])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Add 10 to every element in I</span>
<span class=n>I</span><span class=o>.</span><span class=n>assign_add</span><span class=p>([</span><span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Variable &#39;UnreadVariable&#39; shape=(5,) dtype=int32, numpy=array([10, 11, 12, 13, 60])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Again, the change happens in place</span>
<span class=n>I</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Variable &#39;Variable:0&#39; shape=(5,) dtype=int32, numpy=array([10, 11, 12, 13, 60])&gt;
</code></pre></div> <h2 id=tensors-and-numpy>Tensors and NumPy</h2> <p>We've seen some examples of tensors interact with NumPy arrays, such as, using NumPy arrays to create tensors. </p> <p>Tensors can also be converted to NumPy arrays using:</p> <ul> <li><code>np.array()</code> - pass a tensor to convert to an ndarray (NumPy's main datatype).</li> <li><code>tensor.numpy()</code> - call on a tensor to convert to an ndarray.</li> </ul> <p>Doing this is helpful as it makes tensors iterable as well as allows us to use any of NumPy's methods on them.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a tensor from a NumPy array</span>
<span class=n>J</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>3.</span><span class=p>,</span> <span class=mf>7.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>]))</span>
<span class=n>J</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 3.,  7., 10.])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Convert tensor J to NumPy with np.array()</span>
<span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>J</span><span class=p>),</span> <span class=nb>type</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>J</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([ 3.,  7., 10.]), numpy.ndarray)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Convert tensor J to NumPy with .numpy()</span>
<span class=n>J</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=nb>type</span><span class=p>(</span><span class=n>J</span><span class=o>.</span><span class=n>numpy</span><span class=p>())</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([ 3.,  7., 10.]), numpy.ndarray)
</code></pre></div> <p>By default tensors have <code>dtype=float32</code>, where as NumPy arrays have <code>dtype=float64</code>.</p> <p>This is because neural networks (which are usually built with TensorFlow) can generally work very well with less precision (32-bit rather than 64-bit).</p> <div class=highlight><pre><span></span><code><span class=c1># Create a tensor from NumPy and from an array</span>
<span class=n>numpy_J</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>3.</span><span class=p>,</span> <span class=mf>7.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>]))</span> <span class=c1># will be float64 (due to NumPy)</span>
<span class=n>tensor_J</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>3.</span><span class=p>,</span> <span class=mf>7.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>])</span> <span class=c1># will be float32 (due to being TensorFlow default)</span>
<span class=n>numpy_J</span><span class=o>.</span><span class=n>dtype</span><span class=p>,</span> <span class=n>tensor_J</span><span class=o>.</span><span class=n>dtype</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(tf.float64, tf.float32)
</code></pre></div> <h2 id=using-tffunction>Using <code>@tf.function</code></h2> <p>In your TensorFlow adventures, you might come across Python functions which have the decorator <a href=https://www.tensorflow.org/api_docs/python/tf/function><code>@tf.function</code></a>.</p> <p>If you aren't sure what Python decorators do, <a href=https://realpython.com/primer-on-python-decorators/ >read RealPython's guide on them</a>.</p> <p>But in short, decorators modify a function in one way or another.</p> <p>In the <code>@tf.function</code> decorator case, it turns a Python function into a callable TensorFlow graph. Which is a fancy way of saying, if you've written your own Python function, and you decorate it with <code>@tf.function</code>, when you export your code (to potentially run on another device), TensorFlow will attempt to convert it into a fast(er) version of itself (by making it part of a computation graph).</p> <p>For more on this, read the <a href=https://www.tensorflow.org/guide/function>Better performnace with tf.function</a> guide.</p> <div class=highlight><pre><span></span><code><span class=c1># Create a simple function</span>
<span class=k>def</span><span class=w> </span><span class=nf>function</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
  <span class=k>return</span> <span class=n>x</span> <span class=o>**</span> <span class=mi>2</span> <span class=o>+</span> <span class=n>y</span>

<span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>20</span><span class=p>))</span>
<span class=n>function</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 10,  12,  16,  22,  30,  40,  52,  66,  82, 100])&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Create the same function and decorate it with tf.function</span>
<span class=nd>@tf</span><span class=o>.</span><span class=n>function</span>
<span class=k>def</span><span class=w> </span><span class=nf>tf_function</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
  <span class=k>return</span> <span class=n>x</span> <span class=o>**</span> <span class=mi>2</span> <span class=o>+</span> <span class=n>y</span>

<span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>20</span><span class=p>)</span>
<span class=n>tf_function</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 10,  12,  16,  22,  30,  40,  52,  66,  82, 100])&gt;
</code></pre></div> <p>If you noticed no difference between the above two functions (the decorated one and the non-decorated one) you'd be right.</p> <p>Much of the difference happens behind the scenes. One of the main ones being potential code speed-ups where possible.</p> <h2 id=finding-access-to-gpus>Finding access to GPUs</h2> <p>We've mentioned GPUs plenty of times throughout this notebook.</p> <p>So how do you check if you've got one available?</p> <p>You can check if you've got access to a GPU using <a href=https://www.tensorflow.org/guide/gpu><code>tf.config.list_physical_devices()</code></a>.</p> <div class=highlight><pre><span></span><code><span class=nb>print</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>list_physical_devices</span><span class=p>(</span><span class=s1>&#39;GPU&#39;</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[]
</code></pre></div> <p>If the above outputs an empty array (or nothing), it means you don't have access to a GPU (or at least TensorFlow can't find it).</p> <p>If you're running in Google Colab, you can access a GPU by going to <em>Runtime -&gt; Change Runtime Type -&gt; Select GPU</em> (<strong>note:</strong> after doing this your notebook will restart and any variables you've saved will be lost).</p> <p>Once you've changed your runtime type, run the cell below.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>
<span class=nb>print</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>list_physical_devices</span><span class=p>(</span><span class=s1>&#39;GPU&#39;</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[]
</code></pre></div> <p>If you've got access to a GPU, the cell above should output something like:</p> <p><code>[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]</code></p> <p>You can also find information about your GPU using <code>!nvidia-smi</code>.</p> <div class=highlight><pre><span></span><code><span class=err>!</span><span class=n>nvidia</span><span class=o>-</span><span class=n>smi</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Thu Apr 18 16:24:12 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 517.48       Driver Version: 517.48       CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |
| N/A   45C    P8     2W /  N/A |      0MiB /  4096MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre></div> <blockquote> <p>üîë <strong>Note:</strong> If you have access to a GPU, TensorFlow will automatically use it whenever possible.</p> </blockquote> <h2 id=exercises>üõ† Exercises</h2> <ol> <li>Create a vector, scalar, matrix and tensor with values of your choosing using <code>tf.constant()</code>.</li> <li>Find the shape, rank and size of the tensors you created in 1.</li> <li>Create two tensors containing random values between 0 and 1 with shape <code>[5, 300]</code>.</li> <li>Multiply the two tensors you created in 3 using matrix multiplication.</li> <li>Multiply the two tensors you created in 3 using dot product.</li> <li>Create a tensor with random values between 0 and 1 with shape <code>[224, 224, 3]</code>.</li> <li>Find the min and max values of the tensor you created in 6.</li> <li>Created a tensor with random values of shape <code>[1, 224, 224, 3]</code> then squeeze it to change the shape to <code>[224, 224, 3]</code>.</li> <li>Create a tensor with shape <code>[10]</code> using your own choice of values, then find the index which has the maximum value.</li> <li>One-hot encode the tensor you created in 9.</li> </ol> <h2 id=extra-curriculum>üìñ Extra-curriculum</h2> <ul> <li>Read through the <a href=https://www.tensorflow.org/api_docs/python/ >list of TensorFlow Python APIs</a>, pick one we haven't gone through in this notebook, reverse engineer it (write out the documentation code for yourself) and figure out what it does.</li> <li>Try to create a series of tensor functions to calculate your most recent grocery bill (it's okay if you don't use the names of the items, just the price in numerical form).</li> <li>How would you calculate your grocery bill for the month and for the year using tensors?</li> <li>Go through the <a href=https://www.tensorflow.org/tutorials/quickstart/beginner>TensorFlow 2.x quick start for beginners</a> tutorial (be sure to type out all of the code yourself, even if you don't understand it).</li> <li>Are there any functions we used in here that match what's used in there? Which are the same? Which haven't you seen before?</li> <li>Watch the video <a href="https://www.youtube.com/watch?v=f5liqUk0ZTw">"What's a tensor?"</a> - a great visual introduction to many of the concepts we've covered in this notebook.</li> </ul> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.instant", "navigation.path", "navigation.indexes", "navigation.top", "navigation.tracking"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>