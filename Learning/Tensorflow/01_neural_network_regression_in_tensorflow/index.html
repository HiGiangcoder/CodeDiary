<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=../00_tensorflow_fundamentals/ rel=prev><link href=../02_neural_network_classification_in_tensorflow/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.7"><title>1 Neural network regression - My Documentation</title><link rel=stylesheet href=../../../assets/stylesheets/main.8608ea7d.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Merriweather";--md-code-font:"JetBrains Mono"}</style><link rel=stylesheet href=../../../assets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script> <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#01-neural-network-regression-with-tensorflow class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="My Documentation" class="md-header__button md-logo" aria-label="My Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> My Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 1 Neural network regression </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../home/ class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../AI_Model/lenet/lenet5/ class=md-tabs__link> Ki·∫øn th·ª©c l·∫≠p tr√¨nh </a> </li> <li class=md-tabs__item> <a href=../../../Subject/XSTK/Exercise2/homework/ class=md-tabs__link> M√¥n h·ªçc </a> </li> <li class=md-tabs__item> <a href=../../../Problem/general/ class=md-tabs__link> CP-Problem </a> </li> <li class=md-tabs__item> <a href=../../../other/other/ class=md-tabs__link> Other </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="My Documentation" class="md-nav__button md-logo" aria-label="My Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> My Documentation </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../../../home/ class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../home/about/ class=md-nav__link> <span class=md-ellipsis> About </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Ki·∫øn th·ª©c l·∫≠p tr√¨nh </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Ki·∫øn th·ª©c l·∫≠p tr√¨nh </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1> <label class=md-nav__link for=__nav_2_1 id=__nav_2_1_label tabindex=0> <span class=md-ellipsis> C√°c ki·∫øn tr√∫c, m√¥ h√¨nh AI v√† c√°c kƒ© thu·∫≠t </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> C√°c ki·∫øn tr√∫c, m√¥ h√¨nh AI v√† c√°c kƒ© thu·∫≠t </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_1> <label class=md-nav__link for=__nav_2_1_1 id=__nav_2_1_1_label tabindex=0> <span class=md-ellipsis> Lenet-5 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_1> <span class="md-nav__icon md-icon"></span> Lenet-5 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenet5/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenetpart1/ class=md-nav__link> <span class=md-ellipsis> Tutorial part 1 </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/lenetpart2/ class=md-nav__link> <span class=md-ellipsis> Tutorial part 2 </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/lenet/Lenet5_MNIST/ class=md-nav__link> <span class=md-ellipsis> Lenet-5 (Final Project 1) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_2> <label class=md-nav__link for=__nav_2_1_2 id=__nav_2_1_2_label tabindex=0> <span class=md-ellipsis> UNet </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_2> <span class="md-nav__icon md-icon"></span> UNet </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../AI_Model/unet/unet/ class=md-nav__link> <span class=md-ellipsis> Unet tutorial </span> </a> </li> <li class=md-nav__item> <a href=../../AI_Model/unet/unet_model/ class=md-nav__link> <span class=md-ellipsis> UNet (Final Project 2) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../AI_Model/essential.md class=md-nav__link> <span class=md-ellipsis> C√°c ki·∫øn th·ª©c c·∫ßn thi·∫øt </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> Python </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Python/Numpy_1/ class=md-nav__link> <span class=md-ellipsis> Numpy 1 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Numpy_2/ class=md-nav__link> <span class=md-ellipsis> Numpy 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Python_Pandas_exercise1/ class=md-nav__link> <span class=md-ellipsis> Pandas exercise 1 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas2/ class=md-nav__link> <span class=md-ellipsis> Pandas 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Python_Pandas_exercise2/ class=md-nav__link> <span class=md-ellipsis> Pandas exercise 2 </span> </a> </li> <li class=md-nav__item> <a href=../../Python/Pandas_Join_Combine/ class=md-nav__link> <span class=md-ellipsis> Pandas Join Combine </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> PyTorch </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> PyTorch </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Pytorch/00_pytorch_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 0 Pytorch Fundamentals (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/00_pytorch_and_deep_learning_fundamentals.pdf class=md-nav__link> <span class=md-ellipsis> 0 Deep learning fundamentals (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/01_pytorch_workflow/ class=md-nav__link> <span class=md-ellipsis> 1 Pytorch workflow (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/01_pytorch_workflow.pdf class=md-nav__link> <span class=md-ellipsis> 1 Pytorch workflow (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/02_pytorch_classification/ class=md-nav__link> <span class=md-ellipsis> 2 classification (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/03_pytorch_computer_vision/ class=md-nav__link> <span class=md-ellipsis> 3 Computer vision (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/03_pytorch_computer_vision.pdf class=md-nav__link> <span class=md-ellipsis> 3 Compupter vision (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/04_pytorch_custom_datasets/ class=md-nav__link> <span class=md-ellipsis> 4 Custom datasets (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/05_pytorch_going_modular.pdf class=md-nav__link> <span class=md-ellipsis> 5 Going moduler (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/06_pytorch_transfer_learning/ class=md-nav__link> <span class=md-ellipsis> 6 Transfer learning (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/06_pytorch_transfer_learning.pdf class=md-nav__link> <span class=md-ellipsis> 6 Transfer learning (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/07_pytorch_experiment_tracking/ class=md-nav__link> <span class=md-ellipsis> 7 Experiment tracking (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/07_pytorch_experiment_tracking.pdf class=md-nav__link> <span class=md-ellipsis> 7 Experiment tracking (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/08_pytorch_paper_replicating/ class=md-nav__link> <span class=md-ellipsis> 8 Paper replicating (ipynb) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/08_pytorch_paper_replicating.pdf class=md-nav__link> <span class=md-ellipsis> 8 Paper replicating (pdf) </span> </a> </li> <li class=md-nav__item> <a href=../../Pytorch/09_pytorch_model_deployment.pdf class=md-nav__link> <span class=md-ellipsis> 9 Model deployment(pdf) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4 checked> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class=md-ellipsis> Tensorflow </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=true> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Tensorflow </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../00_tensorflow_fundamentals/ class=md-nav__link> <span class=md-ellipsis> 0 Tensorflow fundamentals </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 1 Neural network regression </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 1 Neural network regression </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-were-going-to-cover class=md-nav__link> <span class=md-ellipsis> What we're going to cover </span> </a> </li> <li class=md-nav__item> <a href=#how-you-can-use-this-notebook class=md-nav__link> <span class=md-ellipsis> How you can use this notebook </span> </a> </li> <li class=md-nav__item> <a href=#typical-architecture-of-a-regresison-neural-network class=md-nav__link> <span class=md-ellipsis> Typical architecture of a regresison neural network </span> </a> </li> <li class=md-nav__item> <a href=#creating-data-to-view-and-fit class=md-nav__link> <span class=md-ellipsis> Creating data to view and fit </span> </a> </li> <li class=md-nav__item> <a href=#regression-input-shapes-and-output-shapes class=md-nav__link> <span class=md-ellipsis> Regression input shapes and output shapes </span> </a> </li> <li class=md-nav__item> <a href=#steps-in-modelling-with-tensorflow class=md-nav__link> <span class=md-ellipsis> Steps in modelling with TensorFlow </span> </a> </li> <li class=md-nav__item> <a href=#improving-a-model class=md-nav__link> <span class=md-ellipsis> Improving a model </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-a-model class=md-nav__link> <span class=md-ellipsis> Evaluating a model </span> </a> <nav class=md-nav aria-label="Evaluating a model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#visualize-visualize-visualize class=md-nav__link> <span class=md-ellipsis> Visualize, visualize, visualize </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#split-data-into-trainingtest-set class=md-nav__link> <span class=md-ellipsis> Split data into training/test set </span> </a> </li> <li class=md-nav__item> <a href=#visualizing-the-data class=md-nav__link> <span class=md-ellipsis> Visualizing the data </span> </a> </li> <li class=md-nav__item> <a href=#visualizing-the-model class=md-nav__link> <span class=md-ellipsis> Visualizing the model </span> </a> </li> <li class=md-nav__item> <a href=#visualizing-the-predictions class=md-nav__link> <span class=md-ellipsis> Visualizing the predictions </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-predictions class=md-nav__link> <span class=md-ellipsis> Evaluating predictions </span> </a> </li> <li class=md-nav__item> <a href=#running-experiments-to-improve-a-model class=md-nav__link> <span class=md-ellipsis> Running experiments to improve a model </span> </a> </li> <li class=md-nav__item> <a href=#comparing-results class=md-nav__link> <span class=md-ellipsis> Comparing results </span> </a> </li> <li class=md-nav__item> <a href=#tracking-your-experiments class=md-nav__link> <span class=md-ellipsis> Tracking your experiments </span> </a> </li> <li class=md-nav__item> <a href=#saving-a-model class=md-nav__link> <span class=md-ellipsis> Saving a model </span> </a> </li> <li class=md-nav__item> <a href=#loading-a-model class=md-nav__link> <span class=md-ellipsis> Loading a model </span> </a> </li> <li class=md-nav__item> <a href=#downloading-a-model-from-google-colab class=md-nav__link> <span class=md-ellipsis> Downloading a model (from Google Colab) </span> </a> </li> <li class=md-nav__item> <a href=#a-larger-example class=md-nav__link> <span class=md-ellipsis> A larger example </span> </a> </li> <li class=md-nav__item> <a href=#preprocessing-data-normalization-and-standardization class=md-nav__link> <span class=md-ellipsis> Preprocessing data (normalization and standardization) </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> üõ† Exercises </span> </a> </li> <li class=md-nav__item> <a href=#extra-curriculum class=md-nav__link> <span class=md-ellipsis> üìñ Extra curriculum </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../02_neural_network_classification_in_tensorflow/ class=md-nav__link> <span class=md-ellipsis> 2 Neural network classification </span> </a> </li> <li class=md-nav__item> <a href=../04_transfer_learning_in_tensorflow_part_1_feature_extraction/ class=md-nav__link> <span class=md-ellipsis> 4 Transfer learning </span> </a> </li> <li class=md-nav__item> <a href=../10_time_series_forecasting_in_tensorflow/ class=md-nav__link> <span class=md-ellipsis> 10 Time series forecasting </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../OOP_Practice/ class=md-nav__link> <span class=md-ellipsis> OOP Practice </span> </a> </li> <li class=md-nav__item> <a href=../../Visualization/ class=md-nav__link> <span class=md-ellipsis> Visualization </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> M√¥n h·ªçc </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> M√¥n h·ªçc </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> X√°c su·∫•t th·ªëng k√™ </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> X√°c su·∫•t th·ªëng k√™ </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Subject/XSTK/Exercise2/homework/ class=md-nav__link> <span class=md-ellipsis> B√†i t·∫≠p v·ªÅ nh√† bu·ªïi 2 </span> </a> </li> <li class=md-nav__item> <a href=../../../Subject/XSTK/Exercise3/homework/ class=md-nav__link> <span class=md-ellipsis> B√†i t·∫≠p v·ªÅ nh√† bu·ªïi 3 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> CP-Problem </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> CP-Problem </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/general/ class=md-nav__link> <span class=md-ellipsis> General </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> COCI 2006 2007 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> COCI 2006 2007 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_2> <label class=md-nav__link for=__nav_4_2_2 id=__nav_4_2_2_label tabindex=0> <span class=md-ellipsis> Contest 1 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_2> <span class="md-nav__icon md-icon"></span> Contest 1 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P1_MODULO/ class=md-nav__link> <span class=md-ellipsis> MODULO </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P2_HERMAN/ class=md-nav__link> <span class=md-ellipsis> HERMAN </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P3_OKVIRI/ class=md-nav__link> <span class=md-ellipsis> OKVIRI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P4_SLIKAR/ class=md-nav__link> <span class=md-ellipsis> SLIKAR </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P5_BOND/ class=md-nav__link> <span class=md-ellipsis> BOND </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest1/P6_DEBUG/ class=md-nav__link> <span class=md-ellipsis> DEBUG </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_3> <label class=md-nav__link for=__nav_4_2_3 id=__nav_4_2_3_label tabindex=0> <span class=md-ellipsis> Contest 2 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_3> <span class="md-nav__icon md-icon"></span> Contest 2 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P1_R2/ class=md-nav__link> <span class=md-ellipsis> R2 </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P2_ABC/ class=md-nav__link> <span class=md-ellipsis> ABC </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P3_KOLONE/ class=md-nav__link> <span class=md-ellipsis> KOLONE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P4_SJECISTA/ class=md-nav__link> <span class=md-ellipsis> SJECISTA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P5_STOL/ class=md-nav__link> <span class=md-ellipsis> STOL </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest2/P6_STRAZA/ class=md-nav__link> <span class=md-ellipsis> STRAZA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_4> <label class=md-nav__link for=__nav_4_2_4 id=__nav_4_2_4_label tabindex=0> <span class=md-ellipsis> Contest 3 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_4> <span class="md-nav__icon md-icon"></span> Contest 3 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P1_PATULJCI/ class=md-nav__link> <span class=md-ellipsis> PATULJCI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P2_NPUZZLE/ class=md-nav__link> <span class=md-ellipsis> NPUZZLE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P3_TROJKE/ class=md-nav__link> <span class=md-ellipsis> TROJKE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P4_TENKICI/ class=md-nav__link> <span class=md-ellipsis> TENKICI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P5_BICIKLI/ class=md-nav__link> <span class=md-ellipsis> BICIKLI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest3/P6_LISTA/ class=md-nav__link> <span class=md-ellipsis> LISTA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_5> <label class=md-nav__link for=__nav_4_2_5 id=__nav_4_2_5_label tabindex=0> <span class=md-ellipsis> Contest 4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_5> <span class="md-nav__icon md-icon"></span> Contest 4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P1_Sibice/ class=md-nav__link> <span class=md-ellipsis> SIBICE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P2_Skener/ class=md-nav__link> <span class=md-ellipsis> SKENER </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P3_Prsteni/ class=md-nav__link> <span class=md-ellipsis> PRSTENI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P4_Zbrka/ class=md-nav__link> <span class=md-ellipsis> ZBRKA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P5_Jogurt/ class=md-nav__link> <span class=md-ellipsis> JOGURT </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest4/P6_Ispiti/ class=md-nav__link> <span class=md-ellipsis> ISPITI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_6> <label class=md-nav__link for=__nav_4_2_6 id=__nav_4_2_6_label tabindex=0> <span class=md-ellipsis> Contest 5 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_6_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_6> <span class="md-nav__icon md-icon"></span> Contest 5 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P1_Trik/ class=md-nav__link> <span class=md-ellipsis> TRIK </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P2_Natrij/ class=md-nav__link> <span class=md-ellipsis> NATRIJ </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P3_Tenis/ class=md-nav__link> <span class=md-ellipsis> TENIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P4_Liga/ class=md-nav__link> <span class=md-ellipsis> LIGA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P5_Ivana/ class=md-nav__link> <span class=md-ellipsis> IVANA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest5/P6_Dvaput/ class=md-nav__link> <span class=md-ellipsis> DVAPUT </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_7> <label class=md-nav__link for=__nav_4_2_7 id=__nav_4_2_7_label tabindex=0> <span class=md-ellipsis> Contest 6 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_7_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_7> <span class="md-nav__icon md-icon"></span> Contest 6 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P1_PRASE/ class=md-nav__link> <span class=md-ellipsis> PRASE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P2_MAGIJA/ class=md-nav__link> <span class=md-ellipsis> MAGIJA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P3_MARATON/ class=md-nav__link> <span class=md-ellipsis> MARATON </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P4_KAMEN/ class=md-nav__link> <span class=md-ellipsis> KAMEN </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P5_V/ class=md-nav__link> <span class=md-ellipsis> V </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Contest6/P6_PROSTOR/ class=md-nav__link> <span class=md-ellipsis> PROSTOR </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_8> <label class=md-nav__link for=__nav_4_2_8 id=__nav_4_2_8_label tabindex=0> <span class=md-ellipsis> Regional </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_8_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_8> <span class="md-nav__icon md-icon"></span> Regional </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P1_BARD/ class=md-nav__link> <span class=md-ellipsis> BARD </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P2_TETRIS/ class=md-nav__link> <span class=md-ellipsis> TETRIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Regional/P4_CIRCLE/ class=md-nav__link> <span class=md-ellipsis> CIRCLE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_9> <label class=md-nav__link for=__nav_4_2_9 id=__nav_4_2_9_label tabindex=0> <span class=md-ellipsis> Croatian Olympiad </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_9_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_9> <span class="md-nav__icon md-icon"></span> Croatian Olympiad </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P1_PATRIK/ class=md-nav__link> <span class=md-ellipsis> PATRIK </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P2_POLICIJA/ class=md-nav__link> <span class=md-ellipsis> POLICIJA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0607/Olympiad/P3_SABOR/ class=md-nav__link> <span class=md-ellipsis> SABOR </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class=md-ellipsis> COCI 2007 2008 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> COCI 2007 2008 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_2> <label class=md-nav__link for=__nav_4_3_2 id=__nav_4_3_2_label tabindex=0> <span class=md-ellipsis> Contest 1 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_2> <span class="md-nav__icon md-icon"></span> Contest 1 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P1_CETVRTA/ class=md-nav__link> <span class=md-ellipsis> CETVRTA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P2_PEG/ class=md-nav__link> <span class=md-ellipsis> PEG </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P3_PRINOVA/ class=md-nav__link> <span class=md-ellipsis> PRINOVA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P4_ZAPIS/ class=md-nav__link> <span class=md-ellipsis> ZAPIS </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P5_SREDNJI/ class=md-nav__link> <span class=md-ellipsis> SREDNJI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest1/P6_STAZA/ class=md-nav__link> <span class=md-ellipsis> STAZA </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_3> <label class=md-nav__link for=__nav_4_3_3 id=__nav_4_3_3_label tabindex=0> <span class=md-ellipsis> Contest 4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_3> <span class="md-nav__icon md-icon"></span> Contest 4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P1_CIRCLE/ class=md-nav__link> <span class=md-ellipsis> CIRCLE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P2_VECI/ class=md-nav__link> <span class=md-ellipsis> VECI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P3_LEKTIRA/ class=md-nav__link> <span class=md-ellipsis> LEKTIRA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P4_MUZICARI/ class=md-nav__link> <span class=md-ellipsis> MUZICARI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P5_POKLON/ class=md-nav__link> <span class=md-ellipsis> POKLON </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest4/P6_KOCKE/ class=md-nav__link> <span class=md-ellipsis> KOCKE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_4> <label class=md-nav__link for=__nav_4_3_4 id=__nav_4_3_4_label tabindex=0> <span class=md-ellipsis> Contest 6 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3_4> <span class="md-nav__icon md-icon"></span> Contest 6 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P1_PARKING/ class=md-nav__link> <span class=md-ellipsis> PARKING </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P2_SEMAFORI/ class=md-nav__link> <span class=md-ellipsis> SEMAFORI </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P3_GRANICA/ class=md-nav__link> <span class=md-ellipsis> GRANICA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P4_GEORGE/ class=md-nav__link> <span class=md-ellipsis> GEORGE </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P5_PRINCEZA/ class=md-nav__link> <span class=md-ellipsis> PRINCEZA </span> </a> </li> <li class=md-nav__item> <a href=../../../Problem/coci0708/Contest6/P6_CESTARINE/ class=md-nav__link> <span class=md-ellipsis> CESTARINE </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../other/other/ class=md-nav__link> <span class=md-ellipsis> Other </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-were-going-to-cover class=md-nav__link> <span class=md-ellipsis> What we're going to cover </span> </a> </li> <li class=md-nav__item> <a href=#how-you-can-use-this-notebook class=md-nav__link> <span class=md-ellipsis> How you can use this notebook </span> </a> </li> <li class=md-nav__item> <a href=#typical-architecture-of-a-regresison-neural-network class=md-nav__link> <span class=md-ellipsis> Typical architecture of a regresison neural network </span> </a> </li> <li class=md-nav__item> <a href=#creating-data-to-view-and-fit class=md-nav__link> <span class=md-ellipsis> Creating data to view and fit </span> </a> </li> <li class=md-nav__item> <a href=#regression-input-shapes-and-output-shapes class=md-nav__link> <span class=md-ellipsis> Regression input shapes and output shapes </span> </a> </li> <li class=md-nav__item> <a href=#steps-in-modelling-with-tensorflow class=md-nav__link> <span class=md-ellipsis> Steps in modelling with TensorFlow </span> </a> </li> <li class=md-nav__item> <a href=#improving-a-model class=md-nav__link> <span class=md-ellipsis> Improving a model </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-a-model class=md-nav__link> <span class=md-ellipsis> Evaluating a model </span> </a> <nav class=md-nav aria-label="Evaluating a model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#visualize-visualize-visualize class=md-nav__link> <span class=md-ellipsis> Visualize, visualize, visualize </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#split-data-into-trainingtest-set class=md-nav__link> <span class=md-ellipsis> Split data into training/test set </span> </a> </li> <li class=md-nav__item> <a href=#visualizing-the-data class=md-nav__link> <span class=md-ellipsis> Visualizing the data </span> </a> </li> <li class=md-nav__item> <a href=#visualizing-the-model class=md-nav__link> <span class=md-ellipsis> Visualizing the model </span> </a> </li> <li class=md-nav__item> <a href=#visualizing-the-predictions class=md-nav__link> <span class=md-ellipsis> Visualizing the predictions </span> </a> </li> <li class=md-nav__item> <a href=#evaluating-predictions class=md-nav__link> <span class=md-ellipsis> Evaluating predictions </span> </a> </li> <li class=md-nav__item> <a href=#running-experiments-to-improve-a-model class=md-nav__link> <span class=md-ellipsis> Running experiments to improve a model </span> </a> </li> <li class=md-nav__item> <a href=#comparing-results class=md-nav__link> <span class=md-ellipsis> Comparing results </span> </a> </li> <li class=md-nav__item> <a href=#tracking-your-experiments class=md-nav__link> <span class=md-ellipsis> Tracking your experiments </span> </a> </li> <li class=md-nav__item> <a href=#saving-a-model class=md-nav__link> <span class=md-ellipsis> Saving a model </span> </a> </li> <li class=md-nav__item> <a href=#loading-a-model class=md-nav__link> <span class=md-ellipsis> Loading a model </span> </a> </li> <li class=md-nav__item> <a href=#downloading-a-model-from-google-colab class=md-nav__link> <span class=md-ellipsis> Downloading a model (from Google Colab) </span> </a> </li> <li class=md-nav__item> <a href=#a-larger-example class=md-nav__link> <span class=md-ellipsis> A larger example </span> </a> </li> <li class=md-nav__item> <a href=#preprocessing-data-normalization-and-standardization class=md-nav__link> <span class=md-ellipsis> Preprocessing data (normalization and standardization) </span> </a> </li> <li class=md-nav__item> <a href=#exercises class=md-nav__link> <span class=md-ellipsis> üõ† Exercises </span> </a> </li> <li class=md-nav__item> <a href=#extra-curriculum class=md-nav__link> <span class=md-ellipsis> üìñ Extra curriculum </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <p><a href=https://colab.research.google.com/github/mrdbourke/tensorflow-deep-learning/blob/main/01_neural_network_regression_in_tensorflow.ipynb target=_parent><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p> <h1 id=01-neural-network-regression-with-tensorflow>01. Neural Network Regression with TensorFlow</h1> <p>There are many definitions for a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression problem</a> but in our case, we're going to simplify it to be: predicting a number.</p> <p>For example, you might want to: - Predict the selling price of houses given information about them (such as number of rooms, size, number of bathrooms). - Predict the coordinates of a bounding box of an item in an image. - Predict the cost of medical insurance for an individual given their demographics (age, sex, gender, race).</p> <p>In this notebook, we're going to set the foundations for how you can take a sample of inputs (this is your data), build a neural network to discover patterns in those inputs and then make a prediction (in the form of a number) based on those inputs.</p> <h2 id=what-were-going-to-cover>What we're going to cover</h2> <p>Specifically, we're going to go through doing the following with TensorFlow: - Architecture of a regression model - Input shapes and output shapes - <code>X</code>: features/data (inputs) - <code>y</code>: labels (outputs) - Creating custom data to view and fit - Steps in modelling - Creating a model - Compiling a model - Defining a loss function - Setting up an optimizer - Creating evaluation metrics - Fitting a model (getting it to find patterns in our data) - Evaluating a model - Visualizng the model ("visualize, visualize, visualize") - Looking at training curves - Compare predictions to ground truth (using our evaluation metrics) - Saving a model (so we can use it later) - Loading a model</p> <p>Don't worry if none of these make sense now, we're going to go through each.</p> <h2 id=how-you-can-use-this-notebook>How you can use this notebook</h2> <p>You can read through the descriptions and the code (it should all run), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to <strong>write more code</strong>.</p> <h2 id=typical-architecture-of-a-regresison-neural-network>Typical architecture of a regresison neural network</h2> <p>The word <em>typical</em> is on purpose.</p> <p>Why?</p> <p>Because there are many different ways (actually, there's almost an infinite number of ways) to write neural networks.</p> <p>But the following is a generic setup for ingesting a collection of numbers, finding patterns in them and then outputting some kind of target number.</p> <p>Yes, the previous sentence is vague but we'll see this in action shortly.</p> <table> <thead> <tr> <th><strong>Hyperparameter</strong></th> <th><strong>Typical value</strong></th> </tr> </thead> <tbody> <tr> <td>Input layer shape</td> <td>Same shape as number of features (e.g. 3 for # bedrooms, # bathrooms, # car spaces in housing price prediction)</td> </tr> <tr> <td>Hidden layer(s)</td> <td>Problem specific, minimum = 1, maximum = unlimited</td> </tr> <tr> <td>Neurons per hidden layer</td> <td>Problem specific, generally 10 to 100</td> </tr> <tr> <td>Output layer shape</td> <td>Same shape as desired prediction shape (e.g. 1 for house price)</td> </tr> <tr> <td>Hidden activation</td> <td>Usually <a href=https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning>ReLU</a> (rectified linear unit)</td> </tr> <tr> <td>Output activation</td> <td>None, ReLU, logistic/tanh</td> </tr> <tr> <td>Loss function</td> <td><a href=https://en.wikipedia.org/wiki/Mean_squared_error>MSE</a> (mean square error) or <a href=https://en.wikipedia.org/wiki/Mean_absolute_error>MAE</a> (mean absolute error)/Huber (combination of MAE/MSE) if outliers</td> </tr> <tr> <td>Optimizer</td> <td><a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD>SGD</a> (stochastic gradient descent), <a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam>Adam</a></td> </tr> </tbody> </table> <p><em>Table 1: Typical architecture of a regression network. Source: Adapted from page 293 of <a href=https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ >Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow Book by Aur√©lien G√©ron</a></em></p> <p>Again, if you're new to neural networks and deep learning in general, much of the above table won't make sense. But don't worry, we'll be getting hands-on with all of it soon.</p> <blockquote> <p>üîë <strong>Note:</strong> A <strong>hyperparameter</strong> in machine learning is something a data analyst or developer can set themselves, where as a <strong>parameter</strong> usually describes something a model learns on its own (a value not explicitly set by an analyst).</p> </blockquote> <p>Okay, enough talk, let's get started writing code.</p> <p>To use TensorFlow, we'll import it as the common alias <code>tf</code> (short for TensorFlow).</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>
<span class=nb>print</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span> <span class=c1># check the version (should be 2.x+)</span>

<span class=kn>import</span><span class=w> </span><span class=nn>datetime</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Notebook last run (end-to-end): </span><span class=si>{</span><span class=n>datetime</span><span class=o>.</span><span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>2.17.0-dev20240226
Notebook last run (end-to-end): 2024-04-18 17:55:36.483369
</code></pre></div> <h2 id=creating-data-to-view-and-fit>Creating data to view and fit</h2> <p>Since we're working on a <strong>regression problem</strong> (predicting a number) let's create some linear data (a straight line) to model.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Create features</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=o>-</span><span class=mf>7.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>4.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>2.0</span><span class=p>,</span> <span class=mf>5.0</span><span class=p>,</span> <span class=mf>8.0</span><span class=p>,</span> <span class=mf>11.0</span><span class=p>,</span> <span class=mf>14.0</span><span class=p>])</span>

<span class=c1># Create labels</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>3.0</span><span class=p>,</span> <span class=mf>6.0</span><span class=p>,</span> <span class=mf>9.0</span><span class=p>,</span> <span class=mf>12.0</span><span class=p>,</span> <span class=mf>15.0</span><span class=p>,</span> <span class=mf>18.0</span><span class=p>,</span> <span class=mf>21.0</span><span class=p>,</span> <span class=mf>24.0</span><span class=p>])</span>

<span class=c1># Visualize it</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_5_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_5_0.png></a></p> <p>Before we do any modelling, can you calculate the pattern between <code>X</code> and <code>y</code>?</p> <p>For example, say I asked you, based on this data what the <code>y</code> value would be if <code>X</code> was 17.0?</p> <p>Or how about if <code>X</code> was -10.0?</p> <p>This kind of pattern discovery is the essence of what we'll be building neural networks to do for us.</p> <h2 id=regression-input-shapes-and-output-shapes>Regression input shapes and output shapes</h2> <p>One of the most important concepts when working with neural networks are the input and output shapes.</p> <p>The <strong>input shape</strong> is the shape of your data that goes into the model.</p> <p>The <strong>output shape</strong> is the shape of your data you want to come out of your model.</p> <p>These will differ depending on the problem you're working on.</p> <p>Neural networks accept numbers and output numbers. These numbers are typically represented as tensors (or arrays).</p> <p>Before, we created data using NumPy arrays, but we could do the same with tensors.</p> <div class=highlight><pre><span></span><code><span class=c1># Example input and output shapes of a regression model</span>
<span class=n>house_info</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=s2>&quot;bedroom&quot;</span><span class=p>,</span> <span class=s2>&quot;bathroom&quot;</span><span class=p>,</span> <span class=s2>&quot;garage&quot;</span><span class=p>])</span>
<span class=n>house_price</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mi>939700</span><span class=p>])</span>
<span class=n>house_info</span><span class=p>,</span> <span class=n>house_price</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;bedroom&#39;, b&#39;bathroom&#39;, b&#39;garage&#39;], dtype=object)&gt;,
 &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([939700])&gt;)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>house_info</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>TensorShape([3])
</code></pre></div> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Create features (using tensors)</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=o>-</span><span class=mf>7.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>4.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>2.0</span><span class=p>,</span> <span class=mf>5.0</span><span class=p>,</span> <span class=mf>8.0</span><span class=p>,</span> <span class=mf>11.0</span><span class=p>,</span> <span class=mf>14.0</span><span class=p>])</span>

<span class=c1># Create labels (using tensors)</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>3.0</span><span class=p>,</span> <span class=mf>6.0</span><span class=p>,</span> <span class=mf>9.0</span><span class=p>,</span> <span class=mf>12.0</span><span class=p>,</span> <span class=mf>15.0</span><span class=p>,</span> <span class=mf>18.0</span><span class=p>,</span> <span class=mf>21.0</span><span class=p>,</span> <span class=mf>24.0</span><span class=p>])</span>

<span class=c1># Visualize it</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_10_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_10_0.png></a></p> <p>Our goal here will be to use <code>X</code> to predict <code>y</code>.</p> <p>So our <strong>input</strong> will be <code>X</code> and our <strong>output</strong> will be <code>y</code>.</p> <p>Knowing this, what do you think our input and output shapes will be?</p> <p>Let's take a look.</p> <div class=highlight><pre><span></span><code><span class=c1># Take a single example of X</span>
<span class=n>input_shape</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span> 

<span class=c1># Take a single example of y</span>
<span class=n>output_shape</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span>

<span class=n>input_shape</span><span class=p>,</span> <span class=n>output_shape</span> <span class=c1># these are both scalars (no shape)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(TensorShape([]), TensorShape([]))
</code></pre></div> <p>Huh?</p> <p>From this it seems our inputs and outputs have no shape?</p> <p>How could that be?</p> <p>It's because no matter what kind of data we pass to our model, it's always going to take as input and return as output some kind of tensor.</p> <p>But in our case because of our dataset (only 2 small lists of numbers), we're looking at a special kind of tensor, more specifically a rank 0 tensor or a scalar.</p> <div class=highlight><pre><span></span><code><span class=c1># Let&#39;s take a look at the single examples invidually</span>
<span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;tf.Tensor: shape=(), dtype=float32, numpy=-7.0&gt;,
 &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;)
</code></pre></div> <p>In our case, we're trying to build a model to predict the pattern between <code>X[0]</code> equalling <code>-7.0</code> and <code>y[0]</code> equalling <code>3.0</code>.</p> <p>So now we get our answer, we're trying to use 1 <code>X</code> value to predict 1 <code>y</code> value.</p> <p>You might be thinking, "this seems pretty complicated for just predicting a straight line...".</p> <p>And you'd be right.</p> <p>But the concepts we're covering here, the concepts of input and output shapes to a model are fundamental. </p> <p>In fact, they're probably two of the things you'll spend the most time on when you work with neural networks: <strong>making sure your input and outputs are in the correct shape</strong>.</p> <p>If it doesn't make sense now, we'll see plenty more examples later on (soon you'll notice the input and output shapes can be almost anything you can imagine).</p> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/01-input-and-output-shapes-housing-prices.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="example of input and output shapes for a housing price prediction problem" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/01-input-and-output-shapes-housing-prices.png></a> <em>If you were working on building a machine learning algorithm for predicting housing prices, your inputs may be number of bedrooms, number of bathrooms and number of garages, giving you an input shape of 3 (3 different features). And since you're trying to predict the price of the house, your output shape would be 1.</em></p> <h2 id=steps-in-modelling-with-tensorflow>Steps in modelling with TensorFlow</h2> <p>Now we know what data we have as well as the input and output shapes, let's see how we'd build a neural network to model it.</p> <p>In TensorFlow, there are typically 3 fundamental steps to creating and training a model.</p> <ol> <li><strong>Creating a model</strong> - piece together the layers of a neural network yourself (using the <a href=https://www.tensorflow.org/guide/keras/functional>Functional</a> or <a href=https://www.tensorflow.org/api_docs/python/tf/keras/Sequential>Sequential API</a>) or import a previously built model (known as transfer learning).</li> <li><strong>Compiling a model</strong> - defining how a models performance should be measured (loss/metrics) as well as defining how it should improve (optimizer). </li> <li><strong>Fitting a model</strong> - letting the model try to find patterns in the data (how does <code>X</code> get to <code>y</code>). </li> </ol> <p>Let's see these in action using the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/Sequential>Keras Sequential API</a> to build a model for our regression data. And then we'll step through each.</p> <blockquote> <p><strong>Note:</strong> If you're using <a href=https://github.com/tensorflow/tensorflow/releases/tag/v2.7.0>TensorFlow 2.7.0</a>+, the <code>fit()</code> function no longer upscales input data to go from <code>(batch_size, )</code> to <code>(batch_size, 1)</code>. To fix this, you'll need to expand the dimension of input data using <code>tf.expand_dims(input_data, axis=-1)</code>.</p> <p>In our case, this means instead of using <code>model.fit(X, y, epochs=5)</code>, use <code>model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)</code>. </p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create a model using the Sequential API</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span> <span class=c1># mae is short for mean absolute error</span>
              <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(),</span> <span class=c1># SGD is short for stochastic gradient descent</span>
              <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=c1># model.fit(X, y, epochs=5) # this will break with TensorFlow 2.7.0+</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>),</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/5
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 382ms/step - loss: 12.8714 - mae: 12.8714
Epoch 2/5
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 33ms/step - loss: 12.7389 - mae: 12.7389
Epoch 3/5
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 12.6064 - mae: 12.6064
Epoch 4/5
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 12.4739 - mae: 12.4739
Epoch 5/5
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 27ms/step - loss: 12.3414 - mae: 12.3414





&lt;keras.src.callbacks.history.History at 0x1d925c69e80&gt;
</code></pre></div> <p>Boom!</p> <p>We've just trained a model to figure out the patterns between <code>X</code> and <code>y</code>.</p> <p>How do you think it went?</p> <div class=highlight><pre><span></span><code><span class=c1># Check out X and y</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)&gt;)
</code></pre></div> <p>What do you think the outcome should be if we passed our model an <code>X</code> value of 17.0?</p> <div class=highlight><pre><span></span><code><span class=c1># Make a prediction with the model</span>
<span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>17.0</span><span class=p>]))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 42ms/step





array([[6.0782204]], dtype=float32)
</code></pre></div> <p>It doesn't go very well... it should've output something close to 27.0.</p> <blockquote> <p>ü§î <strong>Question:</strong> What's Keras? I thought we were working with TensorFlow but every time we write TensorFlow code, <code>keras</code> comes after <code>tf</code> (e.g. <code>tf.keras.layers.Dense()</code>)?</p> </blockquote> <p>Before TensorFlow 2.0+, <a href=https://keras.io/ >Keras</a> was an API designed to be able to build deep learning models with ease. Since TensorFlow 2.0+, its functionality has been tightly integrated within the TensorFlow library.</p> <h2 id=improving-a-model>Improving a model</h2> <p>How do you think you'd improve upon our current model?</p> <p>If you guessed by tweaking some of the things we did above, you'd be correct.</p> <p>To improve our model, we alter almost every part of the 3 steps we went through before.</p> <ol> <li><strong>Creating a model</strong> - here you might want to add more layers, increase the number of hidden units (also called neurons) within each layer, change the activation functions of each layer.</li> <li><strong>Compiling a model</strong> - you might want to choose optimization function or perhaps change the <strong>learning rate</strong> of the optimization function.</li> <li><strong>Fitting a model</strong> - perhaps you could fit a model for more <strong>epochs</strong> (leave it training for longer) or on more data (give the model more examples to learn from).</li> </ol> <p><a class=glightbox href=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-improving-a-model-from-model-perspective.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="various options you can use to improve a neural network model" src=https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-improving-a-model-from-model-perspective.png></a> <em>There are many different ways to potentially improve a neural network. Some of the most common include: increasing the number of layers (making the network deeper), increasing the number of hidden units (making the network wider) and changing the learning rate. Because these values are all human-changeable, they're referred to as <a href=https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)>hyperparameters</a> and the practice of trying to find the best hyperparameters is referred to as <a href=https://en.wikipedia.org/wiki/Hyperparameter_optimization>hyperparameter tuning</a>.</em></p> <p>Woah. We just introduced a bunch of possible steps. The important thing to remember is how you alter each of these will depend on the problem you're working on.</p> <p>And the good thing is, over the next few problems, we'll get hands-on with all of them.</p> <p>For now, let's keep it simple, all we'll do is train our model for longer (everything else will stay the same).</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create a model (same as above)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Compile model (same as above)</span>
<span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
              <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(),</span>
              <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>])</span>

<span class=c1># Fit model (this time we&#39;ll train for longer)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>),</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span> <span class=c1># train for 100 epochs not 10</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 443ms/step - loss: 18.1138 - mae: 18.1138
Epoch 2/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 28ms/step - loss: 17.8326 - mae: 17.8326
Epoch 3/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 27ms/step - loss: 17.5513 - mae: 17.5513
Epoch 4/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 17.2701 - mae: 17.2701
Epoch 5/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 16.9888 - mae: 16.9888
Epoch 6/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 16.7076 - mae: 16.7076
Epoch 7/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 16.4263 - mae: 16.4263
Epoch 8/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 16.1451 - mae: 16.1451
Epoch 9/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 15.8638 - mae: 15.8638
Epoch 10/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 28ms/step - loss: 15.5826 - mae: 15.5826
Epoch 11/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 28ms/step - loss: 15.3013 - mae: 15.3013
Epoch 12/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 15.0201 - mae: 15.0201
Epoch 13/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 33ms/step - loss: 14.7809 - mae: 14.7809
Epoch 14/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 14.6484 - mae: 14.6484
Epoch 15/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 82ms/step - loss: 14.5159 - mae: 14.5159
Epoch 16/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 75ms/step - loss: 14.3834 - mae: 14.3834
Epoch 17/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 30ms/step - loss: 14.2509 - mae: 14.2509
Epoch 18/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 30ms/step - loss: 14.1184 - mae: 14.1184
Epoch 19/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 27ms/step - loss: 13.9859 - mae: 13.9859
Epoch 20/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 30ms/step - loss: 13.8534 - mae: 13.8534
Epoch 21/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 28ms/step - loss: 13.7209 - mae: 13.7209
Epoch 22/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 26ms/step - loss: 13.5884 - mae: 13.5884
Epoch 23/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 30ms/step - loss: 13.4559 - mae: 13.4559
Epoch 24/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 28ms/step - loss: 13.3234 - mae: 13.3234
Epoch 25/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 29ms/step - loss: 13.1909 - mae: 13.1909
Epoch 26/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 29ms/step - loss: 13.0584 - mae: 13.0584
Epoch 27/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 37ms/step - loss: 12.9259 - mae: 12.9259
Epoch 28/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 12.7934 - mae: 12.7934
Epoch 29/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 36ms/step - loss: 12.6609 - mae: 12.6609
Epoch 30/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 40ms/step - loss: 12.5284 - mae: 12.5284
Epoch 31/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 42ms/step - loss: 12.3959 - mae: 12.3959
Epoch 32/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 33ms/step - loss: 12.2634 - mae: 12.2634
Epoch 33/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 36ms/step - loss: 12.1309 - mae: 12.1309
Epoch 34/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 36ms/step - loss: 11.9984 - mae: 11.9984
Epoch 35/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 34ms/step - loss: 11.8659 - mae: 11.8659
Epoch 36/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 35ms/step - loss: 11.7334 - mae: 11.7334
Epoch 37/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 11.6009 - mae: 11.6009
Epoch 38/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 40ms/step - loss: 11.4684 - mae: 11.4684
Epoch 39/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 11.3359 - mae: 11.3359
Epoch 40/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 34ms/step - loss: 11.2034 - mae: 11.2034
Epoch 41/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 37ms/step - loss: 11.0709 - mae: 11.0709
Epoch 42/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 34ms/step - loss: 10.9384 - mae: 10.9384
Epoch 43/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 46ms/step - loss: 10.8059 - mae: 10.8059
Epoch 44/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 41ms/step - loss: 10.6734 - mae: 10.6734
Epoch 45/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 27ms/step - loss: 10.5409 - mae: 10.5409
Epoch 46/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 39ms/step - loss: 10.4084 - mae: 10.4084
Epoch 47/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 36ms/step - loss: 10.2759 - mae: 10.2759
Epoch 48/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 27ms/step - loss: 10.1434 - mae: 10.1434
Epoch 49/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 29ms/step - loss: 10.0109 - mae: 10.0109
Epoch 50/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 33ms/step - loss: 9.8784 - mae: 9.8784
Epoch 51/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 9.7459 - mae: 9.7459
Epoch 52/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 29ms/step - loss: 9.6134 - mae: 9.6134
Epoch 53/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 29ms/step - loss: 9.4809 - mae: 9.4809
Epoch 54/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 28ms/step - loss: 9.3484 - mae: 9.3484
Epoch 55/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 35ms/step - loss: 9.2159 - mae: 9.2159
Epoch 56/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 33ms/step - loss: 9.0834 - mae: 9.0834
Epoch 57/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 30ms/step - loss: 8.9509 - mae: 8.9509
Epoch 58/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 28ms/step - loss: 8.8184 - mae: 8.8184
Epoch 59/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 8.6859 - mae: 8.6859
Epoch 60/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 8.5534 - mae: 8.5534
Epoch 61/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 28ms/step - loss: 8.4209 - mae: 8.4209
Epoch 62/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 28ms/step - loss: 8.2884 - mae: 8.2884
Epoch 63/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 35ms/step - loss: 8.1559 - mae: 8.1559
Epoch 64/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 34ms/step - loss: 8.0234 - mae: 8.0234
Epoch 65/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 7.8909 - mae: 7.8909
Epoch 66/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 55ms/step - loss: 7.7584 - mae: 7.7584
Epoch 67/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 49ms/step - loss: 7.6259 - mae: 7.6259
Epoch 68/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 46ms/step - loss: 7.4934 - mae: 7.4934
Epoch 69/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 45ms/step - loss: 7.3609 - mae: 7.3609
Epoch 70/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 47ms/step - loss: 7.2284 - mae: 7.2284
Epoch 71/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 39ms/step - loss: 7.0959 - mae: 7.0959
Epoch 72/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 41ms/step - loss: 6.9900 - mae: 6.9900
Epoch 73/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 34ms/step - loss: 6.9844 - mae: 6.9844
Epoch 74/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 6.9788 - mae: 6.9788
Epoch 75/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 34ms/step - loss: 6.9731 - mae: 6.9731
Epoch 76/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 6.9675 - mae: 6.9675
Epoch 77/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 30ms/step - loss: 6.9619 - mae: 6.9619
Epoch 78/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 40ms/step - loss: 6.9563 - mae: 6.9563
Epoch 79/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 6.9506 - mae: 6.9506
Epoch 80/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 28ms/step - loss: 6.9450 - mae: 6.9450
Epoch 81/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 30ms/step - loss: 6.9394 - mae: 6.9394
Epoch 82/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 6.9338 - mae: 6.9338
Epoch 83/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 6.9281 - mae: 6.9281
Epoch 84/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 6.9225 - mae: 6.9225
Epoch 85/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 29ms/step - loss: 6.9169 - mae: 6.9169
Epoch 86/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 42ms/step - loss: 6.9113 - mae: 6.9113
Epoch 87/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 6.9056 - mae: 6.9056
Epoch 88/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 6.9000 - mae: 6.9000
Epoch 89/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 29ms/step - loss: 6.8944 - mae: 6.8944
Epoch 90/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 35ms/step - loss: 6.8888 - mae: 6.8888
Epoch 91/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 34ms/step - loss: 6.8831 - mae: 6.8831
Epoch 92/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 41ms/step - loss: 6.8775 - mae: 6.8775
Epoch 93/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 68ms/step - loss: 6.8719 - mae: 6.8719
Epoch 94/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 51ms/step - loss: 6.8663 - mae: 6.8663
Epoch 95/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 47ms/step - loss: 6.8606 - mae: 6.8606
Epoch 96/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 30ms/step - loss: 6.8550 - mae: 6.8550
Epoch 97/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 31ms/step - loss: 6.8494 - mae: 6.8494
Epoch 98/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 29ms/step - loss: 6.8438 - mae: 6.8438
Epoch 99/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step - loss: 6.8381 - mae: 6.8381
Epoch 100/100
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 33ms/step - loss: 6.8325 - mae: 6.8325





&lt;keras.src.callbacks.history.History at 0x1d925c38680&gt;
</code></pre></div> <p>You might've noticed the loss value decrease from before (and keep decreasing as the number of epochs gets higher). </p> <p>What do you think this means for when we make a prediction with our model?</p> <p>How about we try predict on 17.0 again?</p> <div class=highlight><pre><span></span><code><span class=c1># Remind ourselves of what X and y are</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)&gt;)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Try and predict what y would be if X was 17.0</span>
<span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>([</span><span class=mf>17.0</span><span class=p>]))</span> <span class=c1># the right answer is 27.0 (y = X + 10)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 52ms/step





array([[29.34386]], dtype=float32)
</code></pre></div> <p>Much better! </p> <p>We got closer this time. But we could still be better.</p> <p>Now we've trained a model, how could we evaluate it?</p> <h2 id=evaluating-a-model>Evaluating a model</h2> <p>A typical workflow you'll go through when building neural networks is:</p> <div class=highlight><pre><span></span><code>Build a model -&gt; evaluate it -&gt; build (tweak) a model -&gt; evaulate it -&gt; build (tweak) a model -&gt; evaluate it...
</code></pre></div> <p>The tweaking comes from maybe not building a model from scratch but adjusting an existing one.</p> <h3 id=visualize-visualize-visualize>Visualize, visualize, visualize</h3> <p>When it comes to evaluation, you'll want to remember the words: "visualize, visualize, visualize." </p> <p>This is because you're probably better looking at something (doing) than you are thinking about something.</p> <p>It's a good idea to visualize: * <strong>The data</strong> - what data are you working with? What does it look like? * <strong>The model itself</strong> - what does the architecture look like? What are the different shapes? * <strong>The training of a model</strong> - how does a model perform while it learns? * <strong>The predictions of a model</strong> - how do the predictions of a model line up against the ground truth (the original labels)?</p> <p>Let's start by visualizing the model.</p> <p>But first, we'll create a little bit of a bigger dataset and a new model we can use (it'll be the same as before, but the more practice the better).</p> <div class=highlight><pre><span></span><code><span class=c1># Make a bigger dataset</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=o>-</span><span class=mi>100</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
<span class=n>X</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([-100,  -96,  -92,  -88,  -84,  -80,  -76,  -72,  -68,  -64,  -60,
        -56,  -52,  -48,  -44,  -40,  -36,  -32,  -28,  -24,  -20,  -16,
        -12,   -8,   -4,    0,    4,    8,   12,   16,   20,   24,   28,
         32,   36,   40,   44,   48,   52,   56,   60,   64,   68,   72,
         76,   80,   84,   88,   92,   96])
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make labels for the dataset (adhering to the same pattern as before)</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=o>-</span><span class=mi>90</span><span class=p>,</span> <span class=mi>110</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
<span class=n>y</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([-90, -86, -82, -78, -74, -70, -66, -62, -58, -54, -50, -46, -42,
       -38, -34, -30, -26, -22, -18, -14, -10,  -6,  -2,   2,   6,  10,
        14,  18,  22,  26,  30,  34,  38,  42,  46,  50,  54,  58,  62,
        66,  70,  74,  78,  82,  86,  90,  94,  98, 102, 106])
</code></pre></div> <p>Since <span class=arithmatex>\(y=X+10\)</span>, we could make the labels like so:</p> <div class=highlight><pre><span></span><code><span class=c1># Same result as above</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>X</span> <span class=o>+</span> <span class=mi>10</span>
<span class=n>y</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([-90, -86, -82, -78, -74, -70, -66, -62, -58, -54, -50, -46, -42,
       -38, -34, -30, -26, -22, -18, -14, -10,  -6,  -2,   2,   6,  10,
        14,  18,  22,  26,  30,  34,  38,  42,  46,  50,  54,  58,  62,
        66,  70,  74,  78,  82,  86,  90,  94,  98, 102, 106])
</code></pre></div> <h2 id=split-data-into-trainingtest-set>Split data into training/test set</h2> <p>One of the other most common and important steps in a machine learning project is creating a training and test set (and when required, a validation set).</p> <p>Each set serves a specific purpose: * <strong>Training set</strong> - the model learns from this data, which is typically 70-80% of the total data available (like the course materials you study during the semester). * <strong>Validation set</strong> - the model gets tuned on this data, which is typically 10-15% of the total data available (like the practice exam you take before the final exam). * <strong>Test set</strong> - the model gets evaluated on this data to test what it has learned, it's typically 10-15% of the total data available (like the final exam you take at the end of the semester).</p> <p>For now, we'll just use a training and test set, this means we'll have a dataset for our model to learn on as well as be evaluated on.</p> <p>We can create them by splitting our <code>X</code> and <code>y</code> arrays.</p> <blockquote> <p>üîë <strong>Note:</strong> When dealing with real-world data, this step is typically done right at the start of a project (the test set should always be kept separate from all other data). We want our model to learn on training data and then evaluate it on test data to get an indication of how well it <strong>generalizes</strong> to unseen examples.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Check how many samples we have</span>
<span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>50
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Split data into train and test sets</span>
<span class=n>X_train</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:</span><span class=mi>40</span><span class=p>]</span> <span class=c1># first 40 examples (80% of data)</span>
<span class=n>y_train</span> <span class=o>=</span> <span class=n>y</span><span class=p>[:</span><span class=mi>40</span><span class=p>]</span>

<span class=n>X_test</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=mi>40</span><span class=p>:]</span> <span class=c1># last 10 examples (20% of data)</span>
<span class=n>y_test</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=mi>40</span><span class=p>:]</span>

<span class=nb>len</span><span class=p>(</span><span class=n>X_train</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(40, 10)
</code></pre></div> <h2 id=visualizing-the-data>Visualizing the data</h2> <p>Now we've got our training and test data, it's a good idea to visualize it.</p> <p>Let's plot it with some nice colours to differentiate what's what.</p> <div class=highlight><pre><span></span><code><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
<span class=c1># Plot training data in blue</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s1>&#39;b&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training data&#39;</span><span class=p>)</span>
<span class=c1># Plot test data in green</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s1>&#39;g&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Testing data&#39;</span><span class=p>)</span>
<span class=c1># Show the legend</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>();</span>
</code></pre></div> <p><a class=glightbox href=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_38_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_38_0.png></a></p> <p>Beautiful! Any time you can visualize your data, your model, your anything, it's a good idea. </p> <p>With this graph in mind, what we'll be trying to do is build a model which learns the pattern in the blue dots (<code>X_train</code>) to draw the green dots (<code>X_test</code>).</p> <p>Time to build a model. We'll make the exact same one from before (the one we trained for longer).</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create a model (same as above)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Compile model (same as above)</span>
<span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
              <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(),</span>
              <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>])</span>

<span class=c1># Fit model (same as above)</span>
<span class=c1>#model.fit(X_train, y_train, epochs=100) # commented out on purpose (not fitting it just yet)</span>
</code></pre></div> <h2 id=visualizing-the-model>Visualizing the model</h2> <p>After you've built a model, you might want to take a look at it (especially if you haven't built many before).</p> <p>You can take a look at the layers and shapes of your model by calling <a href=https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary><code>summary()</code></a> on it.</p> <blockquote> <p>üîë <strong>Note:</strong> Visualizing a model is particularly helpful when you run into input and output shape mismatches.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Doesn&#39;t work (model not fit/built)</span>
<span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</code></pre></div> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_2"</span>
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ<span style="font-weight: bold"> Layer (type)                         </span>‚îÉ<span style="font-weight: bold"> Output Shape                </span>‚îÉ<span style="font-weight: bold">         Param # </span>‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ dense_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      ‚îÇ ?                           ‚îÇ     <span style="color: #00af00; text-decoration-color: #00af00">0</span> (unbuilt) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre> <p>Ahh, the cell above errors because we haven't fit or built our model.</p> <p>We also haven't told it what input shape it should be expecting.</p> <p>Remember above, how we discussed the input shape was just one number?</p> <p>We can let our model know the input shape of our data using the <code>input_shape</code> parameter to the first layer (usually if <code>input_shape</code> isn't defined, Keras tries to figure it out automatically).</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create a model (same as above)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span> <span class=c1># define the input_shape to our model</span>
<span class=p>])</span>

<span class=c1># Compile model (same as above)</span>
<span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
              <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(),</span>
              <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;mae&quot;</span><span class=p>])</span>
</code></pre></div> <div class=highlight><pre><span></span><code>D:\anaconda\envs\py3-TF2.0\Lib\site-packages\keras\src\layers\core\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># This will work after specifying the input shape</span>
<span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</code></pre></div> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_3"</span>
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ<span style="font-weight: bold"> Layer (type)                         </span>‚îÉ<span style="font-weight: bold"> Output Shape                </span>‚îÉ<span style="font-weight: bold">         Param # </span>‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ dense_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      ‚îÇ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   ‚îÇ               <span style="color: #00af00; text-decoration-color: #00af00">2</span> ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">2</span> (8.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">2</span> (8.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre> <p>Calling <code>summary()</code> on our model shows us the layers it contains, the output shape and the number of parameters. * <strong>Total params</strong> - total number of parameters in the model. * <strong>Trainable parameters</strong> - these are the parameters (patterns) the model can update as it trains. * <strong>Non-trainable parameters</strong> - these parameters aren't updated during training (this is typical when you bring in the already learned patterns from other models during transfer learning).</p> <blockquote> <p>üìñ <strong>Resource:</strong> For a more in-depth overview of the trainable parameters within a layer, check out <a href=https://youtu.be/njKP3FqW3Sk>MIT's introduction to deep learning video</a>.</p> <p>üõ† <strong>Exercise:</strong> Try playing around with the number of hidden units in the <code>Dense</code> layer (e.g. <code>Dense(2)</code>, <code>Dense(3)</code>). How does this change the Total/Trainable params? Investigate what's causing the change.</p> </blockquote> <p>For now, all you need to think about these parameters is that their learnable patterns in the data.</p> <p>Let's fit our model to the training data.</p> <div class=highlight><pre><span></span><code><span class=c1># Fit the model to the training data</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=c1># verbose controls how much gets output</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;keras.src.callbacks.history.History at 0x1d9275bd8b0&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check the model summary</span>
<span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</code></pre></div> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_3"</span>
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ<span style="font-weight: bold"> Layer (type)                         </span>‚îÉ<span style="font-weight: bold"> Output Shape                </span>‚îÉ<span style="font-weight: bold">         Param # </span>‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ dense_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      ‚îÇ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   ‚îÇ               <span style="color: #00af00; text-decoration-color: #00af00">2</span> ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">4</span> (20.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">2</span> (8.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">2</span> (12.00 B)
</pre> <p>Alongside summary, you can also view a 2D plot of the model using <a href=https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model><code>plot_model()</code></a>.</p> <div class=highlight><pre><span></span><code><span class=c1># from plot_model import plot_model</span>

<span class=c1># plot_model(model, show_shapes=True)</span>
</code></pre></div> <p>In our case, the model we used only has an input and an output but visualizing more complicated models can be very helpful for debugging.</p> <h2 id=visualizing-the-predictions>Visualizing the predictions</h2> <p>Now we've got a trained model, let's visualize some predictions.</p> <p>To visualize predictions, it's always a good idea to plot them against the ground truth labels.</p> <p>Often you'll see this in the form of <code>y_test</code> vs. <code>y_pred</code> (ground truth vs. predictions).</p> <p>First, we'll make some predictions on the test data (<code>X_test</code>), remember the model has never seen the test data.</p> <div class=highlight><pre><span></span><code><span class=c1># Make predictions</span>
<span class=n>y_preds</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 40ms/step
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># View the predictions</span>
<span class=n>y_preds</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([[55.658394],
       [59.274494],
       [62.890594],
       [66.5067  ],
       [70.1228  ],
       [73.7389  ],
       [77.355   ],
       [80.97111 ],
       [84.587204],
       [88.20331 ]], dtype=float32)
</code></pre></div> <p>Okay, we get a list of numbers but how do these compare to the ground truth labels?</p> <p>Let's build a plotting function to find out.</p> <blockquote> <p>üîë <strong>Note:</strong> If you think you're going to be visualizing something a lot, it's a good idea to functionize it so you can use it later.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>plot_predictions</span><span class=p>(</span><span class=n>train_data</span><span class=o>=</span><span class=n>X_train</span><span class=p>,</span> 
                     <span class=n>train_labels</span><span class=o>=</span><span class=n>y_train</span><span class=p>,</span> 
                     <span class=n>test_data</span><span class=o>=</span><span class=n>X_test</span><span class=p>,</span> 
                     <span class=n>test_labels</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span> 
                     <span class=n>predictions</span><span class=o>=</span><span class=n>y_preds</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Plots training data, test data and compares predictions.</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
  <span class=c1># Plot training data in blue</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>train_data</span><span class=p>,</span> <span class=n>train_labels</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s2>&quot;b&quot;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Training data&quot;</span><span class=p>)</span>
  <span class=c1># Plot test data in green</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>test_data</span><span class=p>,</span> <span class=n>test_labels</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s2>&quot;g&quot;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Testing data&quot;</span><span class=p>)</span>
  <span class=c1># Plot the predictions in red (predictions were made on the test data)</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>test_data</span><span class=p>,</span> <span class=n>predictions</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s2>&quot;r&quot;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&quot;Predictions&quot;</span><span class=p>)</span>
  <span class=c1># Show the legend</span>
  <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>();</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>plot_predictions</span><span class=p>(</span><span class=n>train_data</span><span class=o>=</span><span class=n>X_train</span><span class=p>,</span>
                 <span class=n>train_labels</span><span class=o>=</span><span class=n>y_train</span><span class=p>,</span>
                 <span class=n>test_data</span><span class=o>=</span><span class=n>X_test</span><span class=p>,</span>
                 <span class=n>test_labels</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span>
                 <span class=n>predictions</span><span class=o>=</span><span class=n>y_preds</span><span class=p>)</span>
</code></pre></div> <p><a class=glightbox href=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_57_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_57_0.png></a></p> <p>From the plot we can see our predictions aren't totally outlandish but they definitely aren't anything special either.</p> <h2 id=evaluating-predictions>Evaluating predictions</h2> <p>Alongisde visualizations, evaulation metrics are your alternative best option for evaluating your model.</p> <p>Depending on the problem you're working on, different models have different evaluation metrics. </p> <p>Two of the main metrics used for regression problems are: * <strong>Mean absolute error (MAE)</strong> - the mean difference between each of the predictions. * <strong>Mean squared error (MSE)</strong> - the squared mean difference between of the predictions (use if larger errors are more detrimental than smaller errors).</p> <p>The lower each of these values, the better.</p> <p>You can also use <a href=https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate><code>model.evaluate()</code></a> which will return the loss of the model as well as any metrics setup during the compile step.</p> <div class=highlight><pre><span></span><code><span class=c1># Evaluate the model on the test set</span>
<span class=n>model</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 93ms/step - loss: 16.0691 - mae: 16.0691





[16.069149017333984, 16.069149017333984]
</code></pre></div> <p>In our case, since we used MAE for the loss function as well as MAE for the metrics, <code>model.evaulate()</code> returns them both.</p> <p>TensorFlow also has built in functions for MSE and MAE.</p> <p>For many evaluation functions, the premise is the same: compare predictions to the ground truth labels.</p> <div class=highlight><pre><span></span><code><span class=c1># Calculate the mean absolute error</span>
<span class=n>mae</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span> 
                                     <span class=n>y_pred</span><span class=o>=</span><span class=n>y_preds</span><span class=p>)</span>
<span class=n>mae</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=
array([14.341606 , 14.725506 , 15.1094055, 15.493301 , 15.877197 ,
       16.2611   , 16.644997 , 17.028893 , 17.412796 , 17.796692 ],
      dtype=float32)&gt;
</code></pre></div> <p>Huh? That's strange, MAE should be a single output.</p> <p>Instead, we get 10 values.</p> <p>This is because our <code>y_test</code> and <code>y_preds</code> tensors are different shapes.</p> <div class=highlight><pre><span></span><code><span class=c1># Check the test label tensor values</span>
<span class=n>y_test</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([ 70,  74,  78,  82,  86,  90,  94,  98, 102, 106])
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check the predictions tensor values (notice the extra square brackets)</span>
<span class=n>y_preds</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([[55.658394],
       [59.274494],
       [62.890594],
       [66.5067  ],
       [70.1228  ],
       [73.7389  ],
       [77.355   ],
       [80.97111 ],
       [84.587204],
       [88.20331 ]], dtype=float32)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check the tensor shapes</span>
<span class=n>y_test</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y_preds</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>((10,), (10, 1))
</code></pre></div> <p>Remember how we discussed dealing with different input and output shapes is one the most common issues you'll come across, this is one of those times.</p> <p>But not to worry.</p> <p>We can fix it using <a href=https://www.tensorflow.org/api_docs/python/tf/squeeze><code>squeeze()</code></a>, it'll remove the the <code>1</code> dimension from our <code>y_preds</code> tensor, making it the same shape as <code>y_test</code>.</p> <blockquote> <p>üîë <strong>Note:</strong> If you're comparing two tensors, it's important to make sure they're the right shape(s) (you won't always have to manipulate the shapes, but always be on the look out, <em>many</em> errors are the result of mismatched tensors, especially mismatched input and output shapes).</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Shape before squeeze()</span>
<span class=n>y_preds</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(10, 1)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Shape after squeeze()</span>
<span class=n>y_preds</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(10,)
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># What do they look like?</span>
<span class=n>y_test</span><span class=p>,</span> <span class=n>y_preds</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(array([ 70,  74,  78,  82,  86,  90,  94,  98, 102, 106]),
 array([55.658394, 59.274494, 62.890594, 66.5067  , 70.1228  , 73.7389  ,
        77.355   , 80.97111 , 84.587204, 88.20331 ], dtype=float32))
</code></pre></div> <p>Okay, now we know how to make our <code>y_test</code> and <code>y_preds</code> tenors the same shape, let's use our evaluation metrics.</p> <div class=highlight><pre><span></span><code><span class=c1># Calcuate the MAE</span>
<span class=n>mae</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span> 
                                     <span class=n>y_pred</span><span class=o>=</span><span class=n>y_preds</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span> <span class=c1># use squeeze() to make same shape</span>
<span class=n>mae</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=16.069149&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Calculate the MSE</span>
<span class=n>mse</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_true</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span>
                                    <span class=n>y_pred</span><span class=o>=</span><span class=n>y_preds</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span>
<span class=n>mse</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=259.4334&gt;
</code></pre></div> <p>We can also calculate the MAE using pure TensorFlow functions.</p> <div class=highlight><pre><span></span><code><span class=c1># Returns the same as tf.metrics.mean_absolute_error()</span>
<span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_test</span><span class=o>-</span><span class=n>y_preds</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()))</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;tf.Tensor: shape=(), dtype=float64, numpy=16.06914939880371&gt;
</code></pre></div> <p>Again, it's a good idea to functionize anything you think you might use over again (or find yourself using over and over again).</p> <p>Let's make functions for our evaluation metrics.</p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>mae</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Calculuates mean absolute error between y_test and y_preds.</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span>
                                        <span class=n>y_pred</span><span class=p>)</span>

<span class=k>def</span><span class=w> </span><span class=nf>mse</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>):</span>
<span class=w>  </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>  Calculates mean squared error between y_test and y_preds.</span>
<span class=sd>  &quot;&quot;&quot;</span>
  <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span>
                                       <span class=n>y_pred</span><span class=p>)</span>
</code></pre></div> <h2 id=running-experiments-to-improve-a-model>Running experiments to improve a model</h2> <p>After seeing the evaluation metrics and the predictions your model makes, it's likely you'll want to improve it.</p> <p>Again, there are many different ways you can do this, but 3 of the main ones are: 1. <strong>Get more data</strong> - get more examples for your model to train on (more opportunities to learn patterns). 2. <strong>Make your model larger (use a more complex model)</strong> - this might come in the form of more layers or more hidden units in each layer. 3. <strong>Train for longer</strong> - give your model more of a chance to find the patterns in the data.</p> <p>Since we created our dataset, we could easily make more data but this isn't always the case when you're working with real-world datasets.</p> <p>So let's take a look at how we can improve our model using 2 and 3.</p> <p>To do so, we'll build 3 models and compare their results: 1. <code>model_1</code> - same as original model, 1 layer, trained for 100 epochs. 2. <code>model_2</code> - 2 layers, trained for 100 epochs. 3. <code>model_3</code> - 2 layers, trained for 500 epochs.</p> <p><strong>Build <code>model_1</code></strong></p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Replicate original model</span>
<span class=n>model_1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_1</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(),</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;mae&#39;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=n>model_1</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>),</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 9ms/step - loss: 10.2953 - mae: 9.9377 
Epoch 2/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.8617 - mae: 8.2765 
Epoch 3/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.5937 - mae: 8.9518 
Epoch 4/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.4829 - mae: 7.9909 
Epoch 5/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.1120 - mae: 9.6343
Epoch 6/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.4625 - mae: 7.9731 
Epoch 7/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 10.0965 - mae: 9.6227
Epoch 8/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.4421 - mae: 7.9552 
Epoch 9/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.0809 - mae: 9.6110
Epoch 10/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.4218 - mae: 7.9374 
Epoch 11/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.0653 - mae: 9.5994
Epoch 12/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.4014 - mae: 7.9195 
Epoch 13/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.0497 - mae: 9.5878
Epoch 14/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.3811 - mae: 7.9017 
Epoch 15/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.0342 - mae: 9.5762
Epoch 16/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.3607 - mae: 7.8838 
Epoch 17/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 3ms/step - loss: 10.0186 - mae: 9.5646
Epoch 18/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.3403 - mae: 7.8660 
Epoch 19/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 10.0030 - mae: 9.5529
Epoch 20/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.3200 - mae: 7.8481 
Epoch 21/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.9874 - mae: 9.5413 
Epoch 22/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.2996 - mae: 7.8303 
Epoch 23/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 3ms/step - loss: 9.9719 - mae: 9.5297 
Epoch 24/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.2792 - mae: 7.8124 
Epoch 25/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.9563 - mae: 9.5181 
Epoch 26/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 3ms/step - loss: 8.2589 - mae: 7.7946 
Epoch 27/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 3ms/step - loss: 9.9407 - mae: 9.5065 
Epoch 28/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.2385 - mae: 7.7767 
Epoch 29/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.9252 - mae: 9.4948 
Epoch 30/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.2181 - mae: 7.7589 
Epoch 31/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.9096 - mae: 9.4832 
Epoch 32/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.1978 - mae: 7.7410 
Epoch 33/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.8940 - mae: 9.4716 
Epoch 34/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.1774 - mae: 7.7232 
Epoch 35/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 9.8784 - mae: 9.4600 
Epoch 36/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.4692 - mae: 7.8304 
Epoch 37/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.1815 - mae: 8.4801 
Epoch 38/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.7880 - mae: 7.4489 
Epoch 39/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.6967 - mae: 10.4088 
Epoch 40/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 7.4362 - mae: 7.2642 
Epoch 41/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.5099 - mae: 11.5898 
Epoch 42/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.0906 - mae: 7.6280 
Epoch 43/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.7864 - mae: 9.3580 
Epoch 44/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.0702 - mae: 7.6101 
Epoch 45/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 9.7708 - mae: 9.3463 
Epoch 46/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.0499 - mae: 7.5923 
Epoch 47/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 9.7553 - mae: 9.3347 
Epoch 48/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 8ms/step - loss: 8.3417 - mae: 7.6995 
Epoch 49/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.0562 - mae: 8.3521 
Epoch 50/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 7.6608 - mae: 7.3184 
Epoch 51/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.5790 - mae: 10.2907 
Epoch 52/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.3098 - mae: 7.1348 
Epoch 53/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.3956 - mae: 11.4761 
Epoch 54/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.9630 - mae: 7.4970 
Epoch 55/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.6632 - mae: 9.2327 
Epoch 56/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.9427 - mae: 7.4792 
Epoch 57/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.6477 - mae: 9.2211 
Epoch 58/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.9223 - mae: 7.4613 
Epoch 59/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.6321 - mae: 9.2095 
Epoch 60/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.2142 - mae: 7.5687 
Epoch 61/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.9309 - mae: 8.2240 
Epoch 62/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 7.5336 - mae: 7.1879 
Epoch 63/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.4614 - mae: 10.1726
Epoch 64/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 7.1834 - mae: 7.0053 
Epoch 65/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.2813 - mae: 11.3625 
Epoch 66/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.8355 - mae: 7.3661 
Epoch 67/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 9.5401 - mae: 9.1074 
Epoch 68/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 7.8151 - mae: 7.3482 
Epoch 69/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.5245 - mae: 9.0958 
Epoch 70/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.7948 - mae: 7.3304 
Epoch 71/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.7113 - mae: 9.1656 
Epoch 72/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 6.9956 - mae: 6.8799 
Epoch 73/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.2257 - mae: 9.5715 
Epoch 74/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.3729 - mae: 11.3429 
Epoch 75/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.7856 - mae: 12.0771 
Epoch 76/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.0629 - mae: 7.4401 
Epoch 77/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.7985 - mae: 8.1201 
Epoch 78/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 7.3635 - mae: 7.0348 
Epoch 79/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.3574 - mae: 10.1055
Epoch 80/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 7.0038 - mae: 6.8399 
Epoch 81/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.3377 - mae: 11.3717 
Epoch 82/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 6.9912 - mae: 6.8153 
Epoch 83/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.2812 - mae: 11.2899 
Epoch 84/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 6.7521 - mae: 6.7014 
Epoch 85/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 3ms/step - loss: 8.6482 - mae: 7.7535 
Epoch 86/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.0388 - mae: 9.2223 
Epoch 87/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.9745 - mae: 10.9090  
Epoch 88/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 3ms/step - loss: 8.6563 - mae: 7.8633 
Epoch 89/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.0469 - mae: 9.3320 
Epoch 90/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.0929 - mae: 11.0629 
Epoch 91/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.7475 - mae: 12.1116 
Epoch 92/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.9262 - mae: 7.3464 
Epoch 93/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.6927 - mae: 8.0665 
Epoch 94/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.5365 - mae: 7.0466 
Epoch 95/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 11ms/step - loss: 9.4372 - mae: 8.8613
Epoch 96/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 7.2300 - mae: 6.8334 
Epoch 97/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.7767 - mae: 10.6792  
Epoch 98/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 8.5304 - mae: 7.6621 
Epoch 99/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 8.9210 - mae: 9.1309 
Epoch 100/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 9.8744 - mae: 10.8247





&lt;keras.src.callbacks.history.History at 0x1d928f0e060&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make and plot predictions for model_1</span>
<span class=n>y_preds_1</span> <span class=o>=</span> <span class=n>model_1</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=n>plot_predictions</span><span class=p>(</span><span class=n>predictions</span><span class=o>=</span><span class=n>y_preds_1</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 58ms/step
</code></pre></div> <p><a class=glightbox href=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_80_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_80_1.png></a></p> <div class=highlight><pre><span></span><code><span class=c1># Calculate model_1 metrics</span>
<span class=n>mae_1</span> <span class=o>=</span> <span class=n>mae</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_preds_1</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
<span class=n>mse_1</span> <span class=o>=</span> <span class=n>mse</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_preds_1</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
<span class=n>mae_1</span><span class=p>,</span> <span class=n>mse_1</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(13.23732, 175.69809)
</code></pre></div> <p><strong>Build <code>model_2</code></strong></p> <p>This time we'll add an extra dense layer (so now our model will have 2 layers) whilst keeping everything else the same.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Replicate model_1 and add an extra layer</span>
<span class=n>model_2</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=c1># add a second layer</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_2</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(),</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;mae&#39;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=n>model_2</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>),</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=c1># set verbose to 0 for less output</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 11ms/step - loss: 40.3570 - mae: 48.5746 
Epoch 2/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 13.8935 - mae: 13.9815 
Epoch 3/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 12.8610 - mae: 13.0419 
Epoch 4/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 15.4269 - mae: 15.1253 
Epoch 5/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.5182 - mae: 10.3873
Epoch 6/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 11.9887 - mae: 11.3604 
Epoch 7/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 12.4901 - mae: 12.3939 
Epoch 8/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 13.7106 - mae: 13.7781 
Epoch 9/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 12.6984 - mae: 12.8543 
Epoch 10/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step - loss: 15.2709 - mae: 14.9481 
Epoch 11/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.3849 - mae: 10.2368
Epoch 12/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.8633 - mae: 11.2205 
Epoch 13/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 12.3729 - mae: 12.2677 
Epoch 14/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 8ms/step - loss: 13.6145 - mae: 13.6830 
Epoch 15/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 12.6006 - mae: 12.7629 
Epoch 16/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 15.2379 - mae: 14.9409 
Epoch 17/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.3052 - mae: 10.1758
Epoch 18/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.7935 - mae: 11.1804 
Epoch 19/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 12.3139 - mae: 12.2506 
Epoch 20/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.2243 - mae: 13.9952 
Epoch 21/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 11.7648 - mae: 11.1722 
Epoch 22/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 12.2910 - mae: 12.2549 
Epoch 23/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 14.7829 - mae: 14.2712 
Epoch 24/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.0433 - mae: 9.7591
Epoch 25/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.5325 - mae: 10.7671 
Epoch 26/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 12.0542 - mae: 11.8418 
Epoch 27/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 13.3082 - mae: 13.2874 
Epoch 28/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 12.3193 - mae: 12.4138 
Epoch 29/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 15.5180 - mae: 14.8908 
Epoch 30/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 7ms/step - loss: 11.0053 - mae: 9.7761
Epoch 31/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 7ms/step - loss: 11.5061 - mae: 10.8093 
Epoch 32/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 7ms/step - loss: 12.0405 - mae: 11.9114 
Epoch 33/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 9ms/step - loss: 13.9948 - mae: 13.7252 
Epoch 34/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.5332 - mae: 10.9144 
Epoch 35/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 12.0768 - mae: 12.0362 
Epoch 36/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 14.6945 - mae: 14.2187 
Epoch 37/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 9ms/step - loss: 10.8666 - mae: 9.6085
Epoch 38/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.3757 - mae: 10.6606 
Epoch 39/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.9193 - mae: 11.7836 
Epoch 40/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.4738 - mae: 13.8969 
Epoch 41/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.7332 - mae: 9.4020
Epoch 42/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 11.2434 - mae: 10.4576
Epoch 43/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.7885 - mae: 11.5848 
Epoch 44/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.3093 - mae: 13.6640 
Epoch 45/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.6226 - mae: 9.2417
Epoch 46/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 11.1354 - mae: 10.3038
Epoch 47/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.6835 - mae: 11.4383 
Epoch 48/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 3ms/step - loss: 14.1997 - mae: 13.5175 
Epoch 49/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.5340 - mae: 9.1265
Epoch 50/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.0508 - mae: 10.1977
Epoch 51/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.6033 - mae: 11.3425 
Epoch 52/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 3ms/step - loss: 14.1434 - mae: 13.4553 
Epoch 53/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.4670 - mae: 9.0550
Epoch 54/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.9891 - mae: 10.1384
Epoch 55/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.5475 - mae: 11.2965 
Epoch 56/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.1415 - mae: 13.4769 
Epoch 57/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.0059 - mae: 10.2200
Epoch 58/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.5723 - mae: 11.3953 
Epoch 59/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 14.8371 - mae: 13.9459 
Epoch 60/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.3723 - mae: 8.9702
Epoch 61/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.9040 - mae: 10.0751
Epoch 62/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.4731 - mae: 11.2571 
Epoch 63/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.7402 - mae: 13.8095 
Epoch 64/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.2880 - mae: 8.8599
Epoch 65/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 10.8232 - mae: 9.9732
Epoch 66/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.3964 - mae: 11.1647 
Epoch 67/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 14.6966 - mae: 13.7567 
Epoch 68/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.2229 - mae: 8.7890
Epoch 69/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.7629 - mae: 9.9134
Epoch 70/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 11.3415 - mae: 11.1172 
Epoch 71/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.7051 - mae: 13.7858 
Epoch 72/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.1765 - mae: 8.7566
Epoch 73/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 10.7226 - mae: 9.8947
Epoch 74/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.3079 - mae: 11.1136 
Epoch 75/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 15.2599 - mae: 14.1015 
Epoch 76/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.6807 - mae: 6.8457 
Epoch 77/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 17.6053 - mae: 16.8410 
Epoch 78/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 3ms/step - loss: 9.6484 - mae: 7.9942 
Epoch 79/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 14.8872 - mae: 15.1902 
Epoch 80/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 15.0601 - mae: 15.7561 
Epoch 81/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 13.1340 - mae: 11.4834 
Epoch 82/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 15.6436 - mae: 17.1007 
Epoch 83/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 8ms/step - loss: 14.1623 - mae: 13.1673 
Epoch 84/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 8ms/step - loss: 15.7980 - mae: 17.4707 
Epoch 85/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.1975 - mae: 13.5455 
Epoch 86/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 15.9466 - mae: 17.8262 
Epoch 87/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 14.2759 - mae: 13.9260 
Epoch 88/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 14.7984 - mae: 14.9875 
Epoch 89/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.9393 - mae: 15.5386 
Epoch 90/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 15.1694 - mae: 16.1068 
Epoch 91/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 13.6207 - mae: 12.0001 
Epoch 92/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 15.5182 - mae: 16.9160 
Epoch 93/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.1112 - mae: 12.9668 
Epoch 94/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 3ms/step - loss: 15.6657 - mae: 17.2686 
Epoch 95/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.1455 - mae: 13.3261 
Epoch 96/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 6ms/step - loss: 15.8068 - mae: 17.6061 
Epoch 97/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 14.1782 - mae: 13.6699 
Epoch 98/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 15.9416 - mae: 17.9290 
Epoch 99/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 4ms/step - loss: 14.5488 - mae: 14.1407 
Epoch 100/100
[1m2/2[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 5ms/step - loss: 15.9239 - mae: 17.9067





&lt;keras.src.callbacks.history.History at 0x1d928dbb320&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make and plot predictions for model_2</span>
<span class=n>y_preds_2</span> <span class=o>=</span> <span class=n>model_2</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=n>plot_predictions</span><span class=p>(</span><span class=n>predictions</span><span class=o>=</span><span class=n>y_preds_2</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>WARNING:tensorflow:5 out of the last 5 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x000001D9291EA3E0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 43ms/step
</code></pre></div> <p><a class=glightbox href=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_84_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_84_1.png></a></p> <p>Woah, that's looking better already! And all it took was an extra layer.</p> <div class=highlight><pre><span></span><code><span class=c1># Calculate model_2 metrics</span>
<span class=n>mae_2</span> <span class=o>=</span> <span class=n>mae</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_preds_2</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
<span class=n>mse_2</span> <span class=o>=</span> <span class=n>mse</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_preds_2</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
<span class=n>mae_2</span><span class=p>,</span> <span class=n>mse_2</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(34.643097, 1215.5195)
</code></pre></div> <p><strong>Build <code>model_3</code></strong></p> <p>For our 3rd model, we'll keep everything the same as <code>model_2</code> except this time we'll train for longer (500 epochs instead of 100).</p> <p>This will give our model more of a chance to learn the patterns in the data.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Replicate model_2</span>
<span class=n>model_3</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>model_3</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
                <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(),</span>
                <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;mae&#39;</span><span class=p>])</span>

<span class=c1># Fit the model (this time for 500 epochs, not 100)</span>
<span class=n>model_3</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>),</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>500</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=c1># set verbose to 0 for less output</span>
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;keras.src.callbacks.history.History at 0x1d9289798b0&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Make and plot predictions for model_3</span>
<span class=n>y_preds_3</span> <span class=o>=</span> <span class=n>model_3</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=n>plot_predictions</span><span class=p>(</span><span class=n>predictions</span><span class=o>=</span><span class=n>y_preds_3</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>WARNING:tensorflow:6 out of the last 6 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x000001D92AEC6020&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 45ms/step
</code></pre></div> <p><a class=glightbox href=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_89_1.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_89_1.png></a></p> <p>Strange, we trained for longer but our model performed worse?</p> <p>As it turns out, our model might've trained too long and has thus resulted in worse results (we'll see ways to prevent training for too long later on).</p> <div class=highlight><pre><span></span><code><span class=c1># Calculate model_3 metrics</span>
<span class=n>mae_3</span> <span class=o>=</span> <span class=n>mae</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_preds_3</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
<span class=n>mse_3</span> <span class=o>=</span> <span class=n>mse</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_preds_3</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
<span class=n>mae_3</span><span class=p>,</span> <span class=n>mse_3</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(45.998524, 2154.496)
</code></pre></div> <h2 id=comparing-results>Comparing results</h2> <p>Now we've got results for 3 similar but slightly different results, let's compare them.</p> <div class=highlight><pre><span></span><code><span class=n>model_results</span> <span class=o>=</span> <span class=p>[[</span><span class=s2>&quot;model_1&quot;</span><span class=p>,</span> <span class=n>mae_1</span><span class=p>,</span> <span class=n>mse_1</span><span class=p>],</span>
                 <span class=p>[</span><span class=s2>&quot;model_2&quot;</span><span class=p>,</span> <span class=n>mae_2</span><span class=p>,</span> <span class=n>mse_2</span><span class=p>],</span>
                 <span class=p>[</span><span class=s2>&quot;model_3&quot;</span><span class=p>,</span> <span class=n>mae_3</span><span class=p>,</span> <span class=n>mae_3</span><span class=p>]]</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=n>all_results</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>model_results</span><span class=p>,</span> <span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;model&quot;</span><span class=p>,</span> <span class=s2>&quot;mae&quot;</span><span class=p>,</span> <span class=s2>&quot;mse&quot;</span><span class=p>])</span>
<span class=n>all_results</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>model</th> <th>mae</th> <th>mse</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>model_1</td> <td>13.237320</td> <td>175.698090</td> </tr> <tr> <th>1</th> <td>model_2</td> <td>34.643097</td> <td>1215.519531</td> </tr> <tr> <th>2</th> <td>model_3</td> <td>45.998524</td> <td>45.998524</td> </tr> </tbody> </table> </div> <p>From our experiments, it looks like <code>model_2</code> performed the best.</p> <p>And now, you might be thinking, "wow, comparing models is tedious..." and it definitely can be, we've only compared 3 models here. </p> <p>But this is part of what machine learning modelling is about, trying many different combinations of models and seeing which performs best.</p> <p>Each model you build is a small experiment. </p> <blockquote> <p>üîë <strong>Note:</strong> One of your main goals should be to minimize the time between your experiments. The more experiments you do, the more things you'll figure out which don't work and in turn, get closer to figuring out what does work. Remember the machine learning practitioner's motto: "experiment, experiment, experiment".</p> </blockquote> <p>Another thing you'll also find is what you thought may work (such as training a model for longer) may not always work and the exact opposite is also often the case.</p> <h2 id=tracking-your-experiments>Tracking your experiments</h2> <p>One really good habit to get into is tracking your modelling experiments to see which perform better than others.</p> <p>We've done a simple version of this above (keeping the results in different variables).</p> <blockquote> <p>üìñ <strong>Resource:</strong> But as you build more models, you'll want to look into using tools such as: * <a href=https://tensorboard.dev/ ><strong>TensorBoard</strong></a> - a component of the TensorFlow library to help track modelling experiments (we'll see this later). * <a href=https://www.wandb.com/ ><strong>Weights &amp; Biases</strong></a> - a tool for tracking all kinds of machine learning experiments (the good news for Weights &amp; Biases is it plugs into TensorBoard).</p> </blockquote> <h2 id=saving-a-model>Saving a model</h2> <p>Once you've trained a model and found one which performs to your liking, you'll probably want to save it for use elsewhere (like a web application or mobile device).</p> <p>You can save a TensorFlow/Keras model using <a href=https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model><code>model.save()</code></a>.</p> <p>There are two ways to save a model in TensorFlow: 1. The <a href=https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format>SavedModel format</a> (default). 2. The <a href=https://www.tensorflow.org/tutorials/keras/save_and_load#hdf5_format>HDF5 format</a>.</p> <p>The main difference between the two is the SavedModel is automatically able to save custom objects (such as special layers) without additional modifications when loading the model back in.</p> <p>Which one should you use?</p> <p>It depends on your situation but the SavedModel format will suffice most of the time.</p> <p>Both methods use the same method call.</p> <div class=highlight><pre><span></span><code><span class=c1># Save a model using the SavedModel format</span>
<span class=c1># model_2.save(&#39;best_model_SavedModel_format&#39;)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check it out - outputs a protobuf binary file (.pb) as well as other files</span>
<span class=c1># !ls best_model_SavedModel_format</span>
</code></pre></div> <p>Now let's save the model in the HDF5 format, we'll use the same method but with a different filename.</p> <div class=highlight><pre><span></span><code><span class=c1># Save a model using the HDF5 format</span>
<span class=n>model_2</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&quot;best_model_HDF5_format.h5&quot;</span><span class=p>)</span> <span class=c1># note the addition of &#39;.h5&#39; on the end</span>
</code></pre></div> <div class=highlight><pre><span></span><code>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save(&#39;my_model.keras&#39;)` or `keras.saving.save_model(model, &#39;my_model.keras&#39;)`.
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check it out</span>
<span class=err>!</span><span class=n>ls</span> <span class=n>best_model_HDF5_format</span><span class=o>.</span><span class=n>h5</span>
</code></pre></div> <div class=highlight><pre><span></span><code>best_model_HDF5_format.h5
</code></pre></div> <h2 id=loading-a-model>Loading a model</h2> <p>We can load a saved model using the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model><code>load_model()</code></a> method.</p> <p>Loading a model for the different formats (SavedModel and HDF5) is the same (as long as the pathnames to the particular formats are correct).</p> <div class=highlight><pre><span></span><code><span class=c1># Load a model from the SavedModel format</span>
<span class=c1># loaded_saved_model = tf.keras.models.load_model(&quot;best_model_SavedModel_format&quot;)</span>
<span class=c1># loaded_saved_model.summary()</span>
</code></pre></div> <p>Now let's test it out.</p> <div class=highlight><pre><span></span><code><span class=c1># Compare model_2 with the SavedModel version (should return True)</span>
<span class=n>model_2_preds</span> <span class=o>=</span> <span class=n>model_2</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=c1># saved_model_preds = loaded_saved_model.predict(X_test)</span>
<span class=c1># mae(y_test, saved_model_preds.squeeze()).numpy() == mae(y_test, model_2_preds.squeeze()).numpy()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 32ms/step
</code></pre></div> <p>Loading in from the HDF5 is much the same.</p> <div class=highlight><pre><span></span><code><span class=c1># Load a model from the HDF5 format</span>
<span class=n>loaded_h5_model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;best_model_HDF5_format.h5&quot;</span><span class=p>)</span>
<span class=n>loaded_h5_model</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
</code></pre></div> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_5"</span>
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ<span style="font-weight: bold"> Layer (type)                         </span>‚îÉ<span style="font-weight: bold"> Output Shape                </span>‚îÉ<span style="font-weight: bold">         Param # </span>‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ dense_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      ‚îÇ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   ‚îÇ               <span style="color: #00af00; text-decoration-color: #00af00">2</span> ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ dense_6 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      ‚îÇ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   ‚îÇ               <span style="color: #00af00; text-decoration-color: #00af00">2</span> ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">6</span> (28.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">4</span> (16.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre> <pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">2</span> (12.00 B)
</pre> <div class=highlight><pre><span></span><code><span class=c1># Compare model_2 with the loaded HDF5 version (should return True)</span>
<span class=n>h5_model_preds</span> <span class=o>=</span> <span class=n>loaded_h5_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=n>mae</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>h5_model_preds</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span> <span class=o>==</span> <span class=n>mae</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>model_2_preds</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 40ms/step





True
</code></pre></div> <h2 id=downloading-a-model-from-google-colab>Downloading a model (from Google Colab)</h2> <p>Say you wanted to get your model from Google Colab to your local machine, you can do one of the following things: * Right click on the file in the files pane and click 'download'. * Use the code below.</p> <div class=highlight><pre><span></span><code><span class=c1># Download the model (or any file) from Google Colab</span>
<span class=kn>from</span><span class=w> </span><span class=nn>google.colab</span><span class=w> </span><span class=kn>import</span> <span class=n>files</span>
<span class=n>files</span><span class=o>.</span><span class=n>download</span><span class=p>(</span><span class=s2>&quot;best_model_HDF5_format.h5&quot;</span><span class=p>)</span>
</code></pre></div> <h2 id=a-larger-example>A larger example</h2> <p>Alright, we've seen the fundamentals of building neural network regression models in TensorFlow.</p> <p>Let's step it up a notch and build a model for a more feature rich dataset.</p> <p>More specifically we're going to try predict the cost of medical insurance for individuals based on a number of different parameters such as, <code>age</code>, <code>sex</code>, <code>bmi</code>, <code>children</code>, <code>smoking_status</code> and <code>residential_region</code>.</p> <p>To do, we'll leverage the pubically available <a href=https://www.kaggle.com/mirichoi0218/insurance>Medical Cost dataset</a> available from Kaggle and <a href=https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/insurance.csv>hosted on GitHub</a>.</p> <blockquote> <p>üîë <strong>Note:</strong> When learning machine learning paradigms, you'll often go through a series of foundational techniques and then practice them by working with open-source datasets and examples. Just as we're doing now, learn foundations, put them to work with different problems. Every time you work on something new, it's a good idea to search for something like "problem X example with Python/TensorFlow" where you substitute X for your problem.</p> </blockquote> <div class=highlight><pre><span></span><code><span class=c1># Import required libraries</span>
<span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Read in the insurance dataset</span>
<span class=n>insurance</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&quot;https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check out the insurance dataset</span>
<span class=n>insurance</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>age</th> <th>sex</th> <th>bmi</th> <th>children</th> <th>smoker</th> <th>region</th> <th>charges</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>19</td> <td>female</td> <td>27.900</td> <td>0</td> <td>yes</td> <td>southwest</td> <td>16884.92400</td> </tr> <tr> <th>1</th> <td>18</td> <td>male</td> <td>33.770</td> <td>1</td> <td>no</td> <td>southeast</td> <td>1725.55230</td> </tr> <tr> <th>2</th> <td>28</td> <td>male</td> <td>33.000</td> <td>3</td> <td>no</td> <td>southeast</td> <td>4449.46200</td> </tr> <tr> <th>3</th> <td>33</td> <td>male</td> <td>22.705</td> <td>0</td> <td>no</td> <td>northwest</td> <td>21984.47061</td> </tr> <tr> <th>4</th> <td>32</td> <td>male</td> <td>28.880</td> <td>0</td> <td>no</td> <td>northwest</td> <td>3866.85520</td> </tr> </tbody> </table> </div> <p>We're going to have to turn the non-numerical columns into numbers (because a neural network can't handle non-numerical inputs).</p> <p>To do so, we'll use the <a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html><code>get_dummies()</code></a> method in pandas.</p> <p>It converts categorical variables (like the <code>sex</code>, <code>smoker</code> and <code>region</code> columns) into numerical variables using one-hot encoding.</p> <div class=highlight><pre><span></span><code><span class=c1># Turn all categories into numbers</span>
<span class=n>insurance_one_hot</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>insurance</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=nb>int</span><span class=p>)</span>
<span class=n>insurance_one_hot</span><span class=o>.</span><span class=n>head</span><span class=p>()</span> <span class=c1># view the converted columns</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>age</th> <th>bmi</th> <th>children</th> <th>charges</th> <th>sex_female</th> <th>sex_male</th> <th>smoker_no</th> <th>smoker_yes</th> <th>region_northeast</th> <th>region_northwest</th> <th>region_southeast</th> <th>region_southwest</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>19</td> <td>27.900</td> <td>0</td> <td>16884.92400</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> </tr> <tr> <th>1</th> <td>18</td> <td>33.770</td> <td>1</td> <td>1725.55230</td> <td>0</td> <td>1</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> </tr> <tr> <th>2</th> <td>28</td> <td>33.000</td> <td>3</td> <td>4449.46200</td> <td>0</td> <td>1</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> </tr> <tr> <th>3</th> <td>33</td> <td>22.705</td> <td>0</td> <td>21984.47061</td> <td>0</td> <td>1</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> </tr> <tr> <th>4</th> <td>32</td> <td>28.880</td> <td>0</td> <td>3866.85520</td> <td>0</td> <td>1</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> </tr> </tbody> </table> </div> <p>Now we'll split data into features (<code>X</code>) and labels (<code>y</code>).</p> <div class=highlight><pre><span></span><code><span class=c1># Create X &amp; y values</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>insurance_one_hot</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s2>&quot;charges&quot;</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>insurance_one_hot</span><span class=p>[</span><span class=s2>&quot;charges&quot;</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># View features</span>
<span class=n>X</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>age</th> <th>bmi</th> <th>children</th> <th>sex_female</th> <th>sex_male</th> <th>smoker_no</th> <th>smoker_yes</th> <th>region_northeast</th> <th>region_northwest</th> <th>region_southeast</th> <th>region_southwest</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>19</td> <td>27.900</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> </tr> <tr> <th>1</th> <td>18</td> <td>33.770</td> <td>1</td> <td>0</td> <td>1</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> </tr> <tr> <th>2</th> <td>28</td> <td>33.000</td> <td>3</td> <td>0</td> <td>1</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> </tr> <tr> <th>3</th> <td>33</td> <td>22.705</td> <td>0</td> <td>0</td> <td>1</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> </tr> <tr> <th>4</th> <td>32</td> <td>28.880</td> <td>0</td> <td>0</td> <td>1</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> </tr> </tbody> </table> </div> <p>And create training and test sets. We could do this manually, but to make it easier, we'll leverage the already available <a href=https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html><code>train_test_split</code></a> function available from Scikit-Learn.</p> <div class=highlight><pre><span></span><code><span class=c1># Create training and test sets</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> 
                                                    <span class=n>y</span><span class=p>,</span> 
                                                    <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> 
                                                    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span> <span class=c1># set random state for reproducible splits</span>
</code></pre></div> <p>Now we can build and fit a model (we'll make it the same as <code>model_2</code>).</p> <div class=highlight><pre><span></span><code><span class=c1># Convert X and y to numpy array</span>
<span class=n>X_train</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
<span class=n>y_train</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y_train</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create a new model (same as model_2)</span>
<span class=n>insurance_model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>insurance_model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
                        <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(),</span>
                        <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;mae&#39;</span><span class=p>])</span>

<span class=c1># Fit the model</span>
<span class=n>insurance_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>Epoch 1/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 10001.6504 - mae: 10003.8008   
Epoch 2/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7677.3208 - mae: 7678.9966 
Epoch 3/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7499.9785 - mae: 7501.6816 
Epoch 4/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7131.3779 - mae: 7132.8643 
Epoch 5/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7260.0825 - mae: 7261.4038 
Epoch 6/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 994us/step - loss: 7563.4419 - mae: 7565.1353
Epoch 7/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step - loss: 7645.0957 - mae: 7645.4868
Epoch 8/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step - loss: 7826.6533 - mae: 7826.3794 
Epoch 9/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7664.6987 - mae: 7666.4385 
Epoch 10/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7340.5503 - mae: 7341.8989 
Epoch 11/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7838.0249 - mae: 7838.3022 
Epoch 12/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7671.2559 - mae: 7673.0312 
Epoch 13/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7689.4292 - mae: 7691.3120 
Epoch 14/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7364.9795 - mae: 7364.3979 
Epoch 15/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7607.3086 - mae: 7607.0210 
Epoch 16/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7353.7129 - mae: 7355.3892 
Epoch 17/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7586.5000 - mae: 7588.1470 
Epoch 18/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7415.3809 - mae: 7417.1250 
Epoch 19/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7483.4326 - mae: 7482.9375 
Epoch 20/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7292.5049 - mae: 7293.6855 
Epoch 21/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7162.9189 - mae: 7164.5073 
Epoch 22/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7111.6421 - mae: 7113.2026 
Epoch 23/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7456.3916 - mae: 7457.7910 
Epoch 24/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7365.7212 - mae: 7367.5229 
Epoch 25/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7279.4243 - mae: 7281.1890 
Epoch 26/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7449.4932 - mae: 7451.3252 
Epoch 27/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7599.8701 - mae: 7601.7549 
Epoch 28/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7583.5103 - mae: 7585.4800 
Epoch 29/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7538.4189 - mae: 7538.3672 
Epoch 30/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7374.6274 - mae: 7373.7964 
Epoch 31/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7512.4541 - mae: 7514.2744 
Epoch 32/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7169.3174 - mae: 7170.9644 
Epoch 33/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7549.1431 - mae: 7548.4082 
Epoch 34/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7474.6357 - mae: 7474.4302 
Epoch 35/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7465.6431 - mae: 7464.9517 
Epoch 36/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7196.7998 - mae: 7198.3965 
Epoch 37/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7352.3115 - mae: 7351.4849 
Epoch 38/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7142.7383 - mae: 7144.3521 
Epoch 39/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7503.1768 - mae: 7504.5981 
Epoch 40/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7118.6587 - mae: 7119.8096 
Epoch 41/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7112.3354 - mae: 7114.0605 
Epoch 42/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7462.2832 - mae: 7461.5244 
Epoch 43/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7346.6943 - mae: 7348.2617 
Epoch 44/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7415.4521 - mae: 7417.4438 
Epoch 45/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7500.2046 - mae: 7502.2544 
Epoch 46/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7337.9058 - mae: 7336.8755 
Epoch 47/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7528.6450 - mae: 7530.6118 
Epoch 48/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7063.4771 - mae: 7062.5732 
Epoch 49/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7507.3882 - mae: 7508.8213 
Epoch 50/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7314.5952 - mae: 7315.7446 
Epoch 51/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7212.9307 - mae: 7213.8398 
Epoch 52/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step - loss: 7003.8774 - mae: 7005.1030
Epoch 53/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step - loss: 7612.7798 - mae: 7612.1279
Epoch 54/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7400.9927 - mae: 7402.2505 
Epoch 55/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7205.1304 - mae: 7206.3662 
Epoch 56/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7654.4155 - mae: 7653.8765 
Epoch 57/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7498.5645 - mae: 7500.0942 
Epoch 58/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7039.6548 - mae: 7041.3203 
Epoch 59/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7052.3594 - mae: 7054.0659 
Epoch 60/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7401.7954 - mae: 7402.9146 
Epoch 61/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7427.4692 - mae: 7428.5938 
Epoch 62/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7220.0957 - mae: 7221.0854 
Epoch 63/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7227.1289 - mae: 7228.6035 
Epoch 64/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7358.0630 - mae: 7359.9058 
Epoch 65/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7313.3682 - mae: 7314.6367 
Epoch 66/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7126.7109 - mae: 7127.9390 
Epoch 67/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7636.0039 - mae: 7637.9336 
Epoch 68/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7545.4844 - mae: 7546.5649 
Epoch 69/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7309.5737 - mae: 7310.5527 
Epoch 70/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 964us/step - loss: 7227.6304 - mae: 7228.5708
Epoch 71/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 990us/step - loss: 7336.1177 - mae: 7337.3599
Epoch 72/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7519.1953 - mae: 7517.5044 
Epoch 73/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 896us/step - loss: 7305.5000 - mae: 7306.3354
Epoch 74/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7256.1470 - mae: 7257.1094 
Epoch 75/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7418.2930 - mae: 7419.3623 
Epoch 76/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7111.0195 - mae: 7111.5967 
Epoch 77/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7270.0205 - mae: 7271.0908 
Epoch 78/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7286.1211 - mae: 7288.2881 
Epoch 79/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step - loss: 7291.9937 - mae: 7293.2183 
Epoch 80/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step - loss: 7030.7515 - mae: 7031.7642
Epoch 81/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7248.4795 - mae: 7250.4644 
Epoch 82/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 955us/step - loss: 7159.0327 - mae: 7157.6870
Epoch 83/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7266.6235 - mae: 7265.4092 
Epoch 84/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7252.4639 - mae: 7251.0972 
Epoch 85/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7100.6050 - mae: 7102.5430 
Epoch 86/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1000us/step - loss: 6894.0845 - mae: 6892.3042
Epoch 87/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 963us/step - loss: 7253.1851 - mae: 7253.9346
Epoch 88/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 985us/step - loss: 6853.4917 - mae: 6854.4634
Epoch 89/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7407.5293 - mae: 7408.5239 
Epoch 90/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7160.0776 - mae: 7161.1372 
Epoch 91/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7438.4497 - mae: 7439.7275 
Epoch 92/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7405.0752 - mae: 7406.1782 
Epoch 93/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7163.6968 - mae: 7165.8018 
Epoch 94/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 6872.2222 - mae: 6871.1763 
Epoch 95/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7041.5376 - mae: 7042.3149 
Epoch 96/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7134.1865 - mae: 7135.0161 
Epoch 97/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 6960.7451 - mae: 6962.7158 
Epoch 98/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 6972.2271 - mae: 6972.9800 
Epoch 99/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7359.0928 - mae: 7360.9976 
Epoch 100/100
[1m34/34[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 7591.9121 - mae: 7593.5176





&lt;keras.src.callbacks.history.History at 0x1d92d75b710&gt;
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check the results of the insurance model</span>
<span class=n>insurance_model</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m9/9[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step - loss: 8473.0605 - mae: 8437.1006





[8610.08203125, 8430.283203125]
</code></pre></div> <p>Our model didn't perform very well, let's try a bigger model.</p> <p>We'll try 3 things: - Increasing the number of layers (2 -&gt; 3). - Increasing the number of units in each layer (except for the output layer). - Changing the optimizer (from SGD to Adam).</p> <p>Everything else will stay the same.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Add an extra layer and increase number of units</span>
<span class=n>insurance_model_2</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>100</span><span class=p>),</span> <span class=c1># 100 units</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>),</span> <span class=c1># 10 units</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=c1># 1 unit (important for output layer)</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>insurance_model_2</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
                          <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span> <span class=c1># Adam works but SGD doesn&#39;t </span>
                          <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;mae&#39;</span><span class=p>])</span>

<span class=c1># Fit the model and save the history (we can plot this)</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>insurance_model_2</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Evaluate our larger model</span>
<span class=n>insurance_model_2</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m9/9[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step - loss: 4680.3613 - mae: 4654.7310





[4663.1640625, 4535.0126953125]
</code></pre></div> <p>Much better! Using a larger model and the Adam optimizer results in almost half the error as the previous model.</p> <blockquote> <p>üîë <strong>Note:</strong> For many problems, the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam>Adam optimizer</a> is a great starting choice. See Andrei Karpathy's "Adam is safe" point from <a href=http://karpathy.github.io/2019/04/25/recipe/ ><em>A Recipe for Training Neural Networks</em></a> for more. </p> </blockquote> <p>Let's check out the loss curves of our model, we should see a downward trend.</p> <div class=highlight><pre><span></span><code><span class=c1># Plot history (also known as a loss curve)</span>
<span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>)</span><span class=o>.</span><span class=n>plot</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;loss&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;epochs&quot;</span><span class=p>);</span>
</code></pre></div> <p><a class=glightbox href=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_130_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_130_0.png></a></p> <p>From this, it looks like our model's loss (and MAE) were both still decreasing (in our case, MAE and loss are the same, hence the lines in the plot overlap eachother).</p> <p>What this tells us is the loss might go down if we try training it for longer.</p> <blockquote> <p>ü§î <strong>Question:</strong> How long should you train for? </p> <p>It depends on what problem you're working on. Sometimes training won't take very long, other times it'll take longer than you expect. A common method is to set your model training for a very long time (e.g. 1000's of epochs) but set it up with an <a href=https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping>EarlyStopping callback</a> so it stops automatically when it stops improving. We'll see this in another module.</p> </blockquote> <p>Let's train the same model as above for a little longer. We can do this but calling fit on it again.</p> <div class=highlight><pre><span></span><code><span class=c1># Try training for a little longer (100 more epochs)</span>
<span class=n>history_2</span> <span class=o>=</span> <span class=n>insurance_model_2</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</code></pre></div> <p>How did the extra training go?</p> <div class=highlight><pre><span></span><code><span class=c1># Evaluate the model trained for 200 total epochs</span>
<span class=n>insurance_model_2_loss</span><span class=p>,</span> <span class=n>insurance_model_2_mae</span> <span class=o>=</span> <span class=n>insurance_model_2</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
<span class=n>insurance_model_2_loss</span><span class=p>,</span> <span class=n>insurance_model_2_mae</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m9/9[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step - loss: 3468.0479 - mae: 3454.6157





(3457.428955078125, 3390.26806640625)
</code></pre></div> <p>Boom! Training for an extra 100 epochs we see about a 10% decrease in error.</p> <p>How does the visual look?</p> <div class=highlight><pre><span></span><code><span class=c1># Plot the model trained for 200 total epochs loss curves</span>
<span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>history_2</span><span class=o>.</span><span class=n>history</span><span class=p>)</span><span class=o>.</span><span class=n>plot</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;loss&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;epochs&quot;</span><span class=p>);</span> <span class=c1># note: epochs will only show 100 since we overrid the history variable</span>
</code></pre></div> <p><a class=glightbox href=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_136_0.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt=png src=../01_neural_network_regression_in_tensorflow_files/01_neural_network_regression_in_tensorflow_136_0.png></a></p> <h2 id=preprocessing-data-normalization-and-standardization>Preprocessing data (normalization and standardization)</h2> <p>A common practice when working with neural networks is to make sure all of the data you pass to them is in the range 0 to 1.</p> <p>This practice is called <strong>normalization</strong> (scaling all values from their original range to, e.g. between 0 and 100,000 to be between 0 and 1).</p> <p>There is another process call <strong>standardization</strong> which converts all of your data to unit variance and 0 mean.</p> <p>These two practices are often part of a preprocessing pipeline (a series of functions to prepare your data for use with neural networks).</p> <p>Knowing this, some of the major steps you'll take to preprocess your data for a neural network include: * Turning all of your data to numbers (a neural network can't handle strings). * Making sure your data is in the right shape (verifying input and output shapes). * <a href=https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler><strong>Feature scaling</strong></a>: * Normalizing data (making sure all values are between 0 and 1). This is done by subtracting the minimum value then dividing by the maximum value minus the minimum. This is also referred to as min-max scaling. * Standardization (making sure all values have a mean of 0 and a variance of 1). This is done by subtracting the mean value from the target feature and then dividing it by the standard deviation. * Which one should you use? * <strong>With neural networks you'll tend to favour normalization</strong> as they tend to prefer values between 0 and 1 (you'll see this espcially with image processing), however, you'll often find a neural network can perform pretty well with minimal feature scaling.</p> <blockquote> <p>üìñ <strong>Resource:</strong> For more on preprocessing data, I'd recommend reading the following resources: * <a href=https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-data>Scikit-Learn's documentation on preprocessing data</a>. * <a href=https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02>Scale, Standardize or Normalize with Scikit-Learn by Jeff Hale</a>.</p> </blockquote> <p>We've already turned our data into numbers using <code>get_dummies()</code>, let's see how we'd normalize it as well.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>

<span class=c1># Read in the insurance dataset</span>
<span class=n>insurance</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&quot;https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv&quot;</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Check out the data</span>
<span class=n>insurance</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div> <div> <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style> <table border=1 class=dataframe> <thead> <tr style="text-align: right;"> <th></th> <th>age</th> <th>sex</th> <th>bmi</th> <th>children</th> <th>smoker</th> <th>region</th> <th>charges</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>19</td> <td>female</td> <td>27.900</td> <td>0</td> <td>yes</td> <td>southwest</td> <td>16884.92400</td> </tr> <tr> <th>1</th> <td>18</td> <td>male</td> <td>33.770</td> <td>1</td> <td>no</td> <td>southeast</td> <td>1725.55230</td> </tr> <tr> <th>2</th> <td>28</td> <td>male</td> <td>33.000</td> <td>3</td> <td>no</td> <td>southeast</td> <td>4449.46200</td> </tr> <tr> <th>3</th> <td>33</td> <td>male</td> <td>22.705</td> <td>0</td> <td>no</td> <td>northwest</td> <td>21984.47061</td> </tr> <tr> <th>4</th> <td>32</td> <td>male</td> <td>28.880</td> <td>0</td> <td>no</td> <td>northwest</td> <td>3866.85520</td> </tr> </tbody> </table> </div> <p>Now, just as before, we need to transform the non-numerical columns into numbers and this time we'll also be normalizing the numerical columns with different ranges (to make sure they're all between 0 and 1).</p> <p>To do this, we're going to use a few classes from Scikit-Learn: * <a href=https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html><code>make_column_transformer</code></a> - build a multi-step data preprocessing function for the folllowing transformations: * <a href=https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html><code>MinMaxScaler</code></a> - make sure all numerical columns are normalized (between 0 and 1). * <a href=https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html><code>OneHotEncoder</code></a> - one hot encode the non-numerical columns.</p> <p>Let's see them in action.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.compose</span><span class=w> </span><span class=kn>import</span> <span class=n>make_column_transformer</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>MinMaxScaler</span><span class=p>,</span> <span class=n>OneHotEncoder</span>

<span class=c1># Create column transformer (this will help us normalize/preprocess our data)</span>
<span class=n>ct</span> <span class=o>=</span> <span class=n>make_column_transformer</span><span class=p>(</span>
    <span class=p>(</span><span class=n>MinMaxScaler</span><span class=p>(),</span> <span class=p>[</span><span class=s2>&quot;age&quot;</span><span class=p>,</span> <span class=s2>&quot;bmi&quot;</span><span class=p>,</span> <span class=s2>&quot;children&quot;</span><span class=p>]),</span> <span class=c1># get all values between 0 and 1</span>
    <span class=p>(</span><span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>handle_unknown</span><span class=o>=</span><span class=s2>&quot;ignore&quot;</span><span class=p>),</span> <span class=p>[</span><span class=s2>&quot;sex&quot;</span><span class=p>,</span> <span class=s2>&quot;smoker&quot;</span><span class=p>,</span> <span class=s2>&quot;region&quot;</span><span class=p>])</span>
<span class=p>)</span>

<span class=c1># Create X &amp; y</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>insurance</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s2>&quot;charges&quot;</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>insurance</span><span class=p>[</span><span class=s2>&quot;charges&quot;</span><span class=p>]</span>

<span class=c1># Build our train and test sets (use random state to ensure same split as before)</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Fit column transformer on the training data only (doing so on test data would result in data leakage)</span>
<span class=n>ct</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>

<span class=c1># Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)</span>
<span class=n>X_train_normal</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
<span class=n>X_test_normal</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <p>Now we've normalized it and one-hot encoding it, what does our data look like now?</p> <div class=highlight><pre><span></span><code><span class=c1># Non-normalized and non-one-hot encoded data example</span>
<span class=n>X_train</span><span class=o>.</span><span class=n>loc</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>age                19
sex            female
bmi              27.9
children            0
smoker            yes
region      southwest
Name: 0, dtype: object
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Normalized and one-hot encoded example</span>
<span class=n>X_train_normal</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</code></pre></div> <div class=highlight><pre><span></span><code>array([0.60869565, 0.10734463, 0.4       , 1.        , 0.        ,
       1.        , 0.        , 0.        , 1.        , 0.        ,
       0.        ])
</code></pre></div> <p>How about the shapes?</p> <div class=highlight><pre><span></span><code><span class=c1># Notice the normalized/one-hot encoded shape is larger because of the extra columns</span>
<span class=n>X_train_normal</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>X_train</span><span class=o>.</span><span class=n>shape</span>
</code></pre></div> <div class=highlight><pre><span></span><code>((1070, 11), (1070, 6))
</code></pre></div> <p>Our data is normalized and numerical, let's model it.</p> <p>We'll use the same model as <code>insurance_model_2</code>.</p> <div class=highlight><pre><span></span><code><span class=c1># Set random seed</span>
<span class=n>tf</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Build the model (3 layers, 100, 10, 1 units)</span>
<span class=n>insurance_model_3</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>100</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>),</span>
  <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Compile the model</span>
<span class=n>insurance_model_3</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mae</span><span class=p>,</span>
                          <span class=n>optimizer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span>
                          <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;mae&#39;</span><span class=p>])</span>

<span class=c1># Fit the model for 200 epochs (same as insurance_model_2)</span>
<span class=n>insurance_model_3</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_normal</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> 
</code></pre></div> <div class=highlight><pre><span></span><code>&lt;keras.src.callbacks.history.History at 0x1d92d8652b0&gt;
</code></pre></div> <p>Let's evaluate the model on normalized test set.</p> <div class=highlight><pre><span></span><code><span class=c1># Evaulate 3rd model</span>
<span class=n>insurance_model_3_loss</span><span class=p>,</span> <span class=n>insurance_model_3_mae</span> <span class=o>=</span> <span class=n>insurance_model_3</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test_normal</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code>[1m9/9[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step - loss: 3221.2153 - mae: 3213.6387
</code></pre></div> <p>And finally, let's compare the results from <code>insurance_model_2</code> (trained on non-normalized data) and <code>insurance_model_3</code> (trained on normalized data).</p> <div class=highlight><pre><span></span><code><span class=c1># Compare modelling results from non-normalized data and normalized data</span>
<span class=n>insurance_model_2_mae</span><span class=p>,</span> <span class=n>insurance_model_3_mae</span>
</code></pre></div> <div class=highlight><pre><span></span><code>(3390.26806640625, 3161.1298828125)
</code></pre></div> <p>From this we can see normalizing the data results in 10% less error using the same model than not normalizing the data.</p> <p>This is <strong>one of the main benefits of normalization: faster convergence time</strong> (a fancy way of saying, your model gets to better results faster).</p> <p><code>insurance_model_2</code> may have eventually achieved the same results as <code>insurance_model_3</code> if we left it training for longer. </p> <p>Also, the results may change if we were to alter the architectures of the models, e.g. more hidden units per layer or more layers.</p> <p>But since our main goal as neural network practitioners is to decrease the time between experiments, anything that helps us get better results sooner is a plus.</p> <h2 id=exercises>üõ† Exercises</h2> <p>We've a covered a whole lot pretty quickly.</p> <p>So now it's time to have a <strong>play around</strong> with a few things and start to build up your intuition.</p> <p>I emphasise the words play around because that's very important. Try a few things out, run the code and see what happens.</p> <ol> <li>Create your own regression dataset (or make the one we created in "Create data to view and fit" bigger) and build fit a model to it.</li> <li>Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform?</li> <li>Try and improve the results we got on the insurance dataset, some things you might want to try include:</li> <li>Building a larger model (how does one with 4 dense layers go?).</li> <li>Increasing the number of units in each layer.</li> <li>Lookup the documentation of <a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam>Adam</a> and find out what the first parameter is, what happens if you increase it by 10x?</li> <li>What happens if you train for longer (say 300 epochs instead of 200)? </li> <li>Import the <a href=https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data>Boston pricing dataset</a> from TensorFlow <a href=https://www.tensorflow.org/api_docs/python/tf/keras/datasets><code>tf.keras.datasets</code></a> and model it.</li> </ol> <h2 id=extra-curriculum>üìñ Extra curriculum</h2> <p>If you're looking for extra materials relating to this notebook, I'd check out the following:</p> <ul> <li><a href=https://youtu.be/njKP3FqW3Sk>MIT introduction deep learning lecture 1</a> - gives a great overview of what's happening behind all of the code we're running.</li> <li>Reading: 1-hour of <a href=http://neuralnetworksanddeeplearning.com/chap1.html>Chapter 1 of Neural Networks and Deep Learning</a> by Michael Nielson - a great in-depth and hands-on example of the intuition behind neural networks.</li> </ul> <p>To practice your regression modelling with TensorFlow, I'd also encourage you to look through <a href=https://lionbridge.ai/datasets/ >Lion Bridge's collection of datasets</a> or <a href=https://www.kaggle.com/data>Kaggle's datasets</a>, find a regression dataset which sparks your interest and try to model.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.instant", "navigation.path", "navigation.indexes", "navigation.top", "navigation.tracking"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>